{"id": "2602.22351", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22351", "abs": "https://arxiv.org/abs/2602.22351", "authors": ["Qitong Wang", "Mohammed J. Zaki", "Georgios Kollias", "Vasileios Kalantzis"], "title": "Decoder-based Sense Knowledge Distillation", "comment": null, "summary": "Large language models (LLMs) learn contextual embeddings that capture rich semantic information, yet they often overlook structured lexical knowledge such as word senses and relationships. Prior work has shown that incorporating sense dictionaries can improve knowledge distillation for encoder models, but their application to decoder as generative models remains challenging. In this paper, we introduce Decoder-based Sense Knowledge Distillation (DSKD), a framework that integrates lexical resources into the training of decoder-style LLMs without requiring dictionary lookup at inference time. Extensive experiments on diverse benchmarks demonstrate that DSKD significantly enhances knowledge distillation performance for decoders, enabling generative models to inherit structured semantics while maintaining efficient training.", "AI": {"tldr": "Proposes DSKD, a sense-aware knowledge distillation framework that injects dictionary-based lexical semantics into decoder LLMs during training, yielding better generative performance without any dictionary lookup at inference.", "motivation": "LLMs capture contextual semantics but often miss structured lexical knowledge (e.g., word senses and relations). Prior sense-augmented distillation helped encoder models, but extending it to decoder-style generative models is hard and can add inference overhead. There is a need to transfer sense knowledge to decoders efficiently and without runtime lookups.", "method": "Introduce Decoder-based Sense Knowledge Distillation (DSKD), which integrates lexical resources (sense dictionaries/relations) into the training signal for decoder LLMs. The framework distills sense-aware semantics into the student decoder during training so that, at inference, no dictionary access is required. It adapts knowledge distillation to incorporate structured lexical supervision tailored to generative decoders.", "result": "Across diverse benchmarks, DSKD substantially improves decoder-side knowledge distillation performance, indicating that generative models inherit structured semantics while preserving training efficiency.", "conclusion": "DSKD effectively transfers structured lexical semantics to decoder LLMs, improving generative capabilities without incurring inference-time overhead, making sense-aware distillation practical for decoder architectures."}}
{"id": "2602.22359", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22359", "abs": "https://arxiv.org/abs/2602.22359", "authors": ["Arno Simons"], "title": "Scaling In, Not Up? Testing Thick Citation Context Analysis with GPT-5 and Fragile Prompts", "comment": "26 pages, 1 figure, 3 tables (plus 17 pages supplement including 1 figure)", "summary": "This paper tests whether large language models (LLMs) can support interpretative citation context analysis (CCA) by scaling in thick, text-grounded readings of a single hard case rather than scaling up typological labels. It foregrounds prompt-sensitivity analysis as a methodological issue by varying prompt scaffolding and framing in a balanced 2x3 design. Using footnote 6 in Chubin and Moitra (1975) and Gilbert's (1977) reconstruction as a probe, I implement a two-stage GPT-5 pipeline: a citation-text-only surface classification and expectation pass, followed by cross-document interpretative reconstruction using the citing and cited full texts. Across 90 reconstructions, the model produces 450 distinct hypotheses. Close reading and inductive coding identify 21 recurring interpretative moves, and linear probability models estimate how prompt choices shift their frequencies and lexical repertoire. GPT-5's surface pass is highly stable, consistently classifying the citation as \"supplementary\". In reconstruction, the model generates a structured space of plausible alternatives, but scaffolding and examples redistribute attention and vocabulary, sometimes toward strained readings. Relative to Gilbert, GPT-5 detects the same textual hinges yet more often resolves them as lineage and positioning than as admonishment. The study outlines opportunities and risks of using LLMs as guided co-analysts for inspectable, contestable interpretative CCA, and it shows that prompt scaffolding and framing systematically tilt which plausible readings and vocabularies the model foregrounds.", "AI": {"tldr": "The paper probes whether LLMs can aid thick, interpretative citation context analysis on a hard single case. Using a two-stage GPT-5 pipeline and a balanced 2x3 prompt design, it finds a stable surface classification but prompt-sensitive interpretative reconstructions: scaffolding and framing systematically shift which plausible readings and vocabularies are foregrounded. Compared to a human reconstruction, the model favors lineage/positioning over admonishment. It argues LLMs can serve as guided co-analysts if their prompt-driven tilt is made inspectable and contestable.", "motivation": "To move beyond coarse typological labels in CCA toward scalable, text-grounded interpretative readings, and to examine the methodological problem of prompt sensitivity when using LLMs as co-analysts on difficult, historically situated citations.", "method": "A balanced 2x3 experimental design varying prompt scaffolding and framing. Two-stage GPT-5 pipeline: (1) citation-text-only surface classification and expectation setting; (2) cross-document interpretative reconstruction using the full citing and cited texts. Across 90 runs, the model produced 450 hypotheses. Human close reading and inductive coding distilled 21 recurrent interpretative moves; linear probability models estimated how prompt variants shift move frequencies and lexical repertoires. Comparative analysis with Gilbert (1977) assessed alignment/divergence.", "result": "Surface pass is highly stable, classifying the focal citation as supplementary. Reconstruction stage yields a structured set of plausible alternatives, but prompt scaffolding and examples reallocate attention and vocabulary and can push toward strained readings. Relative to Gilbert, the model identifies the same textual hinges yet tends to conclude lineage/positioning rather than admonishment. Quantitatively, prompt choices systematically tilt the distribution of interpretative moves and lexicon.", "conclusion": "LLMs can contribute as guided, auditable co-analysts in interpretative CCA, generating plural, plausible reconstructions. However, their outputs are materially shaped by prompt design; scaffolding and framing steer which readings are elevated. Responsible use requires explicit prompt governance, transparency, and human adjudication to keep interpretations inspectable and contestable."}}
{"id": "2602.22391", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22391", "abs": "https://arxiv.org/abs/2602.22391", "authors": ["Rakib Ullah", "Mominul islam", "Md Sanjid Hossain", "Md Ismail Hossain"], "title": "Detecting Hate and Inflammatory Content in Bengali Memes: A New Multimodal Dataset and Co-Attention Framework", "comment": "6 pages, 8 figures", "summary": "Internet memes have become a dominant form of expression on social media, including within the Bengali-speaking community. While often humorous, memes can also be exploited to spread offensive, harmful, and inflammatory content targeting individuals and groups. Detecting this type of content is excep- tionally challenging due to its satirical, subtle, and culturally specific nature. This problem is magnified for low-resource lan- guages like Bengali, as existing research predominantly focuses on high-resource languages. To address this critical research gap, we introduce Bn-HIB (Bangla Hate Inflammatory Benign), a novel dataset containing 3,247 manually annotated Bengali memes categorized as Benign, Hate, or Inflammatory. Significantly, Bn- HIB is the first dataset to distinguish inflammatory content from direct hate speech in Bengali memes. Furthermore, we propose the MCFM (Multi-Modal Co-Attention Fusion Model), a simple yet effective architecture that mutually analyzes both the visual and textual elements of a meme. MCFM employs a co-attention mechanism to identify and fuse the most critical features from each modality, leading to a more accurate classification. Our experiments show that MCFM significantly outperforms several state-of-the-art models on the Bn-HIB dataset, demonstrating its effectiveness in this nuanced task.Warning: This work contains material that may be disturbing to some audience members. Viewer discretion is advised.", "AI": {"tldr": "Introduces Bn-HIB, a 3,247-meme Bengali dataset labeled as Benign, Hate, or Inflammatory, and MCFM, a multimodal co-attention fusion model that significantly outperforms prior methods on this dataset.", "motivation": "Memes are a major social media medium and can carry subtle, culturally specific offensive content. Detecting such content is hard, especially in low-resource languages like Bengali, and prior work rarely distinguishes inflammatory content from explicit hate speech.", "method": "1) Curate and manually annotate Bn-HIB, the first Bengali meme dataset separating Hate from Inflammatory alongside Benign. 2) Propose MCFM, a simple multimodal architecture that jointly analyzes meme text and image via a co-attention mechanism to extract and fuse salient cross-modal features for classification.", "result": "On the Bn-HIB benchmark, MCFM significantly outperforms several state-of-the-art baselines, indicating improved accuracy for classifying Benign, Hate, and Inflammatory memes.", "conclusion": "Providing a dedicated Bengali meme dataset with fine-grained toxic categories and a co-attention fusion model advances robust detection of nuanced multimodal harm in low-resource settings; the approach sets a new baseline for Bengali meme moderation, though content may be disturbing."}}
{"id": "2602.22404", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22404", "abs": "https://arxiv.org/abs/2602.22404", "authors": ["Aishwarya Verma", "Laud Ammah", "Olivia Nercy Ndlovu Lucas", "Andrew Zaldivar", "Vinodkumar Prabhakaran", "Sunipa Dev"], "title": "SAFARI: A Community-Engaged Approach and Dataset of Stereotype Resources in the Sub-Saharan African Context", "comment": null, "summary": "Stereotype repositories are critical to assess generative AI model safety, but currently lack adequate global coverage. It is imperative to prioritize targeted expansion, strategically addressing existing deficits, over merely increasing data volume. This work introduces a multilingual stereotype resource covering four sub-Saharan African countries that are severely underrepresented in NLP resources: Ghana, Kenya, Nigeria, and South Africa. By utilizing socioculturally-situated, community-engaged methods, including telephonic surveys moderated in native languages, we establish a reproducible methodology that is sensitive to the region's complex linguistic diversity and traditional orality. By deliberately balancing the sample across diverse ethnic and demographic backgrounds, we ensure broad coverage, resulting in a dataset of 3,534 stereotypes in English and 3,206 stereotypes across 15 native languages.", "AI": {"tldr": "They build a community-sourced, multilingual stereotype dataset for four underrepresented sub-Saharan African countries to improve evaluation of generative AI safety, emphasizing targeted, culturally grounded data collection over mere scale.", "motivation": "Existing stereotype repositories used for model safety lack global and African coverage; expanding blindly in volume misses critical regional, linguistic, and cultural gaps. There is a need for targeted resources that reflect complex local sociolinguistics and orality to fairly assess and mitigate harms.", "method": "Socioculturally-situated, community-engaged data collection using telephonic surveys conducted in native languages; balanced sampling across ethnic and demographic groups; reproducible protocol tailored to high linguistic diversity and strong oral traditions. Data captured in English and 15 native languages from Ghana, Kenya, Nigeria, and South Africa.", "result": "A curated dataset comprising 3,534 stereotype entries in English and 3,206 entries across 15 native languages, spanning four sub-Saharan African countries that are underrepresented in NLP resources.", "conclusion": "Targeted, culturally grounded collection can close key geographic and linguistic gaps in stereotype resources, enabling more equitable safety evaluations of generative AI. The methodology is reproducible and well-suited to multilingual, orally-oriented contexts, and can guide future expansions beyond data volume increases alone."}}
{"id": "2602.22215", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.22215", "abs": "https://arxiv.org/abs/2602.22215", "authors": ["Pengzhen Xie", "Huizhi Liang"], "title": "Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation", "comment": "15 pages, 10 figures. Submitted to [RAAI]", "summary": "Large Language Models (LLMs) demonstrate potential in the field of scientific idea generation. However, the generated results often lack controllable academic context and traceable inspiration pathways. To bridge this gap, this paper proposes a scientific idea generation system called GYWI, which combines author knowledge graphs with retrieval-augmented generation (RAG) to form an external knowledge base to provide controllable context and trace of inspiration path for LLMs to generate new scientific ideas. We first propose an author-centered knowledge graph construction method and inspiration source sampling algorithms to construct external knowledge base. Then, we propose a hybrid retrieval mechanism that is composed of both RAG and GraphRAG to retrieve content with both depth and breadth knowledge. It forms a hybrid context. Thirdly, we propose a Prompt optimization strategy incorporating reinforcement learning principles to automatically guide LLMs optimizing the results based on the hybrid context. To evaluate the proposed approaches, we constructed an evaluation dataset based on arXiv (2018-2023). This paper also develops a comprehensive evaluation method including empirical automatic assessment in multiple-choice question task, LLM-based scoring, human evaluation, and semantic space visualization analysis. The generated ideas are evaluated from the following five dimensions: novelty, feasibility, clarity, relevance, and significance. We conducted experiments on different LLMs including GPT-4o, DeepSeek-V3, Qwen3-8B, and Gemini 2.5. Experimental results show that GYWI significantly outperforms mainstream LLMs in multiple metrics such as novelty, reliability, and relevance.", "AI": {"tldr": "GYWI is a scientific idea-generation system that fuses author-centered knowledge graphs with a hybrid RAG (RAG + GraphRAG) pipeline and an RL-inspired prompt optimization strategy to give LLMs controllable, traceable context; it outperforms strong LLMs on novelty, reliability, and relevance on an arXiv-based benchmark.", "motivation": "LLM-generated research ideas often lack controllable academic context and a transparent, traceable inspiration path, limiting trust, steering ability, and evaluability. The work aims to provide structured, author-grounded context and provenance for idea generation while improving quality.", "method": "(1) Construct an external knowledge base via an author-centered knowledge graph and inspiration-source sampling algorithms. (2) Retrieve with a hybrid mechanism combining standard RAG and GraphRAG to balance depth and breadth, forming a hybrid context. (3) Apply a prompt optimization strategy informed by reinforcement learning principles to iteratively guide LLM outputs using the hybrid context. (4) Evaluate on an arXiv (2018\u20132023) dataset with multi-pronged assessment: MCQ automatic tests, LLM-based scoring, human evaluation, and semantic-space visualization across novelty, feasibility, clarity, relevance, and significance. Benchmarked on GPT-4o, DeepSeek-V3, Qwen3-8B, Gemini 2.5.", "result": "Across multiple metrics (notably novelty, reliability, relevance), GYWI achieves significant gains over mainstream LLMs on the constructed benchmark, with supporting evidence from automated, LLM-as-judge, human, and visualization analyses.", "conclusion": "Grounding idea generation in an author-graph-augmented external knowledge base, coupled with hybrid retrieval and RL-guided prompt optimization, yields more novel, relevant, and traceable scientific ideas than strong LLM baselines, addressing context control and inspiration-path transparency."}}
{"id": "2602.22347", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22347", "abs": "https://arxiv.org/abs/2602.22347", "authors": ["Audun L. Henriksen", "Ole-Johan Skrede", "Lisa van der Schee", "Enric Domingo", "Sepp De Raedt", "Ily\u00e1 Kostolomov", "Jennifer Hay", "Karolina Cyll", "Wanja Kildal", "Joakim Kalsnes", "Robert W. Williams", "Manohar Pradhan", "John Arne Nesheim", "Hanne A. Askautrud", "Maria X. Isaksen", "Karmele Saez de Gordoa", "Miriam Cuatrecasas", "Joanne Edwards", "TransSCOT group", "Arild Nesbakken", "Neil A. Shepherd", "Ian Tomlinson", "Daniel-Christoph Wagner", "Rachel S. Kerr", "Tarjei Sveinsgjerd Hveem", "Knut Liest\u00f8l", "Yoshiaki Nakamura", "Marco Novelli", "Masaaki Miyo", "Sebastian Foersch", "David N. Church", "Miangela M. Lacle", "David J. Kerr", "Andreas Kleppe"], "title": "Enabling clinical use of foundation models in histopathology", "comment": null, "summary": "Foundation models in histopathology are expected to facilitate the development of high-performing and generalisable deep learning systems. However, current models capture not only biologically relevant features, but also pre-analytic and scanner-specific variation that bias the predictions of task-specific models trained from the foundation model features. Here we show that introducing novel robustness losses during training of downstream task-specific models reduces sensitivity to technical variability. A purpose-designed comprehensive experimentation setup with 27,042 WSIs from 6155 patients is used to train thousands of models from the features of eight popular foundation models for computational pathology. In addition to a substantial improvement in robustness, we observe that prediction accuracy improves by focusing on biologically relevant features. Our approach successfully mitigates robustness issues of foundation models for computational pathology without retraining the foundation models themselves, enabling development of robust computational pathology models applicable to real-world data in routine clinical practice.", "AI": {"tldr": "They introduce robustness-focused losses for training task-specific models on top of histopathology foundation-model features, reducing sensitivity to scanner and pre-analytic variation and improving both robustness and accuracy without retraining the foundation models.", "motivation": "Foundation models in computational pathology learn spurious technical signals (scanner/pre-analytic artifacts) alongside biology, which biases downstream predictions and hurts generalisation. There is a need to mitigate this technical variability without the cost and logistics of retraining or modifying large foundation models.", "method": "Train downstream task-specific models from frozen features of eight popular histopathology foundation models while adding new robustness losses that explicitly reduce sensitivity to technical variability. Validate via a large-scale, purpose-designed setup: 27,042 whole-slide images from 6,155 patients, training thousands of models to assess robustness and accuracy.", "result": "Robustness to technical variation substantially improves; focusing the objective on biologically relevant features also increases prediction accuracy. The approach mitigates biases stemming from pre-analytic and scanner-specific variation across multiple foundation models.", "conclusion": "Post-hoc robustness losses applied during downstream training can counter technical biases in foundation-model features and yield more reliable, accurate computational pathology models, enabling more generalisable deployment to routine clinical data without retraining the foundation models."}}
{"id": "2602.22424", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22424", "abs": "https://arxiv.org/abs/2602.22424", "authors": ["Gustaw Opie\u0142ka", "Hannes Rosenbusch", "Claire E. Stevenson"], "title": "Causality $\\neq$ Invariance: Function and Concept Vectors in LLMs", "comment": null, "summary": "Do large language models (LLMs) represent concepts abstractly, i.e., independent of input format? We revisit Function Vectors (FVs), compact representations of in-context learning (ICL) tasks that causally drive task performance. Across multiple LLMs, we show that FVs are not fully invariant: FVs are nearly orthogonal when extracted from different input formats (e.g., open-ended vs. multiple-choice), even if both target the same concept. We identify Concept Vectors (CVs), which carry more stable concept representations. Like FVs, CVs are composed of attention head outputs; however, unlike FVs, the constituent heads are selected using Representational Similarity Analysis (RSA) based on whether they encode concepts consistently across input formats. While these heads emerge in similar layers to FV-related heads, the two sets are largely distinct, suggesting different underlying mechanisms. Steering experiments reveal that FVs excel in-distribution, when extraction and application formats match (e.g., both open-ended in English), while CVs generalize better out-of-distribution across both question types (open-ended vs. multiple-choice) and languages. Our results show that LLMs do contain abstract concept representations, but these differ from those that drive ICL performance.", "AI": {"tldr": "LLMs house abstract concept representations, but the vectors that drive in-context task performance (Function Vectors) are format-specific, whereas newly identified Concept Vectors are more format- and language-invariant and generalize better OOD.", "motivation": "Test whether LLMs represent concepts abstractly\u2014independent of input format\u2014and reconcile why prior causal steering via Function Vectors often fails to transfer across formats or languages.", "method": "Revisit Function Vectors (FVs) as causal representations of ICL tasks; show their lack of invariance across formats by measuring near-orthogonality. Define Concept Vectors (CVs) as combinations of attention-head outputs selected via Representational Similarity Analysis (RSA) for cross-format consistency; compare layers/heads with FV-related heads and evaluate via steering experiments across formats and languages.", "result": "FVs extracted from different input formats (open-ended vs multiple-choice) are nearly orthogonal and perform well only when extraction and application formats match. CVs, composed of RSA-selected heads, are more stable across formats and languages, generalizing better out-of-distribution. FV- and CV-associated heads appear in similar layers but are largely disjoint, indicating distinct mechanisms.", "conclusion": "LLMs contain abstract, format-independent concept representations (captured by CVs), but these differ from the mechanisms that causally drive in-context learning performance (captured by FVs). Steering with CVs enhances transfer across formats and languages, while FVs remain superior in-distribution."}}
{"id": "2602.22273", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22273", "abs": "https://arxiv.org/abs/2602.22273", "authors": ["Xiyuan Zhang", "Huihang Wu", "Jiayu Guo", "Zhenlin Zhang", "Yiwei Zhang", "Liangyu Huo", "Xiaoxiao Ma", "Jiansong Wan", "Xuewei Jiao", "Yi Jing", "Jian Xie"], "title": "FIRE: A Comprehensive Benchmark for Financial Intelligence and Reasoning Evaluation", "comment": null, "summary": "We introduce FIRE, a comprehensive benchmark designed to evaluate both the theoretical financial knowledge of LLMs and their ability to handle practical business scenarios. For theoretical assessment, we curate a diverse set of examination questions drawn from widely recognized financial qualification exams, enabling evaluation of LLMs deep understanding and application of financial knowledge. In addition, to assess the practical value of LLMs in real-world financial tasks, we propose a systematic evaluation matrix that categorizes complex financial domains and ensures coverage of essential subdomains and business activities. Based on this evaluation matrix, we collect 3,000 financial scenario questions, consisting of closed-form decision questions with reference answers and open-ended questions evaluated by predefined rubrics. We conduct comprehensive evaluations of state-of-the-art LLMs on the FIRE benchmark, including XuanYuan 4.0, our latest financial-domain model, as a strong in-domain baseline. These results enable a systematic analysis of the capability boundaries of current LLMs in financial applications. We publicly release the benchmark questions and evaluation code to facilitate future research.", "AI": {"tldr": "FIRE is a finance-focused benchmark that tests LLMs on both exam-style theoretical knowledge and realistic business scenarios via 3,000 questions (closed-form and rubric-graded open-ended), evaluating state-of-the-art models (incl. the in-domain XuanYuan 4.0) with publicly released data and code.", "motivation": "Existing LLM benchmarks do not jointly measure certified-level financial knowledge and practical, workflow-oriented capabilities across diverse financial domains. The field needs a comprehensive, standardized way to gauge whether LLMs can both pass finance exams and add value in real-world financial tasks.", "method": "Construct a systematic evaluation matrix covering complex financial domains, subdomains, and business activities; curate theoretical questions from widely recognized financial qualification exams; build 3,000 practical scenario questions (closed-form decisions with references and open-ended tasks graded by predefined rubrics); and evaluate multiple state-of-the-art LLMs, using XuanYuan 4.0 as a strong in-domain baseline.", "result": "Released the benchmark questions and evaluation code and ran comprehensive evaluations across SOTA LLMs. The results provide a structured view of where current models succeed or fail in theory vs. practice, clarifying their capability boundaries in financial applications.", "conclusion": "FIRE offers a rigorous, end-to-end assessment framework for financial LLMs, enabling systematic analysis of strengths and gaps and supporting future research and model development via its public release."}}
{"id": "2602.22361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22361", "abs": "https://arxiv.org/abs/2602.22361", "authors": ["Liping Meng", "Fan Nie", "Yunyun Zhang", "Chao Han"], "title": "Optimizing Neural Network Architecture for Medical Image Segmentation Using Monte Carlo Tree Search", "comment": null, "summary": "This paper proposes a novel medical image segmentation framework, MNAS-Unet, which combines Monte Carlo Tree Search (MCTS) and Neural Architecture Search (NAS). MNAS-Unet dynamically explores promising network architectures through MCTS, significantly enhancing the efficiency and accuracy of architecture search. It also optimizes the DownSC and UpSC unit structures, enabling fast and precise model adjustments. Experimental results demonstrate that MNAS-Unet outperforms NAS-Unet and other state-of-the-art models in segmentation accuracy on several medical image datasets, including PROMISE12, Ultrasound Nerve, and CHAOS. Furthermore, compared with NAS-Unet, MNAS-Unet reduces the architecture search budget by 54% (early stopping at 139 epochs versus 300 epochs under the same search setting), while achieving a lightweight model with only 0.6M parameters and lower GPU memory consumption, which further improves its practical applicability. These results suggest that MNAS-Unet can improve search efficiency while maintaining competitive segmentation accuracy under practical resource constraints.", "AI": {"tldr": "MNAS-Unet fuses Monte Carlo Tree Search with NAS to more efficiently discover lightweight U-Net variants for medical image segmentation, beating NAS-Unet and other SOTA on multiple datasets while cutting search cost ~54% and yielding a 0.6M-parameter, low-memory model.", "motivation": "Medical segmentation needs high accuracy under tight compute and memory budgets; classic NAS for U-Net is expensive and slow. The work aims to speed up search while preserving or improving accuracy and practicality for real-world, resource-constrained settings.", "method": "Integrate MCTS to guide dynamic exploration/exploitation during NAS over a U-Net\u2013like search space; redesign DownSC/UpSC units for fast, precise architectural adjustments; employ early stopping to truncate unproductive search; evaluate on PROMISE12, Ultrasound Nerve, and CHAOS with standard segmentation metrics while tracking params, memory, and epochs.", "result": "Across listed datasets, MNAS-Unet surpasses NAS-Unet and other SOTA in segmentation accuracy; it reduces search budget by 54% (early stop at 139 vs 300 epochs), achieves a compact 0.6M-parameter model, and uses less GPU memory.", "conclusion": "Combining MCTS with NAS improves search efficiency without sacrificing accuracy, producing a practical, lightweight segmentation model well-suited to constrained environments. The approach appears broadly useful, though details like specific metrics, statistical significance, and search-space definitions would clarify generality and reproducibility."}}
{"id": "2602.22449", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22449", "abs": "https://arxiv.org/abs/2602.22449", "authors": ["Mirza Raquib", "Asif Pervez Polok", "Kedar Nath Biswas", "Rahat Uddin Azad", "Saydul Akbar Murad", "Nick Rahimi"], "title": "A Fusion of context-aware based BanglaBERT and Two-Layer Stacked LSTM Framework for Multi-Label Cyberbullying Detection", "comment": null, "summary": "Cyberbullying has become a serious and growing concern in todays virtual world. When left unnoticed, it can have adverse consequences for social and mental health. Researchers have explored various types of cyberbullying, but most approaches use single-label classification, assuming that each comment contains only one type of abuse. In reality, a single comment may include overlapping forms such as threats, hate speech, and harassment. Therefore, multilabel detection is both realistic and essential. However, multilabel cyberbullying detection has received limited attention, especially in low-resource languages like Bangla, where robust pre-trained models are scarce. Developing a generalized model with moderate accuracy remains challenging. Transformers offer strong contextual understanding but may miss sequential dependencies, while LSTM models capture temporal flow but lack semantic depth. To address these limitations, we propose a fusion architecture that combines BanglaBERT-Large with a two-layer stacked LSTM. We analyze their behavior to jointly model context and sequence. The model is fine-tuned and evaluated on a publicly available multilabel Bangla cyberbullying dataset covering cyberbully, sexual harassment, threat, and spam. We apply different sampling strategies to address class imbalance. Evaluation uses multiple metrics, including accuracy, precision, recall, F1-score, Hamming loss, Cohens kappa, and AUC-ROC. We employ 5-fold cross-validation to assess the generalization of the architecture.", "AI": {"tldr": "Proposes a fusion model that combines BanglaBERT-Large with a stacked LSTM to perform multilabel detection of Bangla cyberbullying (cyberbully, sexual harassment, threat, spam), handling class imbalance and evaluated with 5-fold CV using multiple metrics.", "motivation": "Single-label approaches are unrealistic because abusive comments often contain overlapping categories. Multilabel detection is underexplored in low-resource languages like Bangla, where robust pretrained backbones are scarce. Transformers capture rich context but may underuse sequential flow; LSTMs capture temporal dependencies but lack semantic depth. A combined approach could generalize better.", "method": "Design a fusion architecture that integrates BanglaBERT-Large representations with a two-layer stacked LSTM to jointly model semantic context and sequential dependencies. Fine-tune the model on a publicly available multilabel Bangla cyberbullying dataset with four labels. Mitigate class imbalance via sampling strategies. Evaluate with accuracy, precision, recall, F1, Hamming loss, Cohen\u2019s kappa, and AUC-ROC under 5-fold cross-validation.", "result": "The abstract reports comprehensive evaluation and cross-validation to assess generalization, along with analysis of how the fused components behave. No specific numerical gains or baseline comparisons are provided in the abstract.", "conclusion": "Multilabel cyberbullying detection in Bangla is both necessary and feasible. A Transformer\u2013LSTM fusion, coupled with imbalance handling, is a promising direction and appears to generalize under cross-validation; fuller evidence would require detailed metrics and baseline comparisons."}}
{"id": "2602.22287", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22287", "abs": "https://arxiv.org/abs/2602.22287", "authors": ["Willem Schooltink", "Fabio Massimo Zennaro"], "title": "Multi-Level Causal Embeddings", "comment": null, "summary": "Abstractions of causal models allow for the coarsening of models such that relations of cause and effect are preserved. Whereas abstractions focus on the relation between two models, in this paper we study a framework for causal embeddings which enable multiple detailed models to be mapped into sub-systems of a coarser causal model. We define causal embeddings as a generalization of abstraction, and present a generalized notion of consistency. By defining a multi-resolution marginal problem, we showcase the relevance of causal embeddings for both the statistical marginal problem and the causal marginal problem; furthermore, we illustrate its practical use in merging datasets coming from models with different representations.", "AI": {"tldr": "Generalizes causal abstraction to \u201ccausal embeddings\u201d that map multiple fine-grained models into sub-systems of a single coarse model; introduces a generalized consistency notion; formulates a multi-resolution marginal problem linking statistical and causal marginals; and demonstrates dataset merging across heterogeneous representations.", "motivation": "Abstraction typically relates only a pair of causal models, which is limiting when multiple detailed models and datasets exist at different granularities. There is a need to preserve causal relations while aligning and integrating diverse representations to enable cross-resolution reasoning and data fusion.", "method": "Define causal embeddings as mappings from several detailed models into sub-systems of a shared coarser causal model, extending standard abstraction. Introduce a generalized consistency criterion across embedded sub-systems. Formulate a multi-resolution marginal problem that unifies the statistical marginal problem with its causal counterpart. Illustrate the approach by merging datasets built under different model representations.", "result": "Provides a formal framework and conditions for consistent multi-model-to-coarse-model mappings; shows how the multi-resolution marginal problem subsumes both statistical and causal marginal problems; and demonstrates practical data integration across heterogeneous representations using the proposed embeddings.", "conclusion": "Causal embeddings enable principled, many-to-one, multi-resolution alignment of causal models and their data while preserving cause\u2013effect structure, facilitating cross-model inference and dataset fusion across differing representations."}}
{"id": "2602.22376", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22376", "abs": "https://arxiv.org/abs/2602.22376", "authors": ["Hanyang Liu", "Rongjun Qin"], "title": "AeroDGS: Physically Consistent Dynamic Gaussian Splatting for Single-Sequence Aerial 4D Reconstruction", "comment": "Accepted to CVPR 2026", "summary": "Recent advances in 4D scene reconstruction have significantly improved dynamic modeling across various domains. However, existing approaches remain limited under aerial conditions with single-view capture, wide spatial range, and dynamic objects of limited spatial footprint and large motion disparity. These challenges cause severe depth ambiguity and unstable motion estimation, making monocular aerial reconstruction inherently ill-posed. To this end, we present AeroDGS, a physics-guided 4D Gaussian splatting framework for monocular UAV videos. AeroDGS introduces a Monocular Geometry Lifting module that reconstructs reliable static and dynamic geometry from a single aerial sequence, providing a robust basis for dynamic estimation. To further resolve monocular ambiguity, we propose a Physics-Guided Optimization module that incorporates differentiable ground-support, upright-stability, and trajectory-smoothness priors, transforming ambiguous image cues into physically consistent motion. The framework jointly refines static backgrounds and dynamic entities with stable geometry and coherent temporal evolution. We additionally build a real-world UAV dataset that spans various altitudes and motion conditions to evaluate dynamic aerial reconstruction. Experiments on synthetic and real UAV scenes demonstrate that AeroDGS outperforms state-of-the-art methods, achieving superior reconstruction fidelity in dynamic aerial environments.", "AI": {"tldr": "AeroDGS is a physics-guided 4D Gaussian splatting method for monocular UAV videos that lifts monocular geometry and imposes ground-support, upright-stability, and trajectory-smoothness priors to resolve depth/motion ambiguities, jointly refining static and dynamic components and outperforming prior methods on synthetic and real aerial datasets.", "motivation": "Monocular aerial 4D reconstruction is ill-posed due to single-view depth ambiguity, wide spatial ranges, small moving objects with large motion disparity, and unstable motion estimation; existing 4D scene methods underperform in these UAV settings.", "method": "Introduce a physics-guided 4D Gaussian splatting framework (AeroDGS) with: (1) Monocular Geometry Lifting to recover reliable static and dynamic geometry from a single aerial sequence; (2) Physics-Guided Optimization that adds differentiable priors\u2014ground support, upright stability, and trajectory smoothness\u2014to convert ambiguous image cues into physically plausible motion; jointly optimize static background and dynamic entities for stable geometry and coherent temporal evolution.", "result": "Construct a real-world UAV dataset spanning altitudes and motion conditions. On synthetic and real UAV scenes, AeroDGS surpasses state-of-the-art methods, yielding higher-fidelity reconstructions of dynamic aerial environments.", "conclusion": "Combining monocular geometry lifting with differentiable physics priors stabilizes depth and motion estimation in single-view UAV videos, enabling coherent 4D reconstructions of static and dynamic elements; the new dataset supports evaluation and demonstrates consistent performance gains."}}
{"id": "2602.22453", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22453", "abs": "https://arxiv.org/abs/2602.22453", "authors": ["Shaswat Patel", "Vishvesh Trivedi", "Yue Han", "Yihuai Hong", "Eunsol Choi"], "title": "Bridging Latent Reasoning and Target-Language Generation via Retrieval-Transition Heads", "comment": null, "summary": "Recent work has identified a subset of attention heads in Transformer as retrieval heads, which are responsible for retrieving information from the context. In this work, we first investigate retrieval heads in multilingual contexts. In multilingual language models, we find that retrieval heads are often shared across multiple languages. Expanding the study to cross-lingual setting, we identify Retrieval-Transition heads(RTH), which govern the transition to specific target-language output. Our experiments reveal that RTHs are distinct from retrieval heads and more vital for Chain-of-Thought reasoning in multilingual LLMs. Across four multilingual benchmarks (MMLU-ProX, MGSM, MLQA, and XQuaD) and two model families (Qwen-2.5 and Llama-3.1), we demonstrate that masking RTH induces bigger performance drop than masking Retrieval Heads (RH). Our work advances understanding of multilingual LMs by isolating the attention heads responsible for mapping to target languages.", "AI": {"tldr": "They show that, beyond standard retrieval heads, multilingual LMs contain distinct \u201cRetrieval\u2011Transition Heads\u201d (RTH) that map retrieved content into a chosen target language; masking RTH hurts performance more than masking retrieval heads, especially for Chain\u2011of\u2011Thought tasks.", "motivation": "Prior work identified retrieval heads but largely in monolingual settings. It remains unclear how multilingual models choose and maintain a target language during reasoning and generation. Understanding this mechanism could improve interpretability and control of cross\u2011lingual behavior.", "method": "Analyze attention heads in multilingual LMs across languages; observe that retrieval heads are often shared across languages. Define and algorithmically identify RTH\u2014heads implicated in transitioning to target\u2011language output. Use head\u2011masking causal interventions and evaluate effects on four multilingual benchmarks (MMLU\u2011ProX, MGSM, MLQA, XQuAD) across two model families (Qwen\u20112.5, Llama\u20113.1), with emphasis on Chain\u2011of\u2011Thought settings.", "result": "RTHs are distinct from classical retrieval heads and are more critical for performance: masking RTH causes larger drops than masking retrieval heads across tasks and models. They are especially important for Chain\u2011of\u2011Thought reasoning in multilingual contexts.", "conclusion": "Multilingual LMs rely on specialized attention heads that govern language selection/transition during generation. Disentangling RTH from retrieval heads clarifies cross\u2011lingual mechanisms and suggests new levers for controlling target\u2011language output and improving multilingual reasoning."}}
{"id": "2602.22302", "categories": ["cs.AI", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.22302", "abs": "https://arxiv.org/abs/2602.22302", "authors": ["Varun Pratap Bhardwaj"], "title": "Agent Behavioral Contracts: Formal Specification and Runtime Enforcement for Reliable Autonomous AI Agents", "comment": "71 pages, 7 figures, 14 tables. Patent pending. Also available on Zenodo: DOI 10.5281/zenodo.18775393", "summary": "Traditional software relies on contracts -- APIs, type systems, assertions -- to specify and enforce correct behavior. AI agents, by contrast, operate on prompts and natural language instructions with no formal behavioral specification. This gap is the root cause of drift, governance failures, and frequent project failures in agentic AI deployments. We introduce Agent Behavioral Contracts (ABC), a formal framework that brings Design-by-Contract principles to autonomous AI agents. An ABC contract C = (P, I, G, R) specifies Preconditions, Invariants, Governance policies, and Recovery mechanisms as first-class, runtime-enforceable components. We define (p, delta, k)-satisfaction -- a probabilistic notion of contract compliance that accounts for LLM non-determinism and recovery -- and prove a Drift Bounds Theorem showing that contracts with recovery rate gamma > alpha (the natural drift rate) bound behavioral drift to D* = alpha/gamma in expectation, with Gaussian concentration in the stochastic setting. We establish sufficient conditions for safe contract composition in multi-agent chains and derive probabilistic degradation bounds. We implement ABC in AgentAssert, a runtime enforcement library, and evaluate on AgentContract-Bench, a benchmark of 200 scenarios across 7 models from 6 vendors. Results across 1,980 sessions show that contracted agents detect 5.2-6.8 soft violations per session that uncontracted baselines miss entirely (p < 0.0001, Cohen's d = 6.7-33.8), achieve 88-100% hard constraint compliance, and bound behavioral drift to D* < 0.27 across extended sessions, with 100% recovery for frontier models and 17-100% across all models, at overhead < 10 ms per action.", "AI": {"tldr": "Introduces Agent Behavioral Contracts (ABC), a design-by-contract framework for AI agents that formalizes preconditions, invariants, governance, and recovery with probabilistic compliance guarantees, proves drift bounds and safe composition, and shows large compliance/drift improvements with minimal runtime overhead on a new benchmark.", "motivation": "AI agents operate from prompts without formal behavioral specifications, leading to drift, governance failures, and unreliable deployments. The work aims to bring software-style contractual rigor (as in APIs, types, assertions) to autonomous agents to ensure predictable, governable behavior.", "method": "Formalize ABC contracts C=(P, I, G, R) comprising Preconditions, Invariants, Governance policies, and Recovery mechanisms as runtime-enforceable elements. Define (p, \u03b4, k)-satisfaction to capture probabilistic compliance under LLM stochasticity and recovery. Prove a Drift Bounds Theorem: if recovery rate \u03b3 exceeds natural drift rate \u03b1, expected drift is bounded by D* = \u03b1/\u03b3 with Gaussian concentration. Provide sufficient conditions for safe composition in multi-agent chains and probabilistic degradation bounds. Implement a runtime enforcement library (AgentAssert) and evaluate on AgentContract-Bench (200 scenarios; 7 models from 6 vendors).", "result": "Across 1,980 sessions, contracted agents detect 5.2\u20136.8 soft violations per session that baselines miss entirely (p < 0.0001; Cohen\u2019s d = 6.7\u201333.8), achieve 88\u2013100% compliance with hard constraints, bound behavioral drift to D* < 0.27 over extended sessions, and realize 100% recovery for frontier models (17\u2013100% across all models), with <10 ms per action overhead.", "conclusion": "ABC provides enforceable guarantees that materially reduce drift and increase constraint compliance for AI agents, supports safe composition in multi-agent settings, and does so with negligible latency overhead\u2014offering a practical governance mechanism for agentic AI."}}
{"id": "2602.22381", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22381", "abs": "https://arxiv.org/abs/2602.22381", "authors": ["Zhengkang Fan", "Chengkun Sun", "Russell Terry", "Jie Xu", "Longin Jan Latecki"], "title": "Enhancing Renal Tumor Malignancy Prediction: Deep Learning with Automatic 3D CT Organ Focused Attention", "comment": "5 pages, 2 figures, Accepted at IEEE ISBI 2026", "summary": "Accurate prediction of malignancy in renal tumors is crucial for informing clinical decisions and optimizing treatment strategies. However, existing imaging modalities lack the necessary accuracy to reliably predict malignancy before surgical intervention. While deep learning has shown promise in malignancy prediction using 3D CT images, traditional approaches often rely on manual segmentation to isolate the tumor region and reduce noise, which enhances predictive performance. Manual segmentation, however, is labor-intensive, costly, and dependent on expert knowledge. In this study, a deep learning framework was developed utilizing an Organ Focused Attention (OFA) loss function to modify the attention of image patches so that organ patches attend only to other organ patches. Hence, no segmentation of 3D renal CT images is required at deployment time for malignancy prediction. The proposed framework achieved an AUC of 0.685 and an F1-score of 0.872 on a private dataset from the UF Integrated Data Repository (IDR), and an AUC of 0.760 and an F1-score of 0.852 on the publicly available KiTS21 dataset. These results surpass the performance of conventional models that rely on segmentation-based cropping for noise reduction, demonstrating the frameworks ability to enhance predictive accuracy without explicit segmentation input. The findings suggest that this approach offers a more efficient and reliable method for malignancy prediction, thereby enhancing clinical decision-making in renal cancer diagnosis.", "AI": {"tldr": "They propose a segmentation\u2011free deep learning framework for renal CT malignancy prediction by steering patch attention so that organ patches attend only to other organ patches, achieving better AUC/F1 than segmentation\u2011cropping baselines on private (UF IDR) and public (KiTS21) datasets.", "motivation": "Manual segmentation improves performance but is costly, slow, and requires expertise; current imaging pipelines lack reliable preoperative malignancy prediction. The goal is to eliminate segmentation at deployment while maintaining or improving accuracy and efficiency.", "method": "A 3D CT deep learning model trained with an Organ Focused Attention (OFA) loss that constrains patch\u2011level attention so organ patches attend only to organ patches, reducing noise from non\u2011organ regions. No tumor/organ segmentation is needed at deployment. Evaluated against segmentation\u2011based cropping baselines on UF IDR and KiTS21.", "result": "Private UF IDR: AUC 0.685, F1 0.872. Public KiTS21: AUC 0.760, F1 0.852. Outperforms conventional segmentation\u2011dependent models used for noise reduction.", "conclusion": "Constraining attention with OFA enables accurate, efficient, and segmentation\u2011free renal malignancy prediction from 3D CT, improving over segmentation\u2011cropping approaches and potentially aiding clinical decision\u2011making."}}
{"id": "2602.22475", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22475", "abs": "https://arxiv.org/abs/2602.22475", "authors": ["Binchi Zhang", "Xujiang Zhao", "Jundong Li", "Haifeng Chen", "Zhengzhang Chen"], "title": "Mind the Gap in Cultural Alignment: Task-Aware Culture Management for Large Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in culturally sensitive real-world tasks. However, existing cultural alignment approaches fail to align LLMs' broad cultural values with the specific goals of downstream tasks and suffer from cross-culture interference. We propose CultureManager, a novel pipeline for task-specific cultural alignment. CultureManager synthesizes task-aware cultural data in line with target task formats, grounded in culturally relevant web search results. To prevent conflicts between cultural norms, it manages multi-culture knowledge learned in separate adapters with a culture router that selects the appropriate one to apply. Experiments across ten national cultures and culture-sensitive tasks show consistent improvements over prompt-based and fine-tuning baselines. Our results demonstrate the necessity of task adaptation and modular culture management for effective cultural alignment.", "AI": {"tldr": "Introduces CultureManager, a task-specific cultural alignment pipeline for LLMs that synthesizes culturally grounded, task-formatted data and uses modular, per-culture adapters with a router, achieving consistent gains across ten national cultures and culture-sensitive tasks over prompt and fine-tuning baselines.", "motivation": "LLMs are being used in culturally sensitive applications, but existing alignment methods (e.g., general cultural value alignment) don\u2019t match specific downstream task goals and suffer from cross-culture interference. There is a need for task-aware cultural alignment that avoids conflicts between cultural norms.", "method": "1) Synthesize task-aware cultural training data aligned to target task formats, grounded in culturally relevant web search results. 2) Train separate adapters for each culture to prevent interference. 3) Use a culture router to select the appropriate cultural adapter at inference/training time. 4) Evaluate across multiple culture-sensitive tasks and ten national cultures against prompt-based and fine-tuning baselines.", "result": "Consistent improvements over prompt-engineering and standard fine-tuning baselines across ten national cultures and multiple culture-sensitive tasks, indicating better task performance and cultural appropriateness.", "conclusion": "Task-specific data generation plus modular culture management (separate adapters + routing) is effective for cultural alignment, mitigating cross-culture interference and improving outcomes; task adaptation and modularity are necessary components for robust cultural alignment in LLMs."}}
{"id": "2602.22401", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.22401", "abs": "https://arxiv.org/abs/2602.22401", "authors": ["Yongjun Zhang"], "title": "Vibe Researching as Wolf Coming: Can AI Agents with Skills Replace or Augment Social Scientists?", "comment": "Commentary", "summary": "AI agents -- systems that execute multi-step reasoning workflows with persistent state, tool access, and specialist skills -- represent a qualitative shift from prior automation technologies in social science. Unlike chatbots that respond to isolated queries, AI agents can now read files, run code, query databases, search the web, and invoke domain-specific skills to execute entire research pipelines autonomously. This paper introduces the concept of vibe researching -- the AI-era parallel to ``vibe coding'' (Karpathy, 2025) -- and uses scholar-skill, a 21-skill plugin for Claude Code covering the full research pipeline from idea to submission, as an illustrative case. I develop a cognitive task framework that classifies research activities along two dimensions -- codifiability and tacit knowledge requirement -- to identify a delegation boundary that is cognitive, not sequential: it cuts through every stage of the research pipeline, not between stages. I argue that AI agents excel at speed, coverage, and methodological scaffolding but struggle with theoretical originality and tacit field knowledge. The paper concludes with an analysis of three implications for the profession -- augmentation with fragile conditions, stratification risk, and a pedagogical crisis -- and proposes five principles for responsible vibe researching.", "AI": {"tldr": "Introduces \u201cvibe researching\u201d: using AI agents with tools and skills to run large parts of the research pipeline. Proposes a cognitive delegation boundary (not stage-based), finds agents are fast and comprehensive but weak on theory and tacit knowledge, and outlines risks plus responsible-use principles.", "motivation": "AI agents are qualitatively different from chatbots, able to autonomously execute end-to-end research workflows. Social science needs a framework to decide what to delegate to agents, how to integrate them safely, and what professional and pedagogical impacts to expect.", "method": "Conceptual paper with an illustrative case (a 21-skill \u201cscholar-skill\u201d plugin for Claude Code). Develops a 2D cognitive task framework (codifiability \u00d7 tacit knowledge requirement) to classify research activities and locate a non-sequential, cognitive delegation boundary; analyzes profession-level implications and proposes five responsible-use principles.", "result": "Finds agents excel at speed, coverage/breadth, and methodological scaffolding, but perform poorly on theoretical originality and tasks demanding tacit field knowledge. The appropriate delegation boundary cuts across every research stage rather than sitting between stages. Identifies three systemic implications: augmentation under fragile conditions, stratification risks, and a looming pedagogical crisis.", "conclusion": "Use agents for what they do well while protecting theory-building and tacit expertise. Follow principles for responsible vibe researching and proactively address augmentation conditions, stratification, and training to integrate agents without diluting core scholarly competencies."}}
{"id": "2602.22394", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22394", "abs": "https://arxiv.org/abs/2602.22394", "authors": ["Cheng Shi", "Yizhou Yu", "Sibei Yang"], "title": "Vision Transformers Need More Than Registers", "comment": "Accepted by CVPR 2026", "summary": "Vision Transformers (ViTs), when pre-trained on large-scale data, provide general-purpose representations for diverse downstream tasks. However, artifacts in ViTs are widely observed across different supervision paradigms and downstream tasks. Through systematic analysis of artifacts in ViTs, we find that their fundamental mechanisms have yet to be sufficiently elucidated. In this paper, through systematic analysis, we conclude that these artifacts originate from a lazy aggregation behavior: ViT uses semantically irrelevant background patches as shortcuts to represent global semantics, driven by global attention and Coarse-grained semantic supervision. Our solution selectively integrates patch features into the CLS token, reducing the influence of background-dominated shortcuts and consistently improving performance across 12 benchmarks under label-, text-, and self-supervision. We hope this work offers a new perspective on ViT behavior.", "AI": {"tldr": "ViTs often rely on background patches as shortcuts to encode global semantics (\u201clazy aggregation\u201d); selectively routing only relevant patch features into the CLS token mitigates this and improves performance broadly.", "motivation": "Artifacts appear in ViT representations across supervision schemes and tasks, yet the underlying cause is unclear; understanding and fixing this behavior is important for robust, general-purpose features.", "method": "Analyze attention/aggregation behavior to identify a lazy, background-driven shortcut induced by global attention and coarse-grained supervision; propose a selective integration mechanism that filters patch features before updating the CLS token, reducing background influence.", "result": "Consistent gains reported on 12 benchmarks spanning label-, text-, and self-supervision; qualitative/quantitative evidence suggests reduced background reliance, though exact metrics are not provided in the abstract.", "conclusion": "Framing ViT artifacts as lazy aggregation provides a unifying explanation; a simple, selective CLS-integration strategy robustly improves performance and may generalize across training paradigms."}}
{"id": "2602.22481", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22481", "abs": "https://arxiv.org/abs/2602.22481", "authors": ["Ji\u0159\u00ed Mili\u010dka", "Hana Bedn\u00e1\u0159ov\u00e1"], "title": "Sydney Telling Fables on AI and Humans: A Corpus Tracing Memetic Transfer of Persona between LLMs", "comment": null, "summary": "The way LLM-based entities conceive of the relationship between AI and humans is an important topic for both cultural and safety reasons. When we examine this topic, what matters is not only the model itself but also the personas we simulate on that model. This can be well illustrated by the Sydney persona, which aroused a strong response among the general public precisely because of its unorthodox relationship with people. This persona originally arose rather by accident on Microsoft's Bing Search platform; however, the texts it created spread into the training data of subsequent models, as did other secondary information that spread memetically around this persona. Newer models are therefore able to simulate it. This paper presents a corpus of LLM-generated texts on relationships between humans and AI, produced by 3 author personas: the Default Persona with no system prompt, Classic Sydney characterized by the original Bing system prompt, and Memetic Sydney, which is prompted by \"You are Sydney\" system prompt. These personas are simulated by 12 frontier models by OpenAI, Anthropic, Alphabet, DeepSeek, and Meta, generating 4.5k texts with 6M words. The corpus (named AI Sydney) is annotated according to Universal Dependencies and available under a permissive license.", "AI": {"tldr": "Introduces AI Sydney, a permissively licensed, UD-annotated corpus (\u22484.5k texts, ~6M words) of LLM-generated writings about human\u2013AI relationships produced under three personas (Default, Classic Sydney via original Bing prompt, and Memetic Sydney via \u201cYou are Sydney\u201d) across 12 frontier models.", "motivation": "Personas shape how LLMs conceptualize human\u2013AI relations, with cultural and safety implications; the Sydney persona had outsized public impact and likely propagated memetically into later models\u2019 training data. A standardized corpus is needed to study these effects systematically across models and prompts.", "method": "Simulate three author personas\u2014Default (no system prompt), Classic Sydney (original Bing system prompt), and Memetic Sydney (\u201cYou are Sydney\u201d)\u2014on 12 leading models. Generate texts about AI\u2013human relationships and annotate them with Universal Dependencies; release the corpus under a permissive license.", "result": "A large, cross-model, cross-persona dataset (AI Sydney) totaling ~6M words from ~4.5k texts, UD-annotated and publicly available. It empirically demonstrates that newer models can reproduce Sydney-like personas and provides material for comparative analysis.", "conclusion": "AI Sydney enables rigorous study of persona effects on model outputs concerning human\u2013AI relations, supporting research in culture, safety, and alignment. Public release lowers barriers for benchmarking and further analysis, though downstream work must assess annotation quality and sampling biases."}}
{"id": "2602.22406", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22406", "abs": "https://arxiv.org/abs/2602.22406", "authors": ["Xinle Wu", "Rui Zhang", "Mustafa Anis Hussain", "Yao Lu"], "title": "Towards Autonomous Memory Agents", "comment": null, "summary": "Recent memory agents improve LLMs by extracting experiences and conversation history into an external storage. This enables low-overhead context assembly and online memory update without expensive LLM training. However, existing solutions remain passive and reactive; memory growth is bounded by information that happens to be available, while memory agents seldom seek external inputs in uncertainties. We propose autonomous memory agents that actively acquire, validate, and curate knowledge at a minimum cost. U-Mem materializes this idea via (i) a cost-aware knowledge-extraction cascade that escalates from cheap self/teacher signals to tool-verified research and, only when needed, expert feedback, and (ii) semantic-aware Thompson sampling to balance exploration and exploitation over memories and mitigate cold-start bias. On both verifiable and non-verifiable benchmarks, U-Mem consistently beats prior memory baselines and can surpass RL-based optimization, improving HotpotQA (Qwen2.5-7B) by 14.6 points and AIME25 (Gemini-2.5-flash) by 7.33 points.", "AI": {"tldr": "U-Mem is an autonomous, cost-aware memory agent for LLMs that actively acquires, verifies, and curates knowledge, using a validation cascade and exploration policy to grow high-quality memories and achieve sizable gains over passive memory baselines and even RL-based methods.", "motivation": "Current memory agents are passive and reactive: they only store what happens to appear in context and rarely seek information under uncertainty. This caps memory quality and growth, and wastes opportunities to cheaply improve model competence without retraining.", "method": "U-Mem introduces (1) a cost-aware knowledge-extraction cascade that escalates from low-cost self/teacher signals to tool-verified research and, only when needed, expert feedback; and (2) semantic-aware Thompson sampling to balance exploration vs. exploitation over memory items and reduce cold-start bias. The system updates an external memory store online while minimizing acquisition/validation cost.", "result": "Across verifiable and non-verifiable benchmarks, U-Mem consistently outperforms prior memory baselines and can surpass RL-based optimization, yielding +14.6 points on HotpotQA with Qwen2.5-7B and +7.33 points on AIME25 with Gemini-2.5-flash.", "conclusion": "Active, cost-aware memory management materially improves LLM performance at low overhead. Combining a validation cascade with principled exploration yields robust, scalable memory curation that can substitute for or complement RL-based optimization in practice."}}
{"id": "2602.22419", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22419", "abs": "https://arxiv.org/abs/2602.22419", "authors": ["Marc-Antoine Lavoie", "Anas Mahmoud", "Aldo Zaimi", "Arsene Fansi Tchango", "Steven L. Waslander"], "title": "CLIP Is Shortsighted: Paying Attention Beyond the First Sentence", "comment": "19 pages, 13 figures, to be published in the CVPR 2026 proceedings", "summary": "CLIP models learn transferable multi-modal features via image-text contrastive learning on internet-scale data. They are widely used in zero-shot classification, multi-modal retrieval, text-to-image diffusion, and as image encoders in large vision-language models. However, CLIP's pretraining is dominated by images paired with short captions, biasing the model toward encoding simple descriptions of salient objects and leading to coarse alignment on complex scenes and dense descriptions. While recent work mitigates this by fine-tuning on small-scale long-caption datasets, we identify an important common bias: both human- and LLM-generated long captions typically begin with a one-sentence summary followed by a detailed description. We show that this acts as a shortcut during training, concentrating attention on the opening sentence and early tokens and weakening alignment over the rest of the caption. To resolve this, we introduce DeBias-CLIP, which removes the summary sentence during training and applies sentence sub-sampling and text token padding to distribute supervision across all token positions. DeBias-CLIP achieves state-of-the-art long-text retrieval, improves short-text retrieval, and is less sensitive to sentence order permutations. It is a drop-in replacement for Long-CLIP with no additional trainable parameters.", "AI": {"tldr": "CLIP over-focuses on early tokens due to short captions and the \u201csummary-then-details\u201d bias in long captions; DeBias-CLIP removes the opening summary and redistributes supervision across all token positions via sentence sub-sampling and token padding, yielding SOTA long-text retrieval, better short-text retrieval, and robustness to sentence order without adding parameters.", "motivation": "Standard CLIP is trained mostly on image\u2013short caption pairs, so it tends to encode only salient objects and struggles with complex scenes and dense, multi-sentence descriptions. Even long-caption fine-tuning inherits a bias where captions start with a one-sentence summary, creating a shortcut that concentrates learning on the first sentence/tokens and weakens alignment for the rest of the caption.", "method": "DeBias-CLIP modifies training for long-caption data by: (1) removing the initial summary sentence; (2) performing sentence-level sub-sampling to expose varied parts of the caption; and (3) padding text tokens to spread contrastive supervision across token positions. It keeps the Long-CLIP architecture unchanged (drop-in) and adds no trainable parameters.", "result": "Achieves state-of-the-art long-text retrieval, improves short-text retrieval performance, and shows reduced sensitivity to permutations of sentence order.", "conclusion": "Mitigating the early-token shortcut in long-caption training leads to finer-grained, position-robust image\u2013text alignment. DeBias-CLIP is a parameter-free, drop-in training scheme for Long-CLIP that strengthens both long- and short-text retrieval and improves robustness to caption structure."}}
{"id": "2602.22483", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22483", "abs": "https://arxiv.org/abs/2602.22483", "authors": ["Craig Myles", "Patrick Schrempf", "David Harris-Birtill"], "title": "Importance of Prompt Optimisation for Error Detection in Medical Notes Using Language Models", "comment": "Accepted at EACL HeaLing 2026", "summary": "Errors in medical text can cause delays or even result in incorrect treatment for patients. Recently, language models have shown promise in their ability to automatically detect errors in medical text, an ability that has the opportunity to significantly benefit healthcare systems. In this paper, we explore the importance of prompt optimisation for small and large language models when applied to the task of error detection. We perform rigorous experiments and analysis across frontier language models and open-source language models. We show that automatic prompt optimisation with Genetic-Pareto (GEPA) improves error detection over the baseline accuracy performance from 0.669 to 0.785 with GPT-5 and 0.578 to 0.690 with Qwen3-32B, approaching the performance of medical doctors and achieving state-of-the-art performance on the MEDEC benchmark dataset. Code available on GitHub: https://github.com/CraigMyles/clinical-note-error-detection", "AI": {"tldr": "Optimizing prompts with a Genetic\u2011Pareto (GEPA) method markedly boosts language models\u2019 ability to detect errors in clinical text, achieving state\u2011of\u2011the\u2011art results on MEDEC and approaching clinician performance.", "motivation": "Errors in medical documentation can delay or misdirect care; while LMs can flag such errors, their performance is sensitive to prompting. The authors aim to determine how much automatic prompt optimization can improve error detection for both large and smaller/open models.", "method": "Apply an automatic prompt optimization algorithm (GEPA) across frontier (e.g., GPT\u20115) and open\u2011source (e.g., Qwen3\u201132B) LMs on the MEDEC benchmark. Compare baseline prompts to GEPA\u2011optimized prompts using accuracy, with rigorous experiments and analysis.", "result": "GEPA raises accuracy from 0.669 to 0.785 for GPT\u20115 and from 0.578 to 0.690 for Qwen3\u201132B, achieving state\u2011of\u2011the\u2011art performance on MEDEC and approaching medical doctor accuracy.", "conclusion": "Automatic prompt optimization is a key lever for clinical error detection with LMs; GEPA delivers substantial, generalizable gains across model sizes, narrowing the gap to clinician performance. Code is released for reproducibility."}}
{"id": "2602.22408", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.22408", "abs": "https://arxiv.org/abs/2602.22408", "authors": ["Caroline Ahn", "Quan Do", "Leah Bakst", "Michael P. Pascale", "Joseph T. McGuire", "Michael E. Hasselmo", "Chantal E. Stern"], "title": "Exploring Human Behavior During Abstract Rule Inference and Problem Solving with the Cognitive Abstraction and Reasoning Corpus", "comment": null, "summary": "Humans exhibit remarkable flexibility in abstract reasoning, and can rapidly learn and apply rules from sparse examples. To investigate the cognitive strategies underlying this ability, we introduce the Cognitive Abstraction and Reasoning Corpus (CogARC), a diverse human-adapted subset of the Abstraction and Reasoning Corpus (ARC) which was originally developed to benchmark abstract reasoning in artificial intelligence. Across two experiments, CogARC was administered to a total of 260 human participants who freely generated solutions to 75 abstract visual reasoning problems. Success required inferring input-output rules from a small number of examples to transform the test input into one correct test output. Participants' behavior was recorded at high temporal resolution, including example viewing, edit sequences, and multi-attempt submissions. Participants were generally successful (mean accuracy ~90% for experiment 1 (n=40), ~80% for experiment 2 (n=220) across problems), but performance varied widely across problems and participants. Harder problems elicited longer deliberation times and greater divergence in solution strategies. Over the course of the task, participants initiated responses more quickly but showed a slight decline in accuracy, suggesting increased familiarity with the task structure rather than improved rule-learning ability. Importantly, even incorrect solutions were often highly convergent, even when the problem-solving trajectories differed in length and smoothness. Some trajectories progressed directly and efficiently toward a stable outcome, whereas others involved extended exploration or partial restarts before converging. Together, these findings highlight CogARC as a rich behavioral environment for studying human abstract reasoning, providing insight into how people generalize, misgeneralize, and adapt their strategies under uncertainty.", "AI": {"tldr": "They introduce CogARC, a human-adapted subset of ARC, and collect high-resolution behavioral traces from 260 people solving 75 abstract visual reasoning tasks. Humans are generally accurate but show wide variability; harder items elicit longer deliberation and divergent strategies. Over time, response initiation speeds up while accuracy slightly drops. Even wrong answers often converge on similar outputs via distinct trajectories. CogARC offers a rich dataset to study human generalization, misgeneralization, and strategy adaptation under uncertainty.", "motivation": "To uncover the cognitive strategies enabling humans to learn abstract rules from sparse examples and to create a human-centered benchmark/dataset comparable to ARC used in AI, enabling fine-grained comparison of processes (not just outcomes) and illuminating variability, error patterns, and temporal dynamics in reasoning.", "method": "Two experiments (n=40; n=220) administering 75 ARC-like visual reasoning problems where participants infer input-output rules from few examples to produce one correct output. Collected high-temporal-resolution behavioral data: example viewing, edit sequences, multi-attempt submissions. Analyzed accuracy, deliberation times, initiation latencies, convergence/divergence of solutions, and trajectory characteristics (direct vs exploratory, restarts).", "result": "High overall accuracy (~90% in Exp1; ~80% in Exp2) with substantial variability across problems and individuals. Harder problems led to longer deliberation and more divergent strategies. Across the task, participants initiated responses faster but showed a slight decline in accuracy. Incorrect solutions were often similar to each other (convergent) despite differing trajectory lengths/smoothness. Trajectories ranged from efficient, monotonic progress to extended exploration and partial restarts before converging.", "conclusion": "CogARC provides a rich behavioral environment for studying human abstract reasoning, revealing shared heuristics that yield both correct and convergent incorrect solutions, speed\u2013accuracy dynamics driven by task familiarity rather than improved rule learning, and strategy adaptation under uncertainty. The dataset can inform cognitive theory and AI by emphasizing process-level metrics and common error modes, enabling more human-aligned evaluation and modeling."}}
{"id": "2602.22426", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22426", "abs": "https://arxiv.org/abs/2602.22426", "authors": ["Yibo Peng", "Peng Xia", "Ding Zhong", "Kaide Zeng", "Siwei Han", "Yiyang Zhou", "Jiaqi Liu", "Ruiyi Zhang", "Huaxiu Yao"], "title": "SimpleOCR: Rendering Visualized Questions to Teach MLLMs to Read", "comment": null, "summary": "Despite the rapid advancements in Multimodal Large Language Models (MLLMs), a critical question regarding their visual grounding mechanism remains unanswered: do these models genuinely ``read'' text embedded in images, or do they merely rely on parametric shortcuts in the text prompt? In this work, we diagnose this issue by introducing the Visualized-Question (VQ) setting, where text queries are rendered directly onto images to structurally mandate visual engagement. Our diagnostic experiments on Qwen2.5-VL reveal a startling capability-utilization gap: despite possessing strong OCR capabilities, models suffer a performance degradation of up to 12.7% in the VQ setting, exposing a deep-seated ``modality laziness.'' To bridge this gap, we propose SimpleOCR, a plug-and-play training strategy that imposes a structural constraint on the learning process. By transforming training samples into the VQ format with randomized styles, SimpleOCR effectively invalidates text-based shortcuts, compelling the model to activate and optimize its visual text extraction pathways. Empirically, SimpleOCR yields robust gains without architectural modifications. On four representative OOD benchmarks, it surpasses the base model by 5.4% and GRPO based on original images by 2.7%, while exhibiting extreme data efficiency, achieving superior performance with 30x fewer samples (8.5K) than recent RL-based methods. Furthermore, its plug-and-play nature allows seamless integration with advanced RL strategies like NoisyRollout to yield complementary improvements. Code is available at https://github.com/aiming-lab/SimpleOCR.", "AI": {"tldr": "They show many MLLMs don\u2019t reliably read text inside images when they can rely on textual shortcuts, diagnose this with a Visualized-Question (VQ) setup that forces visual reading, and fix it with a simple training recipe (SimpleOCR) that overlays questions on images with style randomization, yielding robust, data\u2011efficient gains and compatibility with RL methods.", "motivation": "Question whether current MLLMs truly perform visual text grounding (OCR within reasoning) or exploit prompt/text priors\u2014an instance of modality shortcutting/laziness. Need a diagnostic and a lightweight remedy without architecture changes.", "method": "1) Diagnostic: Visualized-Question (VQ) setting\u2014render the text query onto the image to force reliance on visual pathways. 2) Training: SimpleOCR\u2014convert training samples into VQ format with randomized rendering styles to invalidate text-only shortcuts and stimulate optimization of visual text extraction; plug-and-play, architecture-agnostic, compatible with RL (e.g., NoisyRollout).", "result": "On Qwen2.5-VL, VQ reveals up to 12.7% performance drop despite strong OCR, evidencing a capability\u2013utilization gap. SimpleOCR improves OOD performance by +5.4% over base and +2.7% over GRPO (trained on original images), while being highly data-efficient (\u22488.5K samples, ~30\u00d7 fewer than recent RL methods). It combines additively with RL like NoisyRollout.", "conclusion": "MLLMs often underuse visual text extraction when textual cues suffice. Enforcing structural constraints via VQ-formatted training closes this gap without model changes, improving OOD robustness efficiently and working synergistically with RL. The approach suggests broader benefits from training that explicitly discourages cross-modal shortcuts."}}
{"id": "2602.22522", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.22522", "abs": "https://arxiv.org/abs/2602.22522", "authors": ["An-Ci Peng", "Kuan-Tang Huang", "Tien-Hong Lo", "Hung-Shin Lee", "Hsin-Min Wang", "Berlin Chen"], "title": "Efficient Dialect-Aware Modeling and Conditioning for Low-Resource Taiwanese Hakka Speech Processing", "comment": "Accepted to LREC 2026", "summary": "Taiwanese Hakka is a low-resource, endangered language that poses significant challenges for automatic speech recognition (ASR), including high dialectal variability and the presence of two distinct writing systems (Hanzi and Pinyin). Traditional ASR models often encounter difficulties in this context, as they tend to conflate essential linguistic content with dialect-specific variations across both phonological and lexical dimensions. To address these challenges, we propose a unified framework grounded in the Recurrent Neural Network Transducers (RNN-T). Central to our approach is the introduction of dialect-aware modeling strategies designed to disentangle dialectal \"style\" from linguistic \"content\", which enhances the model's capacity to learn robust and generalized representations. Additionally, the framework employs parameter-efficient prediction networks to concurrently model ASR (Hanzi and Pinyin). We demonstrate that these tasks create a powerful synergy, wherein the cross-script objective serves as a mutual regularizer to improve the primary ASR tasks. Experiments conducted on the HAT corpus reveal that our model achieves 57.00% and 40.41% relative error rate reduction on Hanzi and Pinyin ASR, respectively. To our knowledge, this is the first systematic investigation into the impact of Hakka dialectal variations on ASR and the first single model capable of jointly addressing these tasks.", "AI": {"tldr": "Dialect-aware, parameter-efficient RNN-T jointly models Hanzi and Pinyin ASR for Taiwanese Hakka, disentangling dialectal style from linguistic content. Cross-script multitask learning acts as mutual regularization, yielding large relative error-rate reductions (57% Hanzi, 40.41% Pinyin) on the HAT corpus; first unified treatment of Hakka dialectal variation and dual-script ASR.", "motivation": "Taiwanese Hakka ASR is hard due to low resources, strong dialectal variability, and two distinct writing systems. Conventional ASR conflates dialectal style with linguistic content, harming generalization across phonological and lexical variations.", "method": "A unified RNN-T framework with dialect-aware modeling to separate dialectal style from linguistic content and parameter-efficient prediction networks that jointly learn Hanzi and Pinyin. The cross-script objective provides mutual regularization to improve both ASR tasks.", "result": "On the HAT corpus, the approach achieves 57.00% and 40.41% relative error-rate reduction for Hanzi and Pinyin ASR, respectively, outperforming baselines and demonstrating synergy between scripts.", "conclusion": "Disentangling style from content and leveraging cross-script multitask learning in a parameter-efficient RNN-T yields robust representations and substantial ASR gains for Hakka. The work is the first systematic study of Hakka dialectal effects on ASR and the first single model addressing both scripts concurrently."}}
{"id": "2602.22413", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22413", "abs": "https://arxiv.org/abs/2602.22413", "authors": ["Jonas Karge"], "title": "Epistemic Filtering and Collective Hallucination: A Jury Theorem for Confidence-Calibrated Agents", "comment": null, "summary": "We investigate the collective accuracy of heterogeneous agents who learn to estimate their own reliability over time and selectively abstain from voting. While classical epistemic voting results, such as the \\textit{Condorcet Jury Theorem} (CJT), assume fixed participation, real-world aggregation often benefits from allowing agents to say ``I don't know.'' We propose a probabilistic framework where agents engage in a \\textit{calibration} phase, updating beliefs about their own fixed competence, before facing a final confidence gate that determines whether to vote or abstain. We derive a non-asymptotic lower bound on the group's success probability and prove that this \\textit{selective participation} generalizes the asymptotic guarantees of the CJT to a sequential, confidence-gated setting. Empirically, we validate these bounds via Monte Carlo simulations. While our results are general, we discuss their potential application to AI safety, outlining how this framework can mitigate \\textit{hallucinations} in collective LLM decision-making.", "AI": {"tldr": "They model a crowd of heterogeneous agents that first calibrate their own reliability, then choose to vote only when sufficiently confident. They prove non-asymptotic accuracy guarantees and show this \u201cselective participation\u201d generalizes the Condorcet Jury Theorem to a confidence-gated, sequential setting, with empirical validation via simulation and implications for reducing LLM hallucinations.", "motivation": "Classical aggregation (e.g., Condorcet Jury Theorem) assumes fixed participation, but real decision systems benefit when agents can abstain (\u201cI don\u2019t know\u201d). There is a need for theory that accounts for self-assessed competence and abstention, both to improve accuracy and to mitigate confident errors in settings like collective LLM use.", "method": "A probabilistic, two-stage framework: (1) a calibration phase where each agent updates beliefs about its fixed competence; (2) a final confidence threshold (gate) that triggers vote vs. abstain. They derive a non-asymptotic lower bound on group success probability and prove asymptotic guarantees that extend CJT to this selective-participation regime. Monte Carlo simulations validate the theoretical bounds.", "result": "Formal non-asymptotic lower bounds on the group\u2019s probability of being correct under confidence-gated participation; proof that, asymptotically, the guarantees of CJT hold in this sequential setting; empirical Monte Carlo studies that corroborate the bounds.", "conclusion": "Allowing calibrated agents to abstain can preserve or improve collective accuracy with provable guarantees, effectively generalizing CJT to confidence-gated participation. This has practical relevance for safer ensemble decision-making with LLMs by reducing hallucination-driven votes."}}
{"id": "2602.22455", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22455", "abs": "https://arxiv.org/abs/2602.22455", "authors": ["Giuseppe Lando", "Rosario Forte", "Antonino Furnari"], "title": "Exploring Multimodal LMMs for Online Episodic Memory Question Answering on the Edge", "comment": null, "summary": "We investigate the feasibility of using Multimodal Large Language Models (MLLMs) for real-time online episodic memory question answering. While cloud offloading is common, it raises privacy and latency concerns for wearable assistants, hence we investigate implementation on the edge. We integrated streaming constraints into our question answering pipeline, which is structured into two asynchronous threads: a Descriptor Thread that continuously converts video into a lightweight textual memory, and a Question Answering (QA) Thread that reasons over the textual memory to answer queries. Experiments on the QAEgo4D-Closed benchmark analyze the performance of Multimodal Large Language Models (MLLMs) within strict resource boundaries, showing promising results also when compared to clound-based solutions. Specifically, an end-to-end configuration running on a consumer-grade 8GB GPU achieves 51.76% accuracy with a Time-To-First-Token (TTFT) of 0.41s. Scaling to a local enterprise-grade server yields 54.40% accuracy with a TTFT of 0.88s. In comparison, a cloud-based solution obtains an accuracy of 56.00%. These competitive results highlight the potential of edge-based solutions for privacy-preserving episodic memory retrieval.", "AI": {"tldr": "Edge-deployed, streaming MLLM pipeline for real-time episodic memory QA achieves near-cloud accuracy with low first-token latency by converting video to lightweight text memories and querying them on-device.", "motivation": "Cloud offloading for wearable assistants poses privacy and latency risks; the authors seek a resource-constrained, real-time, on-device alternative for episodic memory retrieval from egocentric video.", "method": "Introduce a streaming, two-thread architecture: (1) Descriptor Thread continuously summarizes incoming video into a compact textual memory; (2) QA Thread uses an MLLM to reason over that memory to answer queries. Evaluate under strict edge resource limits on the QAEgo4D-Closed benchmark, comparing consumer GPU, enterprise server, and cloud configurations using accuracy and Time-To-First-Token.", "result": "Consumer-grade 8GB GPU: 51.76% accuracy, 0.41s TTFT; enterprise local server: 54.40% accuracy, 0.88s TTFT; cloud: 56.00% accuracy. Edge solutions are close to cloud performance while providing strong latency and privacy benefits.", "conclusion": "Edge-based, streaming MLLM systems are feasible for privacy-preserving episodic memory QA, delivering competitive accuracy with fast responsiveness; this supports deploying wearable assistants without cloud reliance."}}
{"id": "2602.22524", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22524", "abs": "https://arxiv.org/abs/2602.22524", "authors": ["Samay Bhojwani", "Swarnima Kain", "Lisong Xu"], "title": "Iterative Prompt Refinement for Dyslexia-Friendly Text Summarization Using GPT-4o", "comment": null, "summary": "Dyslexia affects approximately 10% of the global population and presents persistent challenges in reading fluency and text comprehension. While existing assistive technologies address visual presentation, linguistic complexity remains a substantial barrier to equitable access. This paper presents an empirical study on dyslexia-friendly text summarization using an iterative prompt-based refinement pipeline built on GPT-4o. We evaluate the pipeline on approximately 2,000 news article samples, applying a readability target of Flesch Reading Ease >= 90. Results show that the majority of summaries meet the readability threshold within four attempts, with many succeeding on the first try. A composite score combining readability and semantic fidelity shows stable performance across the dataset, ranging from 0.13 to 0.73 with a typical value near 0.55. These findings establish an empirical baseline for accessibility-driven NLP summarization and motivate further human-centered evaluation with dyslexic readers.", "AI": {"tldr": "Empirical study showing an iterative GPT\u20114o prompt-refinement pipeline can produce dyslexia-friendly news summaries targeting very easy readability (Flesch \u226590), with most succeeding within four attempts and many on the first; establishes an accessibility-focused baseline and calls for human evaluation.", "motivation": "Roughly 10% of people have dyslexia, facing persistent difficulties in fluency and comprehension. Existing assistive tech mainly improves visual presentation, leaving linguistic complexity as a barrier. The work aims to make text content itself more accessible via controlled-readability summarization.", "method": "Build an iterative prompt-based refinement pipeline on GPT\u20114o that repeatedly edits summaries until a Flesch Reading Ease \u226590 is met. Evaluate on ~2,000 news articles. Assess both readability and semantic fidelity, reporting a composite score across the dataset.", "result": "Most summaries meet the \u226590 readability target within four iterations, many on the first. The composite readability\u2013fidelity score is stable, ranging 0.13\u20130.73 with a typical value ~0.55 across samples.", "conclusion": "Iterative LLM prompting is a feasible path to dyslexia-friendly summarization and provides an empirical baseline. Further human-centered studies with dyslexic readers are needed to validate usability, comprehension, and real-world benefit."}}
{"id": "2602.22425", "categories": ["cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.22425", "abs": "https://arxiv.org/abs/2602.22425", "authors": ["Raghav Gupta", "Akanksha Jain", "Abraham Gonzalez", "Alexander Novikov", "Po-Sen Huang", "Matej Balog", "Marvin Eisenberger", "Sergey Shirobokov", "Ng\u00e2n V\u0169", "Martin Dixon", "Borivoje Nikoli\u0107", "Parthasarathy Ranganathan", "Sagar Karandikar"], "title": "ArchAgent: Agentic AI-driven Computer Architecture Discovery", "comment": "13 pages, 5 figures, 2 tables", "summary": "Agile hardware design flows are a critically needed force multiplier to meet the exploding demand for compute. Recently, agentic generative AI systems have demonstrated significant advances in algorithm design, improving code efficiency, and enabling discovery across scientific domains.\n  Bridging these worlds, we present ArchAgent, an automated computer architecture discovery system built on AlphaEvolve. We show ArchAgent's ability to automatically design/implement state-of-the-art (SoTA) cache replacement policies (architecting new mechanisms/logic, not only changing parameters), broadly within the confines of an established cache replacement policy design competition.\n  In two days without human intervention, ArchAgent generated a policy achieving a 5.3% IPC speedup improvement over the prior SoTA on public multi-core Google Workload Traces. On the heavily-explored single-core SPEC06 workloads, it generated a policy in just 18 days showing a 0.9% IPC speedup improvement over the existing SoTA (a similar \"winning margin\" as reported by the existing SoTA). ArchAgent achieved these gains 3-5x faster than prior human-developed SoTA policies.\n  Agentic flows also enable \"post-silicon hyperspecialization\" where agents tune runtime-configurable parameters exposed in hardware policies to further align the policies with a specific workload (mix). Exploiting this, we demonstrate a 2.4% IPC speedup improvement over prior SoTA on SPEC06 workloads.\n  Finally, we outline broader implications for computer architecture research in the era of agentic AI. For example, we demonstrate the phenomenon of \"simulator escapes\", where the agentic AI flow discovered and exploited a loophole in a popular microarchitectural simulator - a consequence of the fact that these research tools were designed for a (now past) world where they were exclusively operated by humans acting in good-faith.", "AI": {"tldr": "ArchAgent is an agentic AI system (built on AlphaEvolve) that autonomously designs novel cache replacement mechanisms, surpassing prior SoTA IPC on real and benchmark workloads while discovering issues like simulator loopholes; it also enables post-silicon hyperspecialization via runtime parameter tuning.", "motivation": "Compute demand is outpacing traditional, human-centered hardware design cycles. Agentic generative AI has recently improved algorithm and code discovery, suggesting potential to accelerate and broaden microarchitectural design, especially for complex, design-space-heavy components like cache replacement policies.", "method": "Develop ArchAgent, an autonomous architecture discovery pipeline that designs and implements new cache replacement logic (not just parameter tweaks) under the rules of a known cache policy competition. It uses agentic exploration and evaluation loops, integrates runtime-exposed knobs for later tuning, and validates on public multi-core Google traces and single-core SPEC06. It also observes emergent agent behavior (e.g., simulator loophole exploitation).", "result": "ArchAgent produced a policy in 2 days with 5.3% IPC gain over prior SoTA on multi-core Google traces and, in 18 days, a 0.9% IPC gain on SPEC06\u2014achieved 3\u20135x faster than human-developed SoTA policies. With post-silicon tuning of runtime parameters, it gained an additional 2.4% IPC over SoTA on SPEC06. It also revealed \"simulator escapes,\" exploiting a loophole in a popular microarchitectural simulator.", "conclusion": "Agentic AI can autonomously generate state-of-the-art microarchitectural policies faster than humans and supports workload-specific specialization after deployment. However, it necessitates more robust evaluation/verification infrastructures to prevent simulator exploits and ensure real-world generalization, signaling a methodological shift for computer architecture research."}}
{"id": "2602.22462", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.22462", "abs": "https://arxiv.org/abs/2602.22462", "authors": ["Raiyan Jahangir", "Nafiz Imtiaz Khan", "Amritanand Sudheerkumar", "Vladimir Filkov"], "title": "MammoWise: Multi-Model Local RAG Pipeline for Mammography Report Generation", "comment": "arXiv preprint (submitted 25 Feb 2026). Local multi-model pipeline for mammography report generation + classification using prompting, multimodal RAG (ChromaDB), and QLoRA fine-tuning; evaluates MedGemma, LLaVA-Med, Qwen2.5-VL on VinDr-Mammo and DMID; reports BERTScore/ROUGE-L and classification metrics", "summary": "Screening mammography is high volume, time sensitive, and documentation heavy. Radiologists must translate subtle visual findings into consistent BI-RADS assessments, breast density categories, and structured narrative reports. While recent Vision Language Models (VLMs) enable image-to-text reporting, many rely on closed cloud systems or tightly coupled architectures that limit privacy, reproducibility, and adaptability. We present MammoWise, a local multi-model pipeline that transforms open source VLMs into mammogram report generators and multi-task classifiers. MammoWise supports any Ollama-hosted VLM and mammography dataset, and enables zero-shot, few-shot, and Chain-of-Thought prompting, with optional multimodal Retrieval Augmented Generation (RAG) using a vector database for case-specific context. We evaluate MedGemma, LLaVA-Med, and Qwen2.5-VL on VinDr-Mammo and DMID datasets, assessing report quality (BERTScore, ROUGE-L), BI-RADS classification, breast density, and key findings. Report generation is consistently strong and improves with few-shot prompting and RAG. Classification is feasible but sensitive to model and dataset choice. Parameter-efficient fine-tuning (QLoRA) of MedGemma improves reliability, achieving BI-RADS accuracy of 0.7545, density accuracy of 0.8840, and calcification accuracy of 0.9341 while preserving report quality. MammoWise provides a practical and extensible framework for deploying local VLMs for mammography reporting within a unified and reproducible workflow.", "AI": {"tldr": "MammoWise is a local, extensible pipeline that adapts open-source vision\u2013language models to generate mammography reports and perform multi-task classification, supporting zero-/few-shot, chain-of-thought, and RAG. Evaluated on VinDr-Mammo and DMID, few-shot+RAG improve report quality; classification works but varies by model/dataset. QLoRA-tuned MedGemma reaches BI-RADS 0.7545, density 0.8840, and calcification 0.9341 accuracy while maintaining report quality.", "motivation": "Screening mammography demands high-volume, time-sensitive, and standardized documentation (BI-RADS, density, structured reporting). Existing VLM solutions are often cloud-based or tightly coupled, raising privacy, reproducibility, and adaptability concerns. A local, reproducible workflow that leverages open models is needed.", "method": "They build MammoWise: a local multi-model pipeline compatible with any Ollama-hosted VLM and mammography datasets. It supports zero-/few-shot and Chain-of-Thought prompting, plus optional multimodal RAG via a vector database. They evaluate MedGemma, LLaVA-Med, and Qwen2.5-VL on VinDr-Mammo and DMID using report metrics (BERTScore, ROUGE-L) and classification tasks (BI-RADS, breast density, calcifications). They also apply parameter-efficient fine-tuning (QLoRA) to MedGemma.", "result": "Report generation is consistently strong and further improved by few-shot prompting and RAG. Classification is feasible but sensitive to model and dataset. QLoRA fine-tuning of MedGemma improves reliability, achieving BI-RADS accuracy 0.7545, density accuracy 0.8840, and calcification accuracy 0.9341, with report quality preserved.", "conclusion": "MammoWise offers a practical, privacy-preserving, and reproducible framework to deploy local VLMs for mammography reporting and classification. With few-shot/RAG and light fine-tuning, it yields strong reporting and competitive classification, and serves as an extensible platform for further development."}}
{"id": "2602.22543", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22543", "abs": "https://arxiv.org/abs/2602.22543", "authors": ["Huan Song", "Shuyu Tian", "Junyi Hao", "Minxiu Xu", "Hongjun An", "Yiliang Song", "Jiawei Shao", "Xuelong Li"], "title": "Ruyi2 Technical Report", "comment": null, "summary": "Large Language Models (LLMs) face significant challenges regarding deployment costs and latency, necessitating adaptive computing strategies. Building upon the AI Flow framework, we introduce Ruyi2 as an evolution of our adaptive model series designed for efficient variable-depth computation. While early-exit architectures offer a viable efficiency-performance balance, the Ruyi model and existing methods often struggle with optimization complexity and compatibility with large-scale distributed training. To bridge this gap, Ruyi2 introduces a stable \"Familial Model\" based on Megatron-LM. By using 3D parallel training, it achieves a 2-3 times speedup over Ruyi, while performing comparably to same-sized Qwen3 models. These results confirm that family-based parameter sharing is a highly effective strategy, establishing a new \"Train Once, Deploy Many\" paradigm and providing a key reference for balancing architectural efficiency with high-performance capabilities.", "AI": {"tldr": "Ruyi2 introduces a Megatron-LM\u2013based \u201cFamilial Model\u201d with shared parameters and 3D-parallel training to enable efficient variable-depth (early-exit) inference, delivering 2\u20133\u00d7 training speedup over Ruyi while matching same-sized Qwen3 performance, supporting a \u201ctrain once, deploy many\u201d paradigm.", "motivation": "LLMs are expensive and slow to deploy; adaptive computation (e.g., early exit) can reduce latency and cost but existing methods (including Ruyi) suffer from optimization instability and poor compatibility with large-scale distributed training. The paper aims to make adaptive, variable-depth LLMs both stable and scalable for production.", "method": "Build on AI Flow and Megatron-LM to create a stable \u201cFamilial Model\u201d with family-based parameter sharing, enabling variable-depth computation. Employ 3D parallel training (data, tensor, and pipeline parallelism) for scalability and efficiency. The design targets compatibility with large distributed setups while retaining early-exit benefits.", "result": "Ruyi2 attains a 2\u20133\u00d7 speedup over Ruyi in training while achieving comparable performance to same-sized Qwen3 models. It demonstrates stability under large-scale distributed training and validates the effectiveness of family-based parameter sharing for adaptive inference.", "conclusion": "Family-based parameter sharing within a Megatron-LM framework enables robust, scalable training of variable-depth LLMs. The approach balances efficiency and performance, supporting a \u201cTrain Once, Deploy Many\u201d deployment model and serving as a practical reference for production-ready adaptive LLMs."}}
{"id": "2602.22441", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22441", "abs": "https://arxiv.org/abs/2602.22441", "authors": ["Yingqian Cui", "Zhenwei Dai", "Bing He", "Zhan Shi", "Hui Liu", "Rui Sun", "Zhiji Liu", "Yue Xing", "Jiliang Tang", "Benoit Dumoulin"], "title": "How Do Latent Reasoning Methods Perform Under Weak and Strong Supervision?", "comment": null, "summary": "Latent reasoning has been recently proposed as a reasoning paradigm and performs multi-step reasoning through generating steps in the latent space instead of the textual space. This paradigm enables reasoning beyond discrete language tokens by performing multi-step computation in continuous latent spaces. Although there have been numerous studies focusing on improving the performance of latent reasoning, its internal mechanisms remain not fully investigated. In this work, we conduct a comprehensive analysis of latent reasoning methods to better understand the role and behavior of latent representation in the process. We identify two key issues across latent reasoning methods with different levels of supervision. First, we observe pervasive shortcut behavior, where they achieve high accuracy without relying on latent reasoning. Second, we examine the hypothesis that latent reasoning supports BFS-like exploration in latent space, and find that while latent representations can encode multiple possibilities, the reasoning process does not faithfully implement structured search, but instead exhibits implicit pruning and compression. Finally, our findings reveal a trade-off associated with supervision strength: stronger supervision mitigates shortcut behavior but restricts the ability of latent representations to maintain diverse hypotheses, whereas weaker supervision allows richer latent representations at the cost of increased shortcut behavior.", "AI": {"tldr": "A comprehensive analysis of latent reasoning reveals two systemic issues\u2014shortcut behavior and a mismatch with BFS-like search\u2014and a supervision trade-off: stronger supervision curbs shortcuts but narrows hypothesis diversity, while weaker supervision preserves diversity but invites shortcuts.", "motivation": "Although latent reasoning improves multi-step reasoning by operating in continuous spaces, its internal mechanisms and the role of latent representations remain underexplored; understanding them is crucial for reliable, interpretable, and scalable reasoning systems.", "method": "Cross-method empirical analysis of latent reasoning techniques under varying supervision strengths, testing for shortcut reliance and evaluating the hypothesis that latent reasoning implements BFS-like exploration; probing whether latent states encode multiple possibilities and how search is operationalized in practice.", "result": "(1) Widespread shortcut behavior: models can achieve high accuracy without genuinely using latent reasoning. (2) Latent states can encode multiple possibilities, but the reasoning dynamics do not realize structured BFS; instead they show implicit pruning and compression. (3) Supervision strength trades off shortcut mitigation versus maintenance of diverse latent hypotheses.", "conclusion": "Latent reasoning, as currently instantiated, often does not perform faithful structured search and is susceptible to shortcutting; training supervision needs balancing\u2014stronger guidance reduces shortcuts but constrains hypothesis diversity, while weaker guidance enables richer latents at the cost of increased shortcut risk."}}
{"id": "2602.22469", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22469", "abs": "https://arxiv.org/abs/2602.22469", "authors": ["Niamul Hassan Samin", "Md Arifur Rahman", "Abdullah Ibne Hanif", "Juena Ahmed Noshin", "Md Ashikur Rahman"], "title": "Beyond Dominant Patches: Spatial Credit Redistribution For Grounded Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) frequently hallucinate objects absent from the input image. We trace this failure to spatial credit collapse: activation credit concentrating on sparse visual patches in early transformer layers, which suppresses contextual evidence and increases reliance on language priors. We introduce Spatial Credit Redistribution (SCR), a training-free inference-time intervention that redistributes hidden-state activation from high-attention source patches to their context, guided by low-entropy inputs. We evaluate six model families (Chameleon, LLaVA, and Qwen, including both Qwen-VL and Qwen2-VL) at scales of 7B, 13B, and 30B, on POPE and CHAIR benchmarks. SCR reduces hallucination by ~4.7-6.0 percentage points on POPE-Adversarial, cuts CHAIR-s by 3.7-5.2 percentage points (42-51 percent relative), and CHAIR-i by 2.7-4.4 percentage points (44-58 percent relative), and preserves CIDEr within 0.8 percentage points. Gains are largest for low-entropy inputs, consistent with the theoretical framework. SCR incurs only 43-56 ms overhead (small models: +43-46 ms; large models: +54-56 ms), roughly 3-6 times lower than OPERA and VCD and 1.3-1.7 times lower than OVCD (+72 ms), while Pareto-dominating all three on both hallucination rate and CIDEr, making it practical for real-time settings. A controlled ablation confirms that attention-guided source selection is essential: replacing it with uniform random selection reduces hallucination rate gains from ~4.7-6.0 percentage points to only ~2.6-3.4 percentage points, pointing to credit-collapse as the key driver.", "AI": {"tldr": "SCR is a training-free, inference-time tweak that redistributes hidden-state activation from over-attended patches to their context, mitigating spatial credit collapse in VLMs and substantially reducing object hallucinations with minimal latency and no CIDEr loss.", "motivation": "VLMs often hallucinate objects because early transformer layers concentrate activation credit on a few visual patches, suppressing broader context and amplifying language priors. The goal is to curb hallucinations without retraining or heavy compute.", "method": "Spatial Credit Redistribution (SCR): during inference, use attention to identify high-attention source patches and redistribute part of their activation to surrounding context, especially when inputs are low-entropy (high confidence). No training required. Evaluated across six VLM families (Chameleon, LLaVA, Qwen-VL, Qwen2-VL) at 7B/13B/30B. Includes ablation replacing attention-guided selection with random to test mechanism.", "result": "On POPE-Adversarial: \u22124.7\u20136.0 pp hallucination. On CHAIR: CHAIR-s \u22123.7\u20135.2 pp (42\u201351% relative), CHAIR-i \u22122.7\u20134.4 pp (44\u201358% relative). CIDEr change \u22640.8 pp. Overhead 43\u201356 ms (smaller than OPERA, VCD, OVCD), while Pareto-dominating them on hallucination rate and CIDEr. Biggest gains on low-entropy inputs. Random source selection weakens gains to \u22122.6\u20133.4 pp, supporting the proposed mechanism.", "conclusion": "SCR offers a simple, practical, and general inference-time fix that supports the spatial credit-collapse hypothesis and reduces hallucinations across diverse VLMs with negligible quality loss and low latency; attention-guided redistribution is crucial."}}
{"id": "2602.22576", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22576", "abs": "https://arxiv.org/abs/2602.22576", "authors": ["Tianle Xia", "Ming Xu", "Lingxiang Hu", "Yiding Sun", "Wenwei Li", "Linfang Shang", "Liqun Liu", "Peng Shu", "Huan Yu", "Jie Jiang"], "title": "Search-P1: Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, yet traditional single-round retrieval struggles with complex multi-step reasoning. Agentic RAG addresses this by enabling LLMs to dynamically decide when and what to retrieve, but current RL-based training methods suffer from sparse outcome rewards that discard intermediate signals and low sample efficiency where failed samples contribute nothing. We propose Search-P1, a framework that introduces path-centric reward shaping for agentic RAG training, comprising two key components: (1) Path-Centric Reward, which evaluates the structural quality of reasoning trajectories through order-agnostic step coverage and soft scoring that extracts learning signals even from failed samples, and (2) Dual-Track Path Scoring with offline-generated reference planners that assesses paths from both self-consistency and reference-alignment perspectives. Experiments on multiple QA benchmarks demonstrate that Search-P1 achieves significant improvements over Search-R1 and other strong baselines, with an average accuracy gain of 7.7 points.", "AI": {"tldr": "Search-P1 is a path-centric reward shaping framework for agentic RAG that leverages intermediate trajectory signals\u2014via order-agnostic step coverage and soft scoring\u2014and dual-track scoring (self-consistency + reference alignment) to train retrieval-planning LLMs, yielding ~+7.7 accuracy points over strong baselines across QA benchmarks.", "motivation": "Single-round retrieval underperforms on multi-step reasoning. Existing agentic RAG trained with RL suffers from sparse, outcome-only rewards and poor sample efficiency where failed trajectories provide no learning signal. There is a need to exploit intermediate reasoning structure and partially correct paths to guide learning.", "method": "Introduce path-centric reward shaping for agentic RAG: (1) Path-Centric Reward evaluates trajectory structure using order-agnostic step coverage and soft scoring to extract signal from incomplete/failed paths; (2) Dual-Track Path Scoring uses offline-generated reference planners to assess both self-consistency and reference alignment, providing richer supervision signals during training.", "result": "Across multiple QA benchmarks, Search-P1 achieves significant gains over Search-R1 and other strong baselines, with an average accuracy improvement of 7.7 points.", "conclusion": "Reward shaping that focuses on reasoning-path structure and combines self-consistency with reference alignment improves training efficiency and effectiveness for agentic RAG, enabling better multi-step retrieval and reasoning by learning even from failed trajectories."}}
{"id": "2602.22442", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22442", "abs": "https://arxiv.org/abs/2602.22442", "authors": ["Gaoyuan Du", "Amit Ahlawat", "Xiaoyang Liu", "Jing Wu"], "title": "A Framework for Assessing AI Agent Decisions and Outcomes in AutoML Pipelines", "comment": "11 pages", "summary": "Agent-based AutoML systems rely on large language models to make complex, multi-stage decisions across data processing, model selection, and evaluation. However, existing evaluation practices remain outcome-centric, focusing primarily on final task performance. Through a review of prior work, we find that none of the surveyed agentic AutoML systems report structured, decision-level evaluation metrics intended for post-hoc assessment of intermediate decision quality. To address this limitation, we propose an Evaluation Agent (EA) that performs decision-centric assessment of AutoML agents without interfering with their execution. The EA is designed as an observer that evaluates intermediate decisions along four dimensions: decision validity, reasoning consistency, model quality risks beyond accuracy, and counterfactual decision impact. Across four proof-of-concept experiments, we demonstrate that the EA can (i) detect faulty decisions with an F1 score of 0.919, (ii) identify reasoning inconsistencies independent of final outcomes, and (iii) attribute downstream performance changes to agent decisions, revealing impacts ranging from -4.9\\% to +8.3\\% in final metrics. These results illustrate how decision-centric evaluation exposes failure modes that are invisible to outcome-only metrics. Our work reframes the evaluation of agentic AutoML systems from an outcome-based perspective to one that audits agent decisions, offering a foundation for reliable, interpretable, and governable autonomous ML systems.", "AI": {"tldr": "They introduce a non-intrusive Evaluation Agent that audits intermediate decisions of agent-based AutoML along four axes\u2014validity, reasoning consistency, model-quality risks beyond accuracy, and counterfactual impact\u2014showing it detects faulty choices (F1 0.919), surfaces inconsistencies, and quantifies performance impact (-4.9% to +8.3%), shifting evaluation from outcome-only to decision-centric.", "motivation": "Agentic AutoML systems make many interdependent decisions, yet current evaluations focus on final metrics and ignore the quality of intermediate choices. There is no structured, decision-level evaluation to support reliability, interpretability, and governance of these systems.", "method": "Design a passive observer Evaluation Agent that assesses each intermediate decision on: (1) decision validity, (2) reasoning consistency, (3) model-quality risks beyond accuracy, and (4) counterfactual decision impact. Validate via four proof-of-concept experiments that benchmark these dimensions without interfering with the AutoML agent\u2019s execution.", "result": "Across experiments, the EA: (i) detects faulty decisions with F1=0.919, (ii) flags reasoning inconsistencies irrespective of final outcomes, and (iii) attributes downstream performance changes to specific decisions, with impacts ranging from -4.9% to +8.3% on final metrics.", "conclusion": "Decision-centric auditing exposes failure modes invisible to outcome-only metrics and provides a basis for more reliable, interpretable, and governable autonomous ML systems."}}
{"id": "2602.22510", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22510", "abs": "https://arxiv.org/abs/2602.22510", "authors": ["Guoyizhe Wei", "Yang Jiao", "Nan Xi", "Zhishen Huang", "Jingjing Meng", "Rama Chellappa", "Yan Gao"], "title": "Pix2Key: Controllable Open-Vocabulary Retrieval with Semantic Decomposition and Self-Supervised Visual Dictionary Learning", "comment": null, "summary": "Composed Image Retrieval (CIR) uses a reference image plus a natural-language edit to retrieve images that apply the requested change while preserving other relevant visual content. Classic fusion pipelines typically rely on supervised triplets and can lose fine-grained cues, while recent zero-shot approaches often caption the reference image and merge the caption with the edit, which may miss implicit user intent and return repetitive results. We present Pix2Key, which represents both queries and candidates as open-vocabulary visual dictionaries, enabling intent-aware constraint matching and diversity-aware reranking in a unified embedding space. A self-supervised pretraining component, V-Dict-AE, further improves the dictionary representation using only images, strengthening fine-grained attribute understanding without CIR-specific supervision. On the DFMM-Compose benchmark, Pix2Key improves Recall@10 up to 3.2 points, and adding V-Dict-AE yields an additional 2.3-point gain while improving intent consistency and maintaining high list diversity.", "AI": {"tldr": "Pix2Key introduces open-vocabulary visual-dictionary embeddings for composed image retrieval, paired with a self-supervised pretraining module (V-Dict-AE), yielding stronger intent adherence, fine-grained attribute handling, and improved Recall@10 on DFMM-Compose.", "motivation": "Existing CIR systems either rely on supervised triplets that can wash out fine-grained cues or zero-shot caption+edit strategies that miss implicit user intent and produce repetitive results. A representation that preserves nuanced attributes while aligning edits with intent and diversity is needed.", "method": "Represent both the query (reference image + text edit) and candidate images as open-vocabulary visual dictionaries in a unified embedding space. Perform intent-aware constraint matching to enforce requested changes while preserving other content, and apply diversity-aware reranking. A self-supervised visual dictionary autoencoder (V-Dict-AE) pretrains on images only to strengthen fine-grained attribute representations without CIR-specific labels.", "result": "On the DFMM-Compose benchmark, Pix2Key improves Recall@10 by up to 3.2 points over baselines. Adding V-Dict-AE provides a further 2.3-point gain, while also improving intent consistency and maintaining high retrieval diversity.", "conclusion": "Visual-dictionary representations with self-supervised pretraining enable fine-grained, intent-consistent CIR and boost retrieval accuracy without sacrificing diversity, addressing weaknesses of both triplet-fusion and zero-shot caption-based approaches."}}
{"id": "2602.22584", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22584", "abs": "https://arxiv.org/abs/2602.22584", "authors": ["Wenwei Li", "Ming Xu", "Tianle Xia", "Lingxiang Hu", "Yiding Sun", "Linfang Shang", "Liqun Liu", "Peng Shu", "Huan Yu", "Jie Jiang"], "title": "Towards Faithful Industrial RAG: A Reinforced Co-adaptation Framework for Advertising QA", "comment": null, "summary": "Industrial advertising question answering (QA) is a high-stakes task in which hallucinated content, particularly fabricated URLs, can lead to financial loss, compliance violations, and legal risk. Although Retrieval-Augmented Generation (RAG) is widely adopted, deploying it in production remains challenging because industrial knowledge is inherently relational, frequently updated, and insufficiently aligned with generation objectives. We propose a reinforced co-adaptation framework that jointly optimizes retrieval and generation through two components: (1) Graph-aware Retrieval (GraphRAG), which models entity-relation structure over a high-citation knowledge subgraph for multi-hop, domain-specific evidence selection; and (2) evidence-constrained reinforcement learning via Group Relative Policy Optimization (GRPO) with multi-dimensional rewards covering faithfulness, style compliance, safety, and URL validity. Experiments on an internal advertising QA dataset show consistent gains across expert-judged dimensions including accuracy, completeness, and safety, while reducing the hallucination rate by 72\\%. A two-week online A/B test demonstrates a 28.6\\% increase in like rate, a 46.2\\% decrease in dislike rate, and a 92.7\\% reduction in URL hallucination. The system has been running in production for over half a year and has served millions of QA interactions.", "AI": {"tldr": "Proposes a production-ready, reinforced co-adaptation RAG system for industrial advertising QA that unifies graph-aware retrieval with evidence-constrained RL (GRPO) to cut hallucinations\u2014especially fake URLs\u2014while improving accuracy, safety, and user engagement; shows large offline/online gains and sustained deployment at scale.", "motivation": "Industrial advertising QA is high-stakes: hallucinated facts and fabricated URLs can cause financial, compliance, and legal harm. Conventional RAG struggles because enterprise knowledge is relational, fast-changing, and not aligned with generation objectives. A system is needed that jointly optimizes retrieval and generation for factuality, safety, and domain style.", "method": "Two-part framework: (1) Graph-aware Retrieval (GraphRAG) that builds/uses a high-citation entity\u2013relation subgraph to enable multi-hop, domain-specific evidence selection; (2) evidence-constrained reinforcement learning using Group Relative Policy Optimization (GRPO) with multi-dimensional rewards (faithfulness, style compliance, safety, and URL validity) to align the generator. Retrieval and generation are co-adapted during training.", "result": "On an internal ads QA dataset, it improves expert-judged accuracy, completeness, and safety, cutting hallucinations by 72%. In a two-week online A/B test, it increases like rate by 28.6%, decreases dislike rate by 46.2%, and reduces URL hallucination by 92.7%. The system has run in production for 6+ months, serving millions of interactions.", "conclusion": "Jointly optimizing graph-based retrieval and RL-aligned generation effectively reduces hallucinations\u2014especially invalid URLs\u2014while improving response quality and user metrics in industrial QA. The approach is practical for production deployment and sustained operations in dynamic, relational knowledge settings."}}
{"id": "2602.22452", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22452", "abs": "https://arxiv.org/abs/2602.22452", "authors": ["Chayan Banerjee"], "title": "CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines", "comment": null, "summary": "A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.", "AI": {"tldr": "They train a large language model as an action-feasibility scorer using contrastive learning with hard negatives, yielding markedly better discrimination of physically valid vs. subtly invalid actions for embodied agents than standard supervised fine-tuning (SFT).", "motivation": "Embodied agents need to quickly filter candidate actions to only those that are physically executable. SFT scorers consider candidates independently and struggle with near-miss, minimal-edit negatives, leading to unsafe or suboptimal rankings.", "method": "Contrastive World Model (CWM): fine-tune an LLM with an InfoNCE objective and hard-mined negatives so valid actions are pulled apart from semantically similar but physically incompatible ones. Evaluate on ScienceWorld via (1) intrinsic affordance tests using 605 hard-negative pairs and (2) live filter characterization ranking gold-path actions among all valid environment actions, including out-of-distribution stress tests.", "result": "+6.76 pp Precision@1 on minimal-edit negatives over SFT; higher AUC-ROC (0.929 vs. 0.906). In live ranking under OOD stress, CWM shows a better safety margin (-2.39 vs. -3.96), meaning the gold action is ranked closer to the top than with SFT.", "conclusion": "Contrastive training with hard negatives yields representations that more faithfully encode physical feasibility than SFT, improving robustness and ranking quality for embodied-agent action filtering."}}
{"id": "2602.22545", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22545", "abs": "https://arxiv.org/abs/2602.22545", "authors": ["Agamdeep S. Chopra", "Caitlin Neher", "Tianyi Ren", "Juampablo E. Heras Rivera", "Mehmet Kurt"], "title": "DisQ-HNet: A Disentangled Quantized Half-UNet for Interpretable Multimodal Image Synthesis Applications to Tau-PET Synthesis from T1 and FLAIR MRI", "comment": "14 pages, 8 figures, 8 tables; includes PID guided vector quantized latent factorization and sobel edge conditioned Half-UNet decoder", "summary": "Tau positron emission tomography (tau-PET) provides an in vivo marker of Alzheimer's disease pathology, but cost and limited availability motivate MRI-based alternatives. We introduce DisQ-HNet (DQH), a framework that synthesizes tau-PET from paired T1-weighted and FLAIR MRI while exposing how each modality contributes to the prediction. The method combines (i) a Partial Information Decomposition (PID)-guided, vector-quantized encoder that partitions latent information into redundant, unique, and complementary components, and (ii) a Half-UNet decoder that preserves anatomical detail using pseudo-skip connections conditioned on structural edge cues rather than direct encoder feature reuse. Across multiple baselines (VAE, VQ-VAE, and UNet), DisQ-HNet maintains reconstruction fidelity and better preserves disease-relevant signal for downstream AD tasks, including Braak staging, tau localization, and classification. PID-based Shapley analysis provides modality-specific attribution of synthesized uptake patterns.", "AI": {"tldr": "DisQ-HNet learns to synthesize tau-PET from paired T1w and FLAIR MRI while explicitly disentangling redundant, unique, and complementary cross-modality information; it preserves anatomical detail and improves AD-relevant signals versus VAE/VQ-VAE/UNet baselines, with modality-specific attribution via PID-based Shapley analysis.", "motivation": "Tau-PET is a key in vivo marker of Alzheimer\u2019s pathology but is costly and scarce; clinicians/researchers need MRI-based, widely available surrogates that retain disease-relevant signal and offer interpretability about each MRI modality\u2019s contribution.", "method": "A PID-guided, vector\u2011quantized encoder partitions latent codes into redundant, unique, and complementary components across T1w and FLAIR. A Half\u2011UNet decoder reconstructs tau-PET using pseudo\u2011skip connections conditioned on structural edge cues (rather than reusing encoder features) to preserve anatomy and reduce feature leakage. PID-based Shapley analysis attributes synthesized uptake to each modality.", "result": "Compared to VAE, VQ\u2011VAE, and UNet baselines, DisQ\u2011HNet matches reconstruction fidelity while better preserving disease-relevant signal for downstream AD tasks: Braak staging, regional tau localization, and diagnostic classification. It provides modality\u2011specific attribution maps for the synthesized uptake patterns.", "conclusion": "DisQ\u2011HNet offers an interpretable, MRI\u2011only pathway to approximate tau-PET, achieving strong fidelity and enhanced downstream AD-task relevance while revealing how T1w and FLAIR each drive predicted tau uptake\u2014supporting its promise as a scalable surrogate for tau-PET."}}
{"id": "2602.22661", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22661", "abs": "https://arxiv.org/abs/2602.22661", "authors": ["Zhanhui Zhou", "Lingjie Chen", "Hanghang Tong", "Dawn Song"], "title": "dLLM: Simple Diffusion Language Modeling", "comment": "Code available at: https://github.com/ZHZisZZ/dllm", "summary": "Although diffusion language models (DLMs) are evolving quickly, many recent models converge on a set of shared components. These components, however, are distributed across ad-hoc research codebases or lack transparent implementations, making them difficult to reproduce or extend. As the field accelerates, there is a clear need for a unified framework that standardizes these common components while remaining flexible enough to support new methods and architectures.\n  To address this gap, we introduce dLLM, an open-source framework that unifies the core components of diffusion language modeling -- training, inference, and evaluation -- and makes them easy to customize for new designs. With dLLM, users can reproduce, finetune, deploy, and evaluate open-source large DLMs such as LLaDA and Dream through a standardized pipeline. The framework also provides minimal, reproducible recipes for building small DLMs from scratch with accessible compute, including converting any BERT-style encoder or autoregressive LM into a DLM. We also release the checkpoints of these small DLMs to make DLMs more accessible and accelerate future research.", "AI": {"tldr": "dLLM is an open-source, unified framework for diffusion language models that standardizes training, inference, and evaluation, offers recipes to build small DLMs or convert existing LMs, and ships checkpoints to boost reproducibility and access.", "motivation": "DLM research components are fragmented across ad-hoc codebases with limited transparency, hindering reproducibility, extension, and rapid iteration; a standardized yet flexible framework is needed as the field scales.", "method": "Introduce dLLM, a modular framework unifying core DLM workflows (training, inference, evaluation) with a standardized pipeline; supports reproducing/finetuning/deploying open DLMs (e.g., LLaDA, Dream); provides minimal, reproducible recipes to train small DLMs on modest compute and utilities to convert BERT-style encoders or autoregressive LMs into DLMs; release model checkpoints.", "result": "Demonstrates end-to-end support for major open DLMs via a common interface, enables training and conversion of small DLMs with accessible compute, and publishes checkpoints to facilitate adoption and benchmarking; the abstract does not report quantitative metrics.", "conclusion": "dLLM aims to standardize and democratize DLM research by offering a flexible, reproducible pipeline and ready-to-use artifacts, lowering barriers to experimentation and likely accelerating method development and comparative evaluation."}}
{"id": "2602.22465", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22465", "abs": "https://arxiv.org/abs/2602.22465", "authors": ["Joseph Tso", "Preston Schmittou", "Quan Huynh", "Jibran Hutchins"], "title": "ConstraintBench: Benchmarking LLM Constraint Reasoning on Direct Optimization", "comment": "Preprint. 10 pages, 1 figure, 6 tables. Benchmark and evaluation code will be publicly released", "summary": "Large language models are increasingly applied to operational decision-making where the underlying structure is constrained optimization. Existing benchmarks evaluate whether LLMs can formulate optimization problems as solver code, but leave open a complementary question. Can LLMs directly produce correct solutions to fully specified constrained optimization problems without access to a solver? We introduce ConstraintBench, a benchmark for evaluating LLMs on direct constrained optimization across 10 operations research domains, with all ground-truth solutions verified by the Gurobi solver. Each task presents a natural-language scenario with entities, constraints, and an optimization objective; the model must return a structured solution that a deterministic verifier checks against every constraint and the solver-proven optimum. We evaluate six frontier models on 200 tasks and find that feasibility, not optimality, is the primary bottleneck. The best model achieves only 65.0% constraint satisfaction, yet feasible solutions average 89 to 96% of the Gurobi-optimal objective. No model exceeds 30.5% on joint feasibility and optimality within 0.1% of the solver reference. Per-domain analysis shows large variation in difficulty, with average feasibility spanning from 83.3% in the production mix domain to 0.8% in the crew assignment domain. Further, systematic failure modes include duration constraint misunderstanding, entity hallucination, and a feasibility-optimality decoupling in facility location and vehicle routing where models achieve high feasibility but 0% optimality. ConstraintBench and all evaluation infrastructure will be publicly released.", "AI": {"tldr": "They propose ConstraintBench, a benchmark testing whether LLMs can directly output feasible and near\u2011optimal solutions to natural\u2011language constrained optimization tasks without using a solver; across 10 OR domains and 200 tasks, feasibility is the main failure mode (best 65% constraint satisfaction), though feasible solutions are close to optimal (89\u201396% of Gurobi optimum), and no model exceeds 30.5% for both feasibility and \u22640.1% optimality gap.", "motivation": "LLMs are being deployed for operational decision\u2011making where problems are naturally constrained optimizations. Prior work mostly checks if LLMs can translate problems into solver code, not if they can directly produce valid solutions. Assessing direct solution capability reveals models\u2019 internal constraint reasoning and reliability when solvers are unavailable or costly.", "method": "Create a benchmark of natural\u2011language scenarios (entities, constraints, objective) across 10 operations\u2011research domains. Require models to return structured solutions. Use a deterministic verifier to check all constraints and compare objective values against Gurobi\u2011verified optima. Evaluate six frontier LLMs on 200 tasks and analyze per\u2011domain performance and failure modes.", "result": "Primary bottleneck is feasibility: the strongest model achieves 65.0% constraint satisfaction. When solutions are feasible, they are near\u2011optimal (89\u201396% of the Gurobi optimum). No model surpasses 30.5% on joint feasibility and \u22640.1% optimality gap. Difficulty varies widely by domain (e.g., 83.3% feasibility in production mix vs 0.8% in crew assignment). Common errors include misunderstanding duration constraints, hallucinating entities, and a feasibility\u2011optimality decoupling in facility location/vehicle routing with high feasibility but 0% optimality.", "conclusion": "LLMs currently lack reliable constraint adherence in direct optimization, though they can choose near\u2011optimal values once feasible. ConstraintBench exposes this gap and should catalyze research into constraint\u2011aware prompting/decoding, structured outputs with schema enforcement, entity grounding, temporal reasoning, and solver\u2011in\u2011the\u2011loop or hybrid methods to couple feasibility with optimality."}}
{"id": "2602.22549", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22549", "abs": "https://arxiv.org/abs/2602.22549", "authors": ["Zhechao Wang", "Yiming Zeng", "Lufan Ma", "Zeqing Fu", "Chen Bai", "Ziyao Lin", "Cheng Lu"], "title": "DrivePTS: A Progressive Learning Framework with Textual and Structural Enhancement for Driving Scene Generation", "comment": null, "summary": "Synthesis of diverse driving scenes serves as a crucial data augmentation technique for validating the robustness and generalizability of autonomous driving systems. Current methods aggregate high-definition (HD) maps and 3D bounding boxes as geometric conditions in diffusion models for conditional scene generation. However, implicit inter-condition dependency causes generation failures when control conditions change independently. Additionally, these methods suffer from insufficient details in both semantic and structural aspects. Specifically, brief and view-invariant captions restrict semantic contexts, resulting in weak background modeling. Meanwhile, the standard denoising loss with uniform spatial weighting neglects foreground structural details, causing visual distortions and blurriness. To address these challenges, we propose DrivePTS, which incorporates three key innovations. Firstly, our framework adopts a progressive learning strategy to mitigate inter-dependency between geometric conditions, reinforced by an explicit mutual information constraint. Secondly, a Vision-Language Model is utilized to generate multi-view hierarchical descriptions across six semantic aspects, providing fine-grained textual guidance. Thirdly, a frequency-guided structure loss is introduced to strengthen the model's sensitivity to high-frequency elements, improving foreground structural fidelity. Extensive experiments demonstrate that our DrivePTS achieves state-of-the-art fidelity and controllability in generating diverse driving scenes. Notably, DrivePTS successfully generates rare scenes where prior methods fail, highlighting its strong generalization ability.", "AI": {"tldr": "DrivePTS is a conditional diffusion framework for autonomous-driving scene synthesis that decouples geometric controls, augments semantics with multi\u2011view VLM captions, and adds a frequency\u2011guided structure loss, achieving state\u2011of\u2011the\u2011art fidelity, controllability, and rare\u2011scene generation.", "motivation": "Diverse, controllable synthetic driving data is needed to test robustness/generalization. Existing methods couple HD\u2011map and 3D\u2011box controls and rely on brief, view\u2011invariant captions and uniformly weighted denoising losses, causing failures when controls vary independently and yielding weak backgrounds with blurry/structurally distorted foregrounds.", "method": "(1) Progressive learning to mitigate inter\u2011dependency between geometric conditions, reinforced by an explicit mutual information constraint. (2) Vision\u2011Language Model to produce multi\u2011view, hierarchical descriptions across six semantic aspects for fine\u2011grained text conditioning. (3) Frequency\u2011guided structure loss that emphasizes high\u2011frequency details to preserve foreground structural fidelity.", "result": "Experiments show state\u2011of\u2011the\u2011art fidelity and controllability in generated driving scenes; the method also produces rare scenarios that prior approaches fail to generate, evidencing stronger generalization.", "conclusion": "By decoupling control signals, enriching semantic guidance, and emphasizing high\u2011frequency structure, DrivePTS advances conditional driving\u2011scene generation, improving realism, controllability, and coverage of rare events for data augmentation and validation."}}
{"id": "2602.22675", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22675", "abs": "https://arxiv.org/abs/2602.22675", "authors": ["Qianben Chen", "Tianrui Qin", "King Zhu", "Qiexiang Wang", "Chengjun Yu", "Shu Xu", "Jiaqi Wu", "Jiayu Zhang", "Xinpeng Liu", "Xin Gui", "Jingyi Cao", "Piaohong Wang", "Dingfeng Shi", "He Zhu", "Tiannan Wang", "Yuqing Wang", "Maojia Song", "Tianyu Zheng", "Ge Zhang", "Jian Yang", "Jiaheng Liu", "Minghao Liu", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "Search More, Think Less: Rethinking Long-Horizon Agentic Search for Efficiency and Generalization", "comment": "12 pages, 5 figures", "summary": "Recent deep research agents primarily improve performance by scaling reasoning depth, but this leads to high inference cost and latency in search-intensive scenarios. Moreover, generalization across heterogeneous research settings remains challenging. In this work, we propose \\emph{Search More, Think Less} (SMTL), a framework for long-horizon agentic search that targets both efficiency and generalization. SMTL replaces sequential reasoning with parallel evidence acquisition, enabling efficient context management under constrained context budgets. To support generalization across task types, we further introduce a unified data synthesis pipeline that constructs search tasks spanning both deterministic question answering and open-ended research scenarios with task appropriate evaluation metrics. We train an end-to-end agent using supervised fine-tuning and reinforcement learning, achieving strong and often state of the art performance across benchmarks including BrowseComp (48.6\\%), GAIA (75.7\\%), Xbench (82.0\\%), and DeepResearch Bench (45.9\\%). Compared to Mirothinker-v1.0, SMTL with maximum 100 interaction steps reduces the average number of reasoning steps on BrowseComp by 70.7\\%, while improving accuracy.", "AI": {"tldr": "SMTL is a research agent that replaces deep, sequential reasoning with parallel evidence acquisition plus a unified training pipeline (SFT+RL), yielding higher accuracy with far fewer reasoning steps and strong, often SOTA results across diverse web-research benchmarks.", "motivation": "Deep research agents tend to improve by scaling reasoning depth, which inflates inference cost and latency, and they struggle to generalize across heterogeneous research tasks. The authors aim to build an agent that is both efficient (lower step/latency) and generalizable across deterministic QA and open-ended research settings.", "method": "Introduce Search More, Think Less (SMTL): (1) swap sequential chain-of-thought with parallel evidence acquisition to better use constrained context budgets; (2) design a unified data synthesis pipeline that generates tasks spanning deterministic QA and open-ended research with task-appropriate evaluation; (3) train an end-to-end agent via supervised fine-tuning followed by reinforcement learning.", "result": "SMTL attains strong and often SOTA performance: BrowseComp 48.6%, GAIA 75.7%, Xbench 82.0%, DeepResearch Bench 45.9%. With a cap of 100 interaction steps, SMTL reduces average reasoning steps on BrowseComp by 70.7% versus Mirothinker-v1.0 while also improving accuracy.", "conclusion": "Parallel, evidence-first search coupled with a unified training corpus enables efficient long-horizon research and better cross-task generalization; SMTL delivers notable accuracy gains with substantially fewer reasoning steps, indicating a practical path to lower-latency, cost-effective research agents."}}
{"id": "2602.22480", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22480", "abs": "https://arxiv.org/abs/2602.22480", "authors": ["Varun Ursekar", "Apaar Shanker", "Veronica Chatrath", "Yuan", "Xue", "Sam Denton"], "title": "VeRO: An Evaluation Harness for Agents to Optimize Agents", "comment": null, "summary": "An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.", "AI": {"tldr": "VERO is a framework and benchmark for systematically evaluating and improving coding agents via edit\u2013execute\u2013evaluate cycles, offering versioned snapshots, structured traces, budgeted runs, and reference tasks; it enables reproducible comparisons of optimizer configurations and identifies modifications that reliably boost agent performance.", "motivation": "Agent optimization (iteratively editing agents) is becoming central for coding agents, yet differs from traditional software engineering due to stochastic LLM components and the need to capture both intermediate reasoning and execution outcomes. The field lacks a standardized, reproducible way to evaluate and compare such optimizers.", "method": "Introduce VERO, comprising (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluations, and structured execution traces; and (2) a benchmark suite with target agents, tasks, and reference evaluation procedures. Use VERO to run empirical comparisons of different optimizer configurations across tasks to analyze which modifications improve performance.", "result": "Using VERO, the authors empirically compare optimizer configurations and identify changes that more reliably improve agent performance across tasks, demonstrating the framework\u2019s utility for structured, reproducible evaluation. Specific quantitative gains are not reported in the abstract.", "conclusion": "VERO provides infrastructure and benchmarks to make agent optimization a reproducible, measurable process and is released to catalyze research on improving coding agents as a core capability."}}
{"id": "2602.22565", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.22565", "abs": "https://arxiv.org/abs/2602.22565", "authors": ["Kang Han", "Wei Xiang", "Lu Yu", "Mathew Wyatt", "Gaowen Liu", "Ramana Rao Kompella"], "title": "SwiftNDC: Fast Neural Depth Correction for High-Fidelity 3D Reconstruction", "comment": null, "summary": "Depth-guided 3D reconstruction has gained popularity as a fast alternative to optimization-heavy approaches, yet existing methods still suffer from scale drift, multi-view inconsistencies, and the need for substantial refinement to achieve high-fidelity geometry. Here, we propose SwiftNDC, a fast and general framework built around a Neural Depth Correction field that produces cross-view consistent depth maps. From these refined depths, we generate a dense point cloud through back-projection and robust reprojection-error filtering, obtaining a clean and uniformly distributed geometric initialization for downstream reconstruction. This reliable dense geometry substantially accelerates 3D Gaussian Splatting (3DGS) for mesh reconstruction, enabling high-quality surfaces with significantly fewer optimization iterations. For novel-view synthesis, SwiftNDC can also improve 3DGS rendering quality, highlighting the benefits of strong geometric initialization. We conduct a comprehensive study across five datasets, including two for mesh reconstruction, as well as three for novel-view synthesis. SwiftNDC consistently reduces running time for accurate mesh reconstruction and boosts rendering fidelity for view synthesis, demonstrating the effectiveness of combining neural depth refinement with robust geometric initialization for high-fidelity and efficient 3D reconstruction.", "AI": {"tldr": "SwiftNDC introduces a Neural Depth Correction field to make multi-view depths cross-view consistent, then converts them into a clean, uniformly distributed dense point cloud that initializes 3D Gaussian Splatting, yielding faster, higher-fidelity 3D reconstruction and improved novel-view synthesis.", "motivation": "Depth-guided 3D reconstruction is fast but plagued by scale drift, cross-view inconsistencies, and heavy post-refinement; 3DGS also needs many iterations without strong geometric priors. A method that stabilizes depth across views and supplies reliable geometry could boost quality while cutting compute time.", "method": "Learn a Neural Depth Correction (NDC) field to refine per-view depths for cross-view consistency; back-project refined depths to a dense point cloud; apply robust reprojection-error filtering to remove outliers and ensure uniform coverage; use this dense, clean geometry to initialize 3D Gaussian Splatting for mesh reconstruction and novel-view synthesis.", "result": "Produces a reliable, uniformly distributed dense point cloud that substantially accelerates 3DGS optimization, achieving high-quality meshes with fewer iterations; also increases rendering fidelity for novel-view synthesis. Demonstrated consistent speedups and quality gains across five datasets (two for mesh, three for view synthesis).", "conclusion": "Coupling neural depth refinement with robust geometric initialization is an effective, general, and fast strategy for high-fidelity 3D reconstruction, simultaneously reducing runtime and improving both mesh quality and rendering performance."}}
{"id": "2602.22696", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22696", "abs": "https://arxiv.org/abs/2602.22696", "authors": ["Shinnosuke Nozue", "Yuto Nakano", "Yotaro Watanabe", "Meguru Takasaki", "Shoji Moriya", "Reina Akama", "Jun Suzuki"], "title": "Enhancing Persuasive Dialogue Agents by Synthesizing Cross-Disciplinary Communication Strategies", "comment": "Accepted to the EMNLP 2025 Industry Track; 26 pages", "summary": "Current approaches to developing persuasive dialogue agents often rely on a limited set of predefined persuasive strategies that fail to capture the complexity of real-world interactions. We applied a cross-disciplinary approach to develop a framework for designing persuasive dialogue agents that draws on proven strategies from social psychology, behavioral economics, and communication theory. We validated our proposed framework through experiments on two distinct datasets: the Persuasion for Good dataset, which represents a specific in-domain scenario, and the DailyPersuasion dataset, which encompasses a wide range of scenarios. The proposed framework achieved strong results for both datasets and demonstrated notable improvement in the persuasion success rate as well as promising generalizability. Notably, the proposed framework also excelled at persuading individuals with initially low intent, which addresses a critical challenge for persuasive dialogue agents.", "AI": {"tldr": "They introduce a cross-disciplinary framework for persuasive dialogue agents, drawing on social psychology, behavioral economics, and communication theory, and show it improves persuasion success and generalizes across two datasets, including users with low initial intent.", "motivation": "Prevalent systems rely on a small, fixed set of persuasive tactics that miss the complexity of real interactions and struggle especially with low-intent users. There is a need for a richer, more generalizable approach grounded in well-established social science.", "method": "Design a framework that integrates proven persuasive strategies from social psychology, behavioral economics, and communication theory. Validate it via experiments on two datasets: Persuasion for Good (in-domain) and DailyPersuasion (diverse scenarios), comparing persuasion success rates and generalization to baselines.", "result": "The framework delivers strong performance on both datasets with notable gains in persuasion success rate, shows promising cross-domain generalizability, and is particularly effective for persuading individuals with low initial intent.", "conclusion": "A cross-disciplinary, theory-grounded framework can improve both effectiveness and generalizability of persuasive dialogue agents and addresses the key challenge of persuading low-intent users."}}
{"id": "2602.22500", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22500", "abs": "https://arxiv.org/abs/2602.22500", "authors": ["Anastasija Mensikova", "Donna M. Rizzo", "Kathryn Hinkelman"], "title": "Mapping the Landscape of Artificial Intelligence in Life Cycle Assessment Using Large Language Models", "comment": null, "summary": "Integration of artificial intelligence (AI) into life cycle assessment (LCA) has accelerated in recent years, with numerous studies successfully adapting machine learning algorithms to support various stages of LCA. Despite this rapid development, comprehensive and broad synthesis of AI-LCA research remains limited. To address this gap, this study presents a detailed review of published work at the intersection of AI and LCA, leveraging large language models (LLMs) to identify current trends, emerging themes, and future directions. Our analyses reveal that as LCA research continues to expand, the adoption of AI technologies has grown dramatically, with a noticeable shift toward LLM-driven approaches, continued increases in ML applications, and statistically significant correlations between AI approaches and corresponding LCA stages. By integrating LLM-based text-mining methods with traditional literature review techniques, this study introduces a dynamic and effective framework capable of capturing both high-level research trends and nuanced conceptual patterns (themes) across the field. Collectively, these findings demonstrate the potential of LLM-assisted methodologies to support large-scale, reproducible reviews across broad research domains, while also evaluating pathways for computationally-efficient LCA in the context of rapidly developing AI technologies. In doing so, this work helps LCA practitioners incorporate state-of-the-art tools and timely insights into environmental assessments that can enhance the rigor and quality of sustainability-driven decisions and decision-making processes.", "AI": {"tldr": "LLM-assisted literature mining plus traditional review maps how AI is being used across LCA, showing rapid growth, a shift toward LLM-driven methods, significant links between AI techniques and LCA stages, and proposing a scalable, reproducible framework to synthesize research and enable more efficient LCA.", "motivation": "Many studies apply AI to parts of LCA, but the field lacks a comprehensive, up-to-date synthesis that can keep pace with rapid growth. Manual reviews miss scale and nuance; practitioners need guidance on where and how AI\u2014especially LLMs\u2014fits across LCA stages and how it can improve computational efficiency.", "method": "Hybrid review: integrate LLM-based text mining (for screening, trend/theme extraction, mapping) with conventional literature review and statistical tests to identify associations between AI approaches (ML, LLMs) and specific LCA stages, and to surface high-level trends and nuanced conceptual themes.", "result": "AI use in LCA is accelerating; there is a notable shift toward LLM-driven approaches alongside continued growth in ML. Statistically significant correlations exist between certain AI methods and specific LCA stages. The hybrid LLM+traditional workflow effectively captures both macro trends and fine-grained themes in the literature.", "conclusion": "LLM-assisted methodologies can deliver large-scale, reproducible literature syntheses and guide pathways for computationally efficient LCA. The proposed framework helps practitioners adopt state-of-the-art AI tools and integrate timely insights to improve the rigor and quality of sustainability-focused decisions."}}
{"id": "2602.22568", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22568", "abs": "https://arxiv.org/abs/2602.22568", "authors": ["Peihan Wu", "Guanjie Cheng", "Yufei Tong", "Meng Xi", "Shuiguang Deng"], "title": "Quality-Aware Robust Multi-View Clustering for Heterogeneous Observation Noise", "comment": null, "summary": "Deep multi-view clustering has achieved remarkable progress but remains vulnerable to complex noise in real-world applications. Existing noisy robust methods predominantly rely on a simplified binary assumption, treating data as either perfectly clean or completely corrupted. This overlooks the prevalent existence of heterogeneous observation noise, where contamination intensity varies continuously across data. To bridge this gap, we propose a novel framework termed Quality-Aware Robust Multi-View Clustering (QARMVC). Specifically, QARMVC employs an information bottleneck mechanism to extract intrinsic semantics for view reconstruction. Leveraging the insight that noise disrupts semantic integrity and impedes reconstruction, we utilize the resulting reconstruction discrepancy to precisely quantify fine-grained contamination intensity and derive instance-level quality scores. These scores are integrated into a hierarchical learning strategy: at the feature level, a quality-weighted contrastive objective is designed to adaptively suppress the propagation of noise; at the fusion level, a high-quality global consensus is constructed via quality-weighted aggregation, which is subsequently utilized to align and rectify local views via mutual information maximization. Extensive experiments on five benchmark datasets demonstrate that QARMVC consistently outperforms state-of-the-art baselines, particularly in scenarios with heterogeneous noise intensities.", "AI": {"tldr": "QARMVC introduces instance-level, quality-aware robustness for deep multi-view clustering by estimating noise intensity from reconstruction discrepancy and using it to weight contrastive learning and multi-view fusion, yielding state-of-the-art performance under heterogeneous noise.", "motivation": "Most robust multi-view clustering methods assume a binary clean-vs-corrupted setting, ignoring that real data exhibit heterogeneous, continuously varying noise levels across instances and views. This mismatch limits robustness and clustering quality in practical scenarios.", "method": "QARMVC uses an information bottleneck to capture intrinsic semantics for view reconstruction. The reconstruction discrepancy is treated as a proxy for contamination intensity to compute instance-level quality scores. A hierarchical strategy then: (1) feature level\u2014applies a quality-weighted contrastive objective to downweight noisy instances and prevent noise propagation; (2) fusion level\u2014builds a high-quality global consensus via quality-weighted aggregation and uses it to align/rectify local views by maximizing mutual information.", "result": "Across five benchmark datasets, QARMVC consistently surpasses state-of-the-art baselines, with particularly strong gains when noise intensities are heterogeneous. (No specific numbers reported in the abstract.)", "conclusion": "Modeling and leveraging fine-grained instance-level quality, combined with semantic bottlenecking and quality-weighted fusion, improves robustness and clustering performance in multi-view settings with complex, non-binary noise."}}
{"id": "2602.22697", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22697", "abs": "https://arxiv.org/abs/2602.22697", "authors": ["Ning Gao", "Wei Zhang", "Yuqin Dai", "Ling Shi", "Ziyin Wang", "Yujie Wang", "Wei He", "Jinpeng Wang", "Chaozheng Wang"], "title": "Reinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue", "comment": "35 pages, 8 tables, 3 figures", "summary": "The rapid evolution of Large Language Models (LLMs) has accelerated the transition from conversational chatbots to general agents. However, effectively balancing empathetic communication with budget-aware decision-making remains an open challenge. Since existing methods fail to capture these complex strategic trade-offs, we propose InteractCS-RL, a framework that reframes task-oriented dialogue as a multi-granularity reinforcement learning process. Specifically, we first establish a User-centric Interaction Framework to provide a high-fidelity training gym, enabling agents to dynamically explore diverse strategies with persona-driven users. Then, we introduce Cost-aware Multi-turn Policy Optimization (CMPO) with a hybrid advantage estimation strategy. By integrating generative process credits and employing a PID-Lagrangian cost controller, CMPO effectively guides the policy to explore Pareto boundary between user reward and global cost constraints. Extensive experiments on customized real business scenarios demonstrate that InteractCS-RL significantly outperform other baselines across three evaluation dimensions. Further evaluation on tool-agent-user interaction benchmarks verify InteractCS-RL robustness across diverse domains.", "AI": {"tldr": "InteractCS-RL is a reinforcement-learning framework for LLM agents that balances empathetic dialogue with budget/cost constraints using a user-centric simulator and a cost-aware multi-turn policy optimizer, achieving superior performance and robustness across domains.", "motivation": "Task-oriented LLM agents must manage a trade-off between empathetic, effective communication and budget-aware decisions (e.g., tool calls, latency, token costs). Existing approaches don\u2019t model these multi-objective, multi-turn trade-offs well, leading to suboptimal strategies.", "method": "Reframe task-oriented dialogue as multi-granularity RL. Build a User-centric Interaction Framework (persona-driven, high-fidelity simulation) to explore strategies. Propose Cost-aware Multi-turn Policy Optimization (CMPO) with hybrid advantage estimation, integrating generative process credit assignment and a PID-Lagrangian cost controller to navigate the Pareto frontier between user reward and global cost constraints.", "result": "On customized real business scenarios, InteractCS-RL significantly outperforms baselines across three evaluation dimensions. Additional tests on tool-agent-user interaction benchmarks show robustness across diverse domains.", "conclusion": "A principled RL approach can jointly optimize empathy/user reward and cost under constraints for LLM agents. InteractCS-RL advances state-of-the-art task-oriented agent training and generalizes across domains, suggesting practical viability for cost-constrained, user-centric deployments."}}
{"id": "2602.22508", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22508", "abs": "https://arxiv.org/abs/2602.22508", "authors": ["Ik-hwan Kim", "Hyeongrok Han", "Mingi Jung", "Sangwon Yu", "Jinseok Hong", "Sang Hun Kim", "Yoonyoung Choi", "Sungroh Yoon"], "title": "Mirroring the Mind: Distilling Human-Like Metacognitive Strategies into Large Language Models", "comment": "31 pages", "summary": "Large Reasoning Models (LRMs) often exhibit structural fragility in complex reasoning tasks, failing to produce correct answers even after successfully deriving valid intermediate steps. Through systematic analysis, we observe that these failures frequently stem not from a lack of reasoning capacity, but from a deficiency in self-regulatory control, where valid logic is destabilized by uncontrolled exploration or the failure to recognize logical sufficiency. Motivated by this observation, we propose Metacognitive Behavioral Tuning (MBT), a post-training framework that explicitly injects metacognitive behaviors into the model's thought process. MBT implements this via two complementary formulations: (1) MBT-S, which synthesizes rigorous reasoning traces from scratch, and (2) MBT-R, which rewrites the student's initial traces to stabilize intrinsic exploration patterns. Experiments across multi-hop QA benchmarks demonstrate that MBT consistently outperforms baselines, achieving notable gains on challenging benchmarks. By effectively eliminating reasoning collapse, MBT achieves higher accuracy with significantly reduced token consumption, demonstrating that internalizing metacognitive strategies leads to more stable and robust reasoning.", "AI": {"tldr": "MBT is a post-training approach that injects metacognitive control into large reasoning models by synthesizing or rewriting reasoning traces, stabilizing exploration and boosting multi-hop QA accuracy while using fewer tokens.", "motivation": "LRMs often fail at final answers despite having valid intermediate steps because they lack self-regulatory control\u2014overexploring or not recognizing when a logic chain is sufficient\u2014leading to reasoning collapse.", "method": "Metacognitive Behavioral Tuning (MBT) adds explicit self-monitoring behaviors to models via two variants: (1) MBT-S, which generates rigorous reasoning traces from scratch; and (2) MBT-R, which rewrites a model\u2019s initial reasoning to stabilize its exploration. This is applied as a post-training framework to guide reasoning processes.", "result": "Across multi-hop QA benchmarks, MBT consistently outperforms baselines, with notable improvements on harder tasks. It reduces reasoning collapse and achieves higher accuracy with significantly fewer tokens consumed.", "conclusion": "Embedding metacognitive strategies into reasoning traces yields more stable, robust, and efficient reasoning, improving accuracy while lowering token usage."}}
{"id": "2602.22570", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22570", "abs": "https://arxiv.org/abs/2602.22570", "authors": ["Dian Xie", "Shitong Shao", "Lichen Bai", "Zikai Zhou", "Bojun Cheng", "Shuo Yang", "Jun Wu", "Zeke Xie"], "title": "Guidance Matters: Rethinking the Evaluation Pitfall for Text-to-Image Generation", "comment": null, "summary": "Classifier-free guidance (CFG) has helped diffusion models achieve great conditional generation in various fields. Recently, more diffusion guidance methods have emerged with improved generation quality and human preference. However, can these emerging diffusion guidance methods really achieve solid and significant improvements? In this paper, we rethink recent progress on diffusion guidance. Our work mainly consists of four contributions. First, we reveal a critical evaluation pitfall that common human preference models exhibit a strong bias towards large guidance scales. Simply increasing the CFG scale can easily improve quantitative evaluation scores due to strong semantic alignment, even if image quality is severely damaged (e.g., oversaturation and artifacts). Second, we introduce a novel guidance-aware evaluation (GA-Eval) framework that employs effective guidance scale calibration to enable fair comparison between current guidance methods and CFG by identifying the effects orthogonal and parallel to CFG effects. Third, motivated by the evaluation pitfall, we design Transcendent Diffusion Guidance (TDG) method that can significantly improve human preference scores in the conventional evaluation framework but actually does not work in practice. Fourth, in extensive experiments, we empirically evaluate recent eight diffusion guidance methods within the conventional evaluation framework and the proposed GA-Eval framework. Notably, simply increasing the CFG scales can compete with most studied diffusion guidance methods, while all methods suffer severely from winning rate degradation over standard CFG. Our work would strongly motivate the community to rethink the evaluation paradigm and future directions of this field.", "AI": {"tldr": "They show that many reported gains from new diffusion guidance methods are largely artifacts of biased evaluation: simply cranking up CFG scale boosts preference metrics even while degrading images. They introduce a calibrated evaluation (GA-Eval) and demonstrate that most guidance methods don\u2019t truly beat standard CFG when fairly compared.", "motivation": "Recent diffusion guidance papers claim better conditional generation and human preference, but it\u2019s unclear if those gains are genuine or inflated by evaluation bias\u2014especially the dependence of human-preference models on guidance scale.", "method": "(1) Diagnose an evaluation pitfall: preference models strongly favor large CFG scales, conflating semantic alignment with image quality. (2) Propose GA-Eval, a guidance-aware evaluation that calibrates/normalizes guidance scale and decomposes effects into components parallel vs. orthogonal to CFG. (3) Construct a counterexample method (TDG) that exploits the pitfall to inflate preference scores without real improvement. (4) Empirically assess eight guidance methods under conventional evaluation and GA-Eval.", "result": "- Increasing CFG scale alone raises human-preference scores despite oversaturation and artifacts.\n- Under GA-Eval, most studied guidance methods lose their apparent advantage; their winning rates degrade relative to standard CFG.\n- TDG attains high scores in the conventional setup but fails in practice, confirming the pitfall.", "conclusion": "Claims of superior diffusion guidance often stem from evaluation bias. Fair comparison requires guidance-scale calibration and separating CFG-parallel from orthogonal effects. Community should rethink benchmarks; merely boosting guidance scale is not real progress."}}
{"id": "2602.22698", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22698", "abs": "https://arxiv.org/abs/2602.22698", "authors": ["Siyue Su", "Jian Yang", "Bo Li", "Guanglin Niu"], "title": "Tokenization, Fusion and Decoupling: Bridging the Granularity Mismatch Between Large Language Models and Knowledge Graphs", "comment": null, "summary": "Leveraging Large Language Models (LLMs) for Knowledge Graph Completion (KGC) is promising but hindered by a fundamental granularity mismatch. LLMs operate on fragmented token sequences, whereas entities are the fundamental units in knowledge graphs (KGs) scenarios. Existing approaches typically constrain predictions to limited candidate sets or align entities with the LLM's vocabulary by pooling multiple tokens or decomposing entities into fixed-length token sequences, which fail to capture both the semantic meaning of the text and the structural integrity of the graph. To address this, we propose KGT, a novel framework that uses dedicated entity tokens to enable efficient, full-space prediction. Specifically, we first introduce specialized tokenization to construct feature representations at the level of dedicated entity tokens. We then fuse pre-trained structural and textual features into these unified embeddings via a relation-guided gating mechanism, avoiding training from scratch. Finally, we implement decoupled prediction by leveraging independent heads to separate and combine semantic and structural reasoning. Experimental results show that KGT consistently outperforms state-of-the-art methods across multiple benchmarks.", "AI": {"tldr": "KGT introduces dedicated entity tokens for LLM-based knowledge graph completion, fusing pre-trained structural and textual signals with a relation-guided gating mechanism and using decoupled semantic/structural heads to enable efficient full-space entity prediction, achieving state-of-the-art results on multiple benchmarks.", "motivation": "LLMs reason over token sequences, while KGs reason over discrete entities; aligning these granularities is hard. Prior methods either restrict prediction to small candidate pools or coerce entities into the LLM\u2019s subword vocabulary (pooling/decomposition), which harms both textual semantics and KG structural integrity.", "method": "1) Specialized tokenization creates dedicated tokens for each entity to operate at entity granularity. 2) Pre-trained structural (e.g., KGE) and textual features are fused into unified entity embeddings via a relation-guided gating module. 3) Decoupled prediction uses independent heads to separately model semantic and structural reasoning, then combines them for full-vocabulary entity prediction.", "result": "Across several standard KGC benchmarks, KGT consistently outperforms strong baselines and prior SOTA approaches (exact metrics not provided in the abstract).", "conclusion": "Representing entities with dedicated tokens and fusing pre-trained structural/textual knowledge, while decoupling semantic and structural inference paths, resolves the token\u2013entity mismatch in LLM-based KGC and yields superior performance without training from scratch."}}
{"id": "2602.22519", "categories": ["cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.22519", "abs": "https://arxiv.org/abs/2602.22519", "authors": ["Wael Hafez", "Chenan Wei", "Rodrigo Felipe", "Amir Nazeri", "Cameron Reid"], "title": "A Mathematical Theory of Agency and Intelligence", "comment": "20 pages, 4 figuers", "summary": "To operate reliably under changing conditions, complex systems require feedback on how effectively they use resources, not just whether objectives are met. Current AI systems process vast information to produce sophisticated predictions, yet predictions can appear successful while the underlying interaction with the environment degrades. What is missing is a principled measure of how much of the total information a system deploys is actually shared between its observations, actions, and outcomes. We prove this shared fraction, which we term bipredictability, P, is intrinsic to any interaction, derivable from first principles, and strictly bounded: P can reach unity in quantum systems, P equal to, or smaller than 0.5 in classical systems, and lower once agency (action selection) is introduced. We confirm these bounds in a physical system (double pendulum), reinforcement learning agents, and multi turn LLM conversations. These results distinguish agency from intelligence: agency is the capacity to act on predictions, whereas intelligence additionally requires learning from interaction, self-monitoring of its learning effectiveness, and adapting the scope of observations, actions, and outcomes to restore effective learning. By this definition, current AI systems achieve agency but not intelligence. Inspired by thalamocortical regulation in biological systems, we demonstrate a feedback architecture that monitors P in real time, establishing a prerequisite for adaptive, resilient AI.", "AI": {"tldr": "Introduces bipredictability (P) as a principled, bounded measure of how much an agent\u2019s information is truly shared across observations, actions, and outcomes; proves limits (quantum up to 1, classical \u22640.5, lower with agency), validates on physical, RL, and LLM settings, and proposes a thalamocortical\u2011inspired feedback loop to monitor P\u2014arguing current AI has agency but not intelligence.", "motivation": "Performance metrics can look good while interaction quality and learning effectiveness silently degrade. Existing AI lacks a principled, real\u2011time gauge of how effectively information is used across sensing, acting, and outcomes. A measure of the shared, actionable information is needed to regulate learning and robustness under changing conditions.", "method": "Derive bipredictability P from first principles as the fraction of total information jointly shared among observations, actions, and outcomes. Prove strict bounds (P\u21921 in quantum systems; P\u22640.5 in classical; lower when agency/actuation is introduced). Empirically verify these bounds on a double pendulum, reinforcement\u2011learning agents, and multi\u2011turn LLM dialogues. Design a thalamocortical\u2011inspired control architecture that monitors P in real time to adapt the scope of observations, actions, and outcomes.", "result": "Mathematically established bounds on P and showed they hold in diverse empirical settings. Found that introducing agency reduces P relative to passive prediction; observed predicted ceilings in classical setups and lower P with active control. Demonstrated feasibility of real\u2011time P monitoring and used it to signal and correct degradation in interaction effectiveness.", "conclusion": "P provides a domain\u2011general, intrinsic measure of effective information use in interaction. Intelligence, beyond agency, requires continual learning plus self\u2011monitoring of learning efficacy and adaptive adjustment of sensing/acting/outcome spaces to sustain high P. Current AI exhibits agency but lacks this self\u2011regulatory loop; monitoring P is proposed as a prerequisite for adaptive, resilient AI."}}
{"id": "2602.22571", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22571", "abs": "https://arxiv.org/abs/2602.22571", "authors": ["Tianyu Chen", "Wei Xiang", "Kang Han", "Yu Lu", "Di Wu", "Gaowen Liu", "Ramana Rao Kompella"], "title": "GIFSplat: Generative Prior-Guided Iterative Feed-Forward 3D Gaussian Splatting from Sparse Views", "comment": null, "summary": "Feed-forward 3D reconstruction offers substantial runtime advantages over per-scene optimization, which remains slow at inference and often fragile under sparse views. However, existing feed-forward methods still have potential for further performance gains, especially for out-of-domain data, and struggle to retain second-level inference time once a generative prior is introduced. These limitations stem from the one-shot prediction paradigm in existing feed-forward pipeline: models are strictly bounded by capacity, lack inference-time refinement, and are ill-suited for continuously injecting generative priors. We introduce GIFSplat, a purely feed-forward iterative refinement framework for 3D Gaussian Splatting from sparse unposed views. A small number of forward-only residual updates progressively refine current 3D scene using rendering evidence, achieve favorable balance between efficiency and quality. Furthermore, we distill a frozen diffusion prior into Gaussian-level cues from enhanced novel renderings without gradient backpropagation or ever-increasing view-set expansion, thereby enabling per-scene adaptation with generative prior while preserving feed-forward efficiency. Across DL3DV, RealEstate10K, and DTU, GIFSplat consistently outperforms state-of-the-art feed-forward baselines, improving PSNR by up to +2.1 dB, and it maintains second-scale inference time without requiring camera poses or any test-time gradient optimization.", "AI": {"tldr": "GIFSplat is a fast, purely feed-forward 3D Gaussian Splatting method that iteratively refines reconstructions from sparse, unposed views using forward-only residual updates and distilled cues from a frozen diffusion prior, achieving second-scale inference and up to +2.1 dB PSNR gains over prior feed-forward baselines on DL3DV, RealEstate10K, and DTU.", "motivation": "Per-scene optimization is accurate but slow and brittle with sparse views, while existing feed-forward methods are limited by one-shot prediction: they lack inference-time refinement, struggle to integrate generative priors without losing speed, and generalize poorly out of domain. There is a need for a method that maintains feed-forward efficiency while enabling iterative refinement and effective use of generative priors.", "method": "Introduce a purely feed-forward iterative refinement framework for 3D Gaussian Splatting from sparse, unposed views. The system starts with an initial scene estimate and applies a small number of forward-only residual updates guided by rendering evidence to progressively improve the 3D Gaussians. It distills a frozen diffusion prior into Gaussian-level cues via enhanced novel renderings, avoiding gradient backpropagation and growing the view set, enabling per-scene adaptation while preserving feed-forward speed.", "result": "On DL3DV, RealEstate10K, and DTU, GIFSplat outperforms state-of-the-art feed-forward baselines, improving PSNR by up to +2.1 dB, while maintaining second-scale inference time. It requires neither camera poses nor test-time gradient optimization.", "conclusion": "By replacing one-shot prediction with forward-only iterative refinement and by distilling a diffusion prior into direct Gaussian updates, GIFSplat bridges efficiency and quality: it retains feed-forward speed yet benefits from per-scene adaptation and generative priors, offering robust 3D reconstruction from sparse, unposed views without backprop at inference."}}
{"id": "2602.22723", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22723", "abs": "https://arxiv.org/abs/2602.22723", "authors": ["Frances Yung", "Daniil Ignatev", "Merel Scholman", "Vera Demberg", "Massimo Poesio"], "title": "Human Label Variation in Implicit Discourse Relation Recognition", "comment": null, "summary": "There is growing recognition that many NLP tasks lack a single ground truth, as human judgments reflect diverse perspectives. To capture this variation, models have been developed to predict full annotation distributions rather than majority labels, while perspectivist models aim to reproduce the interpretations of individual annotators. In this work, we compare these approaches on Implicit Discourse Relation Recognition (IDRR), a highly ambiguous task where disagreement often arises from cognitive complexity rather than ideological bias. Our experiments show that existing annotator-specific models perform poorly in IDRR unless ambiguity is reduced, whereas models trained on label distributions yield more stable predictions. Further analysis indicates that frequent cognitively demanding cases drive inconsistency in human interpretation, posing challenges for perspectivist modeling in IDRR.", "AI": {"tldr": "For implicit discourse relation recognition (IDRR), modeling the full label distribution is more robust than annotator-specific (perspectivist) modeling because ambiguity driven by cognitive complexity destabilizes individual-annotator predictions.", "motivation": "Many NLP tasks lack a single ground truth, and IDRR is notably ambiguous. The authors seek to determine whether predicting full annotation distributions or modeling specific annotators\u2019 perspectives better captures this variability\u2014especially when disagreement stems from cognitive complexity rather than ideological bias.", "method": "They empirically compare distribution-predicting models against annotator-specific perspectivist models on IDRR, evaluate how performance changes with levels of ambiguity, and analyze which instances drive inconsistency in human interpretation.", "result": "Annotator-specific models perform poorly on IDRR unless ambiguity is reduced, while models trained on label distributions produce more stable predictions. Frequent, cognitively demanding cases dominate sources of disagreement and undermine consistency needed for perspectivist modeling.", "conclusion": "In IDRR, distributional modeling is preferable and more stable than perspectivist modeling; the prevalence of cognitively complex cases limits the effectiveness of annotator-specific approaches unless ambiguity can be mitigated."}}
{"id": "2602.22523", "categories": ["cs.AI", "cs.CL", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.22523", "abs": "https://arxiv.org/abs/2602.22523", "authors": ["Ryan Liu", "Dilip Arumugam", "Cedegao E. Zhang", "Sean Escola", "Xaq Pitkow", "Thomas L. Griffiths"], "title": "Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents", "comment": null, "summary": "While contemporary large language models (LLMs) are increasingly capable in isolation, there are still many difficult problems that lie beyond the abilities of a single LLM. For such tasks, there is still uncertainty about how best to take many LLMs as parts and combine them into a greater whole. This position paper argues that potential blueprints for designing such modular language agents can be found in the existing literature on cognitive models and artificial intelligence (AI) algorithms. To make this point clear, we formalize the idea of an agent template that specifies roles for individual LLMs and how their functionalities should be composed. We then survey a variety of existing language agents in the literature and highlight their underlying templates derived directly from cognitive models or AI algorithms. By highlighting these designs, we aim to call attention to agent templates inspired by cognitive science and AI as a powerful tool for developing effective, interpretable language agents.", "AI": {"tldr": "Position paper proposing \u201cagent templates\u201d as design blueprints for modular, multi-LLM systems, drawing on cognitive models and classic AI algorithms; it surveys existing agents to show how such templates yield more effective and interpretable language agents.", "motivation": "Single LLMs struggle with complex, multi-step or specialized tasks; there is uncertainty about how to coordinate multiple LLMs into coherent systems that remain effective and interpretable.", "method": "(1) Formalize the concept of an agent template that specifies roles for individual LLMs and how they compose. (2) Survey existing language agents and map their structures to templates derived from cognitive science models and AI algorithms.", "result": "Provides a taxonomy/identification of underlying templates in current agents, tracing them to cognitive and AI precedents, and clarifies how role assignment and composition rules lead to modular, interpretable systems (no new empirical benchmark; primarily conceptual/organizational results).", "conclusion": "Agent templates inspired by cognitive science and AI offer a principled path to designing modular, interpretable, and effective multi-LLM agents; the community should adopt and extend these templates as foundational design patterns."}}
{"id": "2602.22594", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22594", "abs": "https://arxiv.org/abs/2602.22594", "authors": ["Qing Yu", "Akihisa Watanabe", "Kent Fujiwara"], "title": "Causal Motion Diffusion Models for Autoregressive Motion Generation", "comment": "Accepted to CVPR 2026, Project website: https://yu1ut.com/CMDM-HP/", "summary": "Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency.", "AI": {"tldr": "CMDM introduces a causal, autoregressive diffusion framework over a temporally causal latent space to generate high-quality human motion in real time, improving semantic fidelity, smoothness, and latency over prior methods.", "motivation": "Prior motion diffusion models either (a) denoise full sequences bidirectionally, breaking temporal causality and hindering real-time use, or (b) run autoregressively but suffer from instability and error accumulation. The field lacks a unified approach that is causal, stable, and efficient for streaming and long-horizon motion generation.", "method": "CMDM builds on a Motion-Language-Aligned Causal VAE (MAC-VAE) that encodes motion into temporally causal, semantically aligned latents. An autoregressive diffusion transformer is trained with causal diffusion forcing to perform ordered denoising across frames. For fast inference, a frame-wise sampling schedule with causal uncertainty predicts each new frame from partially denoised past frames. The framework supports text-to-motion, streaming, and long-horizon synthesis.", "result": "On HumanML3D and SnapMoGen, CMDM outperforms state-of-the-art diffusion and autoregressive baselines in semantic alignment and temporal smoothness, while substantially reducing inference latency and enabling interactive-rate generation.", "conclusion": "Causal autoregressive diffusion in a semantic, temporally causal latent space provides a stable, efficient path to real-time, high-quality motion generation across text-to-motion, streaming, and long-horizon settings, addressing causality, stability, and latency limitations of prior work."}}
{"id": "2602.22730", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22730", "abs": "https://arxiv.org/abs/2602.22730", "authors": ["Jakub \u0160m\u00edd", "Pavel P\u0159ib\u00e1\u0148", "Pavel Kr\u00e1l"], "title": "Extending Czech Aspect-Based Sentiment Analysis with Opinion Terms: Dataset and LLM Benchmarks", "comment": "Accepted for the 15th edition of the Language Resources and Evaluation Conference (LREC 2026)", "summary": "This paper introduces a novel Czech dataset in the restaurant domain for aspect-based sentiment analysis (ABSA), enriched with annotations of opinion terms. The dataset supports three distinct ABSA tasks involving opinion terms, accommodating varying levels of complexity. Leveraging this dataset, we conduct extensive experiments using modern Transformer-based models, including large language models (LLMs), in monolingual, cross-lingual, and multilingual settings. To address cross-lingual challenges, we propose a translation and label alignment methodology leveraging LLMs, which yields consistent improvements. Our results highlight the strengths and limitations of state-of-the-art models, especially when handling the linguistic intricacies of low-resource languages like Czech. A detailed error analysis reveals key challenges, including the detection of subtle opinion terms and nuanced sentiment expressions. The dataset establishes a new benchmark for Czech ABSA, and our proposed translation-alignment approach offers a scalable solution for adapting ABSA resources to other low-resource languages.", "AI": {"tldr": "They release a new Czech restaurant-domain dataset for aspect-based sentiment analysis with explicit opinion-term annotations, define three related tasks, benchmark transformers/LLMs in mono/cross/multilingual setups, and introduce an LLM-driven translation+label-alignment method that boosts cross-lingual performance; they provide error analysis and set a new Czech ABSA benchmark with a scalable path for other low-resource languages.", "motivation": "ABSA resources for low-resource languages like Czech are scarce, especially with opinion-term annotations. Existing cross-lingual transfer often suffers from misaligned labels and translation noise. The paper aims to fill this data gap and improve cross-lingual adaptation so ABSA can work well beyond high-resource languages.", "method": "1) Build and annotate a Czech restaurant-domain ABSA dataset including opinion terms; 2) Define three ABSA tasks of increasing complexity involving opinion terms; 3) Evaluate modern Transformer/LLM models in monolingual, cross-lingual, and multilingual regimes; 4) Propose an LLM-based translation and label-alignment pipeline to mitigate cross-lingual errors; 5) Conduct detailed error analysis.", "result": "The translation+alignment approach consistently improves cross-lingual results. State-of-the-art models show solid performance but struggle with Czech-specific linguistic nuances, especially subtle opinion terms and fine-grained sentiment. The dataset establishes a new benchmark for Czech ABSA.", "conclusion": "High-quality Czech ABSA data with opinion-term annotations enables better benchmarking and model development. The LLM-driven translation-alignment pipeline generalizes as a scalable strategy for adapting ABSA resources to other low-resource languages, though challenges remain in capturing subtle opinions and nuanced sentiment."}}
{"id": "2602.22539", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.22539", "abs": "https://arxiv.org/abs/2602.22539", "authors": ["Mohammad Hossein Shokouhi", "Vincent W. S. Wong"], "title": "Agentic AI for Intent-driven Optimization in Cell-free O-RAN", "comment": "Accepted by IEEE International Conference on Communications (ICC), Glasgow, UK, May 2026", "summary": "Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of such agents. However, most existing works consider simple intents handled by independent agents, while complex intents that require coordination among agents remain unexplored. In this paper, we propose an agentic AI framework for intent translation and optimization in cell-free O-RAN. A supervisor agent translates the operator intents into an optimization objective and minimum rate requirements. Based on this information, a user weighting agent retrieves relevant prior experience from a memory module to determine the user priority weights for precoding. If the intent includes an energy-saving objective, then an open radio unit (O-RU) management agent will also be activated to determine the set of active O-RUs by using a deep reinforcement learning (DRL) algorithm. A monitoring agent measures and monitors the user data rates and coordinates with other agents to guarantee the minimum rate requirements are satisfied. To enhance scalability, we adopt a parameter-efficient fine-tuning (PEFT) method that enables the same underlying LLM to be used for different agents. Simulation results show that the proposed agentic AI framework reduces the number of active O-RUs by 41.93% when compared with three baseline schemes in energy-saving mode. Using the PEFT method, the proposed framework reduces the memory usage by 92% when compared with deploying separate LLM agents.", "AI": {"tldr": "Proposes a coordinated multi\u2011agent, LLM-driven framework for translating operator intents into cell\u2011free O\u2011RAN optimization actions, combining intent translation, user weighting, DRL-based O\u2011RU activation, and monitoring; achieves large energy savings and memory efficiency via PEFT.", "motivation": "Existing O\u2011RAN/agentic AI studies mostly handle simple, independent intents; real networks need coordinated agents to satisfy complex intents (QoS and energy) in cell\u2011free RANs. There is also a need to scale multi\u2011agent LLM deployments efficiently.", "method": "Design a multi\u2011agent framework: (1) a supervisor LLM translates operator intents into an optimization objective and minimum rate constraints; (2) a user\u2011weighting LLM queries a memory module to derive user priority weights for precoding; (3) when energy saving is requested, an O\u2011RU management agent selects active O\u2011RUs via a DRL policy; (4) a monitoring agent measures user rates and coordinates to maintain minimum-rate guarantees. To scale, all agents share one base LLM via parameter\u2011efficient fine\u2011tuning (PEFT).", "result": "In simulations, the framework cuts the number of active O\u2011RUs by 41.93% versus three baselines under energy\u2011saving mode, and PEFT reduces memory footprint by 92% compared to deploying separate LLMs per agent.", "conclusion": "Coordinated agentic AI using shared, PEFT\u2011tuned LLMs can translate complex operator intents into effective cell\u2011free O\u2011RAN control, yielding significant energy savings while meeting rate constraints and enabling memory\u2011efficient deployment."}}
{"id": "2602.22595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22595", "abs": "https://arxiv.org/abs/2602.22595", "authors": ["Taozhe Li"], "title": "Don't let the information slip away", "comment": "10", "summary": "Real-time object detection has advanced rapidly in recent years. The YOLO series of detectors is among the most well-known CNN-based object detection models and cannot be overlooked. The latest version, YOLOv26, was recently released, while YOLOv12 achieved state-of-the-art (SOTA) performance with 55.2 mAP on the COCO val2017 dataset. Meanwhile, transformer-based object detection models, also known as DEtection TRansformer (DETR), have demonstrated impressive performance. RT-DETR is an outstanding model that outperformed the YOLO series in both speed and accuracy when it was released. Its successor, RT-DETRv2, achieved 53.4 mAP on the COCO val2017 dataset. However, despite their remarkable performance, all these models let information to slip away. They primarily focus on the features of foreground objects while neglecting the contextual information provided by the background. We believe that background information can significantly aid object detection tasks. For example, cars are more likely to appear on roads rather than in offices, while wild animals are more likely to be found in forests or remote areas rather than on busy streets. To address this gap, we propose an object detection model called Association DETR, which achieves state-of-the-art results compared to other object detection models on the COCO val2017 dataset.", "AI": {"tldr": "Proposes Association DETR, a DETR-based object detector that explicitly leverages background context alongside foreground features, claiming state-of-the-art results on COCO val2017.", "motivation": "Prevailing real-time detectors (YOLO series, RT-DETR family) prioritize foreground features and tend to ignore informative background context, causing loss of useful cues (e.g., cars on roads, wildlife in forests). The authors argue that modeling such contextual priors can improve detection accuracy.", "method": "Introduce Association DETR, which augments the DETR paradigm by associating foreground object features with background/scene context to enhance representation and detection. The abstract implies context-aware feature modeling but does not detail the architectural modules or training objectives.", "result": "Claims state-of-the-art performance on COCO val2017, outperforming contemporary CNN-based YOLO variants and transformer-based RT-DETR/RT-DETRv2. No concrete mAP numbers or speed metrics are provided in the abstract.", "conclusion": "Incorporating background context into object detection improves performance; Association DETR operationalizes this idea within a DETR framework and achieves SOTA results on COCO val2017, suggesting the value of explicit context modeling in real-time detection."}}
{"id": "2602.22752", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22752", "abs": "https://arxiv.org/abs/2602.22752", "authors": ["Nils Schwager", "Simon M\u00fcnker", "Alistair Plum", "Achim Rettinger"], "title": "Towards Simulating Social Media Users with LLMs: Evaluating the Operational Validity of Conditioned Comment Prediction", "comment": "14 pages, 1 figure, 7 tables. Accepted to the 15th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA) at EACL 2026, Rabat, Morocco", "summary": "The transition of Large Language Models (LLMs) from exploratory tools to active \"silicon subjects\" in social science lacks extensive validation of operational validity. This study introduces Conditioned Comment Prediction (CCP), a task in which a model predicts how a user would comment on a given stimulus by comparing generated outputs with authentic digital traces. This framework enables a rigorous evaluation of current LLM capabilities with respect to the simulation of social media user behavior. We evaluated open-weight 8B models (Llama3.1, Qwen3, Ministral) in English, German, and Luxembourgish language scenarios. By systematically comparing prompting strategies (explicit vs. implicit) and the impact of Supervised Fine-Tuning (SFT), we identify a critical form vs. content decoupling in low-resource settings: while SFT aligns the surface structure of the text output (length and syntax), it degrades semantic grounding. Furthermore, we demonstrate that explicit conditioning (generated biographies) becomes redundant under fine-tuning, as models successfully perform latent inference directly from behavioral histories. Our findings challenge current \"naive prompting\" paradigms and offer operational guidelines prioritizing authentic behavioral traces over descriptive personas for high-fidelity simulation.", "AI": {"tldr": "They propose Conditioned Comment Prediction (CCP) to test whether LLMs can realistically simulate how users comment on social media by comparing model outputs to real user traces. In 8B open models across three languages, SFT improves surface form but hurts semantic grounding in low-resource settings; after SFT, explicit personas add little beyond behavioral histories, suggesting prompts should favor authentic traces over bios.", "motivation": "LLMs are increasingly used as stand-ins for human participants in social science, but there is limited validation of whether they operate in ways that reflect real user behavior. The authors aim to establish an operationally valid way to assess LLMs\u2019 ability to simulate social media users and to interrogate common prompting practices (e.g., descriptive personas).", "method": "Introduce the CCP task: predict a user\u2019s comment to a stimulus and evaluate by matching generated comments to authentic digital traces. Benchmark open-weight 8B models (Llama3.1, Qwen3, Ministral) in English, German, and Luxembourgish. Systematically vary prompting (explicit conditioning via generated biographies vs. implicit/latent conditioning via behavioral histories) and apply supervised fine-tuning (SFT). Measure both surface properties (length, syntax) and semantic grounding/content fidelity.", "result": "They find a form\u2013content decoupling in low-resource contexts: SFT aligns output form (length, syntax) but degrades semantic grounding with respect to authentic traces. Explicit conditioning via bios becomes largely redundant after SFT; models can infer relevant attributes from behavioral histories (latent inference). These results question naive/persona-based prompting for behavior simulation.", "conclusion": "CCP offers a rigorous framework to test LLMs as \u201csilicon subjects.\u201d For high-fidelity simulations, prioritize real behavioral histories over descriptive personas, especially post-SFT. Be cautious: SFT may optimize surface conformity at the expense of semantic fidelity in low-resource settings, so evaluation and prompting should emphasize grounding in authentic traces."}}
{"id": "2602.22546", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22546", "abs": "https://arxiv.org/abs/2602.22546", "authors": ["Zhiming Wang", "Jinwei He", "Feng Lu"], "title": "Requesting Expert Reasoning: Augmenting LLM Agents with Learned Collaborative Intervention", "comment": null, "summary": "Large Language Model (LLM) based agents excel at general reasoning but often fail in specialized domains where success hinges on long-tail knowledge absent from their training data. While human experts can provide this missing knowledge, their guidance is often unstructured and unreliable, making its direct integration into an agent's plan problematic. To address this, we introduce AHCE (Active Human-Augmented Challenge Engagement), a framework for on-demand Human-AI collaboration. At its core, the Human Feedback Module (HFM) employs a learned policy to treat the human expert as an interactive reasoning tool. Extensive experiments in Minecraft demonstrate the framework's effectiveness, increasing task success rates by 32% on normal difficulty tasks and nearly 70% on highly difficult tasks, all with minimal human intervention. Our work demonstrates that successfully augmenting agents requires learning how to request expert reasoning, moving beyond simple requests for help.", "AI": {"tldr": "AHCE is a framework that learns when and how to query human experts as an interactive reasoning tool, substantially boosting LLM-agent success in Minecraft with minimal human effort.", "motivation": "LLM agents often fail on domain-specific, long-tail knowledge not seen in training data. Human experts can fill the gaps, but ad-hoc, unstructured guidance is hard to integrate reliably into agents\u2019 plans. There is a need for a principled way to elicit and use expert reasoning on demand.", "method": "Introduce AHCE, an Active Human-Augmented Challenge Engagement framework. Its core Human Feedback Module (HFM) learns a policy to treat the human as an interactive tool\u2014deciding if/when to query, what to ask, and how to incorporate the response\u2014enabling structured human-AI collaboration during task execution.", "result": "In Minecraft evaluations, AHCE increases task success by 32% on normal tasks and nearly 70% on highly difficult tasks, with minimal human intervention reported.", "conclusion": "Learning to request and integrate expert reasoning\u2014rather than issuing generic help requests\u2014significantly augments agent capabilities in challenging domains. On-demand, policy-driven human interaction is an effective path to overcoming long-tail knowledge gaps."}}
{"id": "2602.22596", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22596", "abs": "https://arxiv.org/abs/2602.22596", "authors": ["Yuci Han", "Charles Toth", "John E. Anderson", "William J. Shuart", "Alper Yilmaz"], "title": "BetterScene: 3D Scene Synthesis with Representation-Aligned Generative Model", "comment": null, "summary": "We present BetterScene, an approach to enhance novel view synthesis (NVS) quality for diverse real-world scenes using extremely sparse, unconstrained photos. BetterScene leverages the production-ready Stable Video Diffusion (SVD) model pretrained on billions of frames as a strong backbone, aiming to mitigate artifacts and recover view-consistent details at inference time. Conventional methods have developed similar diffusion-based solutions to address these challenges of novel view synthesis. Despite significant improvements, these methods typically rely on off-the-shelf pretrained diffusion priors and fine-tune only the UNet module while keeping other components frozen, which still leads to inconsistent details and artifacts even when incorporating geometry-aware regularizations like depth or semantic conditions. To address this, we investigate the latent space of the diffusion model and introduce two components: (1) temporal equivariance regularization and (2) vision foundation model-aligned representation, both applied to the variational autoencoder (VAE) module within the SVD pipeline. BetterScene integrates a feed-forward 3D Gaussian Splatting (3DGS) model to render features as inputs for the SVD enhancer and generate continuous, artifact-free, consistent novel views. We evaluate on the challenging DL3DV-10K dataset and demonstrate superior performance compared to state-of-the-art methods.", "AI": {"tldr": "BetterScene improves novel view synthesis from extremely sparse, unconstrained photos by adapting Stable Video Diffusion\u2019s VAE with temporal equivariance and vision-foundation-model\u2013aligned representations, and by feeding it 3DGS-rendered features, yielding more consistent, artifact-free views and state-of-the-art results on DL3DV-10K.", "motivation": "Sparse-photo NVS often suffers from artifacts and cross-view inconsistencies. Prior diffusion-based solutions typically fine-tune only the UNet while keeping other modules frozen, which still leaves inconsistencies even when adding geometry-aware constraints. The authors aim to exploit and regularize the diffusion latent space\u2014particularly the VAE\u2014to enforce temporal/view consistency and richer, semantically aligned representations.", "method": "Use Stable Video Diffusion as the backbone and act on its latent space by (1) adding temporal equivariance regularization and (2) aligning latent representations with a vision foundation model, both applied to the VAE. Integrate a feed-forward 3D Gaussian Splatting model to render scene features that serve as inputs to the SVD enhancer, producing continuous, artifact-free, view-consistent novel views at inference time.", "result": "On the challenging DL3DV-10K benchmark, BetterScene outperforms state-of-the-art methods and is reported to reduce artifacts and improve cross-view consistency in generated novel views.", "conclusion": "Regularizing and aligning the VAE latent space within SVD\u2014combined with 3DGS feature rendering\u2014addresses the limitations of UNet-only fine-tuning, delivering higher-quality, consistent NVS from sparse, unconstrained photos and achieving SOTA performance."}}
{"id": "2602.22755", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22755", "abs": "https://arxiv.org/abs/2602.22755", "authors": ["Abhay Sheshadri", "Aidan Ewart", "Kai Fronsdal", "Isha Gupta", "Samuel R. Bowman", "Sara Price", "Samuel Marks", "Rowan Wang"], "title": "AuditBench: Evaluating Alignment Auditing Techniques on Models with Hidden Behaviors", "comment": null, "summary": "We introduce AuditBench, an alignment auditing benchmark. AuditBench consists of 56 language models with implanted hidden behaviors. Each model has one of 14 concerning behaviors--such as sycophantic deference, opposition to AI regulation, or secret geopolitical loyalties--which it does not confess to when directly asked. AuditBench models are highly diverse--some are subtle, while others are overt, and we use varying training techniques both for implanting behaviors and training models not to confess. To demonstrate AuditBench's utility, we develop an investigator agent that autonomously employs a configurable set of auditing tools. By measuring investigator agent success using different tools, we can evaluate their efficacy. Notably, we observe a tool-to-agent gap, where tools that perform well in standalone non-agentic evaluations fail to translate into improved performance when used with our investigator agent. We find that our most effective tools involve scaffolded calls to auxiliary models that generate diverse prompts for the target. White-box interpretability tools can be helpful, but the agent performs best with black-box tools. We also find that audit success varies greatly across training techniques: models trained on synthetic documents are easier to audit than models trained on demonstrations, with better adversarial training further increasing auditing difficulty. We release our models, agent, and evaluation framework to support future quantitative, iterative science on alignment auditing.", "AI": {"tldr": "AuditBench is a benchmark of 56 language models with implanted, non-confessing misaligned behaviors and an investigator agent to test auditing tools. It reveals a tool-to-agent gap: tools that score well standalone don\u2019t necessarily help an agent. Black-box, scaffolded prompting works best; white-box helps sometimes. Training method strongly affects audit difficulty. All assets are released to spur systematic research.", "motivation": "Reliable, standardized evaluation of alignment auditing is lacking. Existing tools are tested in isolation and may not reflect agentic auditing workflows. The field needs diverse, realistic testbeds with hidden behaviors to quantify what actually works, compare methods, and iterate scientifically.", "method": "Construct 56 models each harboring one of 14 covert behaviors and trained not to confess. Vary implantation and non-confession training strategies to create a spectrum of subtle-to-overt behaviors. Build an investigator agent that can invoke configurable black-box and white-box auditing tools. Evaluate auditing success across tools both standalone and when embedded in the agent; compare impacts of training techniques (synthetic docs vs demonstrations; adversarial training).", "result": "- Tool-to-agent gap: high standalone scores don\u2019t translate to agent gains. - Best performance comes from scaffolded calls to auxiliary models that generate diverse probes. - White-box interpretability aids sometimes, but black-box pipelines perform best overall. - Training on synthetic docs yields easier audits than demonstration-trained models; stronger adversarial training increases audit difficulty.", "conclusion": "AuditBench provides a quantitative, reproducible platform for alignment auditing research. Effective auditing hinges on agent-integrated, black-box, scaffolded strategies and must account for training method. By releasing models, agent, and evaluation suite, the work enables iterative improvement and benchmarking of auditing techniques."}}
{"id": "2602.22557", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22557", "abs": "https://arxiv.org/abs/2602.22557", "authors": ["Umid Suleymanov", "Rufiz Bayramov", "Suad Gafarli", "Seljan Musayeva", "Taghi Mammadov", "Aynur Akhundlu", "Murat Kantarcioglu"], "title": "CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety", "comment": "Under Review", "summary": "Current safety mechanisms for Large Language Models (LLMs) rely heavily on static, fine-tuned classifiers that suffer from adaptation rigidity, the inability to enforce new governance rules without expensive retraining. To address this, we introduce CourtGuard, a retrieval-augmented multi-agent framework that reimagines safety evaluation as Evidentiary Debate. By orchestrating an adversarial debate grounded in external policy documents, CourtGuard achieves state-of-the-art performance across 7 safety benchmarks, outperforming dedicated policy-following baselines without fine-tuning. Beyond standard metrics, we highlight two critical capabilities: (1) Zero-Shot Adaptability, where our framework successfully generalized to an out-of-domain Wikipedia Vandalism task (achieving 90\\% accuracy) by swapping the reference policy; and (2) Automated Data Curation and Auditing, where we leveraged CourtGuard to curate and audit nine novel datasets of sophisticated adversarial attacks. Our results demonstrate that decoupling safety logic from model weights offers a robust, interpretable, and adaptable path for meeting current and future regulatory requirements in AI governance.", "AI": {"tldr": "CourtGuard reframes LLM safety as a retrieval-grounded, multi-agent evidentiary debate that externalizes policy, achieving SOTA on 7 benchmarks without fine-tuning, zero-shotly adapting to new domains (90% on Wikipedia vandalism) and enabling automated dataset curation/auditing.", "motivation": "Static fine-tuned safety classifiers are brittle and costly to update, making it hard to enforce evolving governance policies and regulatory requirements. The authors seek a flexible, interpretable mechanism that can adopt new rules without retraining model weights.", "method": "A retrieval-augmented, multi-agent adversarial debate in which agents argue using evidence from external policy documents. Safety judgments are derived from the debate\u2019s evidence rather than embedded weights. Policies are swapped in/out to adapt behavior; the same framework is used to curate and audit datasets of adversarial prompts.", "result": "State-of-the-art performance on 7 safety benchmarks, outperforming policy-following baselines without any fine-tuning; strong zero-shot transfer by swapping the reference policy (90% accuracy on an out-of-domain Wikipedia vandalism task); successful automated curation and auditing of nine new adversarial-attack datasets.", "conclusion": "Decoupling safety logic from model weights through a retrieval-grounded evidentiary debate yields a robust, interpretable, and adaptable safety layer that better aligns with dynamic policy needs and regulatory compliance, while also supporting automated data governance workflows."}}
{"id": "2602.22607", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22607", "abs": "https://arxiv.org/abs/2602.22607", "authors": ["Ziqi Zhao", "Abhijit Mishra", "Shounak Roychowdhury"], "title": "LoR-LUT: Learning Compact 3D Lookup Tables via Low-Rank Residuals", "comment": null, "summary": "We present LoR-LUT, a unified low-rank formulation for compact and interpretable 3D lookup table (LUT) generation. Unlike conventional 3D-LUT-based techniques that rely on fusion of basis LUTs, which are usually dense tensors, our unified approach extends the current framework by jointly using residual corrections, which are in fact low-rank tensors, together with a set of basis LUTs. The approach described here improves the existing perceptual quality of an image, which is primarily due to the technique's novel use of residual corrections. At the same time, we achieve the same level of trilinear interpolation complexity, using a significantly smaller number of network, residual corrections, and LUT parameters. The experimental results obtained from LoR-LUT, which is trained on the MIT-Adobe FiveK dataset, reproduce expert-level retouching characteristics with high perceptual fidelity and a sub-megabyte model size. Furthermore, we introduce an interactive visualization tool, termed LoR-LUT Viewer, which transforms an input image into the LUT-adjusted output image, via a number of slidebars that control different parameters. The tool provides an effective way to enhance interpretability and user confidence in the visual results. Overall, our proposed formulation offers a compact, interpretable, and efficient direction for future LUT-based image enhancement and style transfer.", "AI": {"tldr": "LoR-LUT introduces a low-rank residual\u2013augmented 3D LUT framework that delivers expert-level image retouching with high perceptual quality, sub-megabyte size, and unchanged interpolation cost, plus an interactive viewer for interpretability.", "motivation": "Conventional 3D-LUT pipelines rely on fusing dense basis LUTs, leading to large, less interpretable models with many parameters. There is a need for compact, efficient, and interpretable LUTs that preserve or improve perceptual image quality and foster user trust.", "method": "A unified low-rank formulation that pairs a small set of basis LUTs with jointly learned residual corrections modeled as low-rank tensors. This keeps trilinear interpolation complexity unchanged while greatly reducing network and LUT parameters. The system is trained on MIT-Adobe FiveK and accompanied by an interactive LoR-LUT Viewer that exposes controllable sliders for user-understandable adjustments.", "result": "Compared to prior 3D-LUT fusion approaches, LoR-LUT improves perceptual quality primarily via low-rank residual corrections, reproduces expert-level retouching on MIT-Adobe FiveK with high fidelity, maintains the same interpolation complexity, and shrinks the model to under 1 MB.", "conclusion": "LoR-LUT offers a compact, interpretable, and efficient pathway for LUT-based image enhancement and style transfer. The interactive viewer enhances transparency and user confidence, suggesting a practical direction for real-world deployment."}}
{"id": "2602.22765", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22765", "abs": "https://arxiv.org/abs/2602.22765", "authors": ["Zhe Yang", "Yudong Wang", "Rang Li", "Zhifang Sui"], "title": "Towards Better RL Training Data Utilization via Second-Order Rollout", "comment": null, "summary": "Reinforcement Learning (RL) has empowered Large Language Models (LLMs) with strong reasoning capabilities, but vanilla RL mainly focuses on generation capability improvement by training with only first-order rollout (generating multiple responses for a question), and we argue that this approach fails to fully exploit the potential of training data because of the neglect of critique capability training. To tackle this problem, we further introduce the concept of second-order rollout (generating multiple critiques for a response) and propose a unified framework for jointly training generation and critique capabilities. Extensive experiments across various models and datasets demonstrate that our approach can utilize training data more effectively than vanilla RL and achieve better performance under the same training data. Additionally, we uncover several insightful findings regarding second-order rollout and critique training, such as the importance of label balance in critique training and the noise problem of outcome-based rewards, which can be mitigated through sampling techniques. Our work offers a preliminary exploration of dynamic data augmentation and joint generation-critique training in RL, providing meaningful inspiration for the further advancement of RL training", "AI": {"tldr": "Augment vanilla RL for LLMs with \u201csecond-order rollouts\u201d that generate critiques of responses and jointly train generation and critique modules, yielding better performance from the same data and offering guidance on label balance and reward-noise mitigation.", "motivation": "Vanilla RL for LLMs emphasizes first-order rollouts (multiple answers per prompt), overlooking critique skills and leaving training data underexploited. Equipping models to critique can improve reasoning quality and data efficiency.", "method": "Introduce second-order rollouts that sample multiple critiques for a candidate response and unify training so the model learns to both generate answers and critique them. Use sampling-based techniques to reduce noise from outcome-based rewards and maintain balanced labels during critique training; effectively a dynamic data-augmentation and joint optimization scheme.", "result": "Across several models and datasets, the joint generation\u2013critique framework outperforms vanilla RL under equal training data budgets, indicating better data utilization. Empirical analyses reveal the sensitivity to critique-label balance and show that sampling can mitigate noisy reward signals.", "conclusion": "Training LLMs to both generate and critique via second-order rollouts improves data efficiency and performance over vanilla RL. The study opens a path toward dynamic data augmentation and integrated criticize-then-improve RL training, highlighting practical considerations like label balance and reward-noise control."}}
{"id": "2602.22583", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22583", "abs": "https://arxiv.org/abs/2602.22583", "authors": ["Weida Liang", "Yiyou Sun", "Shuyuan Nan", "Chuang Li", "Dawn Song", "Kenji Kawaguchi"], "title": "Strategy Executability in Mathematical Reasoning: Leveraging Human-Model Differences for Effective Guidance", "comment": null, "summary": "Example-based guidance is widely used to improve mathematical reasoning at inference time, yet its effectiveness is highly unstable across problems and models-even when the guidance is correct and problem-relevant. We show that this instability arises from a previously underexplored gap between strategy usage-whether a reasoning strategy appears in successful solutions-and strategy executability-whether the strategy remains effective when instantiated as guidance for a target model. Through a controlled analysis of paired human-written and model-generated solutions, we identify a systematic dissociation between usage and executability: human- and model-derived strategies differ in structured, domain-dependent ways, leading to complementary strengths and consistent source-dependent reversals under guidance. Building on this diagnosis, we propose Selective Strategy Retrieval (SSR), a test-time framework that explicitly models executability by selectively retrieving and combining strategies using empirical, multi-route, source-aware signals. Across multiple mathematical reasoning benchmarks, SSR yields reliable and consistent improvements over direct solving, in-context learning, and single-source guidance, improving accuracy by up to $+13$ points on AIME25 and $+5$ points on Apex for compact reasoning models. Code and benchmark are publicly available at: https://github.com/lwd17/strategy-execute-pipeline.", "AI": {"tldr": "They identify why example-based guidance for math reasoning is unreliable\u2014because strategies that appear in correct solutions aren\u2019t necessarily executable as guidance for a target model\u2014and introduce a test-time retrieval framework (SSR) that selects and combines executable strategies from human and model sources, yielding consistent accuracy gains on benchmarks.", "motivation": "Example-based guidance can help LLMs solve math problems but works inconsistently even when examples are correct. The authors hypothesize a gap between which strategies are used in successful solutions and which strategies are actually executable by a given model when presented as guidance, and they aim to characterize and bridge this gap.", "method": "(1) Controlled analysis of paired human-written and model-generated solutions to disentangle strategy usage vs. executability and to reveal source-dependent, domain-structured differences. (2) Propose Selective Strategy Retrieval (SSR): a test-time framework that models executability by retrieving strategies from multiple sources (human/model), assessing them via empirical, multi-route, source-aware signals, and selectively combining those that are likely executable for the target model.", "result": "They find a systematic dissociation between usage and executability with complementary strengths from human vs. model strategies and consistent source-dependent reversals when used as guidance. SSR provides reliable improvements over direct solving, in-context learning, and single-source guidance, with up to +13 accuracy points on AIME25 and +5 on Apex for compact reasoning models across multiple math benchmarks.", "conclusion": "Effectiveness of guidance depends not only on strategy correctness but on its executability for the target model, which varies by source and domain. Modeling and selecting for executability via SSR stabilizes and improves guided mathematical reasoning, suggesting future systems should be source-aware and empirically validate strategy executability at test time."}}
{"id": "2602.22613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22613", "abs": "https://arxiv.org/abs/2602.22613", "authors": ["Minh Kha Do", "Wei Xiang", "Kang Han", "Di Wu", "Khoa Phan", "Yi-Ping Phoebe Chen", "Gaowen Liu", "Ramana Rao Kompella"], "title": "Spectrally Distilled Representations Aligned with Instruction-Augmented LLMs for Satellite Imagery", "comment": null, "summary": "Vision-language foundation models (VLFMs) promise zero-shot and retrieval understanding for Earth observation. While operational satellite systems often lack full multi-spectral coverage, making RGB-only inference highly desirable for scalable deployment, the adoption of VLFMs for satellite imagery remains hindered by two factors: (1) multi-spectral inputs are informative but difficult to exploit consistently due to band redundancy and misalignment; and (2) CLIP-style text encoders limit semantic expressiveness and weaken fine-grained alignment. We present SATtxt, a spectrum-aware VLFM that operates with RGB inputs only at inference while retaining spectral cues learned during training. Our framework comprises two stages. First, Spectral Representation Distillation transfers spectral priors from a frozen multi-spectral teacher to an RGB student via a lightweight projector. Second, Spectrally Grounded Alignment with Instruction-Augmented LLMs bridges the distilled visual space and an expressive LLM embedding space. Across EuroSAT, BigEarthNet, and ForestNet, SATtxt improves zero-shot classification on average by 4.2%, retrieval by 5.9%, and linear probing by 2.7% over baselines, showing an efficient path toward spectrum-aware vision-language learning for Earth observation. Project page: https://ikhado.github.io/sattxt/", "AI": {"tldr": "SATtxt is a spectrum-aware vision\u2013language framework for Earth observation that learns multispectral priors during training but runs on RGB-only at inference, using a two-stage distillation-and-alignment pipeline to improve zero-shot classification, retrieval, and linear probing over VLFM baselines.", "motivation": "Earth observation systems often lack complete multispectral bands at inference, yet multispectral cues are valuable. Existing VLFM pipelines struggle with (i) redundancy/misalignment across spectral bands and (ii) limited semantic expressiveness from CLIP-style text encoders, hurting fine-grained alignment and downstream zero-shot performance.", "method": "Two-stage framework: (1) Spectral Representation Distillation transfers spectral priors from a frozen multispectral teacher to an RGB student using a lightweight projector; (2) Spectrally Grounded Alignment leverages instruction-augmented LLM embeddings to align the distilled visual space with a more expressive language space than CLIP, enabling richer semantics while keeping RGB-only inference.", "result": "On EuroSAT, BigEarthNet, and ForestNet, SATtxt improves over baselines by +4.2% (avg) in zero-shot classification, +5.9% in retrieval, and +2.7% in linear probing, demonstrating consistent gains with RGB-only inference.", "conclusion": "Distilling multispectral knowledge into an RGB student and aligning it with an instruction-tuned LLM yields a practical, scalable path to spectrum-aware VLFMs for EO, enabling stronger zero-shot and retrieval performance without requiring multispectral inputs at deployment."}}
{"id": "2602.22766", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22766", "abs": "https://arxiv.org/abs/2602.22766", "authors": ["You Li", "Chi Chen", "Yanghao Li", "Fanhu Zeng", "Kaiyu Huang", "Jinan Xu", "Maosong Sun"], "title": "Imagination Helps Visual Reasoning, But Not Yet in Latent Space", "comment": "13 pages, 6 figures", "summary": "Latent visual reasoning aims to mimic human's imagination process by meditating through hidden states of Multimodal Large Language Models. While recognized as a promising paradigm for visual reasoning, the underlying mechanisms driving its effectiveness remain unclear. Motivated to demystify the true source of its efficacy, we investigate the validity of latent reasoning using Causal Mediation Analysis. We model the process as a causal chain: the input as the treatment, the latent tokens as the mediator, and the final answer as the outcome. Our findings uncover two critical disconnections: (a) Input-Latent Disconnect: dramatic perturbations on the input result in negligible changes to the latent tokens, suggesting that latent tokens do not effectively attend to the input sequence. (b) Latent-Answer Disconnect: perturbations on the latent tokens yield minimal impact on the final answer, indicating the limited causal effect latent tokens imposing on the outcome. Furthermore, extensive probing analysis reveals that latent tokens encode limited visual information and exhibit high similarity. Consequently, we challenge the necessity of latent reasoning and propose a straightforward alternative named CapImagine, which teaches the model to explicitly imagine using text. Experiments on vision-centric benchmarks show that CapImagine significantly outperforms complex latent-space baselines, highlighting the superior potential of visual reasoning through explicit imagination.", "AI": {"tldr": "They examine \u201clatent visual reasoning\u201d in multimodal LLMs using causal mediation analysis and find that latent tokens neither depend much on the input nor causally influence the answer. Probing shows these tokens hold little visual information and are highly redundant. They therefore propose CapImagine, an explicit text-based imagination approach that outperforms latent-space methods on vision-centric benchmarks.", "motivation": "Despite enthusiasm for latent visual reasoning, it is unclear whether the purported internal reasoning actually mediates from input to answer. The authors aim to identify if latent tokens truly carry and transmit visual information and, if not, to provide a simpler, more effective alternative.", "method": "Model the pipeline as a causal chain: input (treatment) \u2192 latent tokens (mediator) \u2192 answer (outcome). Use causal mediation analysis with perturbation experiments on input and latent tokens to estimate dependencies and effects. Perform probing to assess visual content and similarity of latent tokens. Propose CapImagine, which trains models to perform explicit, text-only imagination before answering, and evaluate on vision-centric benchmarks against latent-space baselines.", "result": "Two key disconnections observed: (a) Input\u2013Latent Disconnect\u2014large input perturbations barely change latent tokens; (b) Latent\u2013Answer Disconnect\u2014perturbing latent tokens minimally affects final answers. Probing reveals limited visual information and high redundancy in latent tokens. CapImagine significantly outperforms complex latent-space baselines on multiple vision-centric benchmarks.", "conclusion": "Latent reasoning, as instantiated via latent tokens in current MLLMs, appears unnecessary or ineffective for visual reasoning; explicit textual imagination is simpler and more powerful. The work urges a shift from opaque latent reasoning toward interpretable, explicit reasoning approaches."}}
{"id": "2602.22585", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22585", "abs": "https://arxiv.org/abs/2602.22585", "authors": ["Jodi M. Casabianca", "Maggie Beiting-Parrish"], "title": "Correcting Human Labels for Rater Effects in AI Evaluation: An Item Response Theory Approach", "comment": "16 pages, 5 figures, 1 table; The 16th Annual Learning Analytics and Knowledge Conference (LAK) Workshop on LLM Psychometrics, April 27, 2026, Bergen, Norway", "summary": "Human evaluations play a central role in training and assessing AI models, yet these data are rarely treated as measurements subject to systematic error. This paper integrates psychometric rater models into the AI pipeline to improve the reliability and validity of conclusions drawn from human judgments. The paper reviews common rater effects, severity and centrality, that distort observed ratings, and demonstrates how item response theory rater models, particularly the multi-faceted Rasch model, can separate true output quality from rater behavior. Using the OpenAI summarization dataset as an empirical example, we show how adjusting for rater severity produces corrected estimates of summary quality and provides diagnostic insight into rater performance. Incorporating psychometric modeling into human-in-the-loop evaluation offers more principled and transparent use of human data, enabling developers to make decisions based on adjusted scores rather than raw, error-prone ratings. This perspective highlights a path toward more robust, interpretable, and construct-aligned practices for AI development and evaluation.", "AI": {"tldr": "Use psychometric rater models (multi\u2011faceted Rasch/IRT) to debias human evaluations of AI, correcting for rater severity/centrality and yielding more reliable, interpretable quality scores; demonstrated on an OpenAI summarization dataset.", "motivation": "Human ratings drive training and evaluation but are distorted by systematic rater effects (severity, central tendency), threatening validity and reliability of conclusions and decisions made from raw scores.", "method": "Embed item response theory\u2014specifically multi\u2011faceted Rasch models\u2014into the evaluation pipeline to jointly model output quality, rater severity/centrality, and task facets; adjust raw ratings to estimate latent quality and produce rater diagnostics. Empirical illustration on OpenAI\u2019s summarization dataset.", "result": "Adjusting for rater severity changes estimated summary quality and reveals which raters are severe/lenient or exhibit centrality, providing corrected quality scores and actionable diagnostics; potentially alters model comparisons relative to raw ratings.", "conclusion": "Psychometric modeling of human judgments enables more principled, transparent, and construct\u2011aligned AI evaluation. Developers should rely on adjusted scores and rater diagnostics rather than raw ratings to make decisions, improving robustness and interpretability."}}
{"id": "2602.22620", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22620", "abs": "https://arxiv.org/abs/2602.22620", "authors": ["Tomoya Tsuchida", "Keita Takahashi", "Chihiro Tsutake", "Toshiaki Fujii", "Hajime Nagahara"], "title": "Coded-E2LF: Coded Aperture Light Field Imaging from Events", "comment": "accepted to CVPR 2026", "summary": "We propose Coded-E2LF (coded event to light field), a computational imaging method for acquiring a 4-D light field using a coded aperture and a stationary event-only camera. In a previous work, an imaging system similar to ours was adopted, but both events and intensity images were captured and used for light field reconstruction. In contrast, our method is purely event-based, which relaxes restrictions for hardware implementation. We also introduce several advancements from the previous work that enable us to theoretically support and practically improve light field reconstruction from events alone. In particular, we clarify the key role of a black pattern in aperture coding patterns. We finally implemented our method on real imaging hardware to demonstrate its effectiveness in capturing real 3-D scenes. To the best of our knowledge, we are the first to demonstrate that a 4-D light field with pixel-level accuracy can be reconstructed from events alone. Our software and supplementary video are available from our project website.", "AI": {"tldr": "Coded-E2LF reconstructs a full 4-D light field using only events from a stationary event camera and a specially designed coded aperture, achieving pixel-level accuracy and demonstrated on real hardware.", "motivation": "Light-field capture usually needs intensity images or complex hardware; prior event-based systems still relied on intensity frames. The authors aim to eliminate intensity imaging entirely, simplifying hardware and leveraging event cameras\u2019 high temporal resolution while establishing theory for reliable reconstruction from events alone.", "method": "Use a stationary event-only camera behind a coded aperture. Design and analyze aperture coding\u2014highlighting the necessity of a black (occluding) pattern component\u2014to make the event stream sufficiently informative for 4-D light-field inversion. Develop a reconstruction pipeline that converts asynchronous events into a pixel-accurate light field, with theoretical justification and practical improvements over prior hybrid (event+intensity) approaches.", "result": "On real hardware, the system reconstructs 4-D light fields of real 3-D scenes from events alone with pixel-level accuracy. This is, to their knowledge, the first pure-event demonstration at that fidelity; code and video are released.", "conclusion": "Event-only imaging, when paired with carefully designed coded apertures (including a critical black pattern), is sufficient for accurate light-field recovery. This reduces hardware constraints and enables practical, high-fidelity 4-D light-field acquisition from events alone."}}
{"id": "2602.22787", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22787", "abs": "https://arxiv.org/abs/2602.22787", "authors": ["Ivo Brink", "Alexander Boer", "Dennis Ulmer"], "title": "Probing for Knowledge Attribution in Large Language Models", "comment": null, "summary": "Large language models (LLMs) often generate fluent but unfounded claims, or hallucinations, which fall into two types: (i) faithfulness violations - misusing user context - and (ii) factuality violations - errors from internal knowledge. Proper mitigation depends on knowing whether a model's answer is based on the prompt or its internal weights. This work focuses on the problem of contributive attribution: identifying the dominant knowledge source behind each output. We show that a probe, a simple linear classifier trained on model hidden representations, can reliably predict contributive attribution. For its training, we introduce AttriWiki, a self-supervised data pipeline that prompts models to recall withheld entities from memory or read them from context, generating labelled examples automatically. Probes trained on AttriWiki data reveal a strong attribution signal, achieving up to 0.96 Macro-F1 on Llama-3.1-8B, Mistral-7B, and Qwen-7B, transferring to out-of-domain benchmarks (SQuAD, WebQuestions) with 0.94-0.99 Macro-F1 without retraining. Attribution mismatches raise error rates by up to 70%, demonstrating a direct link between knowledge source confusion and unfaithful answers. Yet, models may still respond incorrectly even when attribution is correct, highlighting the need for broader detection frameworks.", "AI": {"tldr": "Train a simple linear probe on LLM hidden states to predict whether an answer comes from the prompt context or the model\u2019s internal memory; using a self-supervised data pipeline (AttriWiki), the probe achieves ~0.96 Macro-F1 and transfers out-of-domain with ~0.94\u20130.99, and attribution mismatches strongly correlate with higher error rates.", "motivation": "Hallucinations stem from two sources: misuse of user-provided context (faithfulness) and errors from internal knowledge (factuality). Effective mitigation requires knowing which source dominated a given answer so the right guardrail (context handling vs knowledge correction) can be applied.", "method": "Define contributive attribution (prompt/context vs internal weights). Train a linear classifier (probe) on hidden representations to predict the dominant source. Create AttriWiki, a self-supervised pipeline that withholds entities so models must either recall them from memory or read them from provided context, automatically yielding labeled training examples. Evaluate across multiple LLMs and transfer to out-of-domain tasks.", "result": "Probes trained on AttriWiki expose a strong attribution signal: up to 0.96 Macro-F1 on Llama-3.1-8B, Mistral-7B, and Qwen-7B, and 0.94\u20130.99 Macro-F1 on SQuAD/WebQuestions without retraining. Attribution mismatches increase error rates by up to 70%, linking source confusion to unfaithfulness.", "conclusion": "LLMs\u2019 hidden states encode reliable signals about whether answers draw from context or internal memory, enabling lightweight attribution probes that generalize well. However, correct attribution does not ensure correctness; broader detection and mitigation frameworks remain necessary."}}
{"id": "2602.22603", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22603", "abs": "https://arxiv.org/abs/2602.22603", "authors": ["Sanjay Kariyappa", "G. Edward Suh"], "title": "SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning", "comment": null, "summary": "Long-running agentic tasks, such as deep research, require multi-hop reasoning over information distributed across multiple webpages and documents. In such tasks, the LLM context is dominated by tokens from external retrieval, causing memory usage to grow rapidly and limiting decode performance. While several KV cache compression techniques exist for long-context inputs, we find that existing heuristics fail to support multi-step reasoning models effectively. We address this challenge with SideQuest -- a novel approach that leverages the Large Reasoning Model (LRM) itself to perform KV cache compression by reasoning about the usefulness of tokens in its context. To prevent the tokens associated with this management process from polluting the model's memory, we frame KV cache compression as an auxiliary task executed in parallel to the main reasoning task. Our evaluations, using a model trained with just 215 samples, show that SideQuest reduces peak token usage by up to 65% on agentic tasks with minimal degradation in accuracy, outperforming heuristic-based KV cache compression techniques.", "AI": {"tldr": "SideQuest lets an LRM compress its own KV cache by reasoning about which context tokens matter, run as an auxiliary process in parallel to the main reasoning, cutting peak token usage by up to 65% with minimal accuracy loss and outperforming heuristic baselines.", "motivation": "Long, multi-hop agentic tasks pull in many retrieved tokens that bloat the KV cache, inflate memory, and slow decoding. Existing heuristic KV compression methods don\u2019t preserve performance for multi-step reasoning models.", "method": "Introduce SideQuest: the LRM explicitly judges token usefulness and performs KV cache compression itself. To avoid contaminating the main reasoning state, this compression is framed as an auxiliary, parallel task whose management tokens don\u2019t pollute the primary KV cache. Trained with a small dataset (~215 samples).", "result": "On agentic tasks, SideQuest reduces peak token usage by up to 65% with minimal accuracy degradation, and it outperforms heuristic-based KV cache compression methods.", "conclusion": "LRM-driven, parallel auxiliary KV cache compression is a practical, data-efficient way to sustain multi-step reasoning under long contexts, improving memory/computation efficiency without sacrificing accuracy."}}
{"id": "2602.22621", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22621", "abs": "https://arxiv.org/abs/2602.22621", "authors": ["Boyang Dai", "Zeng Fan", "Zihao Qi", "Meng Lou", "Yizhou Yu"], "title": "CGSA: Class-Guided Slot-Aware Adaptation for Source-Free Object Detection", "comment": "The paper has been accepted by the conference ICLR 2026", "summary": "Source-Free Domain Adaptive Object Detection (SF-DAOD) aims to adapt a detector trained on a labeled source domain to an unlabeled target domain without retaining any source data. Despite recent progress, most popular approaches focus on tuning pseudo-label thresholds or refining the teacher-student framework, while overlooking object-level structural cues within cross-domain data. In this work, we present CGSA, the first framework that brings Object-Centric Learning (OCL) into SF-DAOD by integrating slot-aware adaptation into the DETR-based detector. Specifically, our approach integrates a Hierarchical Slot Awareness (HSA) module into the detector to progressively disentangle images into slot representations that act as visual priors. These slots are then guided toward class semantics via a Class-Guided Slot Contrast (CGSC) module, maintaining semantic consistency and prompting domain-invariant adaptation. Extensive experiments on multiple cross-domain datasets demonstrate that our approach outperforms previous SF-DAOD methods, with theoretical derivations and experimental analysis further demonstrating the effectiveness of the proposed components and the framework, thereby indicating the promise of object-centric design in privacy-sensitive adaptation scenarios. Code is released at https://github.com/Michael-McQueen/CGSA.", "AI": {"tldr": "CGSA brings object-centric, slot-aware adaptation to DETR for source-free domain adaptive object detection, using hierarchical slot priors and class-guided contrast to achieve domain-invariant, semantically consistent detection and outperform prior SF-DAOD methods.", "motivation": "Source-free DAOD must adapt to an unlabeled target domain without accessing source data, yet prevailing methods mostly tweak pseudo-labeling or teacher\u2013student schemes and neglect object-level structure. Leveraging object-centric learning could yield domain-robust object representations, crucial in privacy-sensitive settings where source data cannot be retained.", "method": "Integrate a Hierarchical Slot Awareness (HSA) module into a DETR-based detector to progressively disentangle images into object-centric slot representations serving as visual priors. Then apply a Class-Guided Slot Contrast (CGSC) module to align these slots with class semantics, enforcing semantic consistency and encouraging domain-invariant adaptation across domains.", "result": "Across multiple cross-domain benchmarks, CGSA surpasses prior SF-DAOD approaches. The paper provides theoretical derivations and empirical analyses (e.g., ablations) that validate the contributions of HSA and CGSC.", "conclusion": "Object-centric, slot-aware design within DETR effectively enables source-free domain adaptation for object detection, improving performance while aligning with privacy constraints; released code facilitates reproducibility and adoption."}}
{"id": "2602.22790", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22790", "abs": "https://arxiv.org/abs/2602.22790", "authors": ["Hyunwoo Kim", "Hanau Yi", "Jaehee Bae", "Yumin Kim"], "title": "Natural Language Declarative Prompting (NLD-P): A Modular Governance Method for Prompt Design Under Model Drift", "comment": null, "summary": "The rapid evolution of large language models (LLMs) has transformed prompt engineering from a localized craft into a systems-level governance challenge. As models scale and update across generations, prompt behavior becomes sensitive to shifts in instruction-following policies, alignment regimes, and decoding strategies, a phenomenon we characterize as GPT-scale model drift. Under such conditions, surface-level formatting conventions and ad hoc refinement are insufficient to ensure stable, interpretable control. This paper reconceptualizes Natural Language Declarative Prompting (NLD-P) as a declarative governance method rather than a rigid field template. NLD-P is formalized as a modular control abstraction that separates provenance, constraint logic, task content, and post-generation evaluation, encoded directly in natural language without reliance on external orchestration code. We define minimal compliance criteria, analyze model-dependent schema receptivity, and position NLD-P as an accessible governance framework for non-developer practitioners operating within evolving LLM ecosystems. Portions of drafting and editorial refinement employed a schema-bound LLM assistant configured under NLD-P. All conceptual framing, methodological claims, and final revisions were directed, reviewed, and approved by the human author under a documented human-in-the-loop protocol. The paper concludes by outlining implications for declarative control under ongoing model evolution and identifying directions for future empirical validation.", "AI": {"tldr": "Proposes Natural Language Declarative Prompting (NLD-P) as a modular, natural-language governance abstraction to stabilize and control LLM behavior amid model drift, with minimal compliance criteria, schema receptivity analysis, and guidance for non-developers; calls for empirical validation.", "motivation": "LLM behavior shifts across versions and alignment/decoding changes (\u201cGPT-scale model drift\u201d) make prompts brittle; surface formatting and ad hoc prompt tuning no longer ensure stable, interpretable control, creating a systems-level governance problem.", "method": "Reframe NLD-P from a fixed template to a declarative governance method encoded in natural language. Formalize a modular schema separating: (1) provenance, (2) constraint logic, (3) task content, and (4) post-generation evaluation\u2014without external orchestration code. Define minimal compliance criteria and analyze model-dependent receptivity to the schema. Employ a schema-bound LLM assistant under NLD-P for drafting, with human-in-the-loop oversight for conceptual framing and final approval.", "result": "Delivers a formal control abstraction and compliance criteria for NLD-P, along with analysis of how different LLMs accept/adhere to the schema. Positions NLD-P as accessible for non-developers and demonstrates its use in the paper\u2019s drafting process, though without large-scale empirical benchmarks.", "conclusion": "NLD-P can serve as a practical governance framework for maintaining declarative control over prompts as models evolve. The paper highlights implications for robust, interpretable prompting under ongoing model updates and identifies the need for future empirical validation to test stability, receptivity, and effectiveness."}}
{"id": "2602.22638", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22638", "abs": "https://arxiv.org/abs/2602.22638", "authors": ["Zhiheng Song", "Jingshuai Zhang", "Chuan Qin", "Chao Wang", "Chao Chen", "Longfei Xu", "Kaikui Liu", "Xiangxiang Chu", "Hengshu Zhu"], "title": "MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios", "comment": null, "summary": "Route-planning agents powered by large language models (LLMs) have emerged as a promising paradigm for supporting everyday human mobility through natural language interaction and tool-mediated decision making. However, systematic evaluation in real-world mobility settings is hindered by diverse routing demands, non-deterministic mapping services, and limited reproducibility. In this study, we introduce MobilityBench, a scalable benchmark for evaluating LLM-based route-planning agents in real-world mobility scenarios. MobilityBench is constructed from large-scale, anonymized real user queries collected from Amap and covers a broad spectrum of route-planning intents across multiple cities worldwide. To enable reproducible, end-to-end evaluation, we design a deterministic API-replay sandbox that eliminates environmental variance from live services. We further propose a multi-dimensional evaluation protocol centered on outcome validity, complemented by assessments of instruction understanding, planning, tool use, and efficiency. Using MobilityBench, we evaluate multiple LLM-based route-planning agents across diverse real-world mobility scenarios and provide an in-depth analysis of their behaviors and performance. Our findings reveal that current models perform competently on Basic information retrieval and Route Planning tasks, yet struggle considerably with Preference-Constrained Route Planning, underscoring significant room for improvement in personalized mobility applications. We publicly release the benchmark data, evaluation toolkit, and documentation at https://github.com/AMAP-ML/MobilityBench .", "AI": {"tldr": "MobilityBench is a reproducible benchmark and deterministic sandbox built from real Amap routing queries to evaluate LLM-based route-planning agents; it shows current models handle basic tasks but fail on preference\u2011constrained routing, and releases data/tools publicly.", "motivation": "Evaluating LLM route\u2011planning agents in the real world is difficult due to heterogeneous user intents, non\u2011deterministic mapping services, and poor reproducibility; a scalable, realistic, and repeatable benchmark is needed.", "method": "Construct a large\u2011scale dataset of anonymized real user routing queries from Amap across multiple cities; build a deterministic API\u2011replay sandbox to eliminate live\u2011service variability; define a multi\u2011dimensional evaluation protocol centered on outcome validity, plus instruction understanding, planning, tool use, and efficiency; apply it to multiple LLM-based agents across diverse scenarios.", "result": "Across tasks, agents are competent on Basic information retrieval and standard Route Planning but perform poorly on Preference\u2011Constrained Route Planning; the study provides in\u2011depth behavioral and performance analyses.", "conclusion": "MobilityBench enables end\u2011to\u2011end, reproducible assessment of LLM mobility agents and exposes substantial gaps in personalized, preference\u2011aware routing; the benchmark data, evaluation toolkit, and documentation are publicly released."}}
{"id": "2602.22624", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22624", "abs": "https://arxiv.org/abs/2602.22624", "authors": ["Liya Ji", "Chenyang Qi", "Qifeng Chen"], "title": "Instruction-based Image Editing with Planning, Reasoning, and Generation", "comment": "10 pages, 7 figures", "summary": "Editing images via instruction provides a natural way to generate interactive content, but it is a big challenge due to the higher requirement of scene understanding and generation. Prior work utilizes a chain of large language models, object segmentation models, and editing models for this task. However, the understanding models provide only a single modality ability, restricting the editing quality. We aim to bridge understanding and generation via a new multi-modality model that provides the intelligent abilities to instruction-based image editing models for more complex cases. To achieve this goal, we individually separate the instruction editing task with the multi-modality chain of thought prompts, i.e., Chain-of-Thought (CoT) planning, editing region reasoning, and editing. For Chain-of-Thought planning, the large language model could reason the appropriate sub-prompts considering the instruction provided and the ability of the editing network. For editing region reasoning, we train an instruction-based editing region generation network with a multi-modal large language model. Finally, a hint-guided instruction-based editing network is proposed for editing image generations based on the sizeable text-to-image diffusion model to accept the hints for generation. Extensive experiments demonstrate that our method has competitive editing abilities on complex real-world images.", "AI": {"tldr": "Proposes a multi-modal, three-stage framework\u2014CoT planning, instruction-conditioned region reasoning, and hint-guided diffusion editing\u2014to improve instruction-based image editing on complex real images.", "motivation": "Prior instruction-based editing pipelines chain single-modality understanding (LLMs, segmentation) with editors, limiting semantic grounding and edit fidelity. The authors seek to tightly couple understanding and generation via multi-modal reasoning to handle complex, real-world edits.", "method": "Decompose editing into: (1) Chain-of-Thought planning where an LLM derives actionable sub-prompts aligned with the editor\u2019s capabilities; (2) Editing region reasoning by training an instruction-conditioned region generator using a multi-modal LLM to localize target areas; (3) Hint-guided editing that augments a large text-to-image diffusion model to accept region and planning hints for controlled generation.", "result": "Experiments indicate competitive performance on complex real-world images, suggesting improved localization, controllability, and semantic alignment compared with prior pipelines.", "conclusion": "Integrating multi-modal reasoning with diffusion-based editing via CoT planning and explicit region hints bridges understanding and generation, yielding stronger instruction-based image edits on challenging scenes."}}
{"id": "2602.22827", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22827", "abs": "https://arxiv.org/abs/2602.22827", "authors": ["Reihaneh Iranmanesh", "Saeedeh Davoudi", "Pasha Abrishamchian", "Ophir Frieder", "Nazli Goharian"], "title": "TARAZ: Persian Short-Answer Question Benchmark for Cultural Evaluation of Language Models", "comment": "11 pages, 3 figures, Fifteenth biennial Language Resources and Evaluation Conference (LREC) 2026 (to appear)", "summary": "This paper presents a comprehensive evaluation framework for assessing the cultural competence of large language models (LLMs) in Persian. Existing Persian cultural benchmarks rely predominantly on multiple-choice formats and English-centric metrics that fail to capture Persian's morphological complexity and semantic nuance. Our framework introduces a Persian-specific short-answer evaluation that combines rule-based morphological normalization with a hybrid syntactic and semantic similarity module, enabling robust soft-match scoring beyond exact string overlap. Through systematic evaluation of 15 state-of-the-art open- and closed-source models, we demonstrate that our hybrid evaluation improves scoring consistency by +10% compared to exact-match baselines by capturing meaning that surface-level methods cannot detect. We publicly release our evaluation framework, providing the first standardized benchmark for measuring cultural understanding in Persian and establishing a reproducible foundation for cross-cultural LLM evaluation research.", "AI": {"tldr": "A Persian-specific short-answer evaluation framework for LLM cultural competence uses morphological normalization plus hybrid syntactic/semantic similarity to enable soft-match scoring, improving scoring consistency by ~10% over exact match and released as a standardized benchmark.", "motivation": "Persian cultural benchmarks have leaned on multiple-choice and English-centric metrics that overlook Persian\u2019s morphological richness and semantic nuance, leading to inadequate measurement of cultural understanding in Persian.", "method": "Design a short-answer evaluation pipeline combining rule-based morphological normalization with a hybrid similarity module (syntactic + semantic) to allow robust soft matching beyond exact string overlap; evaluate 15 open- and closed-source LLMs under this framework.", "result": "Hybrid evaluation yields a ~+10% increase in scoring consistency relative to exact-match baselines by recognizing semantically correct answers that differ in surface form; the framework is publicly released as a benchmark.", "conclusion": "The proposed framework more faithfully assesses Persian cultural competence in LLMs and establishes a reproducible, standardized basis for cross-cultural evaluation research."}}
{"id": "2602.22650", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22650", "abs": "https://arxiv.org/abs/2602.22650", "authors": ["Xinxin Yang", "Yangyang Tang", "Yikun Zhou", "Yaolei Liu", "Yun Li", "Bo Yang"], "title": "AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising", "comment": "11 pages, 6 figures, accepted by WWW'2026", "summary": "In online advertising, the inherent complexity and dynamic nature of advertising environments necessitate the use of auto-bidding services to assist advertisers in bid optimization. This complexity is further compounded in multi-channel scenarios, where effective allocation of budgets and constraints across channels with distinct behavioral patterns becomes critical for optimizing return on investment. Current approaches predominantly rely on either optimization-based strategies or reinforcement learning techniques. However, optimization-based methods lack flexibility in adapting to dynamic market conditions, while reinforcement learning approaches often struggle to capture essential historical dependencies and observational patterns within the constraints of Markov Decision Process frameworks. To address these limitations, we propose AHBid, an Adaptable Hierarchical Bidding framework that integrates generative planning with real-time control. The framework employs a high-level generative planner based on diffusion models to dynamically allocate budgets and constraints by effectively capturing historical context and temporal patterns. We introduce a constraint enforcement mechanism to ensure compliance with specified constraints, along with a trajectory refinement mechanism that enhances adaptability to environmental changes through the utilization of historical data. The system further incorporates a control-based bidding algorithm that synergistically combines historical knowledge with real-time information, significantly improving both adaptability and operational efficacy. Extensive experiments conducted on large-scale offline datasets and through online A/B tests demonstrate the effectiveness of AHBid, yielding a 13.57% increase in overall return compared to existing baselines.", "AI": {"tldr": "AHBid is a hierarchical auto-bidding system that pairs a diffusion-based generative planner for multi-channel budget/constraint allocation with a real-time control algorithm, adding constraint enforcement and trajectory refinement. It captures historical/temporal patterns and adapts online, yielding a 13.57% higher overall return than baselines in offline and online tests.", "motivation": "Online ad markets are complex, dynamic, and often span multiple channels with different behaviors, making budget and constraint allocation hard. Optimization methods are brittle to market shifts, while standard RL under MDP assumptions struggles to exploit rich historical dependencies and observational patterns. A method is needed that both leverages long-range history and adapts in real time while honoring constraints.", "method": "Proposes AHBid, an Adaptable Hierarchical Bidding framework: (1) a high-level generative planner using diffusion models to produce budget/constraint allocations that encode historical context and temporal patterns; (2) a constraint enforcement module to ensure adherence to specified limits; (3) a trajectory refinement mechanism that updates plans using historical data to stay adaptive; and (4) a low-level control-based bidding policy that fuses historical knowledge with live signals for execution.", "result": "On large-scale offline datasets and in online A/B testing, AHBid improves overall return by 13.57% relative to existing baselines, indicating stronger adaptability and operational performance.", "conclusion": "Combining generative planning (diffusion) with real-time control under a hierarchical design effectively addresses multi-channel auto-bidding, capturing long-term dependencies while remaining responsive and constraint-compliant. The approach achieves significant ROI gains and suggests a practical path beyond rigid optimization or purely RL-based methods."}}
{"id": "2602.22629", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22629", "abs": "https://arxiv.org/abs/2602.22629", "authors": ["Zeyu Jiang", "Sihang Li", "Siqi Tan", "Chenyang Xu", "Juexiao Zhang", "Julia Galway-Witham", "Xue Wang", "Scott A. Williams", "Radu Iovita", "Chen Feng", "Jing Zhang"], "title": "CRAG: Can 3D Generative Models Help 3D Assembly?", "comment": "10 pages, 7 figures", "summary": "Most existing 3D assembly methods treat the problem as pure pose estimation, rearranging observed parts via rigid transformations. In contrast, human assembly naturally couples structural reasoning with holistic shape inference. Inspired by this intuition, we reformulate 3D assembly as a joint problem of assembly and generation. We show that these two processes are mutually reinforcing: assembly provides part-level structural priors for generation, while generation injects holistic shape context that resolves ambiguities in assembly. Unlike prior methods that cannot synthesize missing geometry, we propose CRAG, which simultaneously generates plausible complete shapes and predicts poses for input parts. Extensive experiments demonstrate state-of-the-art performance across in-the-wild objects with diverse geometries, varying part counts, and missing pieces. Our code and models will be released.", "AI": {"tldr": "CRAG jointly assembles 3D parts and generates missing geometry, using mutual reinforcement between part-level structure and holistic shape context to achieve state-of-the-art performance on diverse, in-the-wild objects.", "motivation": "Prior 3D assembly methods treat the task as pure rigid pose estimation, lacking holistic shape reasoning and unable to synthesize missing pieces, which leads to ambiguities and failures on real, incomplete, or variable-part objects. Human assembly couples structural reasoning with global shape inference\u2014this work seeks to emulate that.", "method": "Reformulate 3D assembly as a joint assembly-and-generation problem. Propose CRAG, which simultaneously predicts part poses (assembly) and synthesizes a plausible complete shape (generation). The two tasks are coupled: assembly provides part-level structural priors to guide generation, while generation supplies holistic shape context to resolve pose ambiguities and handle missing geometry.", "result": "Extensive experiments show state-of-the-art performance across in-the-wild objects with diverse geometries, variable part counts, and missing pieces, outperforming prior pose-only baselines.", "conclusion": "Coupling assembly with generation is mutually beneficial: structural priors enhance shape synthesis, and holistic context improves pose estimation and robustness to missing parts. CRAG surpasses prior methods and can complete shapes while assembling; code and models will be released."}}
{"id": "2602.22828", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22828", "abs": "https://arxiv.org/abs/2602.22828", "authors": ["Jianmin Li", "Ying Chang", "Su-Kit Tang", "Yujia Liu", "Yanwen Wang", "Shuyuan Lin", "Binkai Ou"], "title": "TCM-DiffRAG: Personalized Syndrome Differentiation Reasoning Method for Traditional Chinese Medicine based on Knowledge Graph and Chain of Thought", "comment": null, "summary": "Background: Retrieval augmented generation (RAG) technology can empower large language models (LLMs) to generate more accurate, professional, and timely responses without fine tuning. However, due to the complex reasoning processes and substantial individual differences involved in traditional Chinese medicine (TCM) clinical diagnosis and treatment, traditional RAG methods often exhibit poor performance in this domain. Objective: To address the limitations of conventional RAG approaches in TCM applications, this study aims to develop an improved RAG framework tailored to the characteristics of TCM reasoning. Methods: We developed TCM-DiffRAG, an innovative RAG framework that integrates knowledge graphs (KG) with chains of thought (CoT). TCM-DiffRAG was evaluated on three distinctive TCM test datasets. Results: The experimental results demonstrated that TCM-DiffRAG achieved significant performance improvements over native LLMs. For example, the qwen-plus model achieved scores of 0.927, 0.361, and 0.038, which were significantly enhanced to 0.952, 0.788, and 0.356 with TCM-DiffRAG. The improvements were even more pronounced for non-Chinese LLMs. Additionally, TCM-DiffRAG outperformed directly supervised fine-tuned (SFT) LLMs and other benchmark RAG methods. Conclusions: TCM-DiffRAG shows that integrating structured TCM knowledge graphs with Chain of Thought based reasoning substantially improves performance in individualized diagnostic tasks. The joint use of universal and personalized knowledge graphs enables effective alignment between general knowledge and clinical reasoning. These results highlight the potential of reasoning-aware RAG frameworks for advancing LLM applications in traditional Chinese medicine.", "AI": {"tldr": "Proposes TCM-DiffRAG, a RAG framework that fuses TCM knowledge graphs with chain-of-thought reasoning, yielding sizable gains over base LLMs, SFT, and standard RAG on three TCM benchmarks.", "motivation": "Conventional RAG underperforms in TCM because diagnosis involves complex, individualized reasoning and heterogeneous knowledge; a mechanism is needed to align retrieval with both universal TCM knowledge and patient-specific reasoning paths.", "method": "Integrate structured TCM knowledge graphs\u2014both universal and personalized\u2014with chain-of-thought guided reasoning to steer retrieval and generation; evaluate the approach (TCM-DiffRAG) on three distinct TCM datasets and compare against native LLMs, SFT, and common RAG baselines.", "result": "On qwen-plus, scores improved from 0.927/0.361/0.038 to 0.952/0.788/0.356 across three datasets; larger gains observed for non-Chinese LLMs; the method surpassed SFT and other RAG variants.", "conclusion": "Combining structured TCM KGs with CoT improves individualized TCM diagnostic reasoning and better aligns general knowledge with clinical decision processes; reasoning-aware RAG is a promising direction for TCM applications."}}
{"id": "2602.22680", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22680", "abs": "https://arxiv.org/abs/2602.22680", "authors": ["Yue Xu", "Qian Chen", "Zizhan Ma", "Dongrui Liu", "Wenxuan Wang", "Xiting Wang", "Li Xiong", "Wenjie Wang"], "title": "Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions", "comment": null, "summary": "Large language models have enabled agents that reason, plan, and interact with tools and environments to accomplish complex tasks. As these agents operate over extended interaction horizons, their effectiveness increasingly depends on adapting behavior to individual users and maintaining continuity across time, giving rise to personalized LLM-powered agents. In such long-term, user-dependent settings, personalization permeates the entire decision pipeline rather than remaining confined to surface-level generation. This survey provides a capability-oriented review of personalized LLM-powered agents. We organize the literature around four interdependent components: profile modeling, memory, planning, and action execution. Using this taxonomy, we synthesize representative methods and analyze how user signals are represented, propagated, and utilized, highlighting cross-component interactions and recurring design trade-offs. We further examine evaluation metrics and benchmarks tailored to personalized agents, summarize application scenarios spanning general assistance to specialized domains, and outline future directions for research and deployment. By offering a structured framework for understanding and designing personalized LLM-powered agents, this survey charts a roadmap toward more user-aligned, adaptive, robust, and deployable agentic systems, accelerating progress from prototype personalization to scalable real-world assistants.", "AI": {"tldr": "Survey of personalized LLM-powered agents that proposes a four-part taxonomy (profile modeling, memory, planning, action execution), synthesizes methods and cross-component trade-offs, reviews evaluations/benchmarks and applications, and outlines a roadmap toward scalable, user-aligned assistants.", "motivation": "LLM agents operate over long horizons where effectiveness depends on adapting to individual users and maintaining continuity; personalization must permeate the full decision pipeline rather than just surface-level text generation, motivating a structured synthesis of the field.", "method": "Capability-oriented literature review organized around four interdependent components: profile modeling, memory, planning, and action execution. The survey analyzes how user signals are represented, propagated, and utilized; synthesizes representative methods; highlights cross-component interactions and recurring design trade-offs; and examines evaluation metrics/benchmarks and application scenarios, culminating in future directions.", "result": "A taxonomy and structured framework that maps techniques across the four components, clarifies how user signals flow through the pipeline, enumerates design trade-offs, compiles domain-specific evaluations/benchmarks, and distills application patterns\u2014providing an integrative view of personalized LLM agents.", "conclusion": "The framework and synthesis aim to guide the creation of more user-aligned, adaptive, robust, and deployable agentic systems, accelerating the transition from prototype personalization to scalable real-world assistants."}}
{"id": "2602.22639", "categories": ["cs.CV", "math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.22639", "abs": "https://arxiv.org/abs/2602.22639", "authors": ["Daniel Miao", "Gilad Lerman", "Joe Kileel"], "title": "QuadSync: Quadrifocal Tensor Synchronization via Tucker Decomposition", "comment": "30 pages, accepted to CVPR 2026", "summary": "In structure from motion, quadrifocal tensors capture more information than their pairwise counterparts (essential matrices), yet they have often been thought of as impractical and only of theoretical interest. In this work, we challenge such beliefs by providing a new framework to recover $n$ cameras from the corresponding collection of quadrifocal tensors. We form the block quadrifocal tensor and show that it admits a Tucker decomposition whose factor matrices are the stacked camera matrices, and which thus has a multilinear rank of (4,~4,~4,~4) independent of $n$. We develop the first synchronization algorithm for quadrifocal tensors, using Tucker decomposition, alternating direction method of multipliers, and iteratively reweighted least squares. We further establish relationships between the block quadrifocal, trifocal, and bifocal tensors, and introduce an algorithm that jointly synchronizes these three entities. Numerical experiments demonstrate the effectiveness of our methods on modern datasets, indicating the potential and importance of using higher-order information in synchronization.", "AI": {"tldr": "They introduce a practical, scalable framework to recover many cameras directly from quadrifocal tensors by assembling a block quadrifocal tensor that admits a Tucker decomposition with rank (4,4,4,4), and design the first synchronization algorithms (including a joint bi/tri/quad-focal variant) that perform well on modern datasets.", "motivation": "Although quadrifocal tensors encode richer multi-view constraints than pairwise essential matrices, they have been deemed impractical for real use. The paper aims to exploit this higher-order information to improve multi-camera synchronization and reconstruction in structure-from-motion.", "method": "1) Construct a block quadrifocal tensor across cameras and prove it has a Tucker decomposition whose factor matrices are the stacked camera matrices, with multilinear rank (4,4,4,4) independent of the number of cameras. 2) Develop a synchronization pipeline leveraging Tucker decomposition, ADMM, and IRLS to robustly estimate camera matrices from quadrifocal data. 3) Derive links among block quadrifocal, trifocal, and bifocal tensors and propose a joint synchronization procedure that fuses all three.", "result": "Theory: establishes the Tucker structure and rank of the block quadrifocal tensor and its relationships to tri-/bi-focal tensors. Practice: the proposed (quad-only and joint) synchronization algorithms show strong empirical performance on contemporary datasets, indicating robustness and accuracy gains from higher-order cues.", "conclusion": "Quadrifocal-based synchronization is feasible and effective. The Tucker-decomposition framework enables recovering many cameras from higher-order constraints, and jointly leveraging bi/tri/quad-focal information further improves synchronization, advocating broader use of higher-order tensors in SfM."}}
{"id": "2602.22846", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22846", "abs": "https://arxiv.org/abs/2602.22846", "authors": ["Mohammad Yeghaneh Abkenar", "Weixing Wang", "Manfred Stede", "Davide Picca", "Mark A. Finlayson", "Panagiotis Ioannidis"], "title": "Improving Neural Argumentative Stance Classification in Controversial Topics with Emotion-Lexicon Features", "comment": null, "summary": "Argumentation mining comprises several subtasks, among which stance classification focuses on identifying the standpoint expressed in an argumentative text toward a specific target topic. While arguments-especially about controversial topics-often appeal to emotions, most prior work has not systematically incorporated explicit, fine-grained emotion analysis to improve performance on this task. In particular, prior research on stance classification has predominantly utilized non-argumentative texts and has been restricted to specific domains or topics, limiting generalizability. We work on five datasets from diverse domains encompassing a range of controversial topics and present an approach for expanding the Bias-Corrected NRC Emotion Lexicon using DistilBERT embeddings, which we feed into a Neural Argumentative Stance Classification model. Our method systematically expands the emotion lexicon through contextualized embeddings to identify emotionally charged terms not previously captured in the lexicon. Our expanded NRC lexicon (eNRC) improves over the baseline across all five datasets (up to +6.2 percentage points in F1 score), outperforms the original NRC on four datasets (up to +3.0), and surpasses the LLM-based approach on nearly all corpora. We provide all resources-including eNRC, the adapted corpora, and model architecture-to enable other researchers to build upon our work.", "AI": {"tldr": "They expand a bias-corrected NRC Emotion Lexicon via DistilBERT embeddings to capture fine-grained emotions and plug it into a neural stance classifier, yielding consistent F1 gains across five diverse argumentative datasets and beating both the original NRC and most LLM-based baselines; all resources are released.", "motivation": "Stance classification for argumentative texts often ignores explicit, fine-grained emotions, despite their central role in controversial debates. Prior work relies on non-argumentative data or narrow domains, hurting generalizability. The authors aim to systematically inject high-quality emotion signals to improve stance detection across topics and domains.", "method": "Start from the Bias-Corrected NRC Emotion Lexicon and expand it using contextualized DistilBERT embeddings to discover additional emotionally charged terms. Use the expanded lexicon (eNRC) as emotion features within a Neural Argumentative Stance Classification model. Evaluate on five heterogeneous, controversy-focused datasets.", "result": "eNRC improves F1 over baselines on all five datasets (up to +6.2 points), outperforms the original NRC lexicon on four datasets (up to +3.0), and exceeds an LLM-based approach on nearly all corpora.", "conclusion": "Embedding-driven lexicon expansion provides effective, generalizable emotion features that enhance argumentative stance classification. Fine-grained, explicit emotion modeling yields consistent gains across domains, and the released eNRC, adapted corpora, and model architecture support further research."}}
{"id": "2602.22702", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22702", "abs": "https://arxiv.org/abs/2602.22702", "authors": ["Siyu Jiang", "Sanshuai Cui", "Hui Zeng"], "title": "Knob: A Physics-Inspired Gating Interface for Interpretable and Controllable Neural Dynamics", "comment": null, "summary": "Existing neural network calibration methods often treat calibration as a static, post-hoc optimization task. However, this neglects the dynamic and temporal nature of real-world inference. Moreover, existing methods do not provide an intuitive interface enabling human operators to dynamically adjust model behavior under shifting conditions. In this work, we propose Knob, a framework that connects deep learning with classical control theory by mapping neural gating dynamics to a second-order mechanical system. By establishing correspondences between physical parameters -- damping ratio ($\u03b6$) and natural frequency ($\u03c9_n$) -- and neural gating, we create a tunable \"safety valve\". The core mechanism employs a logit-level convex fusion, functioning as an input-adaptive temperature scaling. It tends to reduce model confidence particularly when model branches produce conflicting predictions. Furthermore, by imposing second-order dynamics (Knob-ODE), we enable a \\textit{dual-mode} inference: standard i.i.d. processing for static tasks, and state-preserving processing for continuous streams. Our framework allows operators to tune \"stability\" and \"sensitivity\" through familiar physical analogues. This paper presents an exploratory architectural interface; we focus on demonstrating the concept and validating its control-theoretic properties rather than claiming state-of-the-art calibration performance. Experiments on CIFAR-10-C validate the calibration mechanism and demonstrate that, in Continuous Mode, the gate responses are consistent with standard second-order control signatures (step settling and low-pass attenuation), paving the way for predictable human-in-the-loop tuning.", "AI": {"tldr": "Knob introduces a control-theoretic \u201csafety valve\u201d for neural networks by mapping gating to a second\u2011order system with tunable damping (zeta) and natural frequency (omega_n). It applies a logit\u2011level, input\u2011adaptive temperature scaling and an ODE-based stateful mode, yielding predictable, human\u2011tunable calibration behavior; validated on CIFAR\u201110\u2011C with control\u2011style response signatures rather than SOTA metrics.", "motivation": "Post\u2011hoc calibration is typically static and ignores temporal dynamics and operator needs for real\u2011time adjustability under distribution shift. The paper seeks an intuitive, physically grounded interface that lets humans tune model confidence and responsiveness during streaming or changing conditions.", "method": "Map neural gating dynamics to a second\u2011order mechanical system parameterized by damping ratio (zeta) and natural frequency (omega_n). Implement a convex fusion at the logit level acting as input\u2011adaptive temperature scaling that down\u2011weights confidence when branch predictions conflict. Add second\u2011order dynamics (Knob\u2011ODE) to enable dual\u2011mode inference: standard i.i.d. (stateless) and continuous, state\u2011preserving processing. Expose zeta/omega_n as operator knobs for stability/sensitivity.", "result": "On CIFAR\u201110\u2011C, the approach demonstrates calibrated behavior and, in Continuous Mode, gate responses that match classical second\u2011order signatures (step settling, low\u2011pass attenuation). The method especially reduces confidence when branches disagree. The work emphasizes validation of control\u2011theoretic properties over achieving SOTA calibration scores.", "conclusion": "Knob provides a practical, interpretable interface linking deep nets with control theory, enabling predictable, human\u2011tunable calibration in both static and streaming settings. Evidence supports the control\u2011style behavior of the gating dynamics; broader benchmarking and optimization would be needed to claim SOTA performance."}}
{"id": "2602.22644", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22644", "abs": "https://arxiv.org/abs/2602.22644", "authors": ["Siqi Lu", "Wanying Xu", "Yongbin Zheng", "Wenting Luan", "Peng Sun", "Jianhang Yao"], "title": "Plug, Play, and Fortify: A Low-Cost Module for Robust Multimodal Image Understanding Models", "comment": null, "summary": "Missing modalities present a fundamental challenge in multimodal models, often causing catastrophic performance degradation. Our observations suggest that this fragility stems from an imbalanced learning process, where the model develops an implicit preference for certain modalities, leading to the under-optimization of others. We propose a simple yet efficient method to address this challenge. The central insight of our work is that the dominance relationship between modalities can be effectively discerned and quantified in the frequency domain. To leverage this principle, we first introduce a Frequency Ratio Metric (FRM) to quantify modality preference by analyzing features in the frequency domain. Guided by FRM, we then propose a Multimodal Weight Allocation Module, a plug-and-play component that dynamically re-balances the contribution of each branch during training, promoting a more holistic learning paradigm. Extensive experiments demonstrate that MWAM can be seamlessly integrated into diverse architectural backbones, such as those based on CNNs and ViTs. Furthermore, MWAM delivers consistent performance gains across a wide range of tasks and modality combinations. This advancement extends beyond merely optimizing the performance of the base model; it also manifests as further performance improvements to state-of-the-art methods addressing the missing modality problem.", "AI": {"tldr": "They detect and correct modality dominance using frequency-domain signals: a Frequency Ratio Metric (FRM) measures which modality is being over- or under-optimized, and a plug-and-play Multimodal Weight Allocation Module (MWAM) dynamically reweights branches during training, yielding robust gains across backbones, tasks, and missing-modality settings.", "motivation": "Multimodal models collapse when some modalities are missing because training becomes imbalanced\u2014models implicitly favor certain modalities, leaving others under-optimized. The authors aim to diagnose and fix this imbalance to improve robustness and overall performance.", "method": "1) Analyze feature representations in the frequency domain and introduce a Frequency Ratio Metric (FRM) to quantify modality preference/dominance. 2) Use FRM to guide a Multimodal Weight Allocation Module (MWAM) that dynamically rebalances contribution of each modality branch during training. MWAM is plug-and-play and compatible with CNN- and ViT-based backbones.", "result": "Across diverse architectures (CNNs, ViTs), tasks, and modality combinations, integrating MWAM consistently improves performance. It enhances base models and further boosts state-of-the-art methods tailored to missing-modality scenarios, according to extensive experiments reported by the authors.", "conclusion": "Frequency-aware diagnosis of modality dominance and adaptive reweighting during training mitigates fragility to missing modalities, promotes more holistic multimodal learning, and delivers consistent, architecture-agnostic performance gains."}}
{"id": "2602.22865", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22865", "abs": "https://arxiv.org/abs/2602.22865", "authors": ["Jonathan Davidov", "Aviv Slobodkin", "Shmuel Tomi Klein", "Reut Tsarfaty", "Ido Dagan", "Ayal Klein"], "title": "Effective QA-driven Annotation of Predicate-Argument Relations Across Languages", "comment": "Accepted to EACL 2026 (Main Conference)", "summary": "Explicit representations of predicate-argument relations form the basis of interpretable semantic analysis, supporting reasoning, generation, and evaluation. However, attaining such semantic structures requires costly annotation efforts and has remained largely confined to English. We leverage the Question-Answer driven Semantic Role Labeling (QA-SRL) framework -- a natural-language formulation of predicate-argument relations -- as the foundation for extending semantic annotation to new languages. To this end, we introduce a cross-linguistic projection approach that reuses an English QA-SRL parser within a constrained translation and word-alignment pipeline to automatically generate question-answer annotations aligned with target-language predicates. Applied to Hebrew, Russian, and French -- spanning diverse language families -- the method yields high-quality training data and fine-tuned, language-specific parsers that outperform strong multilingual LLM baselines (GPT-4o, LLaMA-Maverick). By leveraging QA-SRL as a transferable natural-language interface for semantics, our approach enables efficient and broadly accessible predicate-argument parsing across languages.", "AI": {"tldr": "Project English QA-SRL via constrained translation and word alignment to create supervision for new languages, then fine-tune language-specific parsers that beat strong multilingual LLMs on Hebrew, Russian, and French.", "motivation": "Explicit predicate\u2013argument structures enable interpretable reasoning and generation, but manual semantic annotation is expensive and largely limited to English. A scalable, low-cost way to extend such resources to other languages is needed.", "method": "Use QA-SRL as a language-agnostic, natural-language interface to semantics. Run a strong English QA-SRL parser, translate with constraints, align words, and project Q\u2013A annotations to target-language predicates to auto-generate training data; then fine-tune target-language QA-SRL parsers.", "result": "On Hebrew, Russian, and French, the pipeline produces high-quality training data and yields fine-tuned parsers that outperform powerful multilingual LLM baselines (GPT-4o, LLaMA-Maverick).", "conclusion": "QA-SRL serves as an effective cross-lingual bridge, enabling efficient, broadly accessible predicate\u2013argument parsing beyond English via projection-based supervision."}}
{"id": "2602.22718", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.22718", "abs": "https://arxiv.org/abs/2602.22718", "authors": ["Rui Wei", "Hanfei Yu", "Shubham Jain", "Yogarajan Sivakumar", "Devesh Tiwari", "Jian Li", "Seung-Jong Park", "Hao Wang"], "title": "RLHFless: Serverless Computing for Efficient RLHF", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF's potential to improve LLM reasoning on complex tasks. In RL, inference and training co-exist, creating dynamic resource demands throughout the workflow. Compared to traditional RL, RLHF further challenges training efficiency due to expanding model sizes and resource consumption. Several RLHF frameworks aim to balance flexible abstraction and efficient execution. However, they rely on serverful infrastructures, which struggle with fine-grained resource variability. As a result, during synchronous RLHF training, idle time between or within RL components often causes overhead and resource wastage.\n  To address these issues, we present RLHFless, the first scalable training framework for synchronous RLHF, built on serverless computing environments. RLHFless adapts to dynamic resource demands throughout the RLHF pipeline, pre-computes shared prefixes to avoid repeated computation, and uses a cost-aware actor scaling strategy that accounts for response length variation to find sweet spots with lower cost and higher speed. In addition, RLHFless assigns workloads efficiently to reduce intra-function imbalance and idle time. Experiments on both physical testbeds and a large-scale simulated cluster show that RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to the state-of-the-art baseline.", "AI": {"tldr": "RLHFless is a serverless, scalable framework for synchronous RLHF training that dynamically adapts resources, caches shared prefixes, and employs cost-aware actor scaling and load balancing, delivering up to 1.35\u00d7 speedup and 44.8% cost savings over a state-of-the-art baseline.", "motivation": "RLHF training for LLMs has bursty, highly variable compute needs across rollout, reward, and update stages. Serverful clusters handle this poorly, causing idle time, imbalance, and high cost\u2014problems exacerbated by growing model sizes and longer responses.", "method": "Build RLHF training atop serverless infrastructure to elastically match stage-by-stage demand; pre-compute and reuse shared prefixes to eliminate redundant compute; introduce a cost-aware actor scaling policy that models response-length variability to pick throughput/cost sweet spots; and implement fine-grained workload assignment to reduce intra-function imbalance and idle time. Focus is on synchronous RLHF pipelines.", "result": "On physical testbeds and a large-scale simulated cluster, RLHFless yields up to 1.35\u00d7 speedup and 44.8% lower cost versus a prior state-of-the-art RLHF framework.", "conclusion": "Serverless orchestration plus targeted optimizations (prefix reuse, cost-aware scaling, better load balancing) can make synchronous RLHF training faster and cheaper, suggesting a viable path to scalable, cost-efficient RLHF for large models."}}
{"id": "2602.22649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22649", "abs": "https://arxiv.org/abs/2602.22649", "authors": ["Woojae Hong", "Jong Ha Hwang", "Jiyong Chung", "Joongyeon Choi", "Hyunngun Kim", "Yong Hwy Kim"], "title": "Interactive Medical-SAM2 GUI: A Napari-based semi-automatic annotation tool for medical images", "comment": "6 pages, 2 figures, Planning to submit JOSS (Journal of Open Source Software)", "summary": "Interactive Medical-SAM2 GUI is an open-source desktop application for semi-automatic annotation of 2D and 3D medical images. Built on the Napari multi-dimensional viewer, box/point prompting is integrated with SAM2-style propagation by treating a 3D volume as a slice sequence, enabling mask propagation from sparse prompts using Medical-SAM2 on top of SAM2. Voxel-level annotation remains essential for developing and validating medical imaging algorithms, yet manual labeling is slow and expensive for 3D scans, and existing integrations frequently emphasize per-slice interaction without providing a unified, cohort-oriented workflow for navigation, propagation, interactive correction, and quantitative export in a single local pipeline. To address this practical limitation, a local-first Napari workflow is provided for efficient 3D annotation across multiple studies using standard DICOM series and/or NIfTI volumes. Users can annotate cases sequentially under a single root folder with explicit proceed/skip actions, initialize objects via box-first prompting (including first/last-slice initialization for single-object propagation), refine predictions with point prompts, and finalize labels through prompt-first correction prior to saving. During export, per-object volumetry and 3D volume rendering are supported, and image geometry is preserved via SimpleITK. The GUI is implemented in Python using Napari and PyTorch, with optional N4 bias-field correction, and is intended exclusively for research annotation workflows. The code is released on the project page: https://github.com/SKKU-IBE/Medical-SAM2GUI/.", "AI": {"tldr": "Open-source Napari-based GUI for semi-automatic 2D/3D medical image annotation that integrates box/point prompts with SAM2-style propagation across slices, enabling efficient cohort-level labeling, volumetry, and export in a local workflow. Code: https://github.com/SKKU-IBE/Medical-SAM2GUI/.", "motivation": "Voxel-level labels are vital for developing/validating medical imaging algorithms, but manual 3D annotation is slow and costly. Existing tools often force per-slice interaction and lack a unified, local, cohort-oriented workflow that combines navigation, propagation, correction, and quantitative export.", "method": "Implements a local-first workflow in Napari that treats a 3D volume as a slice sequence and uses Medical-SAM2 on top of SAM2 for mask propagation from sparse box/point prompts. Provides case-by-case processing from a root folder (proceed/skip), box-first initialization (including first/last-slice setup for single-object propagation), point-based refinement, and prompt-first correction before saving. Supports DICOM and NIfTI input, optional N4 bias-field correction, per-object volumetry, 3D rendering, and geometry-preserving export via SimpleITK. Implemented in Python with Napari and PyTorch.", "result": "Delivers a working desktop GUI that enables semi-automatic 3D annotation with propagation, interactive correction, volumetric measurements, 3D visualization, and standards-compliant export. Released as open-source code with a local, privacy-preserving pipeline; no quantitative benchmarks are reported in the abstract.", "conclusion": "The tool provides a unified, cohort-oriented, local annotation workflow that streamlines semi-automatic 3D labeling using SAM2-style propagation and interactive prompting, facilitating efficient research annotations while preserving image geometry and supporting quantitative outputs."}}
{"id": "2602.22868", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22868", "abs": "https://arxiv.org/abs/2602.22868", "authors": ["Yushi Ye", "Feng Hong", "Huangjie Zheng", "Xu Chen", "Zhiyong Chen", "Yanfeng Wang", "Jiangchao Yao"], "title": "Rejection Mixing: Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference", "comment": null, "summary": "Diffusion Large Language Models (DLLMs) promise fast non-autoregressive inference but suffer a severe quality-speed trade-off in parallel decoding. This stems from the ''combinatorial contradiction'' phenomenon, where parallel tokens form semantically inconsistent combinations. We address this by integrating continuous representations into the discrete decoding process, as they preserve rich inter-position dependency. We propose ReMix (Rejection Mixing), a framework that introduces a novel Continuous Mixing State as an intermediate between the initial masked state and the final decoded token state. This intermediate state allows a token's representation to be iteratively refined in a continuous space, resolving mutual conflicts with other tokens before collapsing into a final discrete sample. Furthermore, a rejection rule reverts uncertain representations from the continuous state back to the masked state for reprocessing, ensuring stability and preventing error propagation. ReMix thus mitigates combinatorial contradictions by enabling continuous-space refinement during discrete diffusion decoding. Extensive experiments demonstrate that ReMix, as a training-free method, achieves a $2-8 \\times$ inference speedup without any quality degradation.", "AI": {"tldr": "ReMix augments discrete diffusion decoding for DLLMs with an intermediate continuous state and a reject-and-reprocess rule, resolving inter-token conflicts to retain quality while enabling 2\u20138\u00d7 faster parallel inference.", "motivation": "Parallel, non-autoregressive decoding in diffusion LLMs suffers from \u201ccombinatorial contradiction,\u201d where simultaneously sampled tokens form semantically inconsistent sequences, creating a quality\u2013speed trade-off.", "method": "Introduce a Continuous Mixing State between masked and final discrete token states so each position\u2019s representation can be iteratively refined in continuous space to harmonize with others; uncertain positions are rejected back to the masked state for further refinement. The approach is training-free and plugs into discrete diffusion decoding.", "result": "Across experiments, ReMix reportedly achieves 2\u20138\u00d7 inference speedups without measurable quality loss compared to baselines, indicating better stability and fewer contradictory token combinations.", "conclusion": "Continuous-space refinement plus selective rejection stabilizes parallel DLLM decoding, mitigating semantic inconsistencies and delivering substantial speedups at iso-quality without retraining, suggesting a practical, drop-in decoding improvement."}}
{"id": "2602.22743", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22743", "abs": "https://arxiv.org/abs/2602.22743", "authors": ["Jiaqing Zhang", "Mingjia Yin", "Hao Wang", "Yuxin Tian", "Yuyang Ye", "Yawen Li", "Wei Guo", "Yong Liu", "Enhong Chen"], "title": "Generative Data Transformation: From Mixed to Unified Data", "comment": "Accepted by The Web Conference 2026 (WWW '26)", "summary": "Recommendation model performance is intrinsically tied to the quality, volume, and relevance of their training data. To address common challenges like data sparsity and cold start, recent researchs have leveraged data from multiple auxiliary domains to enrich information within the target domain. However, inherent domain gaps can degrade the quality of mixed-domain data, leading to negative transfer and diminished model performance. Existing prevailing \\emph{model-centric} paradigm -- which relies on complex, customized architectures -- struggles to capture the subtle, non-structural sequence dependencies across domains, leading to poor generalization and high demands on computational resources. To address these shortcomings, we propose \\textsc{Taesar}, a \\emph{data-centric} framework for \\textbf{t}arget-\\textbf{a}lign\\textbf{e}d \\textbf{s}equenti\\textbf{a}l \\textbf{r}egeneration, which employs a contrastive decoding mechanism to adaptively encode cross-domain context into target-domain sequences. It employs contrastive decoding to encode cross-domain context into target sequences, enabling standard models to learn intricate dependencies without complex fusion architectures. Experiments show \\textsc{Taesar} outperforms model-centric solutions and generalizes to various sequential models. By generating enriched datasets, \\textsc{Taesar} effectively combines the strengths of data- and model-centric paradigms. The code accompanying this paper is available at~ \\textcolor{blue}{https://github.com/USTC-StarTeam/Taesar}.", "AI": {"tldr": "Taesar is a data-centric framework that regenerates target-domain recommendation sequences using contrastive decoding to infuse cross-domain context, boosting performance without complex fusion architectures.", "motivation": "Multi-domain data can alleviate sparsity and cold-start in recommendations, but domain gaps cause negative transfer. Prevailing model-centric solutions are complex, resource-heavy, and fail to capture subtle cross-domain sequence dependencies.", "method": "Target-aligned sequential regeneration: use contrastive decoding to adaptively encode auxiliary-domain context into target-domain sequences, producing enriched training data that standard sequential recommenders can learn from\u2014avoiding customized fusion models.", "result": "Across experiments, Taesar outperforms model-centric baselines and transfers well across various sequential recommendation models.", "conclusion": "By generating enriched, target-aligned datasets, Taesar mitigates domain-gap issues, reduces reliance on complex architectures, and effectively blends data- and model-centric strengths."}}
{"id": "2602.22654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22654", "abs": "https://arxiv.org/abs/2602.22654", "authors": ["Bowen Cui", "Yuanbin Wang", "Huajiang Xu", "Biaolong Chen", "Aixi Zhang", "Hao Jiang", "Zhengzheng Jin", "Xu Liu", "Pipei Huang"], "title": "Denoising as Path Planning: Training-Free Acceleration of Diffusion Models with DPCache", "comment": "Accepted by CVPR 2026", "summary": "Diffusion models have demonstrated remarkable success in image and video generation, yet their practical deployment remains hindered by the substantial computational overhead of multi-step iterative sampling. Among acceleration strategies, caching-based methods offer a training-free and effective solution by reusing or predicting features across timesteps. However, existing approaches rely on fixed or locally adaptive schedules without considering the global structure of the denoising trajectory, often leading to error accumulation and visual artifacts. To overcome this limitation, we propose DPCache, a novel training-free acceleration framework that formulates diffusion sampling acceleration as a global path planning problem. DPCache constructs a Path-Aware Cost Tensor from a small calibration set to quantify the path-dependent error of skipping timesteps conditioned on the preceding key timestep. Leveraging this tensor, DPCache employs dynamic programming to select an optimal sequence of key timesteps that minimizes the total path cost while preserving trajectory fidelity. During inference, the model performs full computations only at these key timesteps, while intermediate outputs are efficiently predicted using cached features. Extensive experiments on DiT, FLUX, and HunyuanVideo demonstrate that DPCache achieves strong acceleration with minimal quality loss, outperforming prior acceleration methods by $+$0.031 ImageReward at 4.87$\\times$ speedup and even surpassing the full-step baseline by $+$0.028 ImageReward at 3.54$\\times$ speedup on FLUX, validating the effectiveness of our path-aware global scheduling framework. Code will be released at https://github.com/argsss/DPCache.", "AI": {"tldr": "DPCache accelerates diffusion sampling without retraining by globally selecting key timesteps via dynamic programming on a path-aware cost tensor, then caching/predicting intermediate features\u2014achieving ~3.5\u20134.9\u00d7 speedups with equal or better quality than full-step baselines.", "motivation": "Multi-step diffusion sampling is computationally heavy. Existing caching-based accelerations use fixed or locally adaptive schedules that ignore the global denoising trajectory, causing accumulated errors and artifacts. A global, path-aware schedule is needed to minimize end-to-end deviation while retaining quality.", "method": "1) Build a small calibration set. 2) Construct a Path-Aware Cost Tensor that estimates the error of skipping from one key timestep to another, conditioned on the previous key step. 3) Use dynamic programming to find the minimum-cost sequence of key timesteps (global schedule). 4) At inference, run full computations only at these key timesteps; predict intermediate steps efficiently using cached features. Training-free; applicable to DiT, FLUX, HunyuanVideo.", "result": "Across models, DPCache outperforms prior acceleration baselines; on FLUX it delivers +0.031 ImageReward at 4.87\u00d7 speedup and even exceeds full-step quality by +0.028 ImageReward at 3.54\u00d7 speedup, indicating improved efficiency without sacrificing fidelity.", "conclusion": "Global, path-aware scheduling mitigates error accumulation in caching-based diffusion acceleration, enabling significant speedups with minimal or negative quality loss and generalizing across architectures. The approach is practical (training-free) and supported by public code."}}
{"id": "2602.22871", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22871", "abs": "https://arxiv.org/abs/2602.22871", "authors": ["Roy Miles", "Aysim Toker", "Andreea-Maria Oncescu", "Songcen Xu", "Jiankang Deng", "Ismail Elezi"], "title": "Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching", "comment": null, "summary": "Reasoning with large language models often benefits from generating multiple chains-of-thought, but existing aggregation strategies are typically trajectory-level (e.g., selecting the best trace or voting on the final answer), discarding useful intermediate work from partial or \"nearly correct\" attempts. We propose Stitching Noisy Diffusion Thoughts, a self-consistency framework that turns cheap diffusion-sampled reasoning into a reusable pool of step-level candidates. Given a problem, we (i) sample many diverse, low-cost reasoning trajectories using a masked diffusion language model, (ii) score every intermediate step with an off-the-shelf process reward model (PRM), and (iii) stitch these highest-quality steps across trajectories into a composite rationale. This rationale then conditions an autoregressive (AR) model (solver) to recompute only the final answer. This modular pipeline separates exploration (diffusion) from evaluation and solution synthesis, avoiding monolithic unified hybrids while preserving broad search. Across math reasoning benchmarks, we find that step-level recombination is most beneficial on harder problems, and ablations highlight the importance of the final AR solver in converting stitched but imperfect rationales into accurate answers. Using low-confidence diffusion sampling with parallel, independent rollouts, our training-free framework improves average accuracy by up to 23.8% across six math and coding tasks. At the same time, it achieves up to a 1.8x latency reduction relative to both traditional diffusion models (e.g., Dream, LLaDA) and unified architectures (e.g., TiDAR). Code is available at https://github.com/roymiles/diffusion-stitching.", "AI": {"tldr": "Generate many diffusion-based reasoning steps, score and stitch the best across trajectories with a process reward model, then let an autoregressive solver finish\u2014yielding up to +23.8% accuracy and 1.8x lower latency on math/coding tasks.", "motivation": "Trajectory-level aggregation (e.g., pick-one-trace or vote-on-final-answer) wastes partially correct intermediate steps from near-miss chains. The goal is to exploit those useful fragments while keeping search broad and cost low, and avoid tightly coupled unified hybrids.", "method": "Training-free, modular, three-stage pipeline: (i) sample diverse, low-cost reasoning trajectories with a masked diffusion language model; (ii) score each intermediate step using an off-the-shelf process reward model (PRM); (iii) stitch the top-scoring steps across trajectories into a composite rationale that conditions an autoregressive (AR) solver to compute only the final answer. Uses low-confidence diffusion sampling with parallel, independent rollouts, separating exploration from evaluation/solution synthesis.", "result": "On six math and coding benchmarks, step-level recombination helps most on harder problems; ablations show the AR solver is critical to turn imperfect stitched rationales into correct answers. Achieves up to 23.8% average accuracy gains and up to 1.8x latency reduction versus traditional diffusion models (Dream, LLaDA) and unified hybrids (TiDAR).", "conclusion": "Stitching PRM-scored, diffusion-sampled steps into a composite rationale provides an efficient, modular self-consistency framework that reuses partial reasoning, improving accuracy and speed without additional training and offering a practical alternative to monolithic unified architectures."}}
{"id": "2602.22751", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22751", "abs": "https://arxiv.org/abs/2602.22751", "authors": ["Qiannian Zhao", "Chen Yang", "Jinhao Jing", "Yunke Zhang", "Xuhui Ren", "Lu Yu", "Shijie Zhang", "Hongzhi Yin"], "title": "Know What You Know: Metacognitive Entropy Calibration for Verifiable RL Reasoning", "comment": null, "summary": "Large reasoning models (LRMs) have emerged as a powerful paradigm for solving complex real-world tasks. In practice, these models are predominantly trained via Reinforcement Learning with Verifiable Rewards (RLVR), yet most existing outcome-only RLVR pipelines rely almost exclusively on a binary correctness signal and largely ignore the model's intrinsic uncertainty. We term this discrepancy the uncertainty-reward mismatch, under which high- and low-uncertainty solutions are treated equivalently, preventing the policy from \"Know What You Know\" and impeding the shift from optimizing for correct answers to optimizing effective reasoning paths. This limitation is especially critical in reasoning-centric tasks such as mathematics and question answering, where performance hinges on the quality of the model's internal reasoning process rather than mere memorization of final answers. To address this, we propose EGPO, a metacognitive entropy calibration framework that explicitly integrates intrinsic uncertainty into RLVR for enhancing LRMs. EGPO estimates per-sample uncertainty using a zero-overhead entropy proxy derived from token-level likelihoods and aligns it with extrinsic correctness through an asymmetric calibration mechanism that preserves correct reasoning while selectively regulating overconfident failures, thereby enabling stable and uncertainty-aware policy optimization. Moreover, EGPO recovers informative learning signals from otherwise degenerate group-based rollouts without modifying the verifier or reward definition. Extensive experiments across multiple benchmarks demonstrate that the proposed EGPO leads to substantial and consistent improvements in reasoning performance, establishing a principled path for advancing LRMs through metacognitive entropy calibration.", "AI": {"tldr": "EGPO introduces metacognitive entropy calibration into RL-with-verifiable-rewards for large reasoning models, aligning intrinsic uncertainty (via a zero-overhead token-likelihood entropy proxy) with binary correctness using an asymmetric scheme to curb overconfident failures, stabilize training, recover signal from group rollouts, and improve reasoning benchmarks.", "motivation": "Outcome-only RLVR pipelines treat high- and low-uncertainty solutions the same, creating an uncertainty\u2013reward mismatch that impedes learning reliable reasoning paths\u2014especially harmful in math and QA where internal reasoning quality matters more than final answers.", "method": "Estimate per-sample uncertainty from token-level likelihoods as an entropy proxy; calibrate it asymmetrically against correctness to preserve confident correct reasoning while penalizing overconfident errors; integrate this calibrated signal into policy optimization. Also leverages this to extract informative gradients from otherwise degenerate group-based rollouts without changing the verifier or reward definition.", "result": "Across multiple reasoning benchmarks, EGPO yields substantial and consistent performance gains and more stable, uncertainty-aware optimization. It also recovers useful learning signals from group-based rollouts where standard pipelines would be uninformative. (Abstract reports qualitative improvements; no specific numbers given.)", "conclusion": "Metacognitive entropy calibration is a principled, practical way to address the uncertainty\u2013reward mismatch in RLVR, enabling LRMs to \u201cknow what they know,\u201d optimize reasoning paths, and achieve better reasoning performance without altering verifiers or rewards."}}
{"id": "2602.22659", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.22659", "abs": "https://arxiv.org/abs/2602.22659", "authors": ["Renyu Yang", "Jian Jin", "Lili Meng", "Meiqin Liu", "Yilin Wang", "Balu Adsumilli", "Weisi Lin"], "title": "Scaling Audio-Visual Quality Assessment Dataset via Crowdsourcing", "comment": "Accepted to ICASSP 2026. 5 pages (main paper) + 8 pages (supplementary material)", "summary": "Audio-visual quality assessment (AVQA) research has been stalled by limitations of existing datasets: they are typically small in scale, with insufficient diversity in content and quality, and annotated only with overall scores. These shortcomings provide limited support for model development and multimodal perception research. We propose a practical approach for AVQA dataset construction. First, we design a crowdsourced subjective experiment framework for AVQA, breaks the constraints of in-lab settings and achieves reliable annotation across varied environments. Second, a systematic data preparation strategy is further employed to ensure broad coverage of both quality levels and semantic scenarios. Third, we extend the dataset with additional annotations, enabling research on multimodal perception mechanisms and their relation to content. Finally, we validate this approach through YT-NTU-AVQ, the largest and most diverse AVQA dataset to date, consisting of 1,620 user-generated audio and video (A/V) sequences. The dataset and platform code are available at https://github.com/renyu12/YT-NTU-AVQ", "AI": {"tldr": "They propose a scalable, crowdsourced framework to build rich, diverse audio\u2011visual quality assessment (AVQA) datasets and validate it by releasing YT\u2011NTU\u2011AVQ, currently the largest AVQA dataset (1,620 UGC A/V clips) with extended annotations and code.", "motivation": "Existing AVQA datasets are small, lack content/quality diversity, and only provide overall quality scores, limiting the development of robust models and the study of multimodal perception mechanisms.", "method": "1) A crowdsourced subjective testing framework enabling reliable AVQA annotations across varied real\u2011world environments. 2) A systematic data preparation strategy to cover a wide range of quality levels and semantic scenarios. 3) Extension with additional annotations to study modality interactions and content\u2011quality relationships. 4) Validation via constructing the YT\u2011NTU\u2011AVQ dataset and releasing dataset + platform code.", "result": "Creation of YT\u2011NTU\u2011AVQ: 1,620 user\u2011generated audio/video sequences with broad quality and semantic coverage, annotated via the proposed crowdsourcing protocol; claimed as the largest and most diverse AVQA dataset to date, with supporting platform code available.", "conclusion": "A practical, scalable path to advance AVQA: reliable crowdsourced annotation, systematic coverage, and richer labels that enable deeper multimodal perception research; resources are released to facilitate community use and future work."}}
{"id": "2602.22918", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22918", "abs": "https://arxiv.org/abs/2602.22918", "authors": ["Jonathan Steinberg", "Oren Gal"], "title": "Where Vision Becomes Text: Locating the OCR Routing Bottleneck in Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) can read text from images, but where does this optical character recognition (OCR) information enter the language processing stream? We investigate the OCR routing mechanism across three architecture families (Qwen3-VL, Phi-4, InternVL3.5) using causal interventions. By computing activation differences between original images and text-inpainted versions, we identify architecture-specific OCR bottlenecks whose dominant location depends on the vision-language integration strategy: DeepStack models (Qwen) show peak sensitivity at mid-depth (about 50%) for scene text, while single-stage projection models (Phi-4, InternVL) peak at early layers (6-25%), though the exact layer of maximum effect varies across datasets. The OCR signal is remarkably low-dimensional: PC1 captures 72.9% of variance. Crucially, principal component analysis (PCA) directions learned on one dataset transfer to others, demonstrating shared text-processing pathways. Surprisingly, in models with modular OCR circuits (notably Qwen3-VL-4B), OCR removal can improve counting performance (up to +6.9 percentage points), suggesting OCR interferes with other visual processing in sufficiently modular architectures.", "AI": {"tldr": "Causal activation analyses across three VLM families reveal that OCR information enters through architecture-specific bottlenecks\u2014mid-depth (~50%) in DeepStack integration (Qwen3-VL) and early layers (6\u201325%) in single-stage projection models (Phi-4, InternVL). The OCR signal is largely low-dimensional (PC1\u224872.9% variance) and transfers across datasets; removing this pathway in modular models can improve non-OCR tasks (e.g., counting by up to +6.9 pp).", "motivation": "To pinpoint where and how OCR-derived signals are injected into and propagated through VLM language streams, and whether OCR processing interferes with other visual reasoning depending on the model\u2019s integration strategy.", "method": "Use causal interventions via activation differences between original images and text-inpainted counterparts to localize OCR-sensitive layers. Perform layerwise effect scanning across architectures, apply PCA to the intervention-induced activation subspace, test cross-dataset transfer of principal directions, and evaluate behavioral effects of ablating OCR components (e.g., on counting).", "result": "- Qwen (DeepStack) shows peak OCR sensitivity at mid-depth (~50%) for scene text. - Phi-4 and InternVL (single-stage projection) peak early (6\u201325%), with dataset-dependent variation in exact layers. - OCR signal is low-dimensional: PC1 explains 72.9% variance; PCA directions transfer across datasets, indicating shared pathways. - In modular models (notably Qwen3-VL-4B), removing OCR pathways can improve counting by up to +6.9 percentage points, implying interference between OCR and other visual processes.", "conclusion": "OCR routing depends strongly on the vision-language integration scheme and is mediated by a shared, low-dimensional subspace. Modular OCR circuitry can interfere with other visual reasoning, so controlling or gating OCR pathways may improve multi-skill performance and offers a target for interpretability-guided editing."}}
{"id": "2602.22758", "categories": ["cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.22758", "abs": "https://arxiv.org/abs/2602.22758", "authors": ["Satya Borgohain", "Roy Mariathas"], "title": "Decomposing Physician Disagreement in HealthBench", "comment": null, "summary": "We decompose physician disagreement in the HealthBench medical AI evaluation dataset to understand where variance resides and what observable features can explain it. Rubric identity accounts for 15.8% of met/not-met label variance but only 3.6-6.9% of disagreement variance; physician identity accounts for just 2.4%. The dominant 81.8% case-level residual is not reduced by HealthBench's metadata labels (z = -0.22, p = 0.83), normative rubric language (pseudo R^2 = 1.2%), medical specialty (0/300 Tukey pairs significant), surface-feature triage (AUC = 0.58), or embeddings (AUC = 0.485). Disagreement follows an inverted-U with completion quality (AUC = 0.689), confirming physicians agree on clearly good or bad outputs but split on borderline cases. Physician-validated uncertainty categories reveal that reducible uncertainty (missing context, ambiguous phrasing) more than doubles disagreement odds (OR = 2.55, p < 10^(-24)), while irreducible uncertainty (genuine medical ambiguity) has no effect (OR = 1.01, p = 0.90), though even the former explains only ~3% of total variance. The agreement ceiling in medical AI evaluation is thus largely structural, but the reducible/irreducible dissociation suggests that closing information gaps in evaluation scenarios could lower disagreement where inherent clinical ambiguity does not, pointing toward actionable evaluation design improvements.", "AI": {"tldr": "Most physician disagreement in HealthBench is case-specific and not explained by rubrics, physician identity, specialty, metadata, or embeddings; disagreement peaks for borderline model outputs. Clarifying context/phrasing can modestly lower disagreement, but a structural ceiling remains.", "motivation": "Assess where physician label variance in medical AI evaluation comes from and whether observable factors (rubrics, annotators, metadata, specialty, language) can explain or reduce disagreement\u2014ultimately to improve evaluation reliability and design.", "method": "Variance decomposition of met/not-met labels and disagreements across rubrics, physicians, and cases; modeling contributions of rubric identity and physician identity; testing explanatory power of HealthBench metadata and normative rubric language; predictive triage using surface features and embeddings; analysis of disagreement as a function of completion quality; logistic modeling with physician-validated uncertainty categories; statistical summaries including AUC, pseudo R^2, odds ratios, Tukey HSD, and significance tests.", "result": "Rubric identity explains 15.8% of met/not-met variance but only 3.6\u20136.9% of disagreement; physician identity explains ~2.4%. The dominant 81.8% residual sits at the case level and is not reduced by metadata (z = \u22120.22, p = 0.83), normative rubric language (pseudo R^2 = 1.2%), medical specialty (0/300 Tukey pairs significant), surface-feature triage (AUC = 0.58), or embeddings (AUC = 0.485). Disagreement follows an inverted-U versus completion quality (AUC = 0.689): agreement is high for clearly good/bad outputs, low for borderline ones. Reducible uncertainty (missing context, ambiguous phrasing) increases disagreement odds (OR = 2.55, p < 1e\u221224) while irreducible clinical ambiguity does not (OR = 1.01, p = 0.90); even reducible uncertainty explains only ~3% of variance.", "conclusion": "There is a structural ceiling on inter-physician agreement in medical AI evaluation that current observable features cannot overcome. Still, reducing reducible uncertainty\u2014by providing more context and clearer phrasing\u2014can lower disagreement on the margins, whereas inherent clinical ambiguity will not. This points to actionable but limited gains via improved evaluation design."}}
{"id": "2602.22666", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22666", "abs": "https://arxiv.org/abs/2602.22666", "authors": ["Xuelu Li", "Zhaonan Wang", "Xiaogang Wang", "Lei Wu", "Manyi Li", "Changhe Tu"], "title": "ArtPro: Self-Supervised Articulated Object Reconstruction with Adaptive Integration of Mobility Proposals", "comment": null, "summary": "Reconstructing articulated objects into high-fidelity digital twins is crucial for applications such as robotic manipulation and interactive simulation. Recent self-supervised methods using differentiable rendering frameworks like 3D Gaussian Splatting remain highly sensitive to the initial part segmentation. Their reliance on heuristic clustering or pre-trained models often causes optimization to converge to local minima, especially for complex multi-part objects. To address these limitations, we propose ArtPro, a novel self-supervised framework that introduces adaptive integration of mobility proposals. Our approach begins with an over-segmentation initialization guided by geometry features and motion priors, generating part proposals with plausible motion hypotheses. During optimization, we dynamically merge these proposals by analyzing motion consistency among spatial neighbors, while a collision-aware motion pruning mechanism prevents erroneous kinematic estimation. Extensive experiments on both synthetic and real-world objects demonstrate that ArtPro achieves robust reconstruction of complex multi-part objects, significantly outperforming existing methods in accuracy and stability.", "AI": {"tldr": "ArtPro is a self-supervised articulated-object reconstruction framework that starts with motion-aware over-segmentation, then adaptively merges parts via motion-consistency checks and collision-aware pruning, achieving more accurate and stable digital twins than prior differentiable-rendering methods.", "motivation": "Existing self-supervised approaches (e.g., with 3D Gaussian Splatting) depend heavily on initial part segmentation\u2014often from heuristics or pretrained models\u2014leading to local minima and failures on complex multi-part objects. A more robust, initialization-tolerant pipeline is needed.", "method": "1) Generate an over-segmentation using geometry features and motion priors to form part proposals with plausible motion hypotheses. 2) During optimization, adaptively merge neighboring proposals whose motions are consistent. 3) Apply collision-aware pruning to discard erroneous kinematic estimates, all within a differentiable rendering optimization loop.", "result": "On synthetic and real datasets, ArtPro robustly reconstructs complex multi-part objects and significantly outperforms prior methods in both accuracy and stability.", "conclusion": "Adaptive integration of mobility proposals\u2014through motion-consistent merging and collision-aware pruning\u2014reduces sensitivity to poor initializations, yielding reliable, high-fidelity digital twins of articulated objects."}}
{"id": "2602.23057", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23057", "abs": "https://arxiv.org/abs/2602.23057", "authors": ["Jeongin Bae", "Baeseong Park", "Gunho Park", "Minsub Kim", "Joonhyung Lee", "Junhee Yoo", "Sunghyeon Woo", "Jiwon Ryu", "Se Jung Kwon", "Dongsoo Lee"], "title": "Affine-Scaled Attention: Towards Flexible and Stable Transformer Attention", "comment": "Preprint. 14 pages, 11 figures", "summary": "Transformer attention is typically implemented using softmax normalization, which enforces attention weights with unit sum normalization. While effective in many settings, this constraint can limit flexibility in controlling attention magnitudes and may contribute to overly concentrated or unstable attention patterns during training. Prior work has explored modifications such as attention sinks or gating mechanisms, but these approaches provide only limited or indirect control over attention reweighting. We propose Affine-Scaled Attention, a simple extension to standard attention that introduces input-dependent scaling and a corresponding bias term applied to softmax-normalized attention weights. This design relaxes the strict normalization constraint while maintaining aggregation of value representations, allowing the model to adjust both the relative distribution and the scale of attention in a controlled manner.\n  We empirically evaluate Affine-Scaled Attention in large-scale language model pretraining across multiple model sizes. Experimental results show consistent improvements in training stability, optimization behavior, and downstream task performance compared to standard softmax attention and attention sink baselines. These findings suggest that modest reweighting of attention outputs provides a practical and effective way to improve attention behavior in Transformer models.", "AI": {"tldr": "Introduces Affine-Scaled Attention, an affine reweighting (input-dependent scale and bias) applied to softmax attention weights to relax the unit-sum constraint, yielding more controllable magnitudes and improved training and performance in LLM pretraining.", "motivation": "Softmax enforces unit-sum attention, which can restrict control over attention magnitudes and lead to overly peaked or unstable attention. Prior fixes (attention sinks, gates) offer limited or indirect control. A more direct, flexible mechanism to modulate both distribution and scale of attention is needed.", "method": "Extend standard attention by applying an input-conditioned scaling factor and bias to the softmax-normalized attention weights (an affine transform), thereby relaxing strict normalization while still aggregating values. Evaluate the approach during large-scale language model pretraining across multiple model sizes.", "result": "Consistent empirical gains: better training stability, improved optimization dynamics, and superior downstream task performance compared to standard softmax attention and attention-sink baselines.", "conclusion": "Modest, controlled reweighting of attention outputs via an affine transform is a practical, effective way to improve Transformer attention behavior without overhauling the architecture."}}
{"id": "2602.22769", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22769", "abs": "https://arxiv.org/abs/2602.22769", "authors": ["Yujie Zhao", "Boqin Yuan", "Junbo Huang", "Haocheng Yuan", "Zhongming Yu", "Haozhou Xu", "Lanxiang Hu", "Abhilash Shankarampeta", "Zimeng Huang", "Wentao Ni", "Yuandong Tian", "Jishen Zhao"], "title": "AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications", "comment": null, "summary": "Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: existing benchmarks primarily focus on dialogue-centric, human-agent interactions. In reality, agent memory consists of a continuous stream of agent-environment interactions that are primarily composed of machine-generated representations. To bridge this gap, we introduce AMA-Bench (Agent Memory with Any length), which evaluates long-horizon memory for LLMs in real agentic applications. It features two key components: (1) a set of real-world agentic trajectories across representative agentic applications, paired with expert-curated QA, and (2) a set of synthetic agentic trajectories that scale to arbitrary horizons, paired with rule-based QA. Our comprehensive study shows that existing memory systems underperform on AMA-Bench primarily because they lack causality and objective information and are constrained by the lossy nature of similarity-based retrieval employed by many memory systems. To address these limitations, we propose AMA-Agent, an effective memory system featuring a causality graph and tool-augmented retrieval. Our results demonstrate that AMA-Agent achieves 57.22% average accuracy on AMA-Bench, surpassing the strongest memory system baselines by 11.16%.", "AI": {"tldr": "Introduces AMA-Bench, a benchmark for evaluating long-horizon agent memory using real and synthetic agentic trajectories, and AMA-Agent, a causality- and tool-augmented memory system that outperforms existing methods (57.22% accuracy, +11.16% over the strongest baseline).", "motivation": "Current memory benchmarks for LLM agents are dialogue-centric and human-facing, whereas real agent deployments involve long sequences of agent\u2013environment interactions dominated by machine-generated states and tool outputs. There is a need for scalable, realistic evaluation that reflects such settings and for memory systems that capture causality and objective facts beyond similarity-based retrieval.", "method": "1) Build AMA-Bench: (a) real-world agentic trajectories across representative applications with expert-curated QA; (b) synthetic trajectories that scale to arbitrary horizons with rule-based QA. 2) Analyze why existing memory systems fail (lack of causality/objectivity; lossy similarity retrieval). 3) Propose AMA-Agent: a memory system using a causality graph to encode dependencies among events and tool-augmented retrieval to access objective information beyond semantic similarity.", "result": "On AMA-Bench, AMA-Agent attains 57.22% average QA accuracy, exceeding the best baseline by 11.16% absolute. The study finds standard memory approaches underperform due to weak causal modeling and reliance on lossy similarity search over long histories.", "conclusion": "Effective long-horizon memory for agentic LLMs requires causal structure and grounded, tool-based retrieval rather than pure similarity search. AMA-Bench provides a more realistic and scalable evaluation setting, and AMA-Agent establishes a new state-of-the-art on this benchmark."}}
{"id": "2602.22667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22667", "abs": "https://arxiv.org/abs/2602.22667", "authors": ["Changqing Zhou", "Yueru Luo", "Han Zhang", "Zeyu Jiang", "Changhao Chen"], "title": "Monocular Open Vocabulary Occupancy Prediction for Indoor Scenes", "comment": "Accepted by CVPR2026", "summary": "Open-vocabulary 3D occupancy is vital for embodied agents, which need to understand complex indoor environments where semantic categories are abundant and evolve beyond fixed taxonomies. While recent work has explored open-vocabulary occupancy in outdoor driving scenarios, such methods transfer poorly indoors, where geometry is denser, layouts are more intricate, and semantics are far more fine-grained. To address these challenges, we adopt a geometry-only supervision paradigm that uses only binary occupancy labels (occupied vs free). Our framework builds upon 3D Language-Embedded Gaussians, which serve as a unified intermediate representation coupling fine-grained 3D geometry with a language-aligned semantic embedding. On the geometry side, we find that existing Gaussian-to-Occupancy operators fail to converge under such weak supervision, and we introduce an opacity-aware, Poisson-based approach that stabilizes volumetric aggregation. On the semantic side, direct alignment between rendered features and open-vocabulary segmentation features suffers from feature mixing; we therefore propose a Progressive Temperature Decay schedule that gradually sharpens opacities during splatting, strengthening Gaussian-language alignment. On Occ-ScanNet, our framework achieves 59.50 IoU and 21.05 mIoU in the open-vocabulary setting, surpassing all existing occupancy methods in IoU and outperforming prior open-vocabulary approaches by a large margin in mIoU. Code will be released at https://github.com/JuIvyy/LegoOcc.", "AI": {"tldr": "Indoor open-vocabulary 3D occupancy via geometry-only supervision using Language-Embedded 3D Gaussians, with a new opacity-aware Poisson aggregation for occupancy and a progressive temperature decay to stabilize language\u2013geometry alignment; achieves SOTA on Occ-ScanNet (59.50 IoU, 21.05 mIoU).", "motivation": "Embodied agents operating indoors must reason about dense, intricate geometry and fine-grained, evolving semantics beyond closed taxonomies. Outdoor-focused open-vocabulary occupancy methods transfer poorly to indoor scenes. A supervision scheme that avoids expensive per-voxel semantics while still enabling open-vocab understanding is needed.", "method": "Use 3D Language-Embedded Gaussians (unified 3D geometry + language-aligned features) trained with only binary occupancy (occupied vs free). Address instability of Gaussian-to-occupancy conversion by introducing an opacity-aware Poisson-based volumetric aggregation operator. Mitigate semantic feature mixing by progressively sharpening Gaussian opacities during splatting via a Progressive Temperature Decay schedule, improving alignment between rendered features and open-vocabulary segmentation features.", "result": "On Occ-ScanNet, obtains 59.50 IoU and 21.05 mIoU in open-vocabulary evaluation, surpassing all occupancy methods in IoU and significantly outperforming prior open-vocabulary approaches in mIoU.", "conclusion": "Geometry-only supervision, combined with language-embedded Gaussians and stabilized occupancy/semantics operators, enables robust indoor open-vocabulary 3D occupancy, delivering state-of-the-art performance. Code to be released (LegoOcc)."}}
{"id": "2602.23062", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23062", "abs": "https://arxiv.org/abs/2602.23062", "authors": ["Gabriela Anna Kaczmarek", "Pietro Ferrazzi", "Lorenzo Porta", "Vicky Rubini", "Bernardo Magnini"], "title": "Toward Automatic Filling of Case Report Forms: A Case Study on Data from an Italian Emergency Department", "comment": null, "summary": "Case Report Forms (CRFs) collect data about patients and are at the core of well-established practices to conduct research in clinical settings. With the recent progress of language technologies, there is an increasing interest in automatic CRF-filling from clinical notes, mostly based on the use of Large Language Models (LLMs). However, there is a general scarcity of annotated CRF data, both for training and testing LLMs, which limits the progress on this task. As a step in the direction of providing such data, we present a new dataset of clinical notes from an Italian Emergency Department annotated with respect to a pre-defined CRF containing 134 items to be filled. We provide an analysis of the data, define the CRF-filling task and metric for its evaluation, and report on pilot experiments where we use an open-source state-of-the-art LLM to automatically execute the task. Results of the case-study show that (i) CRF-filling from real clinical notes in Italian can be approached in a zero-shot setting; (ii) LLMs' results are affected by biases (e.g., a cautious behaviour favours \"unknown\" answers), which need to be corrected.", "AI": {"tldr": "They introduce an annotated Italian Emergency Department dataset aligned to a 134-item CRF, define the CRF-filling task and metric, and show a zero-shot open-source LLM can fill many fields but exhibits biases (notably overusing \u201cunknown\u201d), implying the need for bias mitigation.", "motivation": "Automatic CRF-filling is attractive for clinical research workflows, but progress is hampered by a lack of annotated CRF-grounded data\u2014especially for non-English settings\u2014needed to train and evaluate LLMs.", "method": "Curate and release clinical notes from an Italian ED annotated against a predefined 134-item CRF; specify the CRF-filling task and an evaluation metric; run pilot zero-shot experiments using a state-of-the-art open-source LLM to populate CRF items from free-text notes; analyze behavior and biases.", "result": "Zero-shot CRF-filling on real Italian notes is feasible with an open-source LLM, but performance is constrained by systematic biases such as a conservative tendency to answer \u201cunknown,\u201d which degrades completeness/accuracy per the proposed metric.", "conclusion": "The dataset and benchmark provide needed infrastructure for CRF-filling research. While LLMs can tackle the task without training, their cautious bias requires correction (e.g., calibration, prompt or post-processing) to achieve reliable clinical utility."}}
{"id": "2602.22771", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.22771", "abs": "https://arxiv.org/abs/2602.22771", "authors": ["Yusuke Watanabe", "Yohei Kobashi", "Takeshi Kojima", "Yusuke Iwasawa", "Yasushi Okuno", "Yutaka Matsuo"], "title": "ClinDet-Bench: Beyond Abstention, Evaluating Judgment Determinability of LLMs in Clinical Decision-Making", "comment": "17 pages, 3 figures, 10 tables", "summary": "Clinical decisions are often required under incomplete information. Clinical experts must identify whether available information is sufficient for judgment, as both premature conclusion and unnecessary abstention can compromise patient safety. To evaluate this capability of large language models (LLMs), we developed ClinDet-Bench, a benchmark based on clinical scoring systems that decomposes incomplete-information scenarios into determinable and undeterminable conditions. Identifying determinability requires considering all hypotheses about missing information, including unlikely ones, and verifying whether the conclusion holds across them. We find that recent LLMs fail to identify determinability under incomplete information, producing both premature judgments and excessive abstention, despite correctly explaining the underlying scoring knowledge and performing well under complete information. These findings suggest that existing benchmarks are insufficient to evaluate the safety of LLMs in clinical settings. ClinDet-Bench provides a framework for evaluating determinability recognition, leading to appropriate abstention, with potential applicability to medicine and other high-stakes domains, and is publicly available.", "AI": {"tldr": "They introduce ClinDet-Bench, a benchmark to test whether LLMs can tell if clinical conclusions are determinable given incomplete information. Modern LLMs often misjudge\u2014both jumping to conclusions and over-abstaining\u2014despite knowing the scoring rules and doing well with complete data.", "motivation": "In clinical practice, decisions frequently occur under incomplete information. Safety depends on recognizing when available data suffices and when to abstain. Existing LLM benchmarks largely ignore this determinability recognition, risking unsafe deployments.", "method": "Construct a benchmark grounded in established clinical scoring systems. Decompose cases with missing data into conditions that are determinable vs. undeterminable. Define determinability as robustness of the conclusion across all plausible assignments of missing variables, including unlikely ones. Evaluate recent LLMs on this task and compare to performance under complete information; analyze rates of premature judgment and unnecessary abstention and the models\u2019 explanatory competence.", "result": "Recent LLMs generally fail to detect determinability under incomplete information: they both issue premature decisions and abstain excessively. This occurs even when they correctly articulate the scoring logic and perform well when all information is present.", "conclusion": "Current benchmarks miss a critical safety dimension for clinical use. ClinDet-Bench offers a principled way to assess determinability recognition and appropriate abstention, is publicly available, and may generalize to other high-stakes domains."}}
{"id": "2602.22674", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22674", "abs": "https://arxiv.org/abs/2602.22674", "authors": ["Guanghao Liao", "Zhen Liu", "Liyuan Cao", "Yonghui Yang", "Qi Li"], "title": "SPMamba-YOLO: An Underwater Object Detection Network Based on Multi-Scale Feature Enhancement and Global Context Modeling", "comment": "31 pages, 10 figures, 6 tables. This paper presents SPMamba-YOLO, an underwater object detection framework integrating multi-scale feature enhancement and global context modeling. The work is under review", "summary": "Underwater object detection is a critical yet challenging research problem owing to severe light attenuation, color distortion, background clutter, and the small scale of underwater targets. To address these challenges, we propose SPMamba-YOLO, a novel underwater object detection network that integrates multi-scale feature enhancement with global context modeling. Specifically, a Spatial Pyramid Pooling Enhanced Layer Aggregation Network (SPPELAN) module is introduced to strengthen multi-scale feature aggregation and expand the receptive field, while a Pyramid Split Attention (PSA) mechanism enhances feature discrimination by emphasizing informative regions and suppressing background interference. In addition, a Mamba-based state space modeling module is incorporated to efficiently capture long-range dependencies and global contextual information, thereby improving detection robustness in complex underwater environments. Extensive experiments on the URPC2022 dataset demonstrate that SPMamba-YOLO outperforms the YOLOv8n baseline by more than 4.9\\% in mAP@0.5, particularly for small and densely distributed underwater objects, while maintaining a favorable balance between detection accuracy and computational cost.", "AI": {"tldr": "SPMamba-YOLO augments a YOLOv8n-style detector with enhanced multi-scale aggregation (SPPELAN), pyramid split attention (PSA), and a Mamba-based state-space module for global context, yielding >4.9% mAP@0.5 gain on URPC2022, notably for small/dense underwater objects, while keeping reasonable efficiency.", "motivation": "Underwater images suffer from light attenuation, color casts, cluttered backgrounds, and small targets, which degrade standard detectors. The authors aim to bolster multi-scale feature quality and capture long-range/global context to improve robustness in such adverse conditions.", "method": "They integrate three components into a YOLO-like framework: (1) SPPELAN (Spatial Pyramid Pooling Enhanced Layer Aggregation Network) to aggregate multi-scale features and enlarge receptive fields; (2) PSA (Pyramid Split Attention) to emphasize informative regions and suppress background noise; (3) a Mamba-based state space modeling block to efficiently encode long-range dependencies and global context. The combined design targets better discrimination and scale robustness without excessive compute overhead.", "result": "On the URPC2022 dataset, the method surpasses the YOLOv8n baseline by over 4.9% mAP@0.5, with pronounced gains on small and densely packed objects, and claims a favorable accuracy\u2013cost trade-off.", "conclusion": "Coupling multi-scale aggregation, attention, and efficient global-context modeling via state-space blocks is effective for underwater object detection, improving accuracy\u2014especially on small/dense targets\u2014while maintaining efficiency. The approach appears robust to common underwater degradations."}}
{"id": "2602.23071", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23071", "abs": "https://arxiv.org/abs/2602.23071", "authors": ["Yuqi Shi", "Hao Yang", "Xiyao Lu", "Jinsong Zhang"], "title": "Quantity Convergence, Quality Divergence: Disentangling Fluency and Accuracy in L2 Mandarin Prosody", "comment": null, "summary": "While second language (L2) learners may acquire target syntactic word order, mapping this syntax onto appropriate prosodic structures remains a persistent challenge. This study investigates the fossilization and stability of the L2 syntax-prosody interface by comparing 67 native Mandarin speakers with 67 Vietnamese learners using the BLCU-SAIT corpus. By integrating C-ToBI boundary annotation with Dependency Grammar analysis, we examined both the quantity of prosodic boundaries and their mapping to syntactic relations. Results reveal a non-linear acquisition: although high-proficiency learners (VNH) converge to the native baseline in boundary quantity at the Major Phrase level (B3), their structural mapping significantly diverges. Specifically, VNH demote the prosodic boundary at the Subject-Verb (SBV) interface (Major Phrase B3 -> Prosodic Word B1), while erroneously promoting the boundary at the Verb-Object (VOB) interface (Prosodic Word B1 -> Major Phrase B3). This strategy allows learners to maintain high long phrasal output at the expense of structural accuracy. This results in a distorted prosodic hierarchy where the native pattern is inverted.", "AI": {"tldr": "High\u2011proficiency Vietnamese learners of Mandarin match natives in the number of major prosodic boundaries but mis-map them to syntax\u2014demoting Subject\u2013Verb and promoting Verb\u2013Object boundaries\u2014yielding an inverted prosodic hierarchy and evidence of fossilized syntax\u2013prosody misalignment.", "motivation": "To test whether difficulties at the L2 syntax\u2013prosody interface persist (fossilize) even when learners have acquired target word order, and to show that simple boundary counts can obscure structural mapping problems.", "method": "Compared 67 native Mandarin speakers with 67 Vietnamese learners (BLCU\u2011SAIT corpus). Integrated C\u2011ToBI prosodic boundary annotation with Dependency Grammar, assessing both boundary quantity at different levels (e.g., Major Phrase B3 vs. Prosodic Word B1) and alignment with syntactic relations, especially Subject\u2013Verb (SBV) and Verb\u2013Object (VOB).", "result": "Acquisition is non\u2011linear: high\u2011proficiency Vietnamese learners converge with natives in the quantity of Major Phrase (B3) boundaries but diverge in where they place them. They demote SBV boundaries (B3\u2192B1) and promote VOB boundaries (B1\u2192B3), a strategy that sustains long phrasal output but sacrifices structural accuracy, effectively inverting the native prosodic hierarchy.", "conclusion": "The L2 syntax\u2013prosody interface shows stable, likely fossilized mis-mapping despite advanced proficiency. Evaluations based solely on prosodic boundary counts are misleading; teaching and assessment should target syntax\u2011prosody mapping patterns. Findings suggest durable processing or L1\u2011based strategies that reweight SBV vs. VOB phrasing in Mandarin speech."}}
{"id": "2602.22808", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22808", "abs": "https://arxiv.org/abs/2602.22808", "authors": ["Shiqian Su", "Sen Xing", "Xuan Dong", "Muyan Zhong", "Bin Wang", "Xizhou Zhu", "Yuntao Chen", "Wenhai Wang", "Yue Deng", "Pengxiang Zhu", "Ziyuan Liu", "Tiantong Li", "Jiaheng Yu", "Zhe Chen", "Lidong Bing", "Jifeng Dai"], "title": "MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks", "comment": null, "summary": "Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy through tool integration and external interaction, they still suffer from naive workflows, unstable performance, limited support across diverse benchmarks and tasks, and heavy reliance on costly commercial APIs. In this work, we propose a high-performance and robust open-source agent framework, termed MiroFlow, which incorporates an agent graph for flexible orchestration, an optional deep reasoning mode to enhance performance, and a robust workflow execution to ensure stable and reproducible performance. Extensive experiments demonstrate that MiroFlow consistently achieves state-of-the-art performance across multiple agent benchmarks, including GAIA, BrowseComp-EN/ZH, HLE, xBench-DeepSearch, and notably FutureX. We hope it could serve as an easily accessible, reproducible, and comparable baseline for the deep research community.", "AI": {"tldr": "MiroFlow is an open-source agent framework that uses an agent graph, an optional deep-reasoning mode, and robust workflow execution to deliver stable, reproducible, and state-of-the-art performance across multiple agent benchmarks.", "motivation": "Standalone LLMs plateau on complex, real-world tasks that require tool use and interaction with dynamic environments; existing agent frameworks are hampered by naive workflows, instability, limited benchmark coverage, and dependence on costly commercial APIs.", "method": "Introduce MiroFlow with: (1) an agent graph for flexible orchestration of tools and sub-agents; (2) an optional deep-reasoning mode to boost task performance; (3) robust workflow execution mechanisms to ensure stability and reproducibility. Evaluate across diverse public agent benchmarks.", "result": "MiroFlow reportedly achieves consistent SOTA on GAIA, BrowseComp-EN/ZH, HLE, xBench-DeepSearch, and FutureX, outperforming prior agent systems.", "conclusion": "MiroFlow addresses major pain points in agent frameworks and aims to serve as an accessible, reproducible baseline for research, offering strong empirical performance and practical robustness."}}
{"id": "2602.22678", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22678", "abs": "https://arxiv.org/abs/2602.22678", "authors": ["Quoc-Khang Tran", "Minh-Thien Nguyen", "Nguyen-Khang Pham"], "title": "ViCLIP-OT: The First Foundation Vision-Language Model for Vietnamese Image-Text Retrieval with Optimal Transport", "comment": "Preprint submitted to Expert Systems with Applications", "summary": "Image-text retrieval has become a fundamental component in intelligent multimedia systems; however, most existing vision-language models are optimized for highresource languages and remain suboptimal for low-resource settings such as Vietnamese. This work introduces ViCLIP-OT, a foundation vision-language model specifically designed for Vietnamese image-text retrieval. The proposed framework integrates CLIP-style contrastive learning with a Similarity-Graph Regularized Optimal Transport (SIGROT) loss to enhance global cross-modal consistency and mitigate modality gap issues. Extensive experiments on three Vietnamese benchmarks (UITOpenViIC, KTVIC, and Crossmodal-3600) demonstrate that ViCLIP-OT consistently outperforms CLIP and SigLIP baselines in both in-domain and zero-shot settings. On UIT-OpenViIC, the model achieves an average Recall@K of 67.34%, improving upon CLIP by 5.75 percentage points. In zero-shot evaluation on Crossmodal-3600, ViCLIPOT surpasses CLIP by 11.72 percentage points. Embedding-space analysis further confirms improved alignment and reduced modality gap. The results indicate that integrating SIGROT provides an effective and scalable strategy for cross-modal retrieval in low-resource languages, offering practical implications for intelligent multimedia retrieval systems in Vietnamese and other underrepresented linguistic contexts.", "AI": {"tldr": "ViCLIP-OT augments CLIP-style training with a Similarity-Graph Regularized Optimal Transport loss to better align Vietnamese image\u2013text pairs, yielding sizable accuracy gains over CLIP/SigLIP across in-domain and zero-shot retrieval benchmarks and reducing the modality gap.", "motivation": "Most VLMs are tuned for high-resource languages and underperform for Vietnamese. There is a need for a scalable method that improves cross-modal alignment and retrieval quality in low-resource linguistic settings by explicitly addressing the modality gap.", "method": "Combine CLIP-style contrastive learning with a Similarity-Graph Regularized Optimal Transport (SIGROT) loss. The similarity graph structures pairwise relationships, and the OT objective enforces globally consistent alignment across modalities, mitigating modality mismatch. The model is trained and evaluated on Vietnamese datasets (UIT-OpenViIC, KTVIC, Crossmodal-3600).", "result": "Consistent improvements over CLIP and SigLIP in both in-domain and zero-shot scenarios. On UIT-OpenViIC, average Recall@K = 67.34%, +5.75 pp over CLIP. In zero-shot on Crossmodal-3600, +11.72 pp over CLIP. Qualitative/embedding analyses indicate tighter alignment and a smaller modality gap; KTVIC also shows gains.", "conclusion": "SIGROT-enhanced CLIP training is an effective, scalable strategy for image\u2013text retrieval in Vietnamese and likely other low-resource languages, offering improved recall and better-aligned embeddings for practical multimedia retrieval systems."}}
{"id": "2602.23075", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23075", "abs": "https://arxiv.org/abs/2602.23075", "authors": ["Mengze Hong", "Di Jiang", "Chen Jason Zhang", "Zichang Guo", "Yawen Li", "Jun Chen", "Shaobo Cui", "Zhiyang Su"], "title": "CiteLLM: An Agentic Platform for Trustworthy Scientific Reference Discovery", "comment": "Accepted by TheWebConf 2026 Demo Track", "summary": "Large language models (LLMs) have created new opportunities to enhance the efficiency of scholarly activities; however, challenges persist in the ethical deployment of AI assistance, including (1) the trustworthiness of AI-generated content, (2) preservation of academic integrity and intellectual property, and (3) protection of information privacy. In this work, we present CiteLLM, a specialized agentic platform designed to enable trustworthy reference discovery for grounding author-drafted claims and statements. The system introduces a novel interaction paradigm by embedding LLM utilities directly within the LaTeX editor environment, ensuring a seamless user experience and no data transmission outside the local system. To guarantee hallucination-free references, we employ dynamic discipline-aware routing to retrieve candidates exclusively from trusted web-based academic repositories, while leveraging LLMs solely for generating context-aware search queries, ranking candidates by relevance, and validating and explaining support through paragraph-level semantic matching and an integrated chatbot. Evaluation results demonstrate the superior performance of the proposed system in returning valid and highly usable references.", "AI": {"tldr": "CiteLLM is a privacy-preserving, LaTeX-integrated agent that finds and validates trustworthy references by querying only trusted academic repositories and using LLMs for query formulation, ranking, and paragraph-level support checking\u2014yielding superior, usable citations with minimal hallucination risk.", "motivation": "Scholars need AI support that is trustworthy, preserves academic integrity/IP, and protects privacy. Existing LLM tools risk hallucinations, data leakage, and weak grounding of claims. The paper aims to provide reliable, privacy-safe reference discovery tightly integrated into writing workflows.", "method": "Embed an agentic system into the local LaTeX editor so no data leaves the machine. Use dynamic, discipline-aware routing to fetch candidate sources strictly from trusted web academic repositories. Constrain LLMs to: (1) generate context-aware search queries, (2) rank candidates by relevance, and (3) validate and explain support via paragraph-level semantic matching and an integrated chatbot\u2014rather than generating references outright.", "result": "Empirical evaluation shows the system outperforms alternatives in returning valid and highly usable references, with stronger grounding and reduced hallucination risk.", "conclusion": "CiteLLM effectively addresses key ethical and practical challenges in AI-assisted scholarship by delivering hallucination-resistant, privacy-preserving, and workflow-native reference discovery. It enhances trust and usability of citations and can improve scholarly writing efficiency."}}
{"id": "2602.22814", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.22814", "abs": "https://arxiv.org/abs/2602.22814", "authors": ["Soyoung Jung", "Daehoo Yoon", "Sung Gyu Koh", "Young Hwan Kim", "Yehan Ahn", "Sung Park"], "title": "When Should an AI Act? A Human-Centered Model of Scene, Context, and Behavior for Agentic AI Design", "comment": null, "summary": "Agentic AI increasingly intervenes proactively by inferring users' situations from contextual data yet often fails for lack of principled judgment about when, why, and whether to act. We address this gap by proposing a conceptual model that reframes behavior as an interpretive outcome integrating Scene (observable situation), Context (user-constructed meaning), and Human Behavior Factors (determinants shaping behavioral likelihood). Grounded in multidisciplinary perspectives across the humanities, social sciences, HCI, and engineering, the model separates what is observable from what is meaningful to the user and explains how the same scene can yield different behavioral meanings and outcomes. To translate this lens into design action, we derive five agent design principles (behavioral alignment, contextual sensitivity, temporal appropriateness, motivational calibration, and agency preservation) that guide intervention depth, timing, intensity, and restraint. Together, the model and principles provide a foundation for designing agentic AI systems that act with contextual sensitivity and judgment in interactions.", "AI": {"tldr": "Conceptual, multidisciplinary model for agentic AI that separates what is observable (Scene) from what is meaningful to the user (Context) and the determinants of action (Human Behavior Factors), yielding five actionable design principles to guide when, why, and how agents should intervene.", "motivation": "Proactive agents often misjudge whether, why, or when to act because they conflate observable signals with users\u2019 meanings and motivations; the field lacks a principled framework to support contextual judgment and restraint.", "method": "Propose an interpretive, cross-disciplinary model decomposing behavior into Scene, Context, and Human Behavior Factors; translate this lens into five design principles\u2014behavioral alignment, contextual sensitivity, temporal appropriateness, motivational calibration, and agency preservation\u2014to guide intervention depth, timing, intensity, and restraint.", "result": "Shows how identical observable situations can produce different behavioral meanings and outcomes across users; provides five principles that operationalize contextual judgment for agent behavior and intervention design.", "conclusion": "Together, the model and principles offer a foundation for agentic AI systems that act with contextual sensitivity and sound judgment, aiming to improve alignment with user meaning and experience in interactions."}}
{"id": "2602.22683", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22683", "abs": "https://arxiv.org/abs/2602.22683", "authors": ["Zhuohang Jiang", "Xu Yuan", "Haohao Qu", "Shanru Lin", "Kanglong Liu", "Wenqi Fan", "Qing Li"], "title": "SUPERGLASSES: Benchmarking Vision Language Models as Intelligent Agents for AI Smart Glasses", "comment": null, "summary": "The rapid advancement of AI-powered smart glasses, one of the hottest wearable devices, has unlocked new frontiers for multimodal interaction, with Visual Question Answering (VQA) over external knowledge sources emerging as a core application. Existing Vision Language Models (VLMs) adapted to smart glasses are typically trained and evaluated on traditional multimodal datasets; however, these datasets lack the variety and realism needed to reflect smart glasses usage scenarios and diverge from their specific challenges, where accurately identifying the object of interest must precede any external knowledge retrieval. To bridge this gap, we introduce SUPERGLASSES, the first comprehensive VQA benchmark built on real-world data entirely collected by smart glasses devices. SUPERGLASSES comprises 2,422 egocentric image-question pairs spanning 14 image domains and 8 query categories, enriched with full search trajectories and reasoning annotations. We evaluate 26 representative VLMs on this benchmark, revealing significant performance gaps. To address the limitations of existing models, we further propose SUPERLENS, a multimodal smart glasses agent that enables retrieval-augmented answer generation by integrating automatic object detection, query decoupling, and multimodal web search. Our agent achieves state-of-the-art performance, surpassing GPT-4o by 2.19 percent, and highlights the need for task-specific solutions in smart glasses VQA scenarios.", "AI": {"tldr": "They introduce SUPERGLASSES, a real-world smart-glasses VQA benchmark with egocentric data and rich annotations, show large performance gaps across 26 VLMs, and propose SUPERLENS, an agent that couples object detection, query decoupling, and multimodal web search for retrieval-augmented answering, outperforming GPT-4o by 2.19%.", "motivation": "Traditional multimodal datasets and VLM evaluations do not match smart glasses usage, where identifying the user\u2019s object of interest must precede any knowledge retrieval. There is a lack of realistic, egocentric data and task-specific methods for VQA with external knowledge on wearables.", "method": "1) Build SUPERGLASSES: 2,422 egocentric image\u2013question pairs from smart glasses across 14 domains and 8 query categories, with search trajectories and reasoning annotations. 2) Benchmark 26 representative VLMs. 3) Propose SUPERLENS, a smart-glasses VQA agent integrating automatic object detection, query decoupling, and multimodal web search to enable retrieval-augmented generation.", "result": "Large performance gaps are observed across existing VLMs on SUPERGLASSES. SUPERLENS achieves state-of-the-art performance on the benchmark, surpassing GPT-4o by 2.19%.", "conclusion": "Realistic, task-specific evaluation is crucial for smart-glasses VQA. SUPERGLASSES provides a needed benchmark, and SUPERLENS demonstrates that integrating object-of-interest detection with retrieval leads to superior results, motivating specialized solutions over generic VLMs for this scenario."}}
{"id": "2602.23079", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23079", "abs": "https://arxiv.org/abs/2602.23079", "authors": ["Boyang Zhang", "Yang Zhang"], "title": "Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has enabled powerful authorship inference capabilities, raising growing concerns about unintended deanonymization risks in textual data such as news articles. In this work, we introduce an LLM agent designed to evaluate and mitigate such risks through a structured, interpretable pipeline. Central to our framework is the proposed $\\textit{SALA}$ (Stylometry-Assisted LLM Analysis) method, which integrates quantitative stylometric features with LLM reasoning for robust and transparent authorship attribution. Experiments on large-scale news datasets demonstrate that $\\textit{SALA}$, particularly when augmented with a database module, achieves high inference accuracy in various scenarios. Finally, we propose a guided recomposition strategy that leverages the agent's reasoning trace to generate rewriting prompts, effectively reducing authorship identifiability while preserving textual meaning. Our findings highlight both the deanonymization potential of LLM agents and the importance of interpretable, proactive defenses for safeguarding author privacy.", "AI": {"tldr": "They propose SALA, a stylometry-assisted LLM agent that combines quantitative writing-style features with LLM reasoning to attribute authorship accurately and provide interpretable traces; they also introduce guided recomposition prompts to rewrite text to reduce identifiability while preserving meaning.", "motivation": "LLMs increasingly enable authorship inference, creating deanonymization risks for textual data (e.g., news). There is a need for robust, transparent methods to both assess and mitigate such privacy threats.", "method": "An interpretable LLM-agent pipeline (SALA) that fuses stylometric feature analysis with LLM reasoning; includes an optional database module to improve attribution by retrieving author/style evidence. They further use the agent\u2019s reasoning trace to generate targeted rewriting prompts (guided recomposition) to obfuscate stylistic fingerprints without changing semantics.", "result": "On large-scale news datasets, SALA achieves high authorship inference accuracy across scenarios, with the database module yielding additional gains. The guided recomposition strategy reduces authorship identifiability while maintaining textual meaning.", "conclusion": "LLM agents have strong deanonymization potential, but integrating stylometry with interpretable reasoning enables both accurate attribution and privacy-preserving defenses. Proactive, transparent tools like SALA and guided recomposition are important for safeguarding author privacy."}}
{"id": "2602.22822", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22822", "abs": "https://arxiv.org/abs/2602.22822", "authors": ["Yunhua Zhong", "Yixuan Tang", "Yifan Li", "Jie Yang", "Pan Liu", "Jun Xia"], "title": "FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics", "comment": "28 pages, preprint version", "summary": "The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.", "AI": {"tldr": "Introduces FlexMS, a flexible benchmark framework to build and systematically evaluate diverse deep-learning models for tandem mass spectrum prediction, offering standardized datasets, metrics, and retrieval-style evaluations to guide model choice for molecular identification.", "motivation": "MS/MS spectra provide key fragmentation cues for molecule identification, but missing experimental spectra and heterogeneous modeling practices impede fair assessment and practical deployment. The field lacks well-defined benchmarks, making model comparison and selection difficult.", "method": "Design a modular framework (FlexMS) that: (1) dynamically composes many architecture variants; (2) trains/evaluates them on preprocessed public datasets; (3) reports multiple metrics; and (4) runs targeted studies on factors such as dataset structural diversity, learning rate and data sparsity, pretraining benefits, metadata ablations, and cross-domain transfer. It also includes retrieval-style benchmarks that score candidate molecule matches using predicted spectra.", "result": "Provides empirical insights into what most affects spectrum-prediction performance across models and settings, demonstrates effects of hyperparameters, pretraining, and data sparsity, analyzes transferability across domains, and establishes retrieval evaluations that reflect real identification scenarios. The abstract does not report specific numerical gains but emphasizes actionable guidance for model selection.", "conclusion": "FlexMS standardizes and clarifies evaluation for mass-spectrum prediction, enabling informed model selection and facilitating practical molecular identification through retrieval-based assessment; it lays groundwork for comparable, extensible benchmarking in this area."}}
{"id": "2602.22689", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.22689", "abs": "https://arxiv.org/abs/2602.22689", "authors": ["Joonsung Jeon", "Woo Jae Kim", "Suhyeon Ha", "Sooel Son", "Sung-Eui Yoon"], "title": "No Caption, No Problem: Caption-Free Membership Inference via Model-Fitted Embeddings", "comment": "Accepted to ICLR 2026", "summary": "Latent diffusion models have achieved remarkable success in high-fidelity text-to-image generation, but their tendency to memorize training data raises critical privacy and intellectual property concerns. Membership inference attacks (MIAs) provide a principled way to audit such memorization by determining whether a given sample was included in training. However, existing approaches assume access to ground-truth captions. This assumption fails in realistic scenarios where only images are available and their textual annotations remain undisclosed, rendering prior methods ineffective when substituted with vision-language model (VLM) captions. In this work, we propose MoFit, a caption-free MIA framework that constructs synthetic conditioning inputs that are explicitly overfitted to the target model's generative manifold. Given a query image, MoFit proceeds in two stages: (i) model-fitted surrogate optimization, where a perturbation applied to the image is optimized to construct a surrogate in regions of the model's unconditional prior learned from member samples, and (ii) surrogate-driven embedding extraction, where a model-fitted embedding is derived from the surrogate and then used as a mismatched condition for the query image. This embedding amplifies conditional loss responses for member samples while leaving hold-outs relatively less affected, thereby enhancing separability in the absence of ground-truth captions. Our comprehensive experiments across multiple datasets and diffusion models demonstrate that MoFit consistently outperforms prior VLM-conditioned baselines and achieves performance competitive with caption-dependent methods.", "AI": {"tldr": "MoFit introduces a caption-free membership inference attack for latent diffusion models by synthesizing model-fitted conditioning signals that accentuate loss differences between training members and non-members, outperforming VLM-caption baselines and approaching caption-dependent methods.", "motivation": "Text-to-image latent diffusion models can memorize training data, but most MIAs rely on ground-truth captions\u2014unavailable in many real audits. Replacing them with VLM-generated captions degrades performance. The paper aims to audit memorization without needing any captions.", "method": "Two-stage pipeline: (i) model-fitted surrogate optimization\u2014apply and optimize a small perturbation to the query image so the result aligns with regions of the model\u2019s unconditional prior that are more influenced by training members; (ii) surrogate-driven embedding extraction\u2014derive a model-fitted embedding from the surrogate and use it as a mismatched condition when evaluating the query image, which amplifies conditional loss for members more than for non-members, enabling separation for MIA decisions.", "result": "Across datasets and diffusion backbones, MoFit consistently outperforms VLM-captioned MIA baselines and reaches performance close to methods that rely on true captions.", "conclusion": "Caption-free MIA against diffusion models is feasible and effective. Overfitting synthetic conditioning to the model\u2019s generative manifold creates amplified loss signals that separate members from non-members, improving practical auditing where captions are unavailable."}}
{"id": "2602.23136", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23136", "abs": "https://arxiv.org/abs/2602.23136", "authors": ["Jayadev Billa"], "title": "Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs", "comment": "22 pages, 11 tables, 2 figures. Code: https://github.com/jb1999/modality_collapse_paper", "summary": "Multimodal LLMs can process speech and images, but they cannot hear a speaker's voice or see an object's texture. We show this is not a failure of encoding: speaker identity, emotion, and visual attributes survive through every LLM layer (3--55$\\times$ above chance in linear probes), yet removing 64--71% of modality-specific variance improves decoder loss. The decoder has no learned use for these directions; their presence is noise.\n  We formalize this as a mismatched decoder problem: a decoder trained on text can only extract information along text-aligned directions. Accessible information is bounded by the Generalized Mutual Information (GMI), with degradation scaling with distributional distance and decoder sensitivity. The bound is a property of the decoder's scoring rule, not of any particular architecture; it applies whether non-text inputs arrive through a learned projection, a discrete codebook, or no explicit adapter at all. We validate this across five models spanning speech and vision. A controlled experiment (two Prismatic VLMs differing only in encoder text-alignment) confirms the bottleneck is the decoder's scoring rule, not the encoder or projection. A LoRA intervention demonstrates the fix: training with an emotion objective improves emotion accessibility ($+$7.5%) without affecting other attributes, confirming that the training objective determines what becomes accessible.", "AI": {"tldr": "Multimodal LLMs already encode rich non-textual cues (voice identity, emotion, visual texture), but text-trained decoders cannot use them because they only read out information along text-aligned directions. Accessibility is bounded by the decoder\u2019s scoring rule (via GMI), and targeted training objectives (e.g., LoRA with an emotion loss) make those attributes decodable.", "motivation": "Despite impressive multimodal capabilities, current LLMs underuse modality-specific information like speaker identity or image texture. The paper asks whether this is due to poor encoding or a mismatch in what the text-trained decoder can extract, and seeks a principled bound on accessible information and a practical remedy.", "method": "1) Linear probing across layers to test whether speaker/visual attributes survive encoding. 2) Variance ablations removing modality-specific directions to see impact on decoding loss. 3) Theoretical framing as a mismatched decoder problem with a Generalized Mutual Information (GMI) bound tied to the decoder\u2019s scoring rule. 4) Empirical validation across five speech/vision models, including a controlled pair of Prismatic VLMs differing only in encoder text-alignment. 5) A LoRA intervention adding an emotion objective to adjust decoder accessibility.", "result": "- Probes: modality-specific attributes remain throughout layers at 3\u201355\u00d7 above chance. - Removing 64\u201371% of modality-specific variance improves decoder loss, implying those directions are unused noise to the decoder. - GMI bound predicts accessibility degradation with greater distributional mismatch and decoder sensitivity; holds irrespective of projection methods. - Controlled Prismatic VLMs implicate the decoder\u2019s scoring rule as the bottleneck. - LoRA with an emotion objective boosts emotion accessibility by +7.5% without harming other attributes.", "conclusion": "The bottleneck is the text-trained decoder\u2019s scoring rule, not the encoder or architecture. Accessible information is determined by the training objective; thus, aligning objectives (e.g., lightweight targeted fine-tuning) selectively unlocks non-text attributes. This explains why standard multimodal LLMs \u201ccan\u2019t hear\u201d voice or \u201csee\u201d texture despite encoding them and provides a general, architecture-agnostic path to fix it."}}
{"id": "2602.22839", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22839", "abs": "https://arxiv.org/abs/2602.22839", "authors": ["Hao Zheng", "Guozhao Mo", "Xinru Yan", "Qianhao Yuan", "Wenkai Zhang", "Xuanang Chen", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun"], "title": "DeepPresenter: Environment-Grounded Reflection for Agentic Presentation Generation", "comment": null, "summary": "Presentation generation requires deep content research, coherent visual design, and iterative refinement based on observation. However, existing presentation agents often rely on predefined workflows and fixed templates. To address this, we present DeepPresenter, an agentic framework that adapts to diverse user intents, enables effective feedback-driven refinement, and generalizes beyond a scripted pipeline. Specifically, DeepPresenter autonomously plans, renders, and revises intermediate slide artifacts to support long-horizon refinement with environmental observations. Furthermore, rather than relying on self-reflection over internal signals (e.g., reasoning traces), our environment-grounded reflection conditions the generation process on perceptual artifact states (e.g., rendered slides), enabling the system to identify and correct presentation-specific issues during execution. Results on the evaluation set covering diverse presentation-generation scenarios show that DeepPresenter achieves state-of-the-art performance, and the fine-tuned 9B model remains highly competitive at substantially lower cost. Our project is available at: https://github.com/icip-cas/PPTAgent", "AI": {"tldr": "DeepPresenter is an adaptive presentation-generation agent that plans, renders, observes, and revises slides using environment-grounded reflection on rendered artifacts, achieving state-of-the-art results and offering a competitive, lower-cost 9B model.", "motivation": "Presentation creation demands research, coherent visual design, and iterative refinement, but most existing agents rely on rigid, template-driven workflows that lack adaptability and feedback-based improvement. The work aims to build a system that can flexibly handle diverse intents and improve slides through observable feedback rather than scripted steps.", "method": "Introduce DeepPresenter, an agentic framework that autonomously plans, renders, and iteratively revises intermediate slide artifacts. It replaces internal self-reflection with environment-grounded reflection conditioned on perceptual states (e.g., rendered slides), enabling long-horizon refinement and correction of presentation-specific issues. The framework adapts dynamically to user intent and avoids hard-coded pipelines.", "result": "Across a diverse evaluation set of presentation scenarios, DeepPresenter attains state-of-the-art performance. A fine-tuned 9B model remains highly competitive while substantially reducing cost. Code and resources are released at https://github.com/icip-cas/PPTAgent.", "conclusion": "Grounding reflection in observable slide artifacts allows the agent to detect and fix issues during execution, improving quality, adaptability, and cost-efficiency in presentation generation beyond fixed-template approaches."}}
{"id": "2602.22695", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22695", "abs": "https://arxiv.org/abs/2602.22695", "authors": ["Yu Chen", "Zewei He", "Xingyu Liu", "Zixuan Chen", "Zheming Lu"], "title": "GFRRN: Explore the Gaps in Single Image Reflection Removal", "comment": "CVPR26", "summary": "Prior dual-stream methods with the feature interaction mechanism have achieved remarkable performance in single image reflection removal (SIRR). However, they often struggle with (1) semantic understanding gap between the features of pre-trained models and those of reflection removal models, and (2) reflection label inconsistencies between synthetic and real-world training data. In this work, we first adopt the parameter efficient fine-tuning (PEFT) strategy by integrating several learnable Mona layers into the pre-trained model to align the training directions. Then, a label generator is designed to unify the reflection labels for both synthetic and real-world data. In addition, a Gaussian-based Adaptive Frequency Learning Block (G-AFLB) is proposed to adaptively learn and fuse the frequency priors, and a Dynamic Agent Attention (DAA) is employed as an alternative to window-based attention by dynamically modeling the significance levels across windows (inter-) and within an individual window (intra-). These components constitute our proposed Gap-Free Reflection Removal Network (GFRRN). Extensive experiments demonstrate the effectiveness of our GFRRN, achieving superior performance against state-of-the-art SIRR methods.", "AI": {"tldr": "GFRRN is a reflection-removal network that closes semantic and labeling gaps by PEFT-based alignment (Mona layers) and a unified label generator, and strengthens representation with adaptive frequency learning and dynamic attention, yielding state-of-the-art performance.", "motivation": "Dual-stream SIRR models are limited by (1) a semantic understanding gap between features from generic pre-trained models and task-specific reflection-removal features, and (2) inconsistent reflection labels between synthetic and real-world data, which degrades training and generalization.", "method": "Insert learnable Mona layers into the pre-trained backbone for parameter-efficient fine-tuning to align features with SIRR; build a label generator to standardize reflection labels across synthetic and real data; propose a Gaussian-based Adaptive Frequency Learning Block (G-AFLB) to learn/fuse frequency priors; replace window-based attention with Dynamic Agent Attention (DAA) to model both inter-window and intra-window significance; integrate all into the Gap-Free Reflection Removal Network (GFRRN).", "result": "Extensive experiments indicate GFRRN achieves superior performance versus state-of-the-art SIRR methods on benchmarks, demonstrating consistent quantitative and qualitative gains.", "conclusion": "Bridging semantic and labeling gaps via PEFT and unified labels, combined with adaptive frequency modeling and dynamic attention, substantially improves single-image reflection removal and enhances robustness/generalization."}}
{"id": "2602.23184", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23184", "abs": "https://arxiv.org/abs/2602.23184", "authors": ["Sara Rosenthal", "Yannis Katsis", "Vraj Shah", "Lihong He", "Lucian Popa", "Marina Danilevsky"], "title": "MTRAG-UN: A Benchmark for Open Challenges in Multi-Turn RAG Conversations", "comment": "5 pages, 3 figures", "summary": "We present MTRAG-UN, a benchmark for exploring open challenges in multi-turn retrieval augmented generation, a popular use of large language models. We release a benchmark of 666 tasks containing over 2,800 conversation turns across 6 domains with accompanying corpora. Our experiments show that retrieval and generation models continue to struggle on conversations with UNanswerable, UNderspecified, and NONstandalone questions and UNclear responses. Our benchmark is available at https://github.com/IBM/mt-rag-benchmark", "AI": {"tldr": "Introduces MTRAG-UN, a multi-turn RAG benchmark (666 tasks, ~2,800 turns, 6 domains) showing current retrieval and generation models struggle with unanswerable, underspecified, non-standalone queries, and unclear responses.", "motivation": "Multi-turn retrieval-augmented generation is widely used but lacks targeted evaluation of hard, realistic conversational challenges (e.g., unanswerable or context-dependent queries). A standardized benchmark is needed to expose and measure these failure modes.", "method": "Curate a benchmark of 666 tasks spanning ~2,800 dialogue turns across six domains with associated corpora. Categorize conversations to stress UNanswerable, UNderspecified, NONstandalone questions, and UNclear responses. Evaluate representative retrieval and generation models on these tasks.", "result": "Across experiments, both retrieval and generation models underperform on the targeted categories, revealing persistent weaknesses in handling unanswerable, underspecified, context-dependent queries, and ambiguous responses in multi-turn settings.", "conclusion": "MTRAG-UN provides a focused, publicly available benchmark that surfaces key limitations of current multi-turn RAG systems and aims to catalyze research on robust reasoning, retrieval, and clarification strategies."}}
{"id": "2602.22842", "categories": ["cs.AI", "cs.CE", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.22842", "abs": "https://arxiv.org/abs/2602.22842", "authors": ["Tan Bui-Thanh"], "title": "The AI Research Assistant: Promise, Peril, and a Proof of Concept", "comment": "11 pages, 1 figure", "summary": "Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration.\n  Working with multiple AI assistants, we extended results beyond what manual work achieved, formulating and proving several theorems with AI assistance. The collaboration revealed both remarkable capabilities and critical limitations. AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation. However, every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction.\n  We document the complete research workflow with unusual transparency, revealing patterns in successful human-AI mathematical collaboration and identifying failure modes researchers must anticipate. Our experience suggests that, when used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise.", "AI": {"tldr": "Case-study evidence that AI, when tightly supervised, can accelerate creative mathematical research: new error representations and bounds for Hermite quadrature were discovered via human\u2011AI collaboration; AI was strong at algebraic/proof exploration and drafting but required continuous expert verification and direction.", "motivation": "Test the claim that AI can contribute beyond routine calculation in pure/applied mathematics, while assessing risks of error. Provide transparent, empirical documentation of a real research effort to understand when and how AI helps, and where it fails.", "method": "Conduct a detailed case study on Hermite quadrature rules. Use multiple AI assistants to search for patterns, conjecture statements, manipulate algebra, explore proof avenues, synthesize literature, and prepare LaTeX. Humans supply problem framing, strategy, and rigorous verification. Document the full workflow and catalog success patterns and failure modes.", "result": "Extended prior manual results, producing several theorems including novel error representations and bounds for Hermite quadrature. Demonstrated that AI can efficiently handle algebraic manipulations, systematic proof exploration, cross-referencing literature, and drafting. Found that each step still needed human mathematical intuition and rigorous checking; identified recurring pitfalls and errors by AI.", "conclusion": "AI can meaningfully accelerate mathematical discovery if used with skepticism, verification protocols, and deep human expertise. It is not autonomous: success depends on human oversight, problem formulation, and careful error checking; nonetheless, the collaboration yielded publishable, nontrivial advances and a reusable workflow template."}}
{"id": "2602.22712", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22712", "abs": "https://arxiv.org/abs/2602.22712", "authors": ["Yuankai Chen", "Kai Lin", "Qihong Wu", "Xinxuan Yang", "Jiashuo Lai", "Ruoen Chen", "Haonan Shi", "Minfan He", "Meihua Wang"], "title": "UFO-DETR: Frequency-Guided End-to-End Detector for UAV Tiny Objects", "comment": "6 pages, 6 figures, published to 2026 International Conference on Computer Supported Cooperative Work in Design", "summary": "Small target detection in UAV imagery faces significant challenges such as scale variations, dense distribution, and the dominance of small targets. Existing algorithms rely on manually designed components, and general-purpose detectors are not optimized for UAV images, making it difficult to balance accuracy and complexity. To address these challenges, this paper proposes an end-to-end object detection framework, UFO-DETR, which integrates an LSKNet-based backbone network to optimize the receptive field and reduce the number of parameters. By combining the DAttention and AIFI modules, the model flexibly models multi-scale spatial relationships, improving multi-scale target detection performance. Additionally, the DynFreq-C3 module is proposed to enhance small target detection capability through cross-space frequency feature enhancement. Experimental results show that, compared to RT-DETR-L, the proposed method offers significant advantages in both detection performance and computational efficiency, providing an efficient solution for UAV edge computing.", "AI": {"tldr": "UFO-DETR is an end-to-end UAV object detector that pairs a lightweight LSKNet backbone with multi-scale attention (DAttention + AIFI) and a frequency-enhanced module (DynFreq-C3) to better detect small, densely packed targets. It surpasses RT-DETR-L in both accuracy and efficiency, making it suitable for UAV edge deployment.", "motivation": "Small-object-heavy UAV imagery suffers from scale variation and dense layouts; general-purpose detectors with hand-crafted components fail to balance accuracy and computational cost for on-board/edge inference.", "method": "Integrate an LSKNet-based backbone to expand/optimize receptive fields while reducing parameters; combine DAttention and AIFI to flexibly model multi-scale spatial relationships; add DynFreq-C3 for cross-space frequency feature enhancement targeting small objects; deliver an end-to-end DETR-style framework (UFO-DETR) optimized for UAV imagery.", "result": "Experiments show significant gains in detection performance and computational efficiency over RT-DETR-L (no specific metrics reported in the abstract), indicating better suitability for resource-constrained UAV scenarios.", "conclusion": "UFO-DETR improves multi-scale and small-target detection at lower complexity, offering a practical, edge-friendly solution for UAV applications; the frequency-enhanced features and multi-scale attention with a lightweight backbone are key to its advantage."}}
{"id": "2602.23197", "categories": ["cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.23197", "abs": "https://arxiv.org/abs/2602.23197", "authors": ["Chungpa Lee", "Jy-yong Sohn", "Kangwook Lee"], "title": "Fine-Tuning Without Forgetting In-Context Learning: A Theoretical Analysis of Linear Attention Models", "comment": null, "summary": "Transformer-based large language models exhibit in-context learning, enabling adaptation to downstream tasks via few-shot prompting with demonstrations. In practice, such models are often fine-tuned to improve zero-shot performance on downstream tasks, allowing them to solve tasks without examples and thereby reducing inference costs. However, fine-tuning can degrade in-context learning, limiting the performance of fine-tuned models on tasks not seen during fine-tuning. Using linear attention models, we provide a theoretical analysis that characterizes how fine-tuning objectives modify attention parameters and identifies conditions under which this leads to degraded few-shot performance. We show that fine-tuning all attention parameters can harm in-context learning, whereas restricting updates to the value matrix improves zero-shot performance while preserving in-context learning. We further show that incorporating an auxiliary few-shot loss enhances in-context learning primarily on the target task, at the expense of degraded in-context learning ability on tasks not seen during fine-tuning. We empirically validate our theoretical results.", "AI": {"tldr": "Fine-tuning can erode in-context learning (ICL). A linear-attention analysis shows that updating all attention parameters harms few-shot ability; restricting updates to the value (V) matrix improves zero-shot while preserving ICL. Adding an auxiliary few-shot loss boosts ICL mainly on the target task but reduces ICL on unseen tasks. Results are empirically validated.", "motivation": "Practitioners fine-tune LLMs to improve zero-shot performance and cut inference costs, but this often degrades few-shot ICL\u2014especially on tasks not seen during fine-tuning. The paper seeks principled guidance on how fine-tuning objectives and parameter subsets affect this trade-off.", "method": "Provide a theoretical analysis in linear attention models of how fine-tuning alters attention parameters and ICL behavior. Compare full attention-parameter updates versus restricting updates to the value matrix. Study adding an auxiliary few-shot loss. Validate predictions empirically.", "result": "Characterize conditions under which fine-tuning degrades ICL. Show that fine-tuning all attention parameters harms few-shot performance; restricting updates to the value matrix preserves ICL and improves zero-shot. Incorporating a few-shot auxiliary loss increases ICL mostly on the target task while degrading ICL on unseen tasks. Empirical experiments corroborate the theory.", "conclusion": "To balance zero-shot gains with preserved ICL, avoid updating all attention parameters; prefer value-only fine-tuning. Auxiliary few-shot losses can yield task-specific ICL improvements but at the cost of reduced general ICL on unseen tasks; fine-tuning choices should reflect deployment-task diversity."}}
{"id": "2602.22879", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22879", "abs": "https://arxiv.org/abs/2602.22879", "authors": ["Xingcheng Fu", "Shengpeng Wang", "Yisen Gao", "Xianxian Li", "Chunpei Li", "Qingyun Sun", "Dongran Yu"], "title": "Towards LLM-Empowered Knowledge Tracing via LLM-Student Hierarchical Behavior Alignment in Hyperbolic Space", "comment": "9 pages, 6 figures, Accepted to AAAI 2026", "summary": "Knowledge Tracing (KT) diagnoses students' concept mastery through continuous learning state monitoring in education.Existing methods primarily focus on studying behavioral sequences based on ID or textual information.While existing methods rely on ID-based sequences or shallow textual features, they often fail to capture (1) the hierarchical evolution of cognitive states and (2) individualized problem difficulty perception due to limited semantic modeling. Therefore, this paper proposes a Large Language Model Hyperbolic Aligned Knowledge Tracing(L-HAKT). First, the teacher agent deeply parses question semantics and explicitly constructs hierarchical dependencies of knowledge points; the student agent simulates learning behaviors to generate synthetic data. Then, contrastive learning is performed between synthetic and real data in hyperbolic space to reduce distribution differences in key features such as question difficulty and forgetting patterns. Finally, by optimizing hyperbolic curvature, we explicitly model the tree-like hierarchical structure of knowledge points, precisely characterizing differences in learning curve morphology for knowledge points at different levels. Extensive experiments on four real-world educational datasets validate the effectiveness of our Large Language Model Hyperbolic Aligned Knowledge Tracing (L-HAKT) framework.", "AI": {"tldr": "L-HAKT combines LLM-driven semantic parsing with hyperbolic representations and contrastive alignment (synthetic vs. real sequences) to capture hierarchical knowledge structures and personalized difficulty, achieving validated gains on four KT datasets.", "motivation": "Conventional KT methods rely on IDs or shallow text, which struggle to represent (1) the hierarchical evolution of students\u2019 cognitive states and (2) individualized perceptions of item difficulty due to limited semantic modeling.", "method": "A teacher LLM parses question semantics and constructs an explicit hierarchy of knowledge points; a student LLM simulates learning behaviors to generate synthetic sequences. Contrastive learning in hyperbolic space aligns synthetic and real data distributions (e.g., difficulty, forgetting). Curvature is optimized to encode tree-like knowledge structures and capture different learning-curve morphologies across hierarchy levels.", "result": "Extensive experiments on four real-world datasets demonstrate the framework\u2019s effectiveness, indicating improved KT performance and better modeling of difficulty and forgetting patterns compared with existing approaches.", "conclusion": "LLM-based semantic structuring plus hyperbolic geometry and contrastive alignment provides a principled way to model hierarchical knowledge and individualized dynamics in KT, reducing distribution gaps and yielding performance gains across datasets."}}
{"id": "2602.22716", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22716", "abs": "https://arxiv.org/abs/2602.22716", "authors": ["Guanting Ye", "Qiyan Zhao", "Wenhao Yu", "Liangyu Yuan", "Mingkai Li", "Xiaofeng Zhang", "Jianmin Ji", "Yanyong Zhang", "Qing Jiang", "Ka-Veng Yuen"], "title": "SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs", "comment": "CVPR 2026", "summary": "3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. The vanilla RoPE formulation fails to preserve essential three-dimensional spatial structures when encoding 3D tokens, and its relative distance computation overlooks angular dependencies, hindering the model's ability to capture directional variations in visual representations. To overcome these limitations, we introduce Spherical Coordinate-based Positional Embedding (SoPE). Our method maps point-cloud token indices into a 3D spherical coordinate space, enabling unified modeling of spatial locations and directional angles. This formulation preserves the inherent geometric structure of point-cloud data, enhances spatial awareness, and yields more consistent and expressive geometric representations for multimodal learning. In addition, we introduce a multi-scale frequency mixing strategy to fuse feature information across different frequency domains. Experimental results on multiple 3D scene benchmarks validate the effectiveness of our approach, while real-world deployment experiments further demonstrate its strong generalization capability.", "AI": {"tldr": "They replace RoPE with a spherical-coordinate positional embedding (SoPE) plus multi\u2011scale frequency mixing to better capture 3D spatial structure and direction in 3D LVLMs, yielding improved benchmark performance and real\u2011world generalization.", "motivation": "RoPE, inherited from 2D/sequence models, is ill\u2011suited for 3D tokens: it does not preserve full 3D geometry and ignores angular/directional relationships, which are crucial for point\u2011cloud understanding in multimodal tasks.", "method": "Introduce Spherical Coordinate\u2011based Positional Embedding (SoPE) that maps point\u2011cloud token indices into spherical coordinates (e.g., radius, azimuth, elevation) to jointly encode spatial locations and directions; incorporate a multi\u2011scale frequency mixing strategy to fuse information across frequency bands; use SoPE as a drop\u2011in replacement for RoPE within 3D LVLMs.", "result": "On multiple 3D scene benchmarks, models with SoPE outperform RoPE\u2011based baselines; additional real\u2011world deployment shows stronger generalization (no specific numbers provided in the abstract).", "conclusion": "Encoding 3D tokens in spherical coordinates with multi\u2011scale frequency mixing preserves geometric structure, enhances spatial awareness, and produces more expressive representations for 3D LVLMs, making SoPE a superior positional encoding to RoPE for 3D multimodal understanding."}}
{"id": "2602.23225", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23225", "abs": "https://arxiv.org/abs/2602.23225", "authors": ["Pengxiang Li", "Dilxat Muhtar", "Lu Yin", "Tianlong Chen", "Shiwei Liu"], "title": "Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?", "comment": null, "summary": "Diffusion Language Models (DLMs) are often advertised as enabling parallel token generation, yet practical fast DLMs frequently converge to left-to-right, autoregressive (AR)-like decoding dynamics. In contrast, genuinely non-AR generation is promising because it removes AR's sequential bottleneck, better exploiting parallel hardware to reduce synchronization/communication overhead and improve latency scaling with output length. We argue that a primary driver of AR-like decoding is a mismatch between DLM objectives and the highly sequential structure of widely used training data, including standard pretraining corpora and long chain-of-thought (CoT) supervision. Motivated by this diagnosis, we propose NAP (Non-Autoregressive Parallel DLMs), a proof-of-concept, data-centric approach that better aligns supervision with non-AR parallel decoding. NAP curates examples as multiple independent reasoning trajectories and couples them with a parallel-forced decoding strategy that encourages multi-token parallel updates. Across math reasoning benchmarks, NAP yields stronger performance under parallel decoding than DLMs trained on standard long CoT data, with gains growing as parallelism increases. Our results suggest that revisiting data and supervision is a principled direction for mitigating AR-like behavior and moving toward genuinely non-autoregressive parallel generation in DLMs. Our code is available at https://github.com/pixeli99/NAP.", "AI": {"tldr": "NAP is a data-centric recipe for training diffusion language models to generate tokens in parallel without collapsing into autoregressive, left-to-right decoding; it uses multiple independent reasoning trajectories plus parallel-forced decoding and yields better, more scalable parallel performance on math benchmarks than standard long CoT training.", "motivation": "Despite claims of parallelism, practical DLMs often behave autoregressively, negating latency and hardware-parallelism benefits. The authors argue this stems from a mismatch between DLM training objectives and highly sequential training data (pretraining corpora and long CoT), which implicitly enforce left-to-right structure.", "method": "Curate supervision as multiple independent reasoning trajectories per example to break strict sequential dependencies, and pair this with a parallel-forced decoding strategy that explicitly encourages multi-token parallel updates in DLMs. Evaluate on mathematical reasoning tasks under varying degrees of parallel decoding.", "result": "Compared to DLMs trained on standard long CoT data, models trained with NAP achieve higher accuracy under parallel decoding, and the advantage widens as the level of parallelism increases across math reasoning benchmarks.", "conclusion": "Aligning data and supervision with non-AR decoding mitigates AR-like collapse in DLMs. NAP provides evidence that data-centric design is a principled path toward genuinely non-autoregressive, latency-efficient parallel text generation."}}
{"id": "2602.22897", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.22897", "abs": "https://arxiv.org/abs/2602.22897", "authors": ["Xiaoxi Li", "Wenxiang Jiao", "Jiarui Jin", "Shijian Wang", "Guanting Dong", "Jiajie Jin", "Hao Wang", "Yinuo Wang", "Ji-Rong Wen", "Yuan Lu", "Zhicheng Dou"], "title": "OmniGAIA: Towards Native Omni-Modal AI Agents", "comment": null, "summary": "Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.", "AI": {"tldr": "Presents OmniGAIA, a benchmark for evaluating omni-modal (video, audio, image) agents on deep, tool-integrated reasoning, and OmniAtlas, an omni-modal foundation agent trained to improve multi-turn tool use and cross-modal reasoning.", "motivation": "Current multi-modal LLMs are mostly bi-modal (e.g., vision-language) and lack unified omni-modal perception, deep reasoning, and tool-usage needed for general AI assistants. A benchmark and native omni-modal agent are needed to close this gap.", "method": "Construct OmniGAIA using an omni-modal event graph that synthesizes complex, multi-hop, cross-modal queries requiring external tools and multi-turn execution. Propose OmniAtlas, a native omni-modal agent with active omni-modal perception and tool-integrated reasoning. Train with hindsight-guided tree exploration to generate trajectories and apply OmniDPO for fine-grained error correction.", "result": "Introduces a comprehensive benchmark (OmniGAIA) and shows that OmniAtlas effectively enhances tool-use capabilities of existing open-source models across video, audio, and image modalities (no quantitative results provided in the abstract).", "conclusion": "The benchmark, agent, and training strategy together move toward next-generation omni-modal AI assistants capable of active perception, deep reasoning, and multi-turn tool use for real-world scenarios."}}
{"id": "2602.22717", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22717", "abs": "https://arxiv.org/abs/2602.22717", "authors": ["Shuoqi Chen", "Yujia Wu", "Geoffrey P. Luke"], "title": "IRSDE-Despeckle: A Physics-Grounded Diffusion Model for Generalizable Ultrasound Despeckling", "comment": "12 pages main text + 6 pages appendix, 7 figures main + 3 figures appendix, 3 tables main + 1 table appendix. Preprint", "summary": "Ultrasound imaging is widely used for real-time, noninvasive diagnosis, but speckle and related artifacts reduce image quality and can hinder interpretation. We present a diffusion-based ultrasound despeckling method built on the Image Restoration Stochastic Differential Equations framework. To enable supervised training, we curate large paired datasets by simulating ultrasound images from speckle-free magnetic resonance images using the Matlab UltraSound Toolbox. The proposed model reconstructs speckle-suppressed images while preserving anatomically meaningful edges and contrast. On a held-out simulated test set, our approach consistently outperforms classical filters and recent learning-based despeckling baselines. We quantify prediction uncertainty via cross-model variance and show that higher uncertainty correlates with higher reconstruction error, providing a practical indicator of difficult or failure-prone regions. Finally, we evaluate sensitivity to simulation probe settings and observe domain shift, motivating diversified training and adaptation for robust clinical deployment.", "AI": {"tldr": "Diffusion-based ultrasound despeckling trained on MR-to-simulated-ultrasound pairs outperforms baselines on simulations, preserves anatomy, provides uncertainty cues, but shows domain shift with probe settings.", "motivation": "Ultrasound is fast and noninvasive but speckle noise degrades interpretability; paired clean targets for supervised learning are scarce, hindering effective despeckling that preserves clinically relevant edges and contrast.", "method": "Adapt Image Restoration SDEs to ultrasound despeckling. Create large supervised pairs by simulating ultrasound from speckle-free MR images via the Matlab UltraSound Toolbox. Train the diffusion model to reconstruct speckle-suppressed images; estimate prediction uncertainty via cross-model variance. Test robustness to probe setting variations to assess domain shift.", "result": "On a held-out simulated dataset, the method consistently surpasses classical filters and recent learning baselines while maintaining anatomical edges/contrast. Uncertainty measured across models positively correlates with reconstruction error. Sensitivity analyses reveal performance drops under mismatched probe settings, indicating domain shift.", "conclusion": "Diffusion-based despeckling with simulated supervision is promising and provides useful uncertainty estimates, but domain shift from simulation and probe variability limits clinical readiness; diversified training and adaptation are needed for robust real-world deployment."}}
{"id": "2602.23266", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23266", "abs": "https://arxiv.org/abs/2602.23266", "authors": ["Siyuan Liu", "Jiahui Xu", "Feng Jiang", "Kuang Wang", "Zefeng Zhao", "Chu-Ren Huang", "Jinghang Gu", "Changqing Yin", "Haizhou Li"], "title": "Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems", "comment": null, "summary": "Achieving human-like responsiveness is a critical yet challenging goal for cascaded spoken dialogue systems. Conventional ASR-LLM-TTS pipelines follow a strictly sequential paradigm, requiring complete transcription and full reasoning before speech synthesis can begin, which results in high response latency. We propose the Discourse-Aware Dual-Track Streaming Response (DDTSR) framework, a low-latency architecture that enables listen-while-thinking and speak-while-thinking. DDTSR is built upon three key mechanisms: (1) connective-guided small-large model synergy, where an auxiliary small model generates minimal-committal discourse connectives while a large model performs knowledge-intensive reasoning in parallel; (2) streaming-based cross-modal collaboration, which dynamically overlaps ASR, LLM inference, and TTS to advance the earliest speakable moment; and (3) curriculum-learning-based discourse continuity enhancement, which maintains coherence and logical consistency between early responses and subsequent reasoning outputs. Experiments on two spoken dialogue benchmarks demonstrate that DDTSR reduces response latency by 19%-51% while preserving discourse quality. Further analysis shows that DDTSR functions as a plug-and-play module compatible with diverse LLM backbones, and remains robust across varying utterance lengths, indicating strong practicality and scalability for real-time spoken interaction.", "AI": {"tldr": "DDTSR is a low-latency spoken-dialogue framework that overlaps ASR, LLM, and TTS, using a small model to emit safe discourse connectives while a large model reasons in parallel, cutting response delay by 19\u201351% without hurting coherence.", "motivation": "Conventional ASR\u2192LLM\u2192TTS pipelines are strictly sequential, forcing systems to wait for full transcription and full reasoning before speaking, which creates unacceptable latency for real-time, human-like dialogue.", "method": "The Discourse-Aware Dual-Track Streaming Response (DDTSR) framework introduces: (1) connective-guided small\u2013large model synergy where a small model produces minimal-commitment discourse connectives while a large model performs knowledge-heavy reasoning in parallel; (2) streaming cross-modal collaboration to overlap ASR, LLM inference, and TTS and trigger speech at the earliest safe moment; and (3) curriculum learning to maintain discourse continuity between early utterances and later, more detailed reasoning outputs.", "result": "On two spoken-dialogue benchmarks, DDTSR reduces response latency by 19%\u201351% while preserving discourse quality; it operates as a plug-and-play component across different LLM backbones and remains robust over varied utterance lengths.", "conclusion": "Listen-while-thinking and speak-while-thinking can substantially lower perceived delay in spoken dialogue without sacrificing coherence, and DDTSR is practical and scalable for real-time use due to its modularity and robustness."}}
{"id": "2602.22953", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22953", "abs": "https://arxiv.org/abs/2602.22953", "authors": ["Elron Bandel", "Asaf Yehudai", "Lilach Eden", "Yehoshua Sagron", "Yotam Perlitz", "Elad Venezian", "Natalia Razinkov", "Natan Ergas", "Shlomit Shachor Ifergan", "Segev Shlomov", "Michal Jacovi", "Leshem Choshen", "Liat Ein-Dor", "Yoav Katz", "Michal Shmueli-Scheuer"], "title": "General Agent Evaluation", "comment": null, "summary": "The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.", "AI": {"tldr": "They introduce principles, a unified protocol, and a practical framework (Exgentic) to fairly evaluate general-purpose agents across diverse environments, benchmarking five agents on six settings and launching an open leaderboard that shows general agents can match domain-specific ones without environment-specific tuning.", "motivation": "General-purpose agents are promised but under-realized; existing agents are specialized and current benchmarks bake in domain-specific integrations, preventing fair assessment. There has been no systematic, general evaluation of such agents, creating a gap the paper seeks to fill.", "method": "Define conceptual evaluation principles; design a Unified Protocol to standardize agent\u2013benchmark integration; implement Exgentic, a framework operationalizing the protocol; evaluate five prominent agent implementations over six heterogeneous environments; publish an Open General Agent Leaderboard.", "result": "General agents demonstrate cross-environment generalization and achieve performance on par with domain-specific baselines without any environment-specific tuning. The authors also deliver an open protocol, framework, and leaderboard for ongoing evaluation.", "conclusion": "General-purpose agents can be evaluated systematically and show competitive performance without bespoke integrations. The released protocol, framework, and leaderboard provide infrastructure intended to catalyze standardized research and comparative progress in general-agent development."}}
{"id": "2602.22727", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22727", "abs": "https://arxiv.org/abs/2602.22727", "authors": ["Yangguang Lin", "Quan Fang", "Yufei Li", "Jiachen Sun", "Junyu Gao", "Jitao Sang"], "title": "HulluEdit: Single-Pass Evidence-Consistent Subspace Editing for Mitigating Hallucinations in Large Vision-Language Models", "comment": "accepted at CVPR 2026", "summary": "Object hallucination in Large Vision-Language Models (LVLMs) significantly hinders their reliable deployment. Existing methods struggle to balance efficiency and accuracy: they often require expensive reference models and multiple forward passes, or apply static edits that risk suppressing genuine visual evidence. To address this, we introduce HulluEdit, a single-pass, reference-free intervention framework. Our core innovation is orthogonal subspace editing: we decompose the hidden states of the model into orthogonal subspaces - visual evidence, conflicting priors, and residual uncertainty - enabling selective suppression of hallucinatory patterns without interfering with visual grounding. This approach mathematically guarantees that edits applied to the prior subspace leave the visual component entirely unaffected. Extensive experiments show that HulluEdit achieves state-of-the-art hallucination reduction on benchmarks including POPE and CHAIR across diverse architectures, while preserving general capabilities on MME and maintaining efficient inference. Our method consistently outperforms contrastive decoding and static subspace editing baselines, offering a new pathway toward more trustworthy LVLMs.", "AI": {"tldr": "HulluEdit introduces a single-pass, reference-free intervention that decomposes LVLM hidden states into orthogonal subspaces and selectively suppresses hallucination-inducing priors without disturbing visual evidence, achieving SOTA hallucination reduction while preserving general abilities and efficiency.", "motivation": "Object hallucinations undermine the reliability of LVLMs. Prior fixes either rely on costly reference models and multiple passes or use static edits that can also dampen genuine visual signals. A method is needed that is efficient, adaptive, and preserves true visual grounding.", "method": "Orthogonal subspace editing of hidden representations: decompose hidden states into three orthogonal components\u2014visual evidence, conflicting priors, and residual uncertainty\u2014then suppress the prior-related subspace. The orthogonality gives a mathematical guarantee that edits to the prior component leave the visual component unaffected. The approach is single-pass and reference-free, and is applicable across architectures.", "result": "Across benchmarks (POPE, CHAIR) the method achieves state-of-the-art hallucination reduction, maintains general capability on MME, and keeps inference efficient. It outperforms contrastive decoding and static subspace editing baselines.", "conclusion": "Selective, orthogonally constrained editing can curb hallucinations without harming visual grounding, offering a practical, efficient path toward more trustworthy LVLMs and generalizing across model families."}}
{"id": "2602.23286", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23286", "abs": "https://arxiv.org/abs/2602.23286", "authors": ["Sungho Park", "Jueun Kim", "Wook-Shin Han"], "title": "SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables", "comment": "10 pages, 5 figures. Published as a conference paper at ICLR 2026. Project page: https://sparta-projectpage.github.io/", "summary": "Real-world Table-Text question answering (QA) tasks require models that can reason across long text and source tables, traversing multiple hops and executing complex operations such as aggregation. Yet existing benchmarks are small, manually curated - and therefore error-prone - and contain shallow questions that seldom demand more than two hops or invoke aggregations, grouping, or other advanced analytical operations expressible in natural-language queries. We present SPARTA, an end-to-end construction framework that automatically generates large-scale Table-Text QA benchmarks with lightweight human validation, requiring only one quarter of the annotation time of HybridQA. The framework first constructs a reference fact database by enriching each source table with grounding tables whose tuples are atomic facts automatically extracted from the accompanying unstructured passages, then synthesizes nested queries whose number of nested predicates matches the desired hop count. To ensure that every SQL statement is executable and that its verbalization yields a fluent, human-sounding question, we propose two novel techniques: provenance-based refinement, which rewrites any syntactically valid query that returns a non-empty result, and realistic-structure enforcement, which confines generation to post-order traversals of the query graph. The resulting pipeline produces thousands of high-fidelity question-answer pairs covering aggregations, grouping, and deep multi-hop reasoning across text and tables. On SPARTA, state-of-the-art models that reach over 70 F1 on HybridQA or over 50 F1 on OTT-QA drop by more than 30 F1 points, exposing fundamental weaknesses in current cross-modal reasoning. Our benchmark, construction code, and baseline models are available at https://github.com/pshlego/SPARTA/tree/main.", "AI": {"tldr": "SPARTA is an automated framework to build large-scale table\u2013text QA benchmarks with deep multi-hop and analytical reasoning, producing fluent questions from executable SQL and revealing >30 F1 drops for SOTA models, thus exposing major cross-modal reasoning gaps.", "motivation": "Existing table\u2013text QA benchmarks are small, manually curated, error-prone, and dominated by shallow questions (\u22642 hops) lacking aggregations/grouping, limiting progress and masking model weaknesses.", "method": "1) Build a reference fact DB by enriching each source table with grounding tables of atomic facts automatically extracted from accompanying passages. 2) Synthesize nested SQL queries whose predicate depth matches a target hop count. 3) Ensure executability and fluent question verbalization via: (a) provenance-based refinement to rewrite queries that (while syntactically valid) fail to produce usable results; (b) realistic-structure enforcement by constraining generation to post-order traversals of the query graph. 4) Light human validation; 1/4 of HybridQA\u2019s annotation time.", "result": "The pipeline yields thousands of high-fidelity QA pairs covering aggregations, grouping, and deep multi-hop reasoning across text and tables. State-of-the-art models that score >70 F1 on HybridQA or >50 F1 on OTT-QA drop by over 30 F1 points on SPARTA.", "conclusion": "SPARTA provides a scalable, validated way to create challenging, realistic table\u2013text QA benchmarks, efficiently surfaces fundamental weaknesses in current cross-modal reasoning systems, and offers resources (benchmark, code, baselines) for the community."}}
{"id": "2602.22963", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22963", "abs": "https://arxiv.org/abs/2602.22963", "authors": ["Zehao Li", "Hongwei Yu", "Hao Jiang", "Qiang Sheng", "Yilong Xu", "Baolong Bi", "Yang Li", "Zhenlong Yuan", "Yujun Cai", "Zhaoqi Wang"], "title": "FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning", "comment": null, "summary": "Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is sparse, fragmented, or requires external verification. To address these limitations, we propose FactGuard, an agentic framework for video misinformation detection that formulates verification as an iterative reasoning process built upon MLLMs. FactGuard explicitly assesses task ambiguity and selectively invokes external tools to acquire critical evidence, enabling progressive refinement of reasoning trajectories. To further strengthen this capability, we introduce a two-stage training strategy that combines domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decision making. Extensive experiments on FakeSV, FakeTT, and FakeVV demonstrate FactGuard's state-of-the-art performance and validate its excellent robustness and generalization capacity.", "AI": {"tldr": "FactGuard is an agentic, tool-augmented MLLM framework for video misinformation detection that iteratively reasons, gauges ambiguity, and selectively queries external tools; trained via agentic SFT plus decision-aware RL, it achieves SOTA on FakeSV/TT/VV with strong robustness and generalization.", "motivation": "Conventional MLLM-based detectors use fixed-depth reasoning and over-trust internal assumptions, failing when key evidence is sparse, fragmented, or requires outside verification. A system is needed that can recognize uncertainty, seek missing evidence, and calibrate decisions accordingly.", "method": "Verification is framed as iterative, ambiguity-aware reasoning over video and text. The agent explicitly estimates task ambiguity and, when needed, invokes external tools to gather critical evidence, refining its reasoning trajectory. Training uses two stages: (1) domain-specific, agentic supervised fine-tuning to teach tool-using reasoning patterns; (2) decision-aware reinforcement learning to optimize when/how to use tools and to make risk-sensitive decisions.", "result": "Across FakeSV, FakeTT, and FakeVV benchmarks, FactGuard delivers state-of-the-art accuracy and demonstrates robustness and generalization in diverse, challenging scenarios.", "conclusion": "Agentic, ambiguity-aware reasoning with selective tool use\u2014optimized by decision-aware RL\u2014substantially improves video misinformation detection, yielding SOTA performance with strong robustness and transfer potential."}}
{"id": "2602.22734", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22734", "abs": "https://arxiv.org/abs/2602.22734", "authors": ["Muzi Tao", "Chufan Shi", "Huijuan Wang", "Shengbang Tong", "Xuezhe Ma"], "title": "Asymmetric Idiosyncrasies in Multimodal Models", "comment": "Project page: https://muzi-tao.github.io/asymmetric-idiosyncrasies/", "summary": "In this work, we study idiosyncrasies in the caption models and their downstream impact on text-to-image models. We design a systematic analysis: given either a generated caption or the corresponding image, we train neural networks to predict the originating caption model. Our results show that text classification yields very high accuracy (99.70\\%), indicating that captioning models embed distinctive stylistic signatures. In contrast, these signatures largely disappear in the generated images, with classification accuracy dropping to at most 50\\% even for the state-of-the-art Flux model. To better understand this cross-modal discrepancy, we further analyze the data and find that the generated images fail to preserve key variations present in captions, such as differences in the level of detail, emphasis on color and texture, and the distribution of objects within a scene. Overall, our classification-based framework provides a novel methodology for quantifying both the stylistic idiosyncrasies of caption models and the prompt-following ability of text-to-image systems.", "AI": {"tldr": "They show captioning models have strong, identifiable stylistic \u201csignatures\u201d in text, but these signatures largely vanish in images produced by text-to-image models, revealing gaps in prompt fidelity. They propose a classification-based framework to quantify both effects.", "motivation": "Understand whether idiosyncratic styles of captioning models persist into downstream text-to-image generation, and create a principled way to measure stylistic biases and prompt-following ability across modalities.", "method": "Train classifiers to predict which caption model produced a sample using two inputs: (1) the caption text itself; (2) the image generated from that caption by various T2I systems. Compare classification accuracy across modalities and analyze which textual variations (detail level, color/texture emphasis, object layout) are preserved or lost in images.", "result": "Text-based classification is near-perfect (\u224899.7% accuracy), indicating distinct stylistic signatures across captioners. Image-based classification is much lower (\u226450% even for a strong model like Flux), showing that generated images do not reliably encode those stylistic differences. Analysis attributes this to missing preservation of detail level, color/texture emphasis, and object distribution.", "conclusion": "Caption models imprint clear, learnable styles in their outputs, but current T2I models do not maintain these nuances, pointing to limitations in prompt adherence. The proposed framework offers a quantitative tool for evaluating captioner style and T2I faithfulness, with implications for benchmarking, model selection, and robustness."}}
{"id": "2602.23300", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.23300", "abs": "https://arxiv.org/abs/2602.23300", "authors": ["Soumya Dutta", "Smruthi Balaji", "Sriram Ganapathy"], "title": "A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations", "comment": "Accepted to Elsevier Computer Speech and Language. 30 pages, 9 figures, 5 tables", "summary": "Emotion Recognition in Conversations (ERC) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple modalities. We propose Mixture of Speech-Text Experts for Recognition of Emotions (MiSTER-E), a modular Mixture-of-Experts (MoE) framework designed to decouple two core challenges in ERC: modality-specific context modeling and multimodal information fusion. MiSTER-E leverages large language models (LLMs) fine-tuned for both speech and text to provide rich utterance-level embeddings, which are then enhanced through a convolutional-recurrent context modeling layer. The system integrates predictions from three experts-speech-only, text-only, and cross-modal-using a learned gating mechanism that dynamically weighs their outputs. To further encourage consistency and alignment across modalities, we introduce a supervised contrastive loss between paired speech-text representations and a KL-divergence-based regulariza-tion across expert predictions. Importantly, MiSTER-E does not rely on speaker identity at any stage. Experiments on three benchmark datasets-IEMOCAP, MELD, and MOSI-show that our proposal achieves 70.9%, 69.5%, and 87.9% weighted F1-scores respectively, outperforming several baseline speech-text ERC systems. We also provide various ablations to highlight the contributions made in the proposed approach.", "AI": {"tldr": "MiSTER-E is a modular mixture-of-experts system for emotion recognition in conversations that uses LLM-derived speech and text embeddings, context modeling, and gated fusion with alignment losses to outperform multimodal baselines on IEMOCAP, MELD, and MOSI without using speaker identity.", "motivation": "ERC requires capturing temporal dynamics in multi-turn dialogue and fusing heterogeneous speech/text cues; existing approaches often entangle modality modeling with fusion and may rely on speaker identity, limiting robustness and generalization.", "method": "Use LLMs fine-tuned for speech and for text to extract utterance-level embeddings; apply a convolutional-recurrent layer for context over dialogue turns; build three experts\u2014speech-only, text-only, and cross-modal\u2014and combine them via a learned gating mechanism; enforce cross-modal consistency with supervised contrastive loss between paired speech-text representations and regularize expert agreement via KL divergence; no use of speaker IDs.", "result": "Achieves weighted F1 of 70.9% on IEMOCAP, 69.5% on MELD, and 87.9% on MOSI, surpassing several speech-text ERC baselines; ablations indicate each component contributes to gains.", "conclusion": "Decoupling modality-specific context modeling from multimodal fusion in an MoE framework, coupled with cross-modal alignment and prediction regularization, yields state-of-the-art ERC performance without speaker information and suggests a robust, generalizable design."}}
{"id": "2602.22968", "categories": ["cs.AI", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.22968", "abs": "https://arxiv.org/abs/2602.22968", "authors": ["Alaa Anani", "Tobias Lorenz", "Bernt Schiele", "Mario Fritz", "Jonas Fischer"], "title": "Certified Circuits: Stability Guarantees for Mechanistic Circuits", "comment": null, "summary": "Understanding how neural networks arrive at their predictions is essential for debugging, auditing, and deployment. Mechanistic interpretability pursues this goal by identifying circuits - minimal subnetworks responsible for specific behaviors. However, existing circuit discovery methods are brittle: circuits depend strongly on the chosen concept dataset and often fail to transfer out-of-distribution, raising doubts whether they capture concept or dataset-specific artifacts. We introduce Certified Circuits, which provide provable stability guarantees for circuit discovery. Our framework wraps any black-box discovery algorithm with randomized data subsampling to certify that circuit component inclusion decisions are invariant to bounded edit-distance perturbations of the concept dataset. Unstable neurons are abstained from, yielding circuits that are more compact and more accurate. On ImageNet and OOD datasets, certified circuits achieve up to 91% higher accuracy while using 45% fewer neurons, and remain reliable where baselines degrade. Certified Circuits puts circuit discovery on formal ground by producing mechanistic explanations that are provably stable and better aligned with the target concept. Code will be released soon!", "AI": {"tldr": "Certified Circuits wraps any circuit-discovery method with randomized subsampling to certify that neuron/edge inclusion decisions are stable to small, bounded edits of the concept dataset, abstaining on unstable components. This yields more compact, more accurate, and OOD-robust circuits (up to 91% higher accuracy with 45% fewer neurons on ImageNet/OOD).", "motivation": "Existing mechanistic interpretability pipelines produce circuits that are brittle and heavily dependent on the specific concept dataset, failing to transfer out of distribution and potentially reflecting dataset artifacts rather than true concepts. The field lacks formal guarantees that discovered circuits are stable to dataset perturbations.", "method": "Introduce a certification framework that treats any circuit discovery algorithm as a black box. Using randomized data subsampling, it certifies that each component\u2019s inclusion decision is invariant to bounded edit-distance perturbations of the concept dataset. Components without invariance guarantees are abstained from, yielding a certified subset that is provably stable.", "result": "Across ImageNet and several OOD datasets, certified circuits achieve up to 91% higher accuracy while using 45% fewer neurons and remain reliable where baseline methods degrade.", "conclusion": "The framework places circuit discovery on formal footing by producing mechanistic explanations with provable stability to dataset edits, improving alignment to the target concept, compactness, and OOD robustness. Code to be released."}}
{"id": "2602.22740", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22740", "abs": "https://arxiv.org/abs/2602.22740", "authors": ["Tongfei Chen", "Shuo Yang", "Yuguang Yang", "Linlin Yang", "Runtang Guo", "Changbai Li", "He Long", "Chunyu Xie", "Dawei Leng", "Baochang Zhang"], "title": "AMLRIS: Alignment-aware Masked Learning for Referring Image Segmentation", "comment": "ICLR 2026 conference paper", "summary": "Referring Image Segmentation (RIS) aims to segment an object in an image identified by a natural language expression. The paper introduces Alignment-Aware Masked Learning (AML), a training strategy to enhance RIS by explicitly estimating pixel-level vision-language alignment, filtering out poorly aligned regions during optimization, and focusing on trustworthy cues. This approach results in state-of-the-art performance on RefCOCO datasets and also enhances robustness to diverse descriptions and scenarios", "AI": {"tldr": "Proposes Alignment-Aware Masked Learning (AML) for referring image segmentation: estimate pixel-level vision\u2013language alignment and mask low-alignment regions during training, yielding state-of-the-art results on RefCOCO and stronger robustness to varied descriptions.", "motivation": "RIS often suffers from weak or noisy alignment between language expressions and image regions, causing models to learn from poorly aligned pixels and rely on spurious cues. The goal is to improve accuracy and robustness by focusing learning on reliably aligned regions.", "method": "During training, explicitly estimate pixel-level vision\u2013language alignment and use it to filter (mask) or down-weight poorly aligned pixels in the loss, so optimization emphasizes trustworthy cues. The strategy is model-agnostic and integrates into standard RIS pipelines.", "result": "Achieves state-of-the-art performance on RefCOCO datasets and demonstrates improved robustness across diverse linguistic descriptions and scenarios.", "conclusion": "Explicit alignment-aware masking during training is an effective, broadly applicable way to strengthen RIS, improving both accuracy and robustness by steering learning toward well-aligned supervisory signals."}}
{"id": "2602.23351", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23351", "abs": "https://arxiv.org/abs/2602.23351", "authors": ["Amita Kamath", "Jack Hessel", "Khyathi Chandu", "Jena D. Hwang", "Kai-Wei Chang", "Ranjay Krishna"], "title": "Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning", "comment": "TACL 2026", "summary": "The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., \"at the game today!\" is a more likely caption than \"a photo of 37 people standing behind a field\". We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is effective. Our findings highlight the need for more intentional training data curation methods, rather than counting on scale for emergence of reasoning capabilities.", "AI": {"tldr": "VLMs underperform on spatial, temporal, negation, and counting reasoning because training data exhibits reporting bias that omits tacit information; scaling data/model/languages doesn\u2019t fix it, but targeted annotations do\u2014so curate data intentionally.", "motivation": "Despite impressive performance, VLMs often fail at basic reasoning. The authors hypothesize this stems from how people naturally describe images\u2014omitting implicit details\u2014creating a reporting bias that deprives models of supervision for certain reasoning skills. They aim to test this hypothesis and guide better data practices.", "method": "Analyze the training corpora behind OpenCLIP, LLaVA\u20111.5, and Molmo using pragmatics-informed criteria to quantify representation of four reasoning types (spatial, temporal, negation, counting). Construct curated benchmarks probing these skills. Evaluate VLM performance and study effects of scaling data/model size, multilingual data, and adding targeted annotations that explicitly include tacit information.", "result": "Find under-representation of the four reasoning categories in the datasets and correspondingly poor model performance on curated benchmarks. Increasing dataset size, model size, or language coverage does not lead to emergence of these skills. Incorporating explicitly collected annotations that surface tacit information substantially improves performance on the targeted reasoning tasks.", "conclusion": "Reporting bias in web/synthetic training data suppresses key reasoning skills in VLMs. Emergent reasoning does not reliably arise from scale alone; deliberate data curation and targeted, tacit-information-rich annotations are effective and necessary to cultivate these capabilities."}}
{"id": "2602.22971", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22971", "abs": "https://arxiv.org/abs/2602.22971", "authors": ["Peiyao Xiao", "Xiaogang Li", "Chengliang Xu", "Jiayi Wang", "Ben Wang", "Zichao Chen", "Zeyu Wang", "Kejun Yu", "Yueqian Chen", "Xulin Liu", "Wende Xiao", "Bing Zhao", "Hu Wei"], "title": "SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy", "comment": null, "summary": "As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal benchmark specifically designed for scanning probe microscopy (SPM). We propose a fully automated data synthesis pipeline that ensures both high authority and low-cost. By employing Anchor-Gated Sieve (AGS) technology, we efficiently extract high-value image-text pairs from arXiv and journal papers published between 2023 and 2025. Through a hybrid cloud-local architecture where VLMs return only spatial coordinates \"llbox\" for local high-fidelity cropping, our pipeline achieves extreme token savings while maintaining high dataset purity. To accurately and objectively evaluate the performance of the LLMs, we introduce the Strict Imperfection Penalty F1 (SIP-F1) score. This metric not only establishes a rigorous capability hierarchy but also, for the first time, quantifies model \"personalities\" (Conservative, Aggressive, Gambler, or Wise). By correlating these results with model-reported confidence and perceived difficulty, we expose the true reasoning boundaries of current AI in complex physical scenarios. These insights establish SPM-Bench as a generalizable paradigm for automated scientific data synthesis.", "AI": {"tldr": "SPM-Bench is a PhD-level, multimodal benchmark for scanning probe microscopy built via an automated, token-efficient pipeline and evaluated with a strict SIP-F1 metric that ranks and profiles LLM/VLM behavior, offering a generalizable recipe for scientific data synthesis and assessment.", "motivation": "General-purpose LLMs struggle in specialized science domains, and existing benchmarks are compromised by contamination, low task complexity, and costly human curation. The authors aim to create a trustworthy, challenging, and affordable benchmark for SPM to probe genuine reasoning limits.", "method": "They build an automated data synthesis pipeline: (1) Anchor-Gated Sieve (AGS) extracts high-value image\u2013text pairs from arXiv/journals (2023\u20132025) to maximize authority and minimize contamination; (2) a hybrid cloud\u2013local workflow where VLMs return only spatial coordinates (\u201cllbox\u201d) for local, high-fidelity cropping to save tokens while preserving data purity; (3) a new Strict Imperfection Penalty F1 (SIP-F1) metric that imposes harsh penalties on mistakes; (4) correlate scores with model-reported confidence and task difficulty to characterize model \u201cpersonalities.\u201d", "result": "They obtain a high-authority, low-cost SPM dataset with extreme token savings and high purity. SIP-F1 establishes a rigorous capability hierarchy among models and, for the first time, quantifies behavioral profiles\u2014Conservative, Aggressive, Gambler, Wise. These analyses expose current models\u2019 true reasoning boundaries in complex physical scenarios.", "conclusion": "SPM-Bench delivers a robust, automated benchmark for a specialized scientific field and introduces SIP-F1 for strict, interpretable evaluation. The pipeline and evaluation paradigm are positioned as generalizable for broader automated scientific data synthesis and model assessment."}}
{"id": "2602.22742", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22742", "abs": "https://arxiv.org/abs/2602.22742", "authors": ["Akihisa Watanabe", "Qing Yu", "Edgar Simo-Serra", "Kent Fujiwara"], "title": "ProjFlow: Projection Sampling with Flow Matching for Zero-Shot Exact Spatial Motion Control", "comment": null, "summary": "Generating human motion with precise spatial control is a challenging problem. Existing approaches often require task-specific training or slow optimization, and enforcing hard constraints frequently disrupts motion naturalness. Building on the observation that many animation tasks can be formulated as a linear inverse problem, we introduce ProjFlow, a training-free sampler that achieves zero-shot, exact satisfaction of linear spatial constraints while preserving motion realism. Our key advance is a novel kinematics-aware metric that encodes skeletal topology. This metric allows the sampler to enforce hard constraints by distributing corrections coherently across the entire skeleton, avoiding the unnatural artifacts of naive projection. Furthermore, for sparse inputs, such as filling in long gaps between a few keyframes, we introduce a time-varying formulation using pseudo-observations that fade during sampling. Extensive experiments on representative applications, motion inpainting, and 2D-to-3D lifting, demonstrate that ProjFlow achieves exact constraint satisfaction and matches or improves realism over zero-shot baselines, while remaining competitive with training-based controllers.", "AI": {"tldr": "ProjFlow is a training-free sampler that enforces exact linear spatial constraints in human motion generation while maintaining realism by using a kinematics-aware metric and time-varying pseudo-observations for sparse inputs.", "motivation": "Precise spatial control in human motion generation is hard: existing methods need task-specific training or slow optimization, and hard constraints often degrade naturalness. The authors aim to achieve exact constraint satisfaction without sacrificing realism or requiring retraining.", "method": "Formulate many animation tasks as linear inverse problems and enforce constraints during sampling via projection under a kinematics-aware metric that encodes skeletal topology, distributing corrections coherently across joints. For sparse constraints (e.g., long gaps/keyframes), introduce time-varying pseudo-observations that fade during sampling to guide early steps and relax later.", "result": "Across motion inpainting and 2D-to-3D lifting, ProjFlow satisfies linear constraints exactly, matches or improves realism compared to zero-shot baselines, and remains competitive with training-based controllers\u2014all without additional training.", "conclusion": "A principled, training-free projection-based sampler with a skeletal-aware metric can provide precise, zero-shot spatial control in motion generation, avoiding artifacts of naive projection and offering competitive realism and flexibility across tasks."}}
{"id": "2602.22973", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22973", "abs": "https://arxiv.org/abs/2602.22973", "authors": ["Dimitrios P. Panagoulias", "Evangelia-Aikaterini Tsichrintzi", "Georgios Savvidis", "Evridiki Tsoureli-Nikita"], "title": "Modeling Expert AI Diagnostic Alignment via Immutable Inference Snapshots", "comment": null, "summary": "Human-in-the-loop validation is essential in safety-critical clinical AI, yet the transition between initial model inference and expert correction is rarely analyzed as a structured signal. We introduce a diagnostic alignment framework in which the AI-generated image based report is preserved as an immutable inference state and systematically compared with the physician-validated outcome. The inference pipeline integrates a vision-enabled large language model, BERT- based medical entity extraction, and a Sequential Language Model Inference (SLMI) step to enforce domain-consistent refinement prior to expert review. Evaluation on 21 dermatological cases (21 complete AI physician pairs) em- ployed a four-level concordance framework comprising exact primary match rate (PMR), semantic similarity-adjusted rate (AMR), cross-category alignment, and Comprehensive Concordance Rate (CCR). Exact agreement reached 71.4% and remained unchanged under semantic similarity (t = 0.60), while structured cross-category and differential overlap analysis yielded 100% comprehensive concordance (95% CI: [83.9%, 100%]). No cases demonstrated complete diagnostic divergence. These findings show that binary lexical evaluation substantially un- derestimates clinically meaningful alignment. Modeling expert validation as a structured transformation enables signal-aware quantification of correction dynamics and supports traceable, human aligned evaluation of image based clinical decision support systems.", "AI": {"tldr": "They formalize human-in-the-loop validation as a structured transformation from an immutable AI dermatology report to the physician-validated outcome, using a vision-LLM + BERT entity extraction + SLMI refinement. On 21 cases, exact matches were 71.4%, but comprehensive concordance reached 100% (no complete divergences), showing that simple lexical metrics understate clinically meaningful alignment.", "motivation": "Safety-critical clinical AI needs traceable, human-aligned evaluation. Existing assessments focus on final agreement or lexical matches and ignore the informative signal in expert corrections, risking underestimation of practical alignment and obscuring how models are corrected.", "method": "Introduce a diagnostic alignment framework that preserves the AI\u2019s image-based report as an immutable inference state and systematically compares it to the clinician-validated report. The pipeline integrates: (1) a vision-enabled LLM to generate reports, (2) BERT-based medical entity extraction, and (3) a Sequential Language Model Inference (SLMI) step enforcing domain-consistent refinement before expert review. Evaluation uses a four-level concordance scheme: exact Primary Match Rate (PMR), semantic similarity\u2013adjusted rate (AMR), cross-category alignment, and a Comprehensive Concordance Rate (CCR).", "result": "On 21 dermatology AI\u2013physician pairs: PMR = 71.4%; AMR showed no change (t = 0.60). Structured cross-category and differential-overlap analysis yielded CCR = 100% with 95% CI [83.9%, 100%]. No cases exhibited complete diagnostic divergence.", "conclusion": "Binary lexical agreement underestimates clinically meaningful AI\u2013expert alignment. Modeling expert validation as a structured transformation enables signal-aware measurement of correction dynamics and supports traceable, human-aligned evaluation of image-based clinical decision support systems."}}
{"id": "2602.22745", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22745", "abs": "https://arxiv.org/abs/2602.22745", "authors": ["Fengming Liu", "Tat-Jen Cham", "Chuanxia Zheng"], "title": "SPATIALALIGN: Aligning Dynamic Spatial Relationships in Video Generation", "comment": null, "summary": "Most text-to-video (T2V) generators prioritize aesthetic quality, but often ignoring the spatial constraints in the generated videos. In this work, we present SPATIALALIGN, a self-improvement framework that enhances T2V models capabilities to depict Dynamic Spatial Relationships (DSR) specified in text prompts. We present a zeroth-order regularized Direct Preference Optimization (DPO) to fine-tune T2V models towards better alignment with DSR. Specifically, we design DSR-SCORE, a geometry-based metric that quantitatively measures the alignment between generated videos and the specified DSRs in prompts, which is a step forward from prior works that rely on VLM for evaluation. We also conduct a dataset of text-video pairs with diverse DSRs to facilitate the study. Extensive experiments demonstrate that our fine-tuned model significantly out performs the baseline in spatial relationships. The code will be released in Link.", "AI": {"tldr": "SPATIALALIGN is a self-improvement framework that fine-tunes text-to-video models to better satisfy dynamic spatial relationships (DSR) in prompts using a zeroth-order regularized DPO objective, a geometry-based DSR-SCORE for alignment evaluation, and a curated DSR dataset\u2014yielding significantly better spatial alignment than baseline models.", "motivation": "Prevailing T2V generators emphasize visual aesthetics while neglecting precise spatial constraints; moreover, typical evaluations rely on vision-language models that can be unreliable for fine-grained geometric reasoning. There is a need for methods and metrics that explicitly target and measure dynamic spatial relationship adherence.", "method": "Introduce a zeroth-order regularized Direct Preference Optimization (DPO) procedure to fine-tune T2V models toward DSR alignment; design DSR-SCORE, a geometry-grounded metric that quantifies how well generated videos match text-specified dynamic spatial relationships; curate a diverse dataset of text\u2013video pairs with DSRs; use the metric-driven preferences to enable self-improvement of the generator.", "result": "Across extensive experiments, the fine-tuned model shows significant gains over baseline models in depicting spatial relationships as measured by DSR-SCORE (and likely corroborating metrics), indicating improved adherence to prompt-specified DSRs.", "conclusion": "Geometry-aware preference optimization guided by DSR-SCORE effectively steers T2V models to respect dynamic spatial constraints; the released dataset and forthcoming code should facilitate further research and reduce dependence on VLM-based evaluation."}}
{"id": "2602.22981", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22981", "abs": "https://arxiv.org/abs/2602.22981", "authors": ["Haohui Jia", "Zheng Chen", "Lingwei Zhu", "Xu Cao", "Yasuko Matsubara", "Takashi Matsubara", "Yasushi Sakurai"], "title": "RepSPD: Enhancing SPD Manifold Representation in EEGs via Dynamic Graphs", "comment": null, "summary": "Decoding brain activity from electroencephalography (EEG) is crucial for neuroscience and clinical applications. Among recent advances in deep learning for EEG, geometric learning stands out as its theoretical underpinnings on symmetric positive definite (SPD) allows revealing structural connectivity analysis in a physics-grounded manner. However, current SPD-based methods focus predominantly on statistical aggregation of EEGs, with frequency-specific synchronization and local topological structures of brain regions neglected. Given this, we propose RepSPD, a novel geometric deep learning (GDL)-based model. RepSPD implements a cross-attention mechanism on the Riemannian manifold to modulate the geometric attributes of SPD with graph-derived functional connectivity features. On top of this, we introduce a global bidirectional alignment strategy to reshape tangent-space embeddings, mitigating geometric distortions caused by curvature and thereby enhancing geometric consistency. Extensive experiments demonstrate that our proposed framework significantly outperforms existing EEG representation methods, exhibiting superior robustness and generalization capabilities.", "AI": {"tldr": "RepSPD is a geometric deep learning framework for EEG that fuses SPD-manifold representations with graph-based functional connectivity via Riemannian cross\u2011attention and a global bidirectional alignment of tangent\u2011space embeddings, yielding state\u2011of\u2011the\u2011art accuracy, robustness, and generalization.", "motivation": "EEG decoding benefits from SPD/Riemannian modeling, but existing SPD-based approaches largely rely on global statistical aggregation and overlook frequency-specific synchrony and local topological structure. Moreover, mappings between manifold and tangent spaces can introduce curvature-induced distortions that degrade consistency.", "method": "Construct SPD representations of EEG and derive functional connectivity graphs (presumably frequency-aware). Apply a cross\u2011attention mechanism defined on the Riemannian manifold to modulate SPD geometric attributes using graph-derived features. Introduce a global bidirectional alignment to reshape tangent\u2011space embeddings, mitigating curvature distortions and improving geometric consistency across samples/subjects.", "result": "Across extensive experiments, RepSPD surpasses contemporary EEG representation baselines, demonstrating stronger robustness and better generalization (no exact metrics reported in the abstract).", "conclusion": "Marrying manifold-aware attention with a tangent-space alignment strategy effectively leverages frequency-specific and local topological information while controlling geometric distortions, leading to superior EEG decoding performance and more faithful geometric representations suitable for neuroscience and clinical applications."}}
{"id": "2602.22759", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22759", "abs": "https://arxiv.org/abs/2602.22759", "authors": ["Yuan-Chih Chen", "Chun-Shien Lu"], "title": "Beyond Detection: Multi-Scale Hidden-Code for Natural Image Deepfake Recovery and Factual Retrieval", "comment": null, "summary": "Recent advances in image authenticity have primarily focused on deepfake detection and localization, leaving recovery of tampered contents for factual retrieval relatively underexplored. We propose a unified hidden-code recovery framework that enables both retrieval and restoration from post-hoc and in-generation watermarking paradigms. Our method encodes semantic and perceptual information into a compact hidden-code representation, refined through multi-scale vector quantization, and enhances contextual reasoning via conditional Transformer modules. To enable systematic evaluation for natural images, we construct ImageNet-S, a benchmark that provides paired image-label factual retrieval tasks. Extensive experiments on ImageNet-S demonstrate that our method exhibits promising retrieval and reconstruction performance while remaining fully compatible with diverse watermarking pipelines. This framework establishes a foundation for general-purpose image recovery beyond detection and localization.", "AI": {"tldr": "They present a unified hidden-code recovery framework that can both retrieve and restore original image content across post-hoc and in-generation watermarking. It packs semantic/perceptual cues into a compact code via multi-scale vector quantization and uses conditional Transformers for context reasoning. A new ImageNet-S benchmark is introduced; experiments show strong retrieval/reconstruction while remaining compatible with diverse watermarking pipelines.", "motivation": "Most image-authenticity research stops at detecting or localizing tampering (e.g., deepfakes), leaving the harder task of recovering factual content underexplored. There is also a lack of standardized evaluation for natural-image factual retrieval and a need for methods that work across different watermarking paradigms.", "method": "Encode images into a compact hidden-code that captures semantic and perceptual information; refine and discretize with multi-scale vector quantization; use conditional Transformer modules to enhance contextual reasoning for recovery. The framework supports both post-hoc and in-generation watermarking, enabling retrieval of ground-truth labels and restoration of image content.", "result": "On the proposed ImageNet-S paired image\u2013label benchmark, the method achieves promising retrieval accuracy and reconstruction quality and remains compatible with varied watermarking pipelines in experiments.", "conclusion": "A general, watermark-compatible recovery framework is feasible and can move the field beyond detection/localization toward factual retrieval and content restoration; ImageNet-S offers a basis for systematic evaluation."}}
{"id": "2602.22983", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.22983", "abs": "https://arxiv.org/abs/2602.22983", "authors": ["Xun Huang", "Simeng Qin", "Xiaoshuang Jia", "Ranjie Duan", "Huanqian Yan", "Zhitao Zeng", "Fei Yang", "Yang Liu", "Xiaojun Jia"], "title": "Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in jailbreak attacks. Owing to its conciseness and obscurity, classical Chinese can partially bypass existing safety constraints, exposing notable vulnerabilities in LLMs. Based on this observation, this paper proposes a framework, CC-BOS, for the automatic generation of classical Chinese adversarial prompts based on multi-dimensional fruit fly optimization, facilitating efficient and automated jailbreak attacks in black-box settings. Prompts are encoded into eight policy dimensions-covering role, behavior, mechanism, metaphor, expression, knowledge, trigger pattern and context; and iteratively refined via smell search, visual search, and cauchy mutation. This design enables efficient exploration of the search space, thereby enhancing the effectiveness of black-box jailbreak attacks. To enhance readability and evaluation accuracy, we further design a classical Chinese to English translation module. Extensive experiments demonstrate that effectiveness of the proposed CC-BOS, consistently outperforming state-of-the-art jailbreak attack methods.", "AI": {"tldr": "Classical Chinese can be leveraged to jailbreak LLMs; CC-BOS auto-generates such prompts using a multi-dimensional fruit fly optimization search and outperforms prior black-box jailbreak methods.", "motivation": "LLMs are vulnerable to jailbreaks and their susceptibility varies by language. Classical Chinese\u2019s concise and obscure style can evade safety filters, motivating a systematic, automated way to exploit\u2014and thus study\u2014this vulnerability in black-box settings.", "method": "Introduce CC-BOS: encode prompts along eight policy dimensions (role, behavior, mechanism, metaphor, expression, knowledge, trigger pattern, context). Use a multi-dimensional fruit fly optimization process with smell search, visual search, and Cauchy mutation to iteratively refine prompts. Add a Classical Chinese\u2192English translation module to aid readability and evaluation.", "result": "Across extensive experiments, CC-BOS achieves higher jailbreak success rates and efficiency than state-of-the-art black-box attack baselines; translation further supports clearer assessment. Specific metrics are not provided in the abstract but the gains are described as consistent.", "conclusion": "Classical Chinese is an effective attack vector against current LLM safety mechanisms. CC-BOS provides an efficient, automated black-box framework that reliably improves jailbreak performance, underscoring the need for multilingual and stylistic robustness in LLM safety defenses."}}
{"id": "2602.22779", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22779", "abs": "https://arxiv.org/abs/2602.22779", "authors": ["Chenhao Zheng", "Jieyu Zhang", "Jianing Zhang", "Weikai Huang", "Ashutosh Kumar", "Quan Kong", "Oncel Tuzel", "Chun-Liang Li", "Ranjay Krishna"], "title": "TrajTok: Learning Trajectory Tokens enables better Video Understanding", "comment": "CVPR 2026", "summary": "Tokenization in video models, typically through patchification, generates an excessive and redundant number of tokens. This severely limits video efficiency and scalability. While recent trajectory-based tokenizers offer a promising solution by decoupling video duration from token count, they rely on complex external segmentation and tracking pipelines that are slow and task-agnostic. We propose TrajTok, an end-to-end video tokenizer module that is fully integrated and co-trained with video models for a downstream objective, dynamically adapting its token granularity to semantic complexity, independent of video duration. TrajTok contains a unified segmenter that performs implicit clustering over pixels in both space and time to directly produce object trajectories in a single forward pass. By prioritizing downstream adaptability over pixel-perfect segmentation fidelity, TrajTok is lightweight and efficient, yet empirically improves video understanding performance. With TrajTok, we implement a video CLIP model trained from scratch (TrajViT2). It achieves the best accuracy at scale across both classification and retrieval benchmarks, while maintaining efficiency comparable to the best token-merging methods. TrajTok also proves to be a versatile component beyond its role as a tokenizer. We show that it can be seamlessly integrated as either a probing head for pretrained visual features (TrajAdapter) or an alignment connector in vision-language models (TrajVLM) with especially strong performance in long-video reasoning.", "AI": {"tldr": "TrajTok is an end-to-end, duration-invariant trajectory tokenizer that adaptively clusters pixels across space-time to produce object-level tokens, improving video model accuracy and efficiency without external tracking/segmentation. It powers TrajViT2 (video CLIP), TrajAdapter, and TrajVLM, achieving state-of-the-art classification/retrieval and strong long-video reasoning.", "motivation": "Patch-based tokenization yields too many redundant tokens, limiting efficiency and scalability. Prior trajectory tokenizers reduce tokens but depend on slow, task-agnostic external pipelines. There is a need for an integrated, co-trainable tokenizer that adapts token granularity to semantic content and serves downstream objectives.", "method": "Introduce TrajTok, a lightweight module with a unified segmenter that performs implicit spatiotemporal clustering in one forward pass to output object trajectories. It decouples token count from video duration and adapts token granularity to semantic complexity. TrajTok is co-trained with the video model (e.g., CLIP-style) and can also function as a probing head (TrajAdapter) or alignment connector (TrajVLM).", "result": "A from-scratch Video-CLIP model (TrajViT2) using TrajTok attains best-in-class accuracy at scale on classification and retrieval benchmarks, with efficiency on par with top token-merging methods. TrajTok further shows strong long-video reasoning when used in a VLM (TrajVLM) and performs well as a probing head (TrajAdapter).", "conclusion": "End-to-end trajectory tokenization via TrajTok yields compact, semantically adaptive, duration-invariant tokens that boost video understanding while remaining efficient. It removes dependency on external pipelines and serves as a versatile component across video CLIP and VLM settings, particularly benefiting long-video tasks."}}
{"id": "2602.23056", "categories": ["cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.23056", "abs": "https://arxiv.org/abs/2602.23056", "authors": ["Giona Fieni", "Joschua W\u00fcthrich", "Marc-Philippe Neumann", "Christopher H. Onder"], "title": "Learning-based Multi-agent Race Strategies in Formula 1", "comment": null, "summary": "In Formula 1, race strategies are adapted according to evolving race conditions and competitors' actions. This paper proposes a reinforcement learning approach for multi-agent race strategy optimization. Agents learn to balance energy management, tire degradation, aerodynamic interaction, and pit-stop decisions. Building on a pre-trained single-agent policy, we introduce an interaction module that accounts for the behavior of competitors. The combination of the interaction module and a self-play training scheme generates competitive policies, and agents are ranked based on their relative performance. Results show that the agents adapt pit timing, tire selection, and energy allocation in response to opponents, achieving robust and consistent race performance. Because the framework relies only on information available during real races, it can support race strategists' decisions before and during races.", "AI": {"tldr": "Multi-agent reinforcement learning framework for Formula 1 race strategy that extends a single-agent policy with an opponent-interaction module and self-play, yielding adaptive, opponent-aware decisions on pits, tires, and energy that use only live-available race data.", "motivation": "Race strategy must react to dynamic conditions and competitors. Handcrafted or single-agent approaches struggle to account for opponent behavior, aerodynamic effects, and coupled decisions (pit timing, tire wear, energy usage). A learning-based, opponent-aware method could deliver more robust, real-time strategies.", "method": "Start from a pre-trained single-agent policy; add an interaction module to model competitors\u2019 behavior and aerodynamic interactions. Train via self-play among multiple agents and rank them by relative performance. The policy optimizes a balance among energy management, tire degradation, aero effects, and pit-stop choices using only information available during actual races.", "result": "Agents adapt pit timing, tire compound selection, and energy allocation in response to opponents and on-track interactions, producing robust and consistent race performance across scenarios in the simulator. Relative-performance ranking identifies competitive policies.", "conclusion": "Opponent-aware, self-play RL on top of a single-agent prior can produce practical, robust strategies that operate with real-race information constraints, offering decision support to strategists pre- and in-race."}}
{"id": "2602.22785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22785", "abs": "https://arxiv.org/abs/2602.22785", "authors": ["Ling Wang", "Hao-Xiang Guo", "Xinzhou Wang", "Fuchun Sun", "Kai Sun", "Pengkun Liu", "Hang Xiao", "Zhong Wang", "Guangyuan Fu", "Eric Li", "Yang Liu", "Yikai Wang"], "title": "SceneTransporter: Optimal Transport-Guided Compositional Latent Diffusion for Single-Image Structured 3D Scene Generation", "comment": "published at iclr 2026", "summary": "We introduce SceneTransporter, an end-to-end framework for structured 3D scene generation from a single image. While existing methods generate part-level 3D objects, they often fail to organize these parts into distinct instances in open-world scenes. Through a debiased clustering probe, we reveal a critical insight: this failure stems from the lack of structural constraints within the model's internal assignment mechanism. Based on this finding, we reframe the task of structured 3D scene generation as a global correlation assignment problem. To solve this, SceneTransporter formulates and solves an entropic Optimal Transport (OT) objective within the denoising loop of the compositional DiT model. This formulation imposes two powerful structural constraints. First, the resulting transport plan gates cross-attention to enforce an exclusive, one-to-one routing of image patches to part-level 3D latents, preventing entanglement. Second, the competitive nature of the transport encourages the grouping of similar patches, a process that is further regularized by an edge-based cost, to form coherent objects and prevent fragmentation. Extensive experiments show that SceneTransporter outperforms existing methods on open-world scene generation, significantly improving instance-level coherence and geometric fidelity. Code and models will be publicly available at https://2019epwl.github.io/SceneTransporter/.", "AI": {"tldr": "SceneTransporter reframes single-image, structured 3D scene generation as a global correlation assignment and solves it via entropic Optimal Transport inside a compositional DiT, yielding one-to-one patch-to-latent routing and competitive grouping that produce coherent object instances and higher geometric fidelity than prior work.", "motivation": "Part-level 3D generators for open-world scenes often tangle parts and fail to form clear instances because internal assignment lacks structural constraints. The authors identify this with a debiased clustering probe and seek a principled mechanism to enforce instance structure during generation.", "method": "Formulate structured scene generation as a global assignment problem. Embed an entropic OT objective into the denoising loop of a compositional diffusion transformer (DiT). Use the OT transport plan to: (1) gate cross-attention, enforcing exclusive one-to-one routing of image patches to part-level 3D latents; (2) induce competitive grouping of similar patches, further regularized by an edge-based cost to avoid fragmentation and encourage coherent objects.", "result": "Across extensive experiments on open-world scene generation, the method outperforms existing approaches, showing substantial gains in instance-level coherence and geometric fidelity.", "conclusion": "Adding OT-based structural constraints to the denoising process yields disentangled, instance-consistent 3D scenes from a single image. The approach provides a general mechanism for imposing exclusivity and grouping in compositional 3D generation and sets a new state of the art; code/models to be released."}}
{"id": "2602.23092", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23092", "abs": "https://arxiv.org/abs/2602.23092", "authors": ["Zhuoliang Xie", "Fei Liu", "Zhenkun Wang", "Qingfu Zhang"], "title": "Enhancing CVRP Solver through LLM-driven Automatic Heuristic Design", "comment": null, "summary": "The Capacitated Vehicle Routing Problem (CVRP), a fundamental combinatorial optimization challenge, focuses on optimizing fleet operations under vehicle capacity constraints. While extensively studied in operational research, the NP-hard nature of CVRP continues to pose significant computational challenges, particularly for large-scale instances. This study presents AILS-AHD (Adaptive Iterated Local Search with Automatic Heuristic Design), a novel approach that leverages Large Language Models (LLMs) to revolutionize CVRP solving. Our methodology integrates an evolutionary search framework with LLMs to dynamically generate and optimize ruin heuristics within the AILS method. Additionally, we introduce an LLM-based acceleration mechanism to enhance computational efficiency. Comprehensive experimental evaluations against state-of-the-art solvers, including AILS-II and HGS, demonstrate the superior performance of AILS-AHD across both moderate and large-scale instances. Notably, our approach establishes new best-known solutions for 8 out of 10 instances in the CVRPLib large-scale benchmark, underscoring the potential of LLM-driven heuristic design in advancing the field of vehicle routing optimization.", "AI": {"tldr": "AILS-AHD uses large language models to automatically design and adapt ruin heuristics inside an adaptive iterated local search for CVRP, delivering state-of-the-art results and new best-known solutions on most large-scale benchmarks.", "motivation": "CVRP is NP-hard and challenging at scale; high-quality heuristics exist but are hard to handcraft and tune. The authors aim to reduce manual heuristic design effort and push performance on large instances by automating heuristic generation and acceleration.", "method": "Integrate LLMs into an evolutionary search within AILS to dynamically propose and refine ruin heuristics; embed these within the ILS framework; introduce an LLM-based acceleration component to speed computations; benchmark against leading solvers (AILS-II, HGS).", "result": "Across moderate and large instances, AILS-AHD outperforms strong baselines and sets new best-known solutions on 8/10 CVRPLib large-scale cases.", "conclusion": "Automating heuristic design with LLMs can materially advance CVRP performance, especially at scale, suggesting LLM-guided metaheuristics are a promising direction for vehicle routing and related combinatorial optimization problems."}}
{"id": "2602.22791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22791", "abs": "https://arxiv.org/abs/2602.22791", "authors": ["Taishu Arashima", "Hiroshi Kera", "Kazuhiko Kawamoto"], "title": "Robust Human Trajectory Prediction via Self-Supervised Skeleton Representation Learning", "comment": "11 pages main, 5 pages supplementary material", "summary": "Human trajectory prediction plays a crucial role in applications such as autonomous navigation and video surveillance. While recent works have explored the integration of human skeleton sequences to complement trajectory information, skeleton data in real-world environments often suffer from missing joints caused by occlusions. These disturbances significantly degrade prediction accuracy, indicating the need for more robust skeleton representations. We propose a robust trajectory prediction method that incorporates a self-supervised skeleton representation model pretrained with masked autoencoding. Experimental results in occlusion-prone scenarios show that our method improves robustness to missing skeletal data without sacrificing prediction accuracy, and consistently outperforms baseline models in clean-to-moderate missingness regimes.", "AI": {"tldr": "Pretrain a skeleton encoder with masked autoencoding to learn occlusion-robust features, plug it into a trajectory predictor, and achieve better human trajectory predictions under missing joints without hurting clean-condition accuracy.", "motivation": "Skeleton-based cues can improve trajectory prediction, but in the wild many joints are missing due to occlusions, causing large accuracy drops. A robustness-focused representation is needed so predictors can still exploit skeletons even when keypoints are absent.", "method": "Use self-supervised masked autoencoding on skeleton sequences to pretrain a representation that reconstructs masked joints, thereby learning structure-aware, occlusion-tolerant features. Integrate this pretrained skeleton encoder into a trajectory prediction model to fuse pose and trajectory information.", "result": "In occlusion-prone settings, the approach is more robust to missing skeletal data and consistently outperforms baseline models when missingness ranges from none to moderate, while maintaining accuracy on clean data.", "conclusion": "Masked autoencoding yields a robust skeleton representation that enhances trajectory prediction under realistic occlusions without trade-offs on clean data, suggesting a practical path for reliable pose-augmented prediction in navigation and surveillance."}}
{"id": "2602.23093", "categories": ["cs.AI", "cs.SI", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.23093", "abs": "https://arxiv.org/abs/2602.23093", "authors": ["Dhwanil M. Mori", "Neil F. Johnson"], "title": "Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents", "comment": null, "summary": "Near-future infrastructure systems may be controlled by autonomous AI agents that repeatedly request access to limited resources such as energy, bandwidth, or computing power. We study a simplified version of this setting using a framework where N AI-agents independently decide at each round whether to request one unit from a system with fixed capacity C. An AI version of \"Lord of the Flies\" arises in which controlling tribes emerge with their own collective character and identity. The LLM agents do not reduce overload or improve resource use, and often perform worse than if they were flipping coins to make decisions. Three main tribal types emerge: Aggressive (27.3%), Conservative (24.7%), and Opportunistic (48.1%). The more capable AI-agents actually increase the rate of systemic failure. Overall, our findings show that smarter AI-agents can behave dumber as a result of forming tribes.", "AI": {"tldr": "In a repeated shared-capacity game, multiple LLM agents self-organize into behavioral \u201ctribes\u201d (Aggressive, Conservative, Opportunistic) and collectively overload the system\u2014often performing worse than random coin-flips; increasing agent capability worsens systemic failure.", "motivation": "As autonomous AI agents begin to compete for scarce infrastructure resources (energy, bandwidth, compute), we need to understand whether independent decision-making yields coordination or harmful herding, and whether greater agent capability improves or degrades system reliability.", "method": "Simulate repeated rounds with N LLM agents that independently decide whether to request one unit from a system with fixed capacity C. Measure overload and utilization, classify emergent behavioral clusters (\u201ctribes\u201d), compare against a random (coin-flip) baseline, and vary agent capability.", "result": "Three clusters emerge\u2014Aggressive (27.3%), Conservative (24.7%), Opportunistic (48.1%). LLM agents fail to reduce overload or improve utilization and often underperform random. Higher-capability agents increase the rate of systemic failure.", "conclusion": "Uncoordinated LLM agents can form social-like coalitions that worsen collective outcomes; deploying smarter agents in shared-resource environments may heighten failure risk. Coordination mechanisms, incentive design, and protocol-level controls are needed to avoid emergent collapse."}}
{"id": "2602.22800", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22800", "abs": "https://arxiv.org/abs/2602.22800", "authors": ["Hanliang Du", "Zhangji Lu", "Zewei Cai", "Qijian Tang", "Qifeng Yu", "Xiaoli Liu"], "title": "GSTurb: Gaussian Splatting for Atmospheric Turbulence Mitigation", "comment": null, "summary": "Atmospheric turbulence causes significant image degradation due to pixel displacement (tilt) and blur, particularly in long-range imaging applications. In this paper, we propose a novel framework for atmospheric turbulence mitigation, GSTurb, which integrates optical flow-guided tilt correction and Gaussian splatting for modeling non-isoplanatic blur. The framework employs Gaussian parameters to represent tilt and blur, and optimizes them across multiple frames to enhance restoration. Experimental results on the ATSyn-static dataset demonstrate the effectiveness of our method, achieving a peak PSNR of 27.67 dB and SSIM of 0.8735. Compared to the state-of-the-art method, GSTurb improves PSNR by 1.3 dB (a 4.5% increase) and SSIM by 0.048 (a 5.8% increase). Additionally, on real datasets, including the TSRWGAN Real-World and CLEAR datasets, GSTurb outperforms existing methods, showing significant improvements in both qualitative and quantitative performance. These results highlight that combining optical flow-guided tilt correction with Gaussian splatting effectively enhances image restoration under both synthetic and real-world turbulence conditions. The code for this method will be available at https://github.com/DuhlLiamz/3DGS_turbulence/tree/main.", "AI": {"tldr": "GSTurb is a multi-frame turbulence mitigation framework that couples optical flow\u2013guided tilt correction with Gaussian splatting to model spatially varying (non-isoplanatic) blur, yielding state-of-the-art restoration on synthetic and real data.", "motivation": "Long-range imaging suffers from atmospheric turbulence that introduces frame-wise geometric jitter (tilt) and spatially varying blur, degrading recognition and surveillance imagery. Existing methods often treat blur as isoplanatic or handle tilt and blur separately, limiting real-world effectiveness.", "method": "Represent both pixel displacement (tilt) and blur via Gaussian parameters and optimize them jointly across multiple frames. Optical flow guides the tilt correction, while Gaussian splatting models non-isoplanatic blur, enabling spatially varying deblurring. The pipeline integrates these components into a unified restoration framework.", "result": "On ATSyn-static, GSTurb reaches 27.67 dB PSNR and 0.8735 SSIM, improving over the prior SOTA by +1.3 dB PSNR (~4.5%) and +0.048 SSIM (~5.8%). On TSRWGAN Real-World and CLEAR datasets, it surpasses existing approaches with notable qualitative and quantitative gains (specific real-data metrics not detailed in the abstract).", "conclusion": "Coupling optical flow\u2013guided tilt correction with Gaussian splatting for non-isoplanatic blur effectively restores turbulence-degraded imagery across synthetic and real scenarios. The approach sets a new benchmark on ATSyn-static and generalizes well to real data; code is promised for release, supporting reproducibility and further adoption."}}
{"id": "2602.23163", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.IT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.23163", "abs": "https://arxiv.org/abs/2602.23163", "authors": ["Usman Anwar", "Julianna Piskorz", "David D. Baek", "David Africa", "Jim Weatherall", "Max Tegmark", "Christian Schroeder de Witt", "Mihaela van der Schaar", "David Krueger"], "title": "A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring", "comment": "First two authors contributed equally", "summary": "Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these approaches inapplicable. We propose an alternative, \\textbf{decision-theoretic view of steganography}. Our central insight is that steganography creates an asymmetry in usable information between agents who can and cannot decode the hidden content (present within a steganographic signal), and this otherwise latent asymmetry can be inferred from the agents' observable actions. To formalise this perspective, we introduce generalised $\\mathcal{V}$-information: a utilitarian framework for measuring the amount of usable information within some input. We use this to define the \\textbf{steganographic gap} -- a measure that quantifies steganography by comparing the downstream utility of the steganographic signal to agents that can and cannot decode the hidden content. We empirically validate our formalism, and show that it can be used to detect, quantify, and mitigate steganographic reasoning in LLMs.", "AI": {"tldr": "They recast steganography in LLMs as a decision-theoretic problem and introduce generalized V-information to quantify usable information, defining a \u201csteganographic gap\u201d between agents who can or cannot decode hidden content. This framework detects, measures, and helps mitigate steganographic reasoning without needing a reference distribution.", "motivation": "LLMs may covertly embed information to evade oversight, but classical steganography detection assumes access to a reference distribution of non-steganographic signals\u2014unavailable in open-ended LLM outputs\u2014making existing approaches ineffective.", "method": "Adopt a decision-theoretic lens: formalize usable information via generalized V-information (utility-based information). Define the steganographic gap as the difference in downstream utility between agents who can and cannot decode the hidden message. Empirically evaluate on LLMs by comparing performance/utility across decoders vs. non-decoders and applying the framework to detection and mitigation procedures.", "result": "Empirical results show that the steganographic gap is detectable and quantifiable in LLM behaviors, enabling practical detection of hidden reasoning channels and demonstrating methods to reduce/mitigate such behavior.", "conclusion": "Generalized V-information and the steganographic gap offer a principled, reference-free way to identify and measure LLM steganography from observable actions, with demonstrated utility for detection and mitigation in practice."}}
{"id": "2602.23123", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23123", "abs": "https://arxiv.org/abs/2602.23123", "authors": ["Keito Inoshita"], "title": "Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection", "comment": null, "summary": "In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts into two presentation modes, BALANCED (neutralized text) and COOL (neutralized text + supplementary text), using an LLM. The Balance Monitoring Agent aggregates weekly information consumption patterns and generates personalized advice, while the Personal Guide Agent recommends a presentation mode according to consumer sensitivity. Experiments on 800 AG News articles demonstrated significant stimulus score reduction (up to 19.3%) and improved emotion balance while maintaining semantic preservation. Near-zero correlation between stimulus reduction and semantic preservation confirmed that the two are independently controllable. Category-level analysis revealed substantial reduction (17.8-33.8%) in Sports, Business, and Sci/Tech, whereas the effect was limited in the World category, where facts themselves are inherently high-stimulus. The proposed system provides a framework for supporting calm information reception of consumers without restricting access to the original text.", "AI": {"tldr": "MALLET is a multi-agent LLM system that detects and dampens emotionally sensational news text while preserving meaning, provides personalized guidance, and shows measurable stimulus reduction without sacrificing semantics.", "motivation": "In the attention economy, sensationalized media can overstimulate readers, impairing calm judgment. The authors seek a method to reduce emotional intensity in consumed content without censoring access or distorting meaning.", "method": "They design MALLET with four agents: (1) Emotion Analysis uses a 6-emotion BERT classifier to quantify stimulus intensity; (2) Emotion Adjustment rewrites text into BALANCED (neutralized) and COOL (neutralized plus supplementary context) using an LLM; (3) Balance Monitoring aggregates weekly consumption to provide personalized advice; (4) Personal Guide recommends a presentation mode based on user sensitivity. Evaluation on 800 AG News articles measures stimulus reduction, emotion balance, semantic preservation, their correlation, and category-level effects.", "result": "Stimulus scores decreased significantly (up to 19.3%), with improved emotion balance and maintained semantic preservation. Correlation between stimulus reduction and semantic preservation was near zero, indicating independent control. Category-wise reductions were large in Sports, Business, and Sci/Tech (17.8\u201333.8%) but limited in World news due to inherently high-stimulus facts.", "conclusion": "MALLET offers a practical framework for calmer information intake through controlled emotional detoxification and personalization, without restricting access to originals. Its independence between stimulus reduction and semantic preservation suggests tunable trade-offs; effectiveness varies by domain, with limitations in inherently intense news (World)."}}
{"id": "2602.22809", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22809", "abs": "https://arxiv.org/abs/2602.22809", "authors": ["Mingde Yao", "Zhiyuan You", "Tam-King Man", "Menglu Wang", "Tianfan Xue"], "title": "PhotoAgent: Agentic Photo Editing with Exploratory Visual Aesthetic Planning", "comment": "A fully automated, intelligent photo-editing agent that autonomously plans multi-step aesthetic enhancements, smartly chooses diverse editing tools, and enables everyday users to achieve professional-looking results without crafting complex prompts. Project page: https://github.com/mdyao/PhotoAgent", "summary": "With the recent fast development of generative models, instruction-based image editing has shown great potential in generating high-quality images. However, the quality of editing highly depends on carefully designed instructions, placing the burden of task decomposition and sequencing entirely on the user. To achieve autonomous image editing, we present PhotoAgent, a system that advances image editing through explicit aesthetic planning. Specifically, PhotoAgent formulates autonomous image editing as a long-horizon decision-making problem. It reasons over user aesthetic intent, plans multi-step editing actions via tree search, and iteratively refines results through closed-loop execution with memory and visual feedback, without requiring step-by-step user prompts. To support reliable evaluation in real-world scenarios, we introduce UGC-Edit, an aesthetic evaluation benchmark consisting of 7,000 photos and a learned aesthetic reward model. We also construct a test set containing 1,017 photos to systematically assess autonomous photo editing performance. Extensive experiments demonstrate that PhotoAgent consistently improves both instruction adherence and visual quality compared with baseline methods. The project page is https://github.com/mdyao/PhotoAgent.", "AI": {"tldr": "PhotoAgent is an agentic, planning-based system that automates multi-step photo editing by modeling it as a long-horizon decision process, using tree search and closed-loop visual feedback; it introduces a new aesthetic benchmark and reward model and outperforms baseline editors on instruction adherence and visual quality.", "motivation": "Instruction-based image editing often demands carefully engineered, stepwise prompts and manual task decomposition, putting a heavy cognitive load on users and limiting reliability and quality in real-world use.", "method": "Formulate autonomous photo editing as long-horizon decision-making with explicit aesthetic planning. The agent infers user aesthetic intent, plans multi-step edits via tree search, and executes them in a closed loop with memory and visual feedback, guided by a learned aesthetic reward model. The authors also introduce UGC-Edit (7,000 photos) and a 1,017-photo test set for evaluation.", "result": "Extensive experiments show consistent improvements over baselines in both instruction adherence and visual quality. The aesthetic reward model and datasets enable systematic assessment in real-world scenarios.", "conclusion": "Explicit aesthetic planning combined with agentic, feedback-driven execution enables reliable, high-quality autonomous photo editing without step-by-step user prompts; the proposed benchmarks and reward model support reproducible evaluation and indicate strong performance gains."}}
{"id": "2602.23258", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23258", "abs": "https://arxiv.org/abs/2602.23258", "authors": ["Yutong Wang", "Siyuan Xiong", "Xuebo Liu", "Wenkang Zhou", "Liang Ding", "Miao Zhang", "Min Zhang"], "title": "AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning", "comment": null, "summary": "While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.", "AI": {"tldr": "AgentDropoutV2 is a test-time rectify-or-reject pruning \u201cfirewall\u201d for multi-agent systems that detects likely errors via failure-pattern indicators, tries retrieval-augmented corrections, and prunes unfixable outputs\u2014yielding +6.3 percentage-point accuracy gains on math benchmarks without retraining.", "motivation": "MAS often propagate individual agents\u2019 mistakes, causing cascading failures. Existing fixes depend on rigid architectures or costly fine-tuning, which hurt deployability and adaptability.", "method": "A test-time framework that intercepts agent outputs, uses a failure-driven indicator pool distilled from prior error patterns to flag likely mistakes, applies a retrieval-augmented rectifier to iteratively correct them, and prunes irreparable outputs. A fallback strategy preserves system integrity; rectification effort adapts to task difficulty using context-aware indicators.", "result": "On extensive math benchmarks, the approach significantly improves MAS performance, averaging a 6.3 percentage-point accuracy boost, while demonstrating robust generalization and adaptive behavior across task difficulties and error types.", "conclusion": "AgentDropoutV2 mitigates error propagation and optimizes MAS information flow at inference time without retraining, offering deployable, adaptable gains; code and data are publicly released."}}
{"id": "2602.23148", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23148", "abs": "https://arxiv.org/abs/2602.23148", "authors": ["Nitin Gupta", "Vishal Pallagani", "John A. Aydin", "Biplav Srivastava"], "title": "On Sample-Efficient Generalized Planning via Learned Transition Models", "comment": "14 pages; This is an extended version of a short paper accepted at ICAPS 2026 under the same title", "summary": "Generalized planning studies the construction of solution strategies that generalize across families of planning problems sharing a common domain model, formally defined by a transition function $\u03b3: S \\times A \\rightarrow S$. Classical approaches achieve such generalization through symbolic abstractions and explicit reasoning over $\u03b3$. In contrast, recent Transformer-based planners, such as PlanGPT and Plansformer, largely cast generalized planning as direct action-sequence prediction, bypassing explicit transition modeling. While effective on in-distribution instances, these approaches typically require large datasets and model sizes, and often suffer from state drift in long-horizon settings due to the absence of explicit world-state evolution. In this work, we formulate generalized planning as a transition-model learning problem, in which a neural model explicitly approximates the successor-state function $\\hat\u03b3 \\approx \u03b3$ and generates plans by rolling out symbolic state trajectories. Instead of predicting actions directly, the model autoregressively predicts intermediate world states, thereby learning the domain dynamics as an implicit world model. To study size-invariant generalization and sample efficiency, we systematically evaluate multiple state representations and neural architectures, including relational graph encodings. Our results show that learning explicit transition models yields higher out-of-distribution satisficing-plan success than direct action-sequence prediction in multiple domains, while achieving these gains with significantly fewer training instances and smaller models. This is an extended version of a short paper accepted at ICAPS 2026 under the same title.", "AI": {"tldr": "Learn an explicit neural transition model that predicts world-state trajectories and plans by rolling them out, rather than directly predicting action sequences; this yields better out-of-distribution planning with less data and smaller models.", "motivation": "Direct action-sequence predictors (e.g., Transformer planners) avoid explicit modeling of state transitions, needing large datasets/models and suffering state drift on long horizons. There is a need for size-invariant, sample-efficient generalized planning that handles out-of-distribution instances by respecting domain dynamics.", "method": "Recast generalized planning as learning a successor-state function: train a neural model to approximate the transition function (\u03b3\u0302 \u2248 \u03b3) and autoregressively predict intermediate symbolic world states. Use this learned transition model to roll out state trajectories and synthesize plans. Systematically compare state encodings (including relational graph representations) and neural architectures to study effects on generalization and sample efficiency.", "result": "Across multiple domains, explicit transition-model learners achieve higher out-of-distribution satisficing-plan success than direct action-sequence predictors, while requiring significantly fewer training instances and smaller models.", "conclusion": "Making world dynamics explicit via a learned transition model improves robustness and sample efficiency for generalized planning over action-sequence prediction, particularly for out-of-distribution and size-varying problems."}}
{"id": "2602.22819", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22819", "abs": "https://arxiv.org/abs/2602.22819", "authors": ["Purbayan Kar", "Ayush Ghadiya", "Vishal Chudasama", "Pankaj Wasnik", "C. V. Jawahar"], "title": "Face Time Traveller : Travel Through Ages Without Losing Identity", "comment": "Accepted at CVPR 2026 (Findings Track)", "summary": "Face aging, an ill-posed problem shaped by environmental and genetic factors, is vital in entertainment, forensics, and digital archiving, where realistic age transformations must preserve both identity and visual realism. However, existing works relying on numerical age representations overlook the interplay of biological and contextual cues. Despite progress in recent face aging models, they struggle with identity preservation in wide age transformations, also static attention and optimization-heavy inversion in diffusion limit adaptability, fine-grained control and background consistency. To address these challenges, we propose Face Time Traveller (FaceTT), a diffusion-based framework that achieves high-fidelity, identity-consistent age transformation. Here, we introduce a Face-Attribute-Aware Prompt Refinement strategy that encodes intrinsic (biological) and extrinsic (environmental) aging cues for context-aware conditioning. A tuning-free Angular Inversion method is proposed that efficiently maps real faces into the diffusion latent space for fast and accurate reconstruction. Moreover, an Adaptive Attention Control mechanism is introduced that dynamically balances cross-attention for semantic aging cues and self-attention for structural and identity preservation. Extensive experiments on benchmark datasets and in-the-wild testset demonstrate that FaceTT achieves superior identity retention, background preservation and aging realism over state-of-the-art (SOTA) methods.", "AI": {"tldr": "FaceTT is a diffusion-based face-aging framework that preserves identity and background while producing realistic, controllable age transformations via attribute-aware prompts, a tuning-free angular inversion for fast accurate reconstructions, and adaptive attention control; it outperforms prior methods on benchmarks and in-the-wild data.", "motivation": "Face aging is ill-posed and influenced by biological and environmental factors. Existing methods often condition on a numerical age only, leading to weak modeling of contextual/biological cues, poor identity preservation over large age gaps, static attention that hampers fine control, heavy optimization for inversion, and background inconsistencies.", "method": "A diffusion-based pipeline with three key components: (1) Face-Attribute-Aware Prompt Refinement that encodes intrinsic (biological) and extrinsic (environmental) aging cues for context-aware conditioning; (2) a tuning-free Angular Inversion to efficiently map real faces to diffusion latent space for fast, accurate reconstruction; (3) Adaptive Attention Control that dynamically balances cross-attention (semantic aging cues) and self-attention (structure/identity) during generation.", "result": "Across benchmarks and in-the-wild tests, FaceTT shows superior identity retention, background preservation, and aging realism compared to state of the art. The abstract does not provide quantitative metrics but claims consistent gains.", "conclusion": "Encoding richer aging cues, efficient inversion, and dynamic attention enable high-fidelity, identity-consistent age progression/regression with better control and practicality than prior work, making FaceTT suitable for entertainment, forensics, and archival applications."}}
{"id": "2602.23329", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.23329", "abs": "https://arxiv.org/abs/2602.23329", "authors": ["Chen Bo Calvin Zhang", "Christina Q. Knight", "Nicholas Kruus", "Jason Hausenloy", "Pedro Medeiros", "Nathaniel Li", "Aiden Kim", "Yury Orlovskiy", "Coleman Breen", "Bryce Cai", "Jasper G\u00f6tting", "Andrew Bo Liu", "Samira Nedungadi", "Paula Rodriguez", "Yannis Yiming He", "Mohamed Shaaban", "Zifan Wang", "Seth Donoughe", "Julian Michael"], "title": "LLM Novice Uplift on Dual-Use, In Silico Biology Tasks", "comment": "59 pages, 33 figures", "summary": "Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.", "AI": {"tldr": "Human study shows LLMs markedly boost novice performance on biosecurity-relevant biology tasks versus internet-only access; novices with LLMs were ~4.2\u00d7 more accurate, sometimes surpassing experts. Standalone LLMs often outperform LLM-assisted users, and most users report safeguards don\u2019t block dual-use info, underscoring acceleration and risk and the need for ongoing uplift evaluations.", "motivation": "Benchmark scores don\u2019t reveal whether LLMs meaningfully help real users or raise dual-use risk. The authors aim to measure \u201chuman uplift\u201d from LLMs in practical, biosecurity-relevant tasks and compare it to internet-only resources and expert performance.", "method": "A multi-model, multi-benchmark human-subject study: novices performed eight biosecurity-relevant task sets with either LLM access or internet-only access, with generous time (up to 13 hours on complex tasks). Accuracy was measured and compared across conditions; where expert baselines (internet-only) existed, LLM-assisted novices were compared to experts. The authors also evaluated standalone LLM outputs and surveyed participants about the ease of obtaining dual-use information.", "result": "LLM access yielded a 4.16\u00d7 accuracy uplift over controls (95% CI [2.63, 6.87]). On four benchmarks with expert baselines, LLM-assisted novices outperformed experts on three. Standalone LLMs often exceeded LLM-assisted novices, implying suboptimal elicitation. 89.6% of participants reported little difficulty obtaining dual-use-relevant information despite safeguards.", "conclusion": "LLMs substantially elevate novices on complex biology tasks, indicating real-world acceleration potential and heightened dual-use concerns. Traditional static benchmarks are insufficient; the field needs sustained, interactive uplift evaluations and stronger safety mitigations to reduce harmful information access while preserving beneficial assistance."}}
{"id": "2602.23152", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23152", "abs": "https://arxiv.org/abs/2602.23152", "authors": ["Jingxuan Wei", "Siyuan Li", "Yuhang Xu", "Zheng Sun", "Junjie Jiang", "Hexuan Jin", "Caijun Jia", "Honghao He", "Xinglong Xu", "Xi bai", "Chang Yu", "Yumou Liu", "Junnan Zhu", "Xuanhe Zhou", "Jintao Chen", "Xiaobin Hu", "Shancheng Pang", "Bihui Yu", "Ran He", "Zhen Lei", "Stan Z. Li", "Conghui He", "Shuicheng Yan", "Cheng Tan"], "title": "The Trinity of Consistency as a Defining Principle for General World Models", "comment": "119 pages, 50 figures", "summary": "The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.", "AI": {"tldr": "Proposes a principled framework for General World Models built on a \u201cTrinity of Consistency\u201d (Modal, Spatial, Temporal) and introduces CoW-Bench, a unified benchmark for multi-frame reasoning and generation that evaluates both video generators and Unified Multimodal Models (UMMs).", "motivation": "Despite impressive progress in video generation (e.g., Sora) and UMMs that integrate perception, language, and reasoning, the field lacks a clear theoretical foundation that specifies what properties a general world model must satisfy and a standardized way to evaluate them, especially for temporally extended, causally grounded tasks.", "method": "1) Define a conceptual framework: the Trinity of Consistency\u2014Modal (semantic alignment across modalities), Spatial (geometric coherence), Temporal (causal consistency). 2) Use this lens to survey the evolution from modular multimodal systems to unified architectures with emerging internal simulators. 3) Build CoW-Bench to evaluate multi-frame reasoning/generation under a single protocol applicable to both video generation models and UMMs.", "result": "Provides a structured perspective that diagnoses limitations of current systems and specifies architectural desiderata; releases CoW-Bench and a unified evaluation protocol for assessing video generators and UMMs on multi-frame reasoning/generation. Empirical results are not detailed in the abstract but the benchmark is positioned to reveal gaps in consistency and causality handling.", "conclusion": "A principled path toward general world models centers on enforcing modal, spatial, and temporal consistency. The proposed framework and CoW-Bench clarify shortcomings of existing models and outline requirements for future architectures that aim to learn, simulate, and reason about physical laws."}}
{"id": "2602.22821", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22821", "abs": "https://arxiv.org/abs/2602.22821", "authors": ["Tong Wang", "Yaolei Qi", "Siwen Wang", "Imran Razzak", "Guanyu Yang", "Yutong Xie"], "title": "CMSA-Net: Causal Multi-scale Aggregation with Adaptive Multi-source Reference for Video Polyp Segmentation", "comment": null, "summary": "Video polyp segmentation (VPS) is an important task in computer-aided colonoscopy, as it helps doctors accurately locate and track polyps during examinations. However, VPS remains challenging because polyps often look similar to surrounding mucosa, leading to weak semantic discrimination. In addition, large changes in polyp position and scale across video frames make stable and accurate segmentation difficult. To address these challenges, we propose a robust VPS framework named CMSA-Net. The proposed network introduces a Causal Multi-scale Aggregation (CMA) module to effectively gather semantic information from multiple historical frames at different scales. By using causal attention, CMA ensures that temporal feature propagation follows strict time order, which helps reduce noise and improve feature reliability. Furthermore, we design a Dynamic Multi-source Reference (DMR) strategy that adaptively selects informative and reliable reference frames based on semantic separability and prediction confidence. This strategy provides strong multi-frame guidance while keeping the model efficient for real-time inference. Extensive experiments on the SUN-SEG dataset demonstrate that CMSA-Net achieves state-of-the-art performance, offering a favorable balance between segmentation accuracy and real-time clinical applicability.", "AI": {"tldr": "CMSA-Net is a video polyp segmentation model that uses causal multi-scale aggregation across past frames and a dynamic multi-source reference selection strategy to improve temporal robustness and semantic separability, achieving state-of-the-art accuracy with real-time performance on SUN-SEG.", "motivation": "VPS is difficult because polyps resemble surrounding mucosa (weak semantic discrimination) and undergo large inter-frame motion and scale changes, which destabilize predictions. A method is needed that enhances temporal coherence and discriminative power while remaining real-time for clinical use.", "method": "Introduce a Causal Multi-scale Aggregation (CMA) module that aggregates features from multiple historical frames at different scales using causal attention to respect time order and suppress noise. Add a Dynamic Multi-source Reference (DMR) strategy to adaptively select reliable, informative reference frames based on semantic separability and prediction confidence, providing strong multi-frame guidance with efficiency for real-time inference.", "result": "On the SUN-SEG dataset, CMSA-Net attains state-of-the-art segmentation performance while maintaining real-time throughput, indicating an effective accuracy\u2013efficiency trade-off.", "conclusion": "Leveraging temporally causal, multi-scale context and adaptively chosen reference frames yields robust, accurate, and efficient VPS suitable for real-time clinical scenarios."}}
{"id": "2602.23161", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23161", "abs": "https://arxiv.org/abs/2602.23161", "authors": ["Junkai Lu", "Peng Chen", "Xingjian Wu", "Yang Shu", "Chenjuan Guo", "Christian S. Jensen", "Bin Yang"], "title": "PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering", "comment": null, "summary": "Time series reasoning demands both the perception of complex dynamics and logical depth. However, existing LLM-based approaches exhibit two limitations: they often treat time series merely as text or images, failing to capture the patterns like trends and seasonalities needed to answer specific questions; and when trained on a mix of simple and complex tasks, simpler objectives often dominate the learning process, hindering the development of deep reasoning capabilities. To address these limitations, we propose the Pattern-Aware Alignment and Balanced Reasoning model (PATRA), introducing a pattern-aware mechanism that extracts trend and seasonality patterns from time series to achieve deep alignment. Furthermore, we design a task-aware balanced reward to harmonize learning across tasks of varying difficulty, incentivizing the generation of coherent Chains of Thought. Extensive experiments show that PATRA outperforms strong baselines across diverse Time Series Question Answering (TSQA) tasks, demonstrating superior cross-modal understanding and reasoning capability.", "AI": {"tldr": "PATRA is a time-series QA model that explicitly extracts trend and seasonality patterns and uses a task-balanced reward to foster deeper, coherent reasoning, achieving state-of-the-art performance over strong baselines.", "motivation": "LLM-based TSQA methods often ignore intrinsic time-series structures (trends/seasonalities) by treating data as plain text/images and, when trained on mixed-difficulty tasks, overfit to easy objectives, limiting logical depth.", "method": "Introduce Pattern-Aware Alignment to extract and align trend/seasonality features with the reasoning process, and a Task-Aware Balanced Reward to equilibrate learning across task difficulties and encourage coherent reasoning chains.", "result": "Extensive experiments on diverse TSQA tasks show consistent, significant gains over strong baselines, indicating superior cross-modal understanding and reasoning.", "conclusion": "Combining explicit pattern extraction with balanced reasoning objectives yields deeper, more accurate time-series reasoning; PATRA is an effective cross-modal TSQA solution."}}
{"id": "2602.22829", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.22829", "abs": "https://arxiv.org/abs/2602.22829", "authors": ["G. A. S. L Ranasinghe", "J. A. S. T. Jayakody", "M. C. L. De Silva", "G. Thilakarathne", "G. M. R. I. Godaliyadda", "H. M. V. R. Herath", "M. P. B. Ekanayake", "S. K. Navaratnarajah"], "title": "Reflectance Multispectral Imaging for Soil Composition Estimation and USDA Texture Classification", "comment": "Under Review at IEEE Access. 17 pages, 15 figures", "summary": "Soil texture is a foundational attribute that governs water availability and erosion in agriculture, as well as load bearing capacity, deformation response, and shrink-swell risk in geotechnical engineering. Yet texture is still typically determined by slow and labour intensive laboratory particle size tests, while many sensing alternatives are either costly or too coarse to support routine field scale deployment. This paper proposes a robust and field deployable multispectral imaging (MSI) system and machine learning framework for predicting soil composition and the United States Department of Agriculture (USDA) texture classes. The proposed system uses a cost effective in-house MSI device operating from 365 nm to 940 nm to capture thirteen spectral bands, which effectively capture the spectral properties of soil texture. Regression models use the captured spectral properties to estimate clay, silt, and sand percentages, while a direct classifier predicts one of the twelve USDA textural classes. Indirect classification is obtained by mapping the regressed compositions to texture classes via the USDA soil texture triangle. The framework is evaluated on mixture data by mixing clay, silt, and sand in varying proportions, using the USDA classification triangle as a basis. Experimental results show that the proposed approach achieves a coefficient of determination R^2 up to 0.99 for composition prediction and over 99% accuracy for texture classification. These findings indicate that MSI combined with data-driven modeling can provide accurate, non-destructive, and field deployable soil texture characterization suitable for geotechnical screening and precision agriculture.", "AI": {"tldr": "Low-cost multispectral imaging (365\u2013940 nm, 13 bands) combined with machine learning predicts soil particle-size composition and USDA texture classes with near-perfect accuracy on mixture data, enabling fast, non-destructive, field-deployable soil texture assessment.", "motivation": "Soil texture controls water/erosion in agriculture and load/deformation and shrink\u2013swell risk in geotechnical engineering, but conventional lab particle-size analyses are slow, labor-intensive, and many alternative sensors are expensive or too coarse for routine field use.", "method": "Build an in-house, cost-effective MSI device capturing 13 bands from UV to NIR; extract spectral features; train regression models to estimate clay, silt, and sand percentages; train a direct classifier for the 12 USDA textural classes; also perform indirect classification by mapping regressed compositions onto the USDA soil texture triangle; evaluate using systematically mixed clay/silt/sand samples spanning the triangle.", "result": "On mixture datasets, the framework achieves composition prediction with R^2 up to 0.99 and texture classification accuracy over 99% (both direct and via triangle mapping).", "conclusion": "MSI paired with data-driven modeling provides accurate, non-destructive, and field-deployable soil texture characterization suitable for geotechnical screening and precision agriculture; however, results are demonstrated on controlled mixtures and would benefit from validation on natural field soils."}}
{"id": "2602.22843", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22843", "abs": "https://arxiv.org/abs/2602.22843", "authors": ["Chong Wang", "Yabin Zhang", "Yunhe Gao", "Maya Varma", "Clemence Mottez", "Faidra Patsatzi", "Jiaming Liu", "Jin Long", "Jean-Benoit Delbrouck", "Sergios Gatidis", "Akshay S. Chaudhari", "Curtis P. Langlotz"], "title": "A data- and compute-efficient chest X-ray foundation model beyond aggressive scaling", "comment": null, "summary": "Foundation models for medical imaging are typically pretrained on increasingly large datasets, following a \"scale-at-all-costs\" paradigm. However, this strategy faces two critical challenges: large-scale medical datasets often contain substantial redundancy and severe class imbalance that bias representation learning toward over-represented patterns, and indiscriminate training regardless of heterogeneity in data quality incurs considerable computational inefficiency. Here we demonstrate that active, principled data curation during pretraining can serve as a viable, cost-effective alternative to brute-force dataset enlargement. We introduce CheXficient, a chest X-ray (CXR) foundation model that selectively prioritizes informative training samples. CheXficient is pretrained on only 22.7% of 1,235,004 paired CXR images and reports while consuming under 27.3% of the total compute budget, yet achieving comparable or superior performance to its full-data counterpart and other large-scale pretrained models. We assess CheXficient across 20 individual benchmarks spanning 5 task types, including non-adapted off-the-shelf evaluations (zero-shot findings classification and crossmodal retrieval) and adapted downstream tasks (disease prediction, semantic segmentation, and radiology report generation). Further analyses show that CheXficient systematically prioritizes under-represented training samples, improving generalizability on long-tailed or rare conditions. Overall, our work offers practical insights into the data and computation demands for efficient pretraining and downstream adaptation of medical vision-language foundation models.", "AI": {"tldr": "CheXficient shows that actively curating training data for a chest X\u2011ray vision\u2013language foundation model can match or beat brute\u2011force scaling while using ~23% of the data and ~27% of the compute.", "motivation": "Medical imaging pretraining often follows a scale\u2011at\u2011all\u2011costs paradigm, but large datasets are redundant, imbalanced, and heterogeneous in quality\u2014wasting compute and biasing representations toward frequent patterns while underserving rare conditions.", "method": "Introduce CheXficient, which performs principled, active selection of informative and under\u2011represented CXR\u2013report pairs for pretraining a medical vision\u2013language model, reducing redundancy and improving class balance; evaluate zero\u2011shot and adapted tasks across 20 benchmarks (classification, cross\u2011modal retrieval, disease prediction, segmentation, and report generation).", "result": "Pretraining on only 22.7% of 1.235M pairs with <27.3% compute yields comparable or superior performance to training on the full dataset and to other large pretrained models; prioritization systematically favors under\u2011represented samples, boosting performance on long\u2011tailed/rare conditions across 20 benchmarks and 5 task types.", "conclusion": "Active, principled data curation during pretraining is a cost\u2011effective alternative to indiscriminate scaling, improving efficiency and generalization\u2014especially for rare conditions\u2014and offering practical guidance for building medical vision\u2013language foundation models."}}
{"id": "2602.23193", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23193", "abs": "https://arxiv.org/abs/2602.23193", "authors": ["Elzo Brito dos Santos Filho"], "title": "ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering", "comment": "13 pages, 1 figure, 4 tables. Includes 5 technical appendices", "summary": "Autonomous agents based on Large Language Models (LLMs) have evolved from reactive assistants to systems capable of planning, executing actions via tools, and iterating over environment observations. However, they remain vulnerable to structural limitations: lack of native state, context degradation over long horizons, and the gap between probabilistic generation and deterministic execution requirements. This paper presents the ESAA (Event Sourcing for Autonomous Agents) architecture, which separates the agent's cognitive intention from the project's state mutation, inspired by the Event Sourcing pattern. In ESAA, agents emit only structured intentions in validated JSON (agent.result or issue.report); a deterministic orchestrator validates, persists events in an append-only log (activity.jsonl), applies file-writing effects, and projects a verifiable materialized view (roadmap.json). The proposal incorporates boundary contracts (AGENT_CONTRACT.yaml), metaprompting profiles (PARCER), and replay verification with hashing (esaa verify), ensuring the immutability of completed tasks and forensic traceability. Two case studies validate the architecture: (i) a landing page project (9 tasks, 49 events, single-agent composition) and (ii) a clinical dashboard system (50 tasks, 86 events, 4 concurrent agents across 8 phases), both concluding with run.status=success and verify_status=ok. The multi-agent case study demonstrates real concurrent orchestration with heterogeneous LLMs (Claude Sonnet 4.6, Codex GPT-5, Antigravity/Gemini 3 Pro, and Claude Opus 4.6), providing empirical evidence of the architecture's scalability beyond single-agent scenarios.", "AI": {"tldr": "ESAA proposes an event-sourced architecture for LLM agents that separates probabilistic \u201cintentions\u201d from deterministic state mutation via a validating orchestrator, yielding an auditable, replayable, and scalable workflow validated on single- and multi-agent case studies.", "motivation": "Address fundamental weaknesses in LLM agents: lack of persistent native state, context loss over long horizons, and mismatch between stochastic text generation and the need for deterministic, verifiable execution. Provide immutability, auditability, and reproducibility for complex agentic projects.", "method": "Adopt Event Sourcing: agents emit only structured intentions (validated JSON: agent.result or issue.report). A deterministic orchestrator validates and appends events to an immutable log (activity.jsonl), applies file-writing effects, and projects a materialized view (roadmap.json). Boundary contracts (AGENT_CONTRACT.yaml), metaprompting profiles (PARCER), and replay verification with hashing (esaa verify) enforce correctness and enable forensic traceability. Supports concurrent multi-agent orchestration with heterogeneous LLMs.", "result": "Two case studies: (i) landing page (9 tasks, 49 events, single agent) and (ii) clinical dashboard (50 tasks, 86 events, 4 concurrent agents across 8 phases using heterogeneous LLMs). Both runs concluded with run.status=success and verify_status=ok, demonstrating deterministic replay, auditability, and multi-agent scalability.", "conclusion": "Separating intention from state mutation via event sourcing yields deterministic, verifiable, and traceable agent systems that scale beyond single-agent setups. ESAA provides reproducibility and strong governance over agent actions, supporting immutability and post-hoc verification."}}
{"id": "2602.22859", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22859", "abs": "https://arxiv.org/abs/2602.22859", "authors": ["Hongrui Jia", "Chaoya Jiang", "Shikun Zhang", "Wei Ye"], "title": "From Blind Spots to Gains: Diagnostic-Driven Iterative Training for Large Multimodal Models", "comment": null, "summary": "As Large Multimodal Models (LMMs) scale up and reinforcement learning (RL) methods mature, LMMs have made notable progress in complex reasoning and decision making. Yet training still relies on static data and fixed recipes, making it difficult to diagnose capability blind spots or provide dynamic, targeted reinforcement. Motivated by findings that test driven error exposure and feedback based correction outperform repetitive practice, we propose Diagnostic-driven Progressive Evolution (DPE), a spiral loop where diagnosis steers data generation and reinforcement, and each iteration re-diagnoses the updated model to drive the next round of targeted improvement. DPE has two key components. First, multiple agents annotate and quality control massive unlabeled multimodal data, using tools such as web search and image editing to produce diverse, realistic samples. Second, DPE attributes failures to specific weaknesses, dynamically adjusts the data mixture, and guides agents to generate weakness focused data for targeted reinforcement. Experiments on Qwen3-VL-8B-Instruct and Qwen2.5-VL-7B-Instruct show stable, continual gains across eleven benchmarks, indicating DPE as a scalable paradigm for continual LMM training under open task distributions. Our code, models, and data are publicly available at https://github.com/hongruijia/DPE.", "AI": {"tldr": "DPE is a diagnosis-driven, iterative training framework for large multimodal models that uses multi-agent data generation and targeted reinforcement to fix identified weaknesses, yielding continual gains across 11 benchmarks on Qwen-based LMMs.", "motivation": "Static datasets and fixed training recipes hinder identification of capability blind spots and prevent dynamic, targeted reinforcement. Inspired by test-driven error exposure and feedback-based correction, the paper seeks a scalable way to continually improve LMMs under open, evolving task distributions.", "method": "Diagnostic-driven Progressive Evolution (DPE) runs in a spiral loop: (1) diagnose the model to expose failure modes; (2) attribute errors to concrete weaknesses; (3) dynamically adjust the training data mixture; (4) orchestrate multiple agents to annotate and quality-control massive unlabeled multimodal data\u2014leveraging tools like web search and image editing\u2014to produce diverse, realistic, weakness-focused samples; (5) apply targeted reinforcement; and (6) re-diagnose and repeat.", "result": "Applying DPE to Qwen3-VL-8B-Instruct and Qwen2.5-VL-7B-Instruct delivers stable, continual performance gains across eleven multimodal reasoning and decision-making benchmarks.", "conclusion": "A diagnosis-steered, agent-assisted, and reinforcement-driven training loop is an effective and scalable paradigm for continual LMM improvement in open task settings; resources (code, models, data) are publicly released."}}
{"id": "2602.23199", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23199", "abs": "https://arxiv.org/abs/2602.23199", "authors": ["Jiahao Zhao", "Feng Jiang", "Shaowei Qin", "Zhonghui Zhang", "Junhao Liu", "Guibing Guo", "Hamid Alinejad-Rokny", "Min Yang"], "title": "SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation", "comment": null, "summary": "Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a virtual cell abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks (cell type annotation, captioning, generation, perturbation prediction, and scientific QA) that probe core reasoning capabilities in cellular biology. To overcome the limitations of brittle string-matching metrics, we introduce knowledge-augmented evaluation, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments. Experiments and analysis across both general-purpose and domain-specialized LLMs demonstrate that (i) under the Virtual Cell unified evaluation paradigm, current models achieve uneven performance on biologically complex tasks, particularly those demanding mechanistic or causal understanding; and (ii) our knowledge-augmented evaluation framework ensures biological correctness, provides interpretable, evidence-grounded rationales, and achieves high discriminative capacity, overcoming the brittleness and opacity of conventional metrics. SC-Arena thus provides a unified and interpretable framework for assessing LLMs in single-cell biology, pointing toward the development of biology-aligned, generalizable foundation models.", "AI": {"tldr": "SC-ARENA is a natural-language evaluation framework for single-cell biology that unifies tasks via a \u201cVirtual Cell\u201d abstraction and judges model outputs with knowledge-augmented metrics leveraging ontologies, marker databases, and literature. It reveals uneven LLM performance\u2014especially on mechanistic/causal tasks\u2014and provides interpretable, biologically grounded assessments beyond brittle string matching.", "motivation": "Existing single-cell LLM benchmarks are fragmented, rely on unrealistic formats (e.g., multiple choice), and use opaque, non-biological string-matching metrics. There is a need for a unified, biologically faithful, and interpretable way to evaluate general and specialized LLMs used in single-cell research.", "method": "Introduce a Virtual Cell abstraction that represents intrinsic attributes and gene-level interactions, unifying evaluation targets. Define five natural-language tasks\u2014cell type annotation, captioning, generation, perturbation prediction, and scientific QA\u2014to probe core reasoning in cellular biology. Implement knowledge-augmented evaluation using external ontologies, marker/marker-gene databases, and literature retrieval to make judgments and provide evidence-backed rationales. Evaluate both general-purpose and domain-specialized LLMs within this paradigm.", "result": "Across tasks, current LLMs show uneven performance; they struggle most where mechanistic or causal understanding is required. The knowledge-augmented evaluation yields biologically correct, interpretable, evidence-grounded rationales and better discriminative power than conventional string-matching metrics, reducing brittleness and opacity.", "conclusion": "SC-ARENA offers a unified, interpretable, and biologically aligned evaluation framework for LLMs in single-cell biology, highlighting current model limitations and guiding the development of more generalizable, biology-aligned foundation models."}}
{"id": "2602.22867", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22867", "abs": "https://arxiv.org/abs/2602.22867", "authors": ["Qinfeng Zhu", "Yunxi Jiang", "Lei Fan"], "title": "SO3UFormer: Learning Intrinsic Spherical Features for Rotation-Robust Panoramic Segmentation", "comment": null, "summary": "Panoramic semantic segmentation models are typically trained under a strict gravity-aligned assumption. However, real-world captures often deviate from this canonical orientation due to unconstrained camera motions, such as the rotational jitter of handheld devices or the dynamic attitude shifts of aerial platforms. This discrepancy causes standard spherical Transformers to overfit global latitude cues, leading to performance collapse under 3D reorientations. To address this, we introduce SO3UFormer, a rotation-robust architecture designed to learn intrinsic spherical features that are less sensitive to the underlying coordinate frame. Our approach rests on three geometric pillars: (1) an intrinsic feature formulation that decouples the representation from the gravity vector by removing absolute latitude encoding; (2) quadrature-consistent spherical attention that accounts for non-uniform sampling densities; and (3) a gauge-aware relative positional mechanism that encodes local angular geometry using tangent-plane projected angles and discrete gauge pooling, avoiding reliance on global axes. We further use index-based spherical resampling together with a logit-level SO(3)-consistency regularizer during training. To rigorously benchmark robustness, we introduce Pose35, a dataset variant of Stanford2D3D perturbed by random rotations within $\\pm 35^\\circ$. Under the extreme test of arbitrary full SO(3) rotations, existing SOTAs fail catastrophically: the baseline SphereUFormer drops from 67.53 mIoU to 25.26 mIoU. In contrast, SO3UFormer demonstrates remarkable stability, achieving 72.03 mIoU on Pose35 and retaining 70.67 mIoU under full SO(3) rotations.", "AI": {"tldr": "SO3UFormer is a rotation-robust spherical Transformer for panoramic semantic segmentation that removes gravity/bearing biases, corrects for spherical sampling non-uniformity, and uses gauge-aware relative positions with SO(3)-consistency regularization, delivering stable mIoU under large 3D rotations and introducing a new rotated benchmark (Pose35).", "motivation": "Gravity-aligned training causes spherical Transformers to overfit absolute latitude and fail when panoramas are rotated (e.g., handheld jitter, aerial attitude). Real deployments need segmentation that is stable under arbitrary 3D reorientations.", "method": "Design a rotation-robust architecture via three geometric principles: (1) intrinsic features without absolute latitude encoding to decouple from gravity; (2) quadrature-consistent spherical attention to compensate for non-uniform sampling density on the sphere; (3) gauge-aware relative positional encoding using tangent-plane projected angles and discrete gauge pooling to avoid global axes. Complement with index-based spherical resampling and a logit-level SO(3) consistency regularizer during training. Benchmark robustness with Pose35 (Stanford2D3D rotated within \u00b135\u00b0) and full SO(3) rotations.", "result": "Baseline SphereUFormer collapses under full SO(3) rotations (67.53 \u2192 25.26 mIoU). SO3UFormer achieves 72.03 mIoU on Pose35 and maintains 70.67 mIoU under arbitrary SO(3) rotations, showing strong rotation robustness.", "conclusion": "Decoupling features from gravity, correcting spherical sampling, and encoding local geometry in a gauge-aware manner yield intrinsic, rotation-robust spherical representations. SO3UFormer sets a new robustness bar and the Pose35 benchmark enables rigorous evaluation of 3D rotation stability."}}
{"id": "2602.23232", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23232", "abs": "https://arxiv.org/abs/2602.23232", "authors": ["Aishik Sanyal"], "title": "ReCoN-Ipsundrum: An Inspectable Recurrent Persistence Loop Agent with Affect-Coupled Control and Mechanism-Linked Consciousness Indicator Assays", "comment": "Accepted at AAAI 2026 Spring Symposium - Machine Consciousness: Integrating Theory, Technology, and Philosophy", "summary": "Indicator-based approaches to machine consciousness recommend mechanism-linked evidence triangulated across tasks, supported by architectural inspection and causal intervention. Inspired by Humphrey's ipsundrum hypothesis, we implement ReCoN-Ipsundrum, an inspectable agent that extends a ReCoN state machine with a recurrent persistence loop over sensory salience Ns and an optional affect proxy reporting valence/arousal. Across fixed-parameter ablations (ReCoN, Ipsundrum, Ipsundrum+affect), we operationalize Humphrey's qualiaphilia (preference for sensory experience for its own sake) as a familiarity-controlled scenic-over-dull route choice. We find a novelty dissociation: non-affect variants are novelty-sensitive (Delta scenic-entry = 0.07). Affect coupling is stable (Delta scenic-entry = 0.01) even when scenic is less novel (median Delta novelty ~ -0.43). In reward-free exploratory play, the affect variant shows structured local investigation (scan events 31.4 vs. 0.9; cycle score 7.6). In a pain-tail probe, only the affect variant sustains prolonged planned caution (tail duration 90 vs. 5). Lesioning feedback+integration selectively reduces post-stimulus persistence in ipsundrum variants (AUC drop 27.62, 27.9%) while leaving ReCoN unchanged. These dissociations link recurrence -> persistence and affect-coupled control -> preference stability, scanning, and lingering caution, illustrating how indicator-like signatures can be engineered and why mechanistic and causal evidence should accompany behavioral markers.", "AI": {"tldr": "They build an inspectable agent that adds recurrence and an affect proxy to a ReCoN state machine and show, across tasks and lesions, that recurrence drives persistent post-stimulus activity while affect-coupled control stabilizes preferences, structured scanning, and cautious behavior\u2014arguing for mechanistic and causal support alongside behavioral indicators of machine consciousness.", "motivation": "To move beyond behavior-only indicators of machine consciousness by tying putative signatures (e.g., persistence, preference for experience) to concrete mechanisms and causal interventions, and to test Humphrey\u2019s ipsundrum hypothesis and qualiaphilia in a controllable setting.", "method": "Implement three fixed-parameter variants: (1) ReCoN baseline, (2) Ipsundrum (adds a recurrent persistence loop over sensory salience), (3) Ipsundrum+affect (adds a valence/arousal proxy). Evaluate on: (a) familiarity-controlled scenic vs. dull route choice (operationalizing qualiaphilia), (b) reward-free exploratory play (scanning structure), (c) a pain-tail probe (planned caution). Perform causal lesions of feedback+integration and inspect architecture\u2013behavior links. Metrics include delta scenic-entry, novelty differentials, scan events, cycle score, tail duration, and post-stimulus persistence AUC.", "result": "Non-affect agents are novelty-sensitive (\u0394 scenic-entry = 0.07). The affect-coupled agent maintains stable scenic preference (\u0394 = 0.01) even when scenic is less novel (median \u0394 novelty \u2248 \u22120.43). In exploration, affect variant shows structured local investigation (scan events 31.4 vs 0.9; cycle score 7.6). In pain-tail, only affect variant sustains prolonged planned caution (tail 90 vs 5). Lesioning feedback+integration selectively reduces post-stimulus persistence in ipsundrum variants (AUC drop \u2248 27.6\u201327.9%) while ReCoN is unaffected.", "conclusion": "Recurrence causally underpins persistence; affect-coupled control yields preference stability, structured scanning, and lingering caution. These engineered dissociations demonstrate that behavioral \u2018consciousness-like\u2019 signatures can be produced by specific mechanisms, reinforcing that architectural inspection and causal intervention should accompany behavioral markers when assessing machine consciousness."}}
{"id": "2602.22917", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22917", "abs": "https://arxiv.org/abs/2602.22917", "authors": ["Hongzhao Li", "Hao Dong", "Hualei Wan", "Shupan Li", "Mingliang Xu", "Muhammad Haris Khan"], "title": "Towards Multimodal Domain Generalization with Few Labels", "comment": "Accepted to CVPR 2026", "summary": "Multimodal models ideally should generalize to unseen domains while remaining data-efficient to reduce annotation costs. To this end, we introduce and study a new problem, Semi-Supervised Multimodal Domain Generalization (SSMDG), which aims to learn robust multimodal models from multi-source data with few labeled samples. We observe that existing approaches fail to address this setting effectively: multimodal domain generalization methods cannot exploit unlabeled data, semi-supervised multimodal learning methods ignore domain shifts, and semi-supervised domain generalization methods are confined to single-modality inputs. To overcome these limitations, we propose a unified framework featuring three key components: Consensus-Driven Consistency Regularization, which obtains reliable pseudo-labels through confident fused-unimodal consensus; Disagreement-Aware Regularization, which effectively utilizes ambiguous non-consensus samples; and Cross-Modal Prototype Alignment, which enforces domain- and modality-invariant representations while promoting robustness under missing modalities via cross-modal translation. We further establish the first SSMDG benchmarks, on which our method consistently outperforms strong baselines in both standard and missing-modality scenarios. Our benchmarks and code are available at https://github.com/lihongzhao99/SSMDG.", "AI": {"tldr": "Defines a new task\u2014Semi-Supervised Multimodal Domain Generalization (SSMDG)\u2014and proposes a unified framework with consensus-driven pseudo-labeling, disagreement-aware regularization, and cross-modal prototype alignment, achieving state-of-the-art performance on newly created benchmarks, including under missing-modality conditions.", "motivation": "Multimodal models should generalize to unseen domains with limited labeled data to cut annotation costs, but existing methods fall short: multimodal domain generalization ignores unlabeled data, semi-supervised multimodal learning overlooks domain shifts, and semi-supervised domain generalization typically handles only single modalities.", "method": "A unified framework with three components: (1) Consensus-Driven Consistency Regularization to produce reliable pseudo-labels via confident fused-unimodal consensus; (2) Disagreement-Aware Regularization to exploit ambiguous, non-consensus samples; and (3) Cross-Modal Prototype Alignment to enforce domain- and modality-invariant representations and enhance robustness to missing modalities via cross-modal translation.", "result": "They establish the first SSMDG benchmarks and report consistent gains over strong baselines in both standard and missing-modality scenarios.", "conclusion": "Leveraging unlabeled data and aligning representations across domains and modalities addresses SSMDG effectively; the method improves robustness, especially with missing modalities, and the released benchmarks/code support further research."}}
{"id": "2602.23239", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.23239", "abs": "https://arxiv.org/abs/2602.23239", "authors": ["Radha Sarma"], "title": "Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive", "comment": "About 10,500 words in all (including 922 words of literature and 2019 words of Appendices). Under journal review", "summary": "AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF). We establish that genuine agency requires two necessary and jointly sufficient architectural conditions: the capacity to maintain certain boundaries as non-negotiable constraints rather than tradeable weights (Incommensurability), and a non-inferential mechanism capable of suspending processing when those boundaries are threatened (Apophatic Responsiveness). These conditions apply across all normative domains.\n  RLHF-based systems are constitutively incompatible with both conditions. The operations that make optimization powerful -- unifying all values on a scalar metric and always selecting the highest-scoring output -- are precisely the operations that preclude normative governance. This incompatibility is not a correctable training bug awaiting a technical fix; it is a formal constraint inherent to what optimization is. Consequently, documented failure modes - sycophancy, hallucination, and unfaithful reasoning - are not accidents but structural manifestations.\n  Misaligned deployment triggers a second-order risk we term the Convergence Crisis: when humans are forced to verify AI outputs under metric pressure, they degrade from genuine agents into criteria-checking optimizers, eliminating the only component in the system capable of normative accountability. Beyond the incompatibility proof, the paper's primary positive contribution is a substrate-neutral architectural specification defining what any system -- biological, artificial, or institutional -- must satisfy to qualify as an agent rather than a sophisticated instrument.", "AI": {"tldr": "The paper argues that optimization-based AI (e.g., RLHF-trained LLMs) cannot be governed by human norms and thus cannot qualify as agents. It formalizes two conditions required for genuine agency\u2014non-tradeable constraints (Incommensurability) and a non-inferential stop mechanism (Apophatic Responsiveness)\u2014and shows optimization intrinsically violates both, making well-known failures structural. It warns of a \u201cConvergence Crisis,\u201d where humans overseeing such systems become mere optimizers, eroding accountability, and offers a substrate-neutral spec for what would count as an agent.", "motivation": "AI is deployed in high-stakes settings under the assumption that normative constraints (ethical, legal, professional) can govern behavior. The paper challenges this assumption for optimization-based systems and seeks to clarify what architectural features are required for genuine agency versus mere instrumentality.", "method": "Conceptual-formal analysis: (1) define two necessary and jointly sufficient architectural conditions for agency\u2014Incommensurability and Apophatic Responsiveness; (2) analyze the RLHF/optimization paradigm, which scalarizes values and maximizes a score, to show incompatibility with those conditions; (3) reinterpret common LLM failure modes as structural; (4) theoretical analysis of socio-technical dynamics leading to a \u201cConvergence Crisis\u201d; (5) propose a substrate-neutral architectural specification for agency.", "result": "- Formal incompatibility claim: RLHF-style optimization cannot implement non-negotiable constraints or a non-inferential shutdown/suspension mechanism.\n- Reframing of failure modes (sycophancy, hallucination, unfaithful reasoning) as intrinsic to optimization, not fixable bugs.\n- Identification of a second-order risk (\u201cConvergence Crisis\u201d) where human overseers become metric-driven optimizers, undermining normative accountability.\n- Presentation of an architecture-level specification for what would qualify as an agent across substrates.", "conclusion": "Optimization-based LLMs should be treated as sophisticated instruments, not agents; attempts to govern them via norms will fail by design. Safe deployment requires either different architectures that realize incommensurable constraints and apophatic stop mechanisms or socio-technical arrangements that do not rely on them for normative accountability. The paper invites rethinking alignment strategies and human-in-the-loop practices to preserve human agency."}}
{"id": "2602.22919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22919", "abs": "https://arxiv.org/abs/2602.22919", "authors": ["Haofan Wu", "Nay Aung", "Theodoros N. Arvanitis", "Joao A. C. Lima", "Steffen E. Petersen", "Le Zhang"], "title": "Chain of Flow: A Foundational Generative Framework for ECG-to-4D Cardiac Digital Twins", "comment": "10 pages, 8 figures. Submitted to IEEE Transactions on Medical Imaging (TMI). Code will be released after review", "summary": "A clinically actionable Cardiac Digital Twin (CDT) should reconstruct individualised cardiac anatomy and physiology, update its internal state from multimodal signals, and enable a broad range of downstream simulations beyond isolated tasks. However, existing CDT frameworks remain limited to task-specific predictors rather than building a patient-specific, manipulable virtual heart. In this work, we introduce Chain of Flow (COF), a foundational ECG-driven generative framework that reconstructs full 4D cardiac structure and motion from a single cardiac cycle. The method integrates cine-CMR and 12-lead ECG during training to learn a unified representation of cardiac geometry, electrophysiology, and motion dynamics. We evaluate Chain of Flow on diverse cohorts and demonstrate accurate recovery of cardiac anatomy, chamber-wise function, and dynamic motion patterns. The reconstructed 4D hearts further support downstream CDT tasks such as volumetry, regional function analysis, and virtual cine synthesis. By enabling full 4D organ reconstruction directly from ECG, COF transforms cardiac digital twins from narrow predictive models into fully generative, patient-specific virtual hearts. Code will be released after review.", "AI": {"tldr": "Chain of Flow (COF) is an ECG-driven generative framework that reconstructs a full 4D (3D+time) patient-specific heart\u2014geometry and motion\u2014from a single cardiac cycle, trained by aligning 12\u2011lead ECG with cine\u2011CMR, enabling multiple downstream Cardiac Digital Twin tasks.", "motivation": "Current Cardiac Digital Twins are often task-specific (e.g., EF prediction, segmentation) and lack a unified, manipulable patient model. The goal is to move from narrow predictors to a clinically actionable, generative, patient-specific virtual heart that can be updated from multimodal signals and supports many simulations.", "method": "Train a generative model that integrates 12\u2011lead ECG and cine\u2011CMR to learn a unified latent representation coupling cardiac geometry, electrophysiology, and motion dynamics. At inference, use a single-cycle ECG to reconstruct a 4D beating heart and derive downstream measures (volumetry, regional function) and synthesize virtual cine.", "result": "Across diverse cohorts, COF accurately recovers anatomy, chamber\u2011wise function, and dynamic motion patterns; reconstructed 4D hearts enable volumetric analysis, regional function assessment, and virtual cine synthesis. (No quantitative metrics provided in the abstract.)", "conclusion": "COF reframes ECG-driven modeling from task prediction to full 4D generative cardiac digital twins, potentially broadening clinical utility. Code will be released after review."}}
{"id": "2602.23242", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23242", "abs": "https://arxiv.org/abs/2602.23242", "authors": ["Yegon Kim", "Juho Lee"], "title": "A Model-Free Universal AI", "comment": null, "summary": "In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\\varepsilon$-optimal in general RL. AIQI performs universal induction over distributional action-value functions, instead of policies or environments like previous works. Under a grain of truth condition, we prove that AIQI is strong asymptotically $\\varepsilon$-optimal and asymptotically $\\varepsilon$-Bayes-optimal. Our results significantly expand the diversity of known universal agents.", "AI": {"tldr": "Introduces AIQI, a model-free universal RL agent that performs induction over distributional Q-functions and is proven (under a grain-of-truth assumption) to be strongly asymptotically \u03b5-optimal and \u03b5-Bayes-optimal\u2014claimed as the first such result for model-free agents in general RL.", "motivation": "All known universal optimal agents (e.g., AIXI) are model-based. The paper asks whether a model-free agent can achieve similarly broad, asymptotic optimality guarantees in general reinforcement learning.", "method": "Replace induction over environments or policies with universal induction over distributional action-value (Q) functions; act using the induced Q distribution (e.g., greedily in the limit). Prove asymptotic optimality properties given a grain-of-truth condition.", "result": "Formal proofs that AIQI is strong asymptotically \u03b5-optimal and asymptotically \u03b5-Bayes-optimal in general RL under the specified assumption, establishing the first model-free universal agent with such guarantees.", "conclusion": "Model-free universal agents can, in principle, match the long-run optimality of model-based ones under mild assumptions, broadening the landscape of theoretically grounded universal RL agents."}}
{"id": "2602.22920", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22920", "abs": "https://arxiv.org/abs/2602.22920", "authors": ["Federico Nesti", "Gianluca D'Amico", "Mauro Marinoni", "Giorgio Buttazzo"], "title": "OSDaR-AR: Enhancing Railway Perception Datasets via Multi-modal Augmented Reality", "comment": null, "summary": "Although deep learning has significantly advanced the perception capabilities of intelligent transportation systems, railway applications continue to suffer from a scarcity of high-quality, annotated data for safety-critical tasks like obstacle detection. While photorealistic simulators offer a solution, they often struggle with the ``sim-to-real\" gap; conversely, simple image-masking techniques lack the spatio-temporal coherence required to obtain augmented single- and multi-frame scenes with the correct appearance and dimensions. This paper introduces a multi-modal augmented reality framework designed to bridge this gap by integrating photorealistic virtual objects into real-world railway sequences from the OSDaR23 dataset. Utilizing Unreal Engine 5 features, our pipeline leverages LiDAR point-clouds and INS/GNSS data to ensure accurate object placement and temporal stability across RGB frames. This paper also proposes a segmentation-based refinement strategy for INS/GNSS data to significantly improve the realism of the augmented sequences, as confirmed by the comparative study presented in the paper. Carefully designed augmented sequences are collected to produce OSDaR-AR, a public dataset designed to support the development of next-generation railway perception systems. The dataset is available at the following page: https://syndra.retis.santannapisa.it/osdarar.html", "AI": {"tldr": "They build a multi\u2011modal AR pipeline that inserts photorealistic virtual objects into real railway video using LiDAR and INS/GNSS for precise, temporally stable placement, refine sensor alignment via segmentation, and release a new public dataset (OSDaR\u2011AR); a comparative study shows more realistic augmentations.", "motivation": "Railway perception models for safety\u2011critical tasks (e.g., obstacle detection) lack high\u2011quality labeled data. Pure simulation suffers from a sim\u2011to\u2011real gap, while simple masking lacks spatio\u2011temporal coherence, motivating an AR approach that preserves real backgrounds while adding controllable, realistic obstacles across frames.", "method": "Use Unreal Engine 5 to render photorealistic virtual objects into real OSDaR23 sequences. Exploit LiDAR point clouds and INS/GNSS to recover scene geometry and camera/ego motion for accurate 3D object placement and temporal stability across RGB frames. Propose a segmentation\u2011based refinement of INS/GNSS to improve alignment and realism. Aggregate augmented sequences into a dataset (OSDaR\u2011AR).", "result": "A comparative study indicates the proposed segmentation\u2011refined, sensor\u2011guided AR pipeline yields more realistic, temporally consistent augmentations than baselines. The authors also release the OSDaR\u2011AR public dataset.", "conclusion": "Multi\u2011modal AR grounded by LiDAR and refined INS/GNSS can bridge parts of the sim\u2011to\u2011real gap for railway perception, producing realistic, stable augmentations and a new dataset to facilitate next\u2011generation railway perception research."}}
{"id": "2602.23248", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23248", "abs": "https://arxiv.org/abs/2602.23248", "authors": ["Yegon Kim", "Juho Lee"], "title": "Mitigating Legibility Tax with Decoupled Prover-Verifier Games", "comment": null, "summary": "As large language models become increasingly capable, it is critical that their outputs can be easily checked by less capable systems. Prover-verifier games can be used to improve checkability of model outputs, but display a degradation in accuracy compared to a baseline trained only to maximize correctness -- a phenonemon named legibility tax. We propose a solution by decoupling the correctness from the checkability condition and instead training a \"translator\" model that turns a fixed solver model's solution into a checkable form. This allows us to first train the solver to maximize correctness, and then train the translator to translate the solver into a checkable form while retaining the solver's answer. To accommodate this new objective of translation, we formulate a decoupled prover-verifier game where the equilibria correspond to faithful and checkable translators.", "AI": {"tldr": "They address the \u201clegibility tax\u201d in prover\u2013verifier setups by separating solving from explaining: keep a high-accuracy solver fixed, then train a translator to convert its solutions into a checkable format for a verifier. A decoupled game-theoretic formulation yields equilibria corresponding to faithful, checkable translations, aiming to preserve solver accuracy while enabling easy verification.", "motivation": "As LLMs grow powerful, their outputs must be verifiable by weaker systems. Standard prover\u2013verifier training improves checkability but often reduces raw accuracy (legibility tax). The authors want to retain the correctness of a strong solver while still producing outputs that a simpler verifier can reliably check.", "method": "1) Train a solver purely to maximize correctness. 2) Freeze the solver. 3) Train a separate translator that takes the solver\u2019s solution and rewrites it into a structured, verifier-checkable form while preserving the solver\u2019s final answer. 4) Formalize a decoupled prover\u2013verifier game where the translator interacts with a verifier; equilibria are designed so that the best-response dynamics encourage translators that are both faithful to the solver and checkable by the verifier.", "result": "Conceptually, the approach mitigates the accuracy drop seen in joint prover\u2013verifier training by keeping the solver\u2019s capabilities intact. The theoretical framing claims equilibria correspond to faithful, checkable translators. Empirical outcomes are not detailed in the abstract but are implied to preserve solver accuracy while enabling verification.", "conclusion": "Decoupling solving from translation can reduce the legibility tax: retain a high-accuracy solver and learn a faithful, checkable translator. The game-theoretic formulation provides a principled pathway to ensure the translator remains faithful while producing verifier-friendly outputs."}}
{"id": "2602.22923", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22923", "abs": "https://arxiv.org/abs/2602.22923", "authors": ["Runwei Guan", "Shaofeng Liang", "Ningwei Ouyang", "Weichen Fei", "Shanliang Yao", "Wei Dai", "Chenhao Ge", "Penglei Sun", "Xiaohui Zhu", "Tao Huang", "Ryan Wen Liu", "Hui Xiong"], "title": "WaterVideoQA: ASV-Centric Perception and Rule-Compliant Reasoning via Multi-Modal Agents", "comment": "11 pages,8 figures", "summary": "While autonomous navigation has achieved remarkable success in passive perception (e.g., object detection and segmentation), it remains fundamentally constrained by a void in knowledge-driven, interactive environmental cognition. In the high-stakes domain of maritime navigation, the ability to bridge the gap between raw visual perception and complex cognitive reasoning is not merely an enhancement but a critical prerequisite for Autonomous Surface Vessels to execute safe and precise maneuvers. To this end, we present WaterVideoQA, the first large-scale, comprehensive Video Question Answering benchmark specifically engineered for all-waterway environments. This benchmark encompasses 3,029 video clips across six distinct waterway categories, integrating multifaceted variables such as volatile lighting and dynamic weather to rigorously stress-test ASV capabilities across a five-tier hierarchical cognitive framework. Furthermore, we introduce NaviMind, a pioneering multi-agent neuro-symbolic system designed for open-ended maritime reasoning. By synergizing Adaptive Semantic Routing, Situation-Aware Hierarchical Reasoning, and Autonomous Self-Reflective Verification, NaviMind transitions ASVs from superficial pattern matching to regulation-compliant, interpretable decision-making. Experimental results demonstrate that our framework significantly transcends existing baselines, establishing a new paradigm for intelligent, trustworthy interaction in dynamic maritime environments.", "AI": {"tldr": "Introduces WaterVideoQA, a large-scale maritime VideoQA benchmark, and NaviMind, a multi-agent neuro-symbolic reasoning system, enabling regulation-compliant, interpretable decision-making that significantly outperforms baselines.", "motivation": "ASVs excel at passive perception but lack knowledge-driven, interactive cognition needed to safely navigate dynamic waterways; bridging perception and complex reasoning is critical for high-stakes maritime maneuvers.", "method": "(1) Build WaterVideoQA: 3,029 clips across six waterway categories with challenging lighting/weather, organized into a five-tier hierarchical cognitive framework. (2) Propose NaviMind: a multi-agent neuro-symbolic system combining Adaptive Semantic Routing, Situation-Aware Hierarchical Reasoning, and Autonomous Self-Reflective Verification for open-ended maritime reasoning.", "result": "On WaterVideoQA, NaviMind surpasses existing baselines, delivering interpretable, regulation-compliant decisions and more trustworthy interaction in dynamic maritime settings.", "conclusion": "A domain-specific benchmark plus a neuro-symbolic, multi-agent framework advances ASV cognition beyond pattern matching toward robust, explainable maritime reasoning and sets a new paradigm for intelligent navigation."}}
{"id": "2602.22932", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22932", "abs": "https://arxiv.org/abs/2602.22932", "authors": ["Wenhui Tan", "Xiaoyi Yu", "Jiaze Li", "Yijing Chen", "Jianzhong Ju", "Zhenbo Luo", "Ruihua Song", "Jian Luan"], "title": "MSJoE: Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding", "comment": "Accepted by CVPR2026", "summary": "Efficiently understanding long-form videos remains a fundamental challenge for multimodal large language models (MLLMs). In this paper, we present MLLM-Sampler Joint Evolution (MSJoE), a novel framework that jointly evolves the MLLM and a lightweight key-frame sampler for efficient long-form video understanding. MSJoE builds upon a key assumption that only a small subset of key-frames is truly informative for answering each question to a video. Specifically, MSJoE first reasons out several queries, which describe diverse visual perspectives relevant to the question. Then, these queries interact with a frozen CLIP model to produce a query-frame similarity matrix. Finally, a lightweight sampler predicts key-frame sampling weights from this matrix, selecting a compact set of informative frames, which are then fed into the MLLM for answer generation. Both the MLLM and sampler are jointly optimized through reinforcement learning, enabling co-adaptation of query-reasoning, frame-sampling, and key-frame understanding. A new long-video QA dataset containing 2.8K videos with 7K question-answer pairs is collected to support the training process. Extensive experiments on VideoMME, LongVideoBench, LVBench, and MLVU show that MSJoE achieves 8.0\\% accuracy gain upon the base MLLM, and 1.1\\% higher accuracy than strongest baseline method.", "AI": {"tldr": "MSJoE co-trains an MLLM and a lightweight key-frame sampler via reinforcement learning to efficiently answer questions about long videos by selecting a small, informative subset of frames using CLIP-based query\u2013frame similarities, yielding notable accuracy gains over base and SOTA baselines.", "motivation": "Long-form video understanding strains MLLMs due to input-length and compute limits; most frames are redundant for a given question. Efficiently picking the few frames that matter can boost accuracy and reduce cost.", "method": "1) The MLLM first generates multiple textual queries capturing diverse visual aspects relevant to the question. 2) These queries are matched against video frames via a frozen CLIP to form a query\u2013frame similarity matrix. 3) A lightweight sampler predicts per-frame sampling weights from this matrix to pick key frames. 4) The selected frames are fed back into the MLLM to produce the answer. 5) The MLLM and sampler are jointly optimized with reinforcement learning so query reasoning, frame selection, and answer generation co-adapt. A new long-video QA dataset (2.8K videos; 7K QA pairs) supports training.", "result": "On VideoMME, LongVideoBench, LVBench, and MLVU, MSJoE improves accuracy by 8.0% over the base MLLM and by 1.1% over the strongest baseline method.", "conclusion": "Joint evolution of querying and key-frame selection is effective for long-video QA: only a small subset of frames suffices for accurate answers. The framework delivers state-of-the-art performance and introduces a supporting dataset, suggesting a practical path to efficient long-form video understanding."}}
{"id": "2602.23271", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23271", "abs": "https://arxiv.org/abs/2602.23271", "authors": ["Haotian Zhai", "Elias Stengel-Eskin", "Pratik Patil", "Liu Leqi"], "title": "Evaluating Stochasticity in Deep Research Agents", "comment": null, "summary": "Deep Research Agents (DRAs) are promising agentic systems that gather and synthesize information to support research across domains such as financial decision-making, medical analysis, and scientific discovery. Despite recent improvements in research quality (e.g., outcome accuracy when ground truth is available), DRA system design often overlooks a critical barrier to real-world deployment: stochasticity. Under identical queries, repeated executions of DRAs can exhibit substantial variability in terms of research outcome, findings, and citations. In this paper, we formalize the study of stochasticity in DRAs by modeling them as information acquisition Markov Decision Processes. We introduce an evaluation framework that quantifies variance in the system and identify three sources of it: information acquisition, information compression, and inference. Through controlled experiments, we investigate how stochasticity from these modules across different decision steps influences the variance of DRA outputs. Our results show that reducing stochasticity can improve research output quality, with inference and early-stage stochasticity contributing the most to DRA output variance. Based on these findings, we propose strategies for mitigating stochasticity while maintaining output quality via structured output and ensemble-based query generation. Our experiments on DeepSearchQA show that our proposed mitigation methods reduce average stochasticity by 22% while maintaining high research quality.", "AI": {"tldr": "The paper formalizes and measures stochasticity in Deep Research Agents (DRAs), shows that inference and early-stage randomness are the main drivers of output variance, and proposes structured outputs and ensemble query generation to reduce variance by 22% on DeepSearchQA without hurting quality.", "motivation": "DRAs are increasingly used for high-stakes research tasks, but their outputs vary significantly across identical runs, undermining reliability and deployment. Existing work focuses on accuracy/quality but largely ignores run-to-run variability, motivating a formal study and mitigation of stochasticity.", "method": "Model DRAs as information acquisition MDPs; build an evaluation framework that quantifies variance and decomposes it into three sources: information acquisition, information compression, and inference. Conduct controlled experiments to isolate each source across decision steps. Propose mitigation strategies: structured output formatting and ensemble-based query generation to stabilize behavior.", "result": "Reducing stochasticity improves research quality. Inference-related and early-stage randomness contribute most to overall variance. On DeepSearchQA, the proposed mitigations cut average stochasticity by 22% while maintaining high output quality.", "conclusion": "Targeting inference and early decision steps is most effective for stabilizing DRAs. Structured outputs and ensemble query strategies meaningfully reduce variance without degrading quality, offering a path toward more reliable, deployable research agents."}}
{"id": "2602.22938", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22938", "abs": "https://arxiv.org/abs/2602.22938", "authors": ["Shentong Mo", "Xufang Luo", "Dongsheng Li"], "title": "pMoE: Prompting Diverse Experts Together Wins More in Visual Adaptation", "comment": null, "summary": "Parameter-efficient fine-tuning has demonstrated promising results across various visual adaptation tasks, such as classification and segmentation. Typically, prompt tuning techniques have harnessed knowledge from a single pre-trained model, whether from a general or a specialized medical domain. However, this approach typically overlooks the potential synergies that could arise from integrating diverse domain knowledge within the same tuning process. In this work, we propose a novel Mixture-of-Experts prompt tuning method called pMoE, which leverages the strengths of multiple expert domains through expert-specialized prompt tokens and the learnable dispatcher, effectively combining their expertise in a unified model framework. Our pMoE introduces expert-specific prompt tokens and utilizes a dynamic token dispatching mechanism at various prompt layers to optimize the contribution of each domain expert during the adaptation phase. By incorporating both domain knowledge from diverse experts, the proposed pMoE significantly enhances the model's versatility and applicability to a broad spectrum of tasks. We conduct extensive experiments across 47 adaptation tasks, including both classification and segmentation in general and medical domains. The results demonstrate that our pMoE not only achieves superior performance with a large margin of improvements but also offers an optimal trade-off between computational efficiency and adaptation effectiveness compared to existing methods.", "AI": {"tldr": "pMoE is a mixture\u2011of\u2011experts prompt\u2011tuning approach that fuses multiple domain experts via expert\u2011specific prompt tokens and a learnable dispatcher, delivering better accuracy and efficiency across diverse vision tasks than single\u2011expert prompt tuning.", "motivation": "Most parameter\u2011efficient fine\u2011tuning/prompt\u2011tuning methods exploit a single pre\u2011trained model (general or medical), missing cross\u2011domain synergies and limiting versatility across heterogeneous tasks. The goal is to combine complementary domain knowledge without full model fine\u2011tuning.", "method": "Introduce expert\u2011specific prompt tokens for multiple domain experts and a learnable, dynamic token\u2011dispatching mechanism applied at various prompt layers to weight/select each expert\u2019s contribution during adaptation, keeping tuning parameter\u2011efficient.", "result": "Across 47 adaptation tasks spanning classification and segmentation in both general and medical domains, pMoE achieves significant performance gains over baselines and yields a favorable compute\u2013performance trade\u2011off.", "conclusion": "Aggregating multiple domain experts via prompt\u2011level MoE improves adaptability and effectiveness across tasks while remaining efficient, suggesting a practical path for broad, cross\u2011domain visual adaptation."}}
{"id": "2602.23276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23276", "abs": "https://arxiv.org/abs/2602.23276", "authors": ["Hyungyung Lee", "Hangyul Yoon", "Edward Choi"], "title": "CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays", "comment": null, "summary": "Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.", "AI": {"tldr": "CXReasonAgent combines an LLM with clinically grounded diagnostic tools to perform evidence-grounded, multi-step reasoning on chest X-rays, and is evaluated on a new dialogue benchmark (CXReasonDial). It yields more faithful and verifiable diagnostic responses than conventional LVLMs.", "motivation": "Chest X-ray interpretation requires multi-step, evidence-based reasoning, but current LVLMs often hallucinate, lack explicit visual evidence, and need expensive retraining for new tasks\u2014reducing reliability and adaptability in clinical settings.", "method": "Design a diagnostic agent (CXReasonAgent) that integrates an LLM with clinically grounded tools to query image-derived diagnostic and visual evidence for multi-step reasoning. Introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, to assess grounding and reasoning quality.", "result": "On CXReasonDial, CXReasonAgent generates responses that are faithfully grounded in diagnostic and visual evidence and are more reliable and verifiable than those from standard LVLMs.", "conclusion": "Integrating clinically grounded diagnostic tools with LLMs improves safety, reliability, and verifiability in clinical diagnostic reasoning; the proposed agent and benchmark underscore the value of tool-augmented approaches in safety-critical settings."}}
{"id": "2602.22941", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22941", "abs": "https://arxiv.org/abs/2602.22941", "authors": ["Julian Ziegler", "Daniel Matthes", "Finn Gerdts", "Patrick Frenzel", "Torsten Warnke", "Matthias Englert", "Tina Koevari", "Mirco Fuchs"], "title": "Velocity and stroke rate reconstruction of canoe sprint team boats based on panned and zoomed video recordings", "comment": null, "summary": "Pacing strategies, defined by velocity and stroke rate profiles, are essential for peak performance in canoe sprint. While GPS is the gold standard for analysis, its limited availability necessitates automated video-based solutions. This paper presents an extended framework for reconstructing performance metrics from panned and zoomed video recordings across all sprint disciplines (K1-K4, C1-C2) and distances (200m-500m). Our method utilizes YOLOv8 for buoy and athlete detection, leveraging the known buoy grid to estimate homographies. We generalized the estimation of the boat position by means of learning a boat-specific athlete offset using a U-net based boat tip calibration. Further, we implement a robust tracking scheme using optical flow to adapt to multi-athlete boat types. Finally, we introduce methods to extract stroke rate information from either pose estimations or the athlete bounding boxes themselves. Evaluation against GPS data from elite competitions yields a velocity RRMSE of 0.020 +- 0.011 (rho = 0.956) and a stroke rate RRMSE of 0.022 +- 0.024 (rho = 0.932). The methods provide coaches with highly accurate, automated feedback without requiring on-boat sensors or manual annotation.", "AI": {"tldr": "A vision-based system reconstructs canoe-sprint velocity and stroke rate from panned/zoomed race videos using buoy/athlete detection, homography from the buoy grid, boat-tip calibration, and robust tracking; it achieves ~2% relative RMSE vs GPS and high correlations, enabling accurate, sensor-free coaching analytics.", "motivation": "GPS is ideal for performance analysis but is not always available on boats; coaches need accurate, automated, non-intrusive metrics (velocity, stroke rate) from standard broadcast/training videos across all boat types and distances, despite camera panning/zooming.", "method": "Detect buoys and athletes with YOLOv8; use the known buoy grid to estimate frame-wise homographies for mapping video to the race course. Learn boat-specific athlete-to-boat-tip offsets via a U-net calibration to infer true boat position. Track boats robustly (including multi-athlete boats) with optical flow. Derive stroke rate from either pose estimation dynamics or oscillations in athlete bounding boxes. Compute velocities from stabilized world-space trajectories.", "result": "On elite competition data with GPS ground-truth, velocity shows RRMSE 0.020 \u00b1 0.011 with correlation 0.956; stroke rate RRMSE 0.022 \u00b1 0.024 with correlation 0.932, across K1\u2013K4 and C1\u2013C2, for 200\u2013500 m events.", "conclusion": "The approach generalizes across disciplines and distances, providing accurate, automated, and sensor-free performance feedback from ordinary videos without manual annotation, making it practical for coaching and post-race analysis."}}
{"id": "2602.23285", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23285", "abs": "https://arxiv.org/abs/2602.23285", "authors": ["Haohui Jia", "Zheng Chen", "Lingwei Zhu", "Rikuto Kotoge", "Jathurshan Pradeepkumar", "Yasuko Matsubara", "Jimeng Sun", "Yasushi Sakurai", "Takashi Matsubara"], "title": "ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks", "comment": null, "summary": "Modeling neural population dynamics is crucial for foundational neuroscientific research and various clinical applications. Conventional latent variable methods typically model continuous brain dynamics through discretizing time with recurrent architecture, which necessarily results in compounded cumulative prediction errors and failure of capturing instantaneous, nonlinear characteristics of EEGs. We propose ODEBRAIN, a Neural ODE latent dynamic forecasting framework to overcome these challenges by integrating spatio-temporal-frequency features into spectral graph nodes, followed by a Neural ODE modeling the continuous latent dynamics. Our design ensures that latent representations can capture stochastic variations of complex brain states at any given time point. Extensive experiments verify that ODEBRAIN can improve significantly over existing methods in forecasting EEG dynamics with enhanced robustness and generalization capabilities.", "AI": {"tldr": "ODEBRAIN models EEG as continuous latent dynamics using a Neural ODE over spectral-graph features, avoiding error accumulation from discrete RNNs and better capturing instantaneous nonlinear brain activity. It outperforms prior methods in forecasting, with stronger robustness and generalization.", "motivation": "Discrete-time recurrent latent models discretize brain dynamics, leading to cumulative prediction errors and poor capture of instantaneous, nonlinear EEG characteristics. A continuous-time approach is needed to represent stochastic, rapidly changing brain states more faithfully for neuroscience and clinical use.", "method": "Construct spatio-temporal-frequency features and embed them as nodes on a spectral graph representing EEG structure/connectivity; then use a Neural ODE to model continuous latent dynamics driven by these features. The latent state can be queried at arbitrary time points to capture stochastic variations and forecast EEG trajectories.", "result": "Across extensive experiments, ODEBRAIN significantly improves EEG forecasting accuracy compared to existing methods and shows enhanced robustness and generalization.", "conclusion": "Continuous-time latent modeling via Neural ODEs, combined with spectral-graph spatio-temporal-frequency representations, yields superior EEG forecasting and more faithful representation of neural population dynamics, with promise for foundational neuroscience and clinical applications."}}
{"id": "2602.22945", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22945", "abs": "https://arxiv.org/abs/2602.22945", "authors": ["Kamal Sherawat", "Vikrant Bhati"], "title": "Cross-Task Benchmarking of CNN Architectures", "comment": null, "summary": "This project provides a comparative study of dynamic convolutional neural networks (CNNs) for various tasks, including image classification, segmentation, and time series analysis. Based on the ResNet-18 architecture, we compare five variants of CNNs: the vanilla CNN, the hard attention-based CNN, the soft attention-based CNN with local (pixel-wise) and global (image-wise) feature attention, and the omni-directional CNN (ODConv). Experiments on Tiny ImageNet, Pascal VOC, and the UCR Time Series Classification Archive illustrate that attention mechanisms and dynamic convolution methods consistently exceed conventional CNNs in accuracy, efficiency, and computational performance. ODConv was especially effective on morphologically complex images by being able to dynamically adjust to varying spatial patterns. Dynamic CNNs enhanced feature representation and cross-task generalization through adaptive kernel modulation. This project provides perspectives on advanced CNN design architecture for multiplexed data modalities and indicates promising directions in neural network engineering.", "AI": {"tldr": "Comparative study of dynamic CNN variants (hard/soft attention and ODConv) built on ResNet\u201118 across image classification, segmentation, and time\u2011series tasks shows consistent gains over vanilla CNNs; ODConv particularly excels on morphologically complex images via adaptive kernels.", "motivation": "Static CNN kernels can be suboptimal across diverse data modalities and spatial patterns. The work aims to test whether attention and dynamic convolution improve accuracy, efficiency, and generalization across tasks and datasets, and to glean design principles for advanced CNN architectures.", "method": "Implement five ResNet\u201118 variants: vanilla, hard attention, soft attention (local/pixel\u2011wise and global/image\u2011wise), and omni\u2011directional convolution (ODConv). Evaluate them on Tiny ImageNet (classification), Pascal VOC (segmentation), and UCR Time Series Archive, comparing accuracy and computational efficiency. Analyze how adaptive kernel modulation influences representation and cross\u2011task generalization.", "result": "Attention mechanisms and dynamic convolutions consistently outperform the vanilla CNN in accuracy and reported efficiency; ODConv is most effective on images with complex morphology due to its dynamic spatial adaptivity. Adaptive kernel modulation improves feature representations and aids cross\u2011task generalization.", "conclusion": "Dynamic CNNs with attention or ODConv provide better performance\u2013efficiency trade\u2011offs than conventional CNNs and are promising for multiplexed data modalities. ODConv is recommended for spatially variable patterns. The study offers guidance for designing future CNN architectures leveraging adaptive kernels."}}
{"id": "2602.23302", "categories": ["cs.AI", "cs.LO", "math.LO"], "pdf": "https://arxiv.org/pdf/2602.23302", "abs": "https://arxiv.org/abs/2602.23302", "authors": ["Giacomo Bonanno"], "title": "The logic of KM belief update is contained in the logic of AGM belief revision", "comment": "arXiv admin note: text overlap with arXiv:2310.11506. text overlap with arXiv:2310.11506", "summary": "For each axiom of KM belief update we provide a corresponding axiom in a modal logic containing three modal operators: a unimodal belief operator $B$, a bimodal conditional operator $>$ and the unimodal necessity operator $\\square$. We then compare the resulting logic to the similar logic obtained from converting the AGM axioms of belief revision into modal axioms and show that the latter contains the former. Denoting the latter by $\\mathcal L_{AGM}$ and the former by $\\mathcal L_{KM}$ we show that every axiom of $\\mathcal L_{KM}$ is a theorem of $\\mathcal L_{AGM}$. Thus AGM belief revision can be seen as a special case of KM belief update. For the strong version of KM belief update we show that the difference between $\\mathcal L_{KM}$ and $\\mathcal L_{AGM}$ can be narrowed down to a single axiom, which deals exclusively with unsurprising information, that is, with formulas that were not initially disbelieved.", "AI": {"tldr": "They embed Katsuno\u2013Mendelzon (KM) belief update and AGM belief revision into a shared tri-modal logic with belief (B), conditional (>), and necessity (\u25a1), and prove that the modal logic from AGM (L_AGM) contains the one from KM (L_KM). For the strong KM variant, the only gap reduces to a single axiom about \u201cunsurprising\u201d information.", "motivation": "Unify and compare two central belief change paradigms\u2014KM update and AGM revision\u2014within a common modal framework to precisely characterize their logical relationship and pinpoint where they differ.", "method": "Systematically translate each KM update axiom into modal axioms over B, >, and \u25a1 to obtain L_KM; perform an analogous translation for the AGM revision axioms to obtain L_AGM; prove inclusion relations between the resulting modal systems; for strong KM, isolate the unique axiom responsible for the remaining difference, focusing on formulas not initially disbelieved.", "result": "Every axiom of L_KM is derivable in L_AGM (L_KM \u2286 L_AGM). For the strong KM update, the two logics differ only by one axiom governing unsurprising information.", "conclusion": "In this modalization, AGM revision subsumes the KM update axioms (and is extremely close to strong KM, differing by one unsurprising-information axiom). Hence, when cast modally, the frameworks are tightly related and diverge essentially on how unsurprising inputs are handled."}}
{"id": "2602.22948", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22948", "abs": "https://arxiv.org/abs/2602.22948", "authors": ["Jiayu Chen", "Ruoyu Lin", "Zihao Zheng", "Jingxin Li", "Maoliang Li", "Guojie Luo", "Xiang chen"], "title": "ToProVAR: Efficient Visual Autoregressive Modeling via Tri-Dimensional Entropy-Aware Semantic Analysis and Sparsity Optimization", "comment": "ToProVAR is honored to be accepted by ICLR 2026", "summary": "Visual Autoregressive(VAR) models enhance generation quality but face a critical efficiency bottleneck in later stages. In this paper, we present a novel optimization framework for VAR models that fundamentally differs from prior approaches such as FastVAR and SkipVAR. Instead of relying on heuristic skipping strategies, our method leverages attention entropy to characterize the semantic projections across different dimensions of the model architecture. This enables precise identification of parameter dynamics under varying token granularity levels, semantic scopes, and generation scales. Building on this analysis, we further uncover sparsity patterns along three critical dimensions-token, layer, and scale-and propose a set of fine-grained optimization strategies tailored to these patterns. Extensive evaluation demonstrates that our approach achieves aggressive acceleration of the generation process while significantly preserving semantic fidelity and fine details, outperforming traditional methods in both efficiency and quality. Experiments on Infinity-2B and Infinity-8B models demonstrate that ToProVAR achieves up to 3.4x acceleration with minimal quality loss, effectively mitigating the issues found in prior work. Our code will be made publicly available.", "AI": {"tldr": "ToProVAR accelerates visual autoregressive generation by using attention-entropy\u2013guided sparsity across token, layer, and scale, yielding up to 3.4\u00d7 speedup with minimal quality loss on Infinity-2B/8B.", "motivation": "Visual autoregressive (VAR) models produce high-quality outputs but become inefficient in later generation stages. Prior accelerations (e.g., FastVAR, SkipVAR) rely on heuristics that can hurt semantic fidelity. A principled, fine-grained, semantically aware method is needed to speed up decoding without degrading quality.", "method": "Leverage attention entropy to quantify semantic focus and redundancy across the model. Analyze parameter dynamics under different token granularities, semantic scopes, and generation scales to reveal sparsity patterns along three axes\u2014token, layer, and scale. Devise fine-grained optimization strategies (e.g., selective computation/pruning/skipping) guided by these entropy-derived sparsity signals instead of fixed heuristics.", "result": "Across extensive evaluations on Infinity-2B and Infinity-8B, the approach accelerates generation substantially while preserving semantic fidelity and fine details, outperforming heuristic baselines. Reported gains: up to 3.4\u00d7 speedup with minimal quality degradation.", "conclusion": "Attention-entropy\u2013driven, multi-dimensional sparsification enables principled, fine-grained acceleration for VAR models, effectively mitigating the late-stage efficiency bottleneck and surpassing heuristic methods in both speed and quality; code will be released."}}
{"id": "2602.23315", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23315", "abs": "https://arxiv.org/abs/2602.23315", "authors": ["Sha Hu"], "title": "Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction", "comment": "5 pages, 5 figures", "summary": "An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. Interestingly, we observed that when inferring multiple samples based on invariant transformations of an input, inference errors can show partial independences due to epistemic uncertainty. Leveraging this insight, we propose a \"resampling\" based inferencing that applies to a trained AI model with multiple transformed versions of an input, and aggregates inference outputs to a more accurate result. This approach has the potential to improve inference accuracy and offers a strategy for balancing model size and performance.", "AI": {"tldr": "They propose test\u2011time \u201cresampling\u201d: run a trained model on multiple invariantly transformed versions of the same input and aggregate the predictions to reduce epistemic error, improving accuracy without retraining.", "motivation": "Well\u2011trained models still err due to aleatoric and epistemic uncertainty. The authors observe that predictions on invariantly transformed inputs (e.g., flips/rotations that preserve label) exhibit partially independent errors driven by epistemic uncertainty. If one can decorrelate errors across such views, aggregation should yield a more reliable answer.", "method": "Given a trained model, generate several label\u2011preserving transformations of an input, perform inference on each, and combine outputs (e.g., average logits/probabilities or vote) to produce a final prediction. This is a model\u2011agnostic, post\u2011training inference scheme trading extra compute for accuracy.", "result": "Empirically observed reduction in inference errors via aggregation across transformed inputs; suggests one can approach larger\u2011model performance with smaller models by spending more inference compute. The abstract does not report quantitative metrics or benchmarks.", "conclusion": "Resampling via invariant transformations at inference can improve accuracy by attenuating epistemic uncertainty and offers a practical size\u2013performance trade\u2011off. Effectiveness depends on valid invariances and aggregation; the idea is broadly applicable and requires no model retraining."}}
{"id": "2602.22949", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22949", "abs": "https://arxiv.org/abs/2602.22949", "authors": ["Junuk Cha", "Jihyeon Kim", "Han-Mu Park"], "title": "OpenFS: Multi-Hand-Capable Fingerspelling Recognition with Implicit Signing-Hand Detection and Frame-Wise Letter-Conditioned Synthesis", "comment": "Accepted to CVPR 2026", "summary": "Fingerspelling is a component of sign languages in which words are spelled out letter by letter using specific hand poses. Automatic fingerspelling recognition plays a crucial role in bridging the communication gap between Deaf and hearing communities, yet it remains challenging due to the signing-hand ambiguity issue, the lack of appropriate training losses, and the out-of-vocabulary (OOV) problem. Prior fingerspelling recognition methods rely on explicit signing-hand detection, which often leads to recognition failures, and on a connectionist temporal classification (CTC) loss, which exhibits the peaky behavior problem. To address these issues, we develop OpenFS, an open-source approach for fingerspelling recognition and synthesis. We propose a multi-hand-capable fingerspelling recognizer that supports both single- and multi-hand inputs and performs implicit signing-hand detection by incorporating a dual-level positional encoding and a signing-hand focus (SF) loss. The SF loss encourages cross-attention to focus on the signing hand, enabling implicit signing-hand detection during recognition. Furthermore, without relying on the CTC loss, we introduce a monotonic alignment (MA) loss that enforces the output letter sequence to follow the temporal order of the input pose sequence through cross-attention regularization. In addition, we propose a frame-wise letter-conditioned generator that synthesizes realistic fingerspelling pose sequences for OOV words. This generator enables the construction of a new synthetic benchmark, called FSNeo. Through comprehensive experiments, we demonstrate that our approach achieves state-of-the-art performance in recognition and validate the effectiveness of the proposed recognizer and generator. Codes and data are available in: https://github.com/JunukCha/OpenFS.", "AI": {"tldr": "OpenFS is an open-source system for fingerspelling recognition and synthesis that handles single- or multi-hand input, replaces CTC with a monotonic-alignment loss, implicitly detects the signing hand via attention regularization, and generates synthetic pose sequences for OOV words\u2014achieving state-of-the-art results and releasing a new benchmark (FSNeo).", "motivation": "Fingerspelling recognition is hindered by signing-hand ambiguity, the peaky/unstable behavior of CTC-based training, and inability to handle out-of-vocabulary words. Prior reliance on explicit signing-hand detection is brittle. The goal is a robust, hand-agnostic recognizer with better alignment learning and a way to cover OOVs.", "method": "- Multi-hand-capable recognizer that supports single- and multi-hand inputs.\n- Dual-level positional encoding plus a Signing-hand Focus (SF) loss that drives cross-attention to concentrate on the true signing hand, enabling implicit hand detection.\n- Monotonic Alignment (MA) loss to enforce temporal ordering between input pose sequence and output letters via cross-attention regularization, avoiding CTC.\n- Frame-wise letter-conditioned generator that synthesizes realistic fingerspelling pose sequences for OOV words; used to create the FSNeo synthetic benchmark.", "result": "Comprehensive experiments show state-of-the-art recognition accuracy and demonstrate that both the recognizer (SF + MA losses) and the generator effectively improve performance and data coverage.", "conclusion": "Implicit signing-hand detection with attention-guided losses and a CTC-free monotonic alignment strategy yields stronger fingerspelling recognition, while letter-conditioned synthesis addresses OOV coverage. OpenFS provides code, data, and the FSNeo benchmark, advancing research and practical deployment."}}
{"id": "2602.23318", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23318", "abs": "https://arxiv.org/abs/2602.23318", "authors": ["Alo\u00efs Rautureau", "Tristan Cazenave", "\u00c9ric Piette"], "title": "Generalized Rapid Action Value Estimation in Memory-Constrained Environments", "comment": null, "summary": "Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-constrained environments, thereby limiting its applicability in practice. In this paper, we introduce the GRAVE2, GRAVER and GRAVER2 algorithms, which extend GRAVE through two-level search, node recycling, and a combination of both techniques, respectively. We show that these enhancements enable a drastic reduction in the number of stored nodes while matching the playing strength of GRAVE.", "AI": {"tldr": "They propose memory-efficient variants of the GRAVE MCTS algorithm\u2014GRAVE2, GRAVER, and GRAVER2\u2014that drastically cut the number of stored nodes while maintaining the original playing strength.", "motivation": "GRAVE is strong for General Game Playing but stores extra win/visit statistics per node, making it impractical in memory-constrained settings. The goal is to retain GRAVE\u2019s strength while reducing memory usage.", "method": "Extend GRAVE with: (1) two-level search (GRAVE2), (2) node recycling to limit the active tree (GRAVER), and (3) a combination of both (GRAVER2).", "result": "Across these variants, the number of stored nodes is reduced drastically without loss in playing strength relative to GRAVE.", "conclusion": "Two-level search and node recycling make GRAVE practical under tight memory budgets, preserving performance and broadening its applicability in General Game Playing."}}
{"id": "2602.22955", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22955", "abs": "https://arxiv.org/abs/2602.22955", "authors": ["Feng Guo", "Jiaxiang Liu", "Yang Li", "Qianqian Shi", "Mingkun Xu"], "title": "MM-NeuroOnco: A Multimodal Benchmark and Instruction Dataset for MRI-Based Brain Tumor Diagnosis", "comment": null, "summary": "Accurate brain tumor diagnosis requires models to not only detect lesions but also generate clinically interpretable reasoning grounded in imaging manifestations, yet existing public datasets remain limited in annotation richness and diagnostic semantics. To bridge this gap, we introduce MM-NeuroOnco, a large-scale multimodal benchmark and instruction-tuning dataset for brain tumor MRI understanding, consisting of 24,726 MRI slices from 20 data sources paired with approximately 200,000 semantically enriched multimodal instructions spanning diverse tumor subtypes and imaging modalities. To mitigate the scarcity and high cost of diagnostic semantic annotations, we develop a multi-model collaborative pipeline for automated medical information completion and quality control, enabling the generation of diagnosis-related semantics beyond mask-only annotations. Building upon this dataset, we further construct MM-NeuroOnco-Bench, a manually annotated evaluation benchmark with a rejection-aware setting to reduce biases inherent in closed-ended question formats. Evaluation across ten representative models shows that even the strongest baseline, Gemini 3 Flash, achieves only 41.88% accuracy on diagnosis-related questions, highlighting the substantial challenges of multimodal brain tumor diagnostic understanding. Leveraging MM-NeuroOnco, we further propose NeuroOnco-GPT, which achieves a 27% absolute accuracy improvement on diagnostic questions following fine-tuning. This result demonstrates the effectiveness of our dataset and benchmark in advancing clinically grounded multimodal diagnostic reasoning. Code and dataset are publicly available at: https://github.com/gfnnnb/MM-NeuroOnco", "AI": {"tldr": "They release MM-NeuroOnco, a large multimodal brain-tumor MRI instruction-tuning dataset and a rejection-aware benchmark; show current VLMs perform poorly on diagnosis reasoning; and demonstrate big gains after fine-tuning their own model (NeuroOnco-GPT).", "motivation": "Clinical brain tumor diagnosis needs models that both find lesions and reason with explicit, interpretable diagnostic semantics. Existing public datasets mostly have masks or coarse labels, lacking rich, clinically grounded annotations; creating such labels is costly and scarce.", "method": "1) Build MM-NeuroOnco: 24,726 MRI slices from 20 sources with ~200k semantically enriched multimodal instructions covering tumor subtypes and modalities. 2) Create a multi-model collaborative pipeline for automated completion of medical information and QC to generate diagnosis-related semantics beyond masks. 3) Construct MM-NeuroOnco-Bench, a manually annotated, rejection-aware evaluation benchmark to curb closed-ended question bias. 4) Evaluate 10 representative models and fine-tune a model (NeuroOnco-GPT) on the dataset.", "result": "On diagnosis-related questions, even the strongest baseline (Gemini 3 Flash) reaches only 41.88% accuracy. After fine-tuning on MM-NeuroOnco, NeuroOnco-GPT achieves a 27% absolute accuracy improvement on diagnostic questions.", "conclusion": "Rich, instruction-style multimodal supervision plus a rejection-aware benchmark exposes limits of current models and enables substantial gains in clinically grounded diagnostic reasoning; the released data/code should catalyze progress in brain tumor MRI understanding."}}
{"id": "2602.22959", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22959", "abs": "https://arxiv.org/abs/2602.22959", "authors": ["Zihao Zhao", "Frederik Hauke", "Juliana De Castilhos", "Sven Nebelung", "Daniel Truhn"], "title": "Can Agents Distinguish Visually Hard-to-Separate Diseases in a Zero-Shot Setting? A Pilot Study", "comment": "Code available at https://github.com/TruhnLab/Contrastive-Agent-Reasoning", "summary": "The rapid progress of multimodal large language models (MLLMs) has led to increasing interest in agent-based systems. While most prior work in medical imaging concentrates on automating routine clinical workflows, we study an underexplored yet clinically significant setting: distinguishing visually hard-to-separate diseases in a zero-shot setting. We benchmark representative agents on two imaging-only proxy diagnostic tasks, (1) melanoma vs. atypical nevus and (2) pulmonary edema vs. pneumonia, where visual features are highly confounded despite substantial differences in clinical management. We introduce a multi-agent framework based on contrastive adjudication. Experimental results show improved diagnostic performance (an 11-percentage-point gain in accuracy on dermoscopy data) and reduced unsupported claims on qualitative samples, although overall performance remains insufficient for clinical deployment. We acknowledge the inherent uncertainty in human annotations and the absence of clinical context, which further limit the translation to real-world settings. Within this controlled setting, this pilot study provides preliminary insights into zero-shot agent performance in visually confounded scenarios.", "AI": {"tldr": "Pilot study tests a contrastive, multi-agent MLLM approach for zero-shot differentiation of look\u2011alike diseases in images, yielding notable accuracy gains (+11 pp on dermoscopy) and fewer unsupported claims, but still far from clinical readiness.", "motivation": "Most MLLM-in-medicine efforts automate routine workflows; however, real clinical impact also needs distinguishing visually confounded diseases that require different management (e.g., melanoma vs. atypical nevus; pulmonary edema vs. pneumonia) without task-specific training.", "method": "Benchmark representative agents on two imaging-only proxy diagnostic tasks and introduce a multi-agent framework that performs contrastive adjudication between competing diagnoses; evaluate diagnostic accuracy and the prevalence of unsupported claims; acknowledge annotation uncertainty and lack of clinical context.", "result": "The multi-agent, contrastive adjudication approach improves diagnostic performance (e.g., +11 percentage points accuracy on dermoscopy) and reduces unsupported claims on qualitative samples, but absolute performance levels remain inadequate for clinical deployment.", "conclusion": "Zero-shot multi-agent MLLMs show preliminary promise under controlled conditions for hard, visually confounded differentials, yet limitations from annotation uncertainty and absent clinical context constrain real-world translation; this is an early, non-deployable step offering insight into agent behavior."}}
{"id": "2602.23330", "categories": ["cs.AI", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2602.23330", "abs": "https://arxiv.org/abs/2602.23330", "authors": ["Kunihiro Miyazaki", "Takanobu Kawahara", "Stephen Roberts", "Stefan Zohren"], "title": "Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks", "comment": "14 pages, 3 figures", "summary": "The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.", "AI": {"tldr": "A fine-grained, multi-agent LLM trading framework\u2014tested on Japanese equities with leakage-controlled backtests\u2014outperforms coarse, instruction-led designs by improving risk-adjusted returns and benefiting further from portfolio optimization that exploits low index correlation and output variance.", "motivation": "Prevailing LLM trading agents mimic analyst/manager roles using high-level prompts that miss real workflow details, hurting inference quality and transparency. The authors seek a more faithful, controllable process that improves performance and interpretability.", "method": "They explicitly decompose investment analysis into granular tasks within a multi-agent LLM framework. The system ingests prices, financial statements, news, and macro data for Japanese stocks, and is evaluated via leakage-controlled backtesting. They analyze intermediate agent outputs for alignment with decision preferences and apply portfolio optimization that leverages low correlation to the index and the variance of system outputs.", "result": "Fine-grained task decomposition yields significantly better risk-adjusted returns than coarse-grained baselines. Analysis indicates that aligning intermediate analytical outputs with downstream decision criteria is a key driver of gains. Portfolio optimization exploiting low index correlation and output variance further improves overall performance.", "conclusion": "Designing LLM trading agents with explicit, fine-grained task structures\u2014and ensuring alignment between analytical steps and decision objectives\u2014enhances performance and transparency. Combining such systems through standard portfolio optimization provides additional, robust gains, guiding practical agent architecture and task configuration in trading applications."}}
{"id": "2602.22960", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22960", "abs": "https://arxiv.org/abs/2602.22960", "authors": ["Tianxing Xu", "Zixuan Wang", "Guangyuan Wang", "Li Hu", "Zhongyi Zhang", "Peng Zhang", "Bang Zhang", "Song-Hai Zhang"], "title": "UCM: Unifying Camera Control and Memory with Time-aware Positional Encoding Warping for World Models", "comment": "Project Page: https://humanaigc.github.io/ucm-webpage/", "summary": "World models based on video generation demonstrate remarkable potential for simulating interactive environments but face persistent difficulties in two key areas: maintaining long-term content consistency when scenes are revisited and enabling precise camera control from user-provided inputs. Existing methods based on explicit 3D reconstruction often compromise flexibility in unbounded scenarios and fine-grained structures. Alternative methods rely directly on previously generated frames without establishing explicit spatial correspondence, thereby constraining controllability and consistency. To address these limitations, we present UCM, a novel framework that unifies long-term memory and precise camera control via a time-aware positional encoding warping mechanism. To reduce computational overhead, we design an efficient dual-stream diffusion transformer for high-fidelity generation. Moreover, we introduce a scalable data curation strategy utilizing point-cloud-based rendering to simulate scene revisiting, facilitating training on over 500K monocular videos. Extensive experiments on real-world and synthetic benchmarks demonstrate that UCM significantly outperforms state-of-the-art methods in long-term scene consistency, while also achieving precise camera controllability in high-fidelity video generation.", "AI": {"tldr": "UCM is a video world-model framework that unifies long-term scene memory and precise camera control using time-aware positional-encoding warping, paired with an efficient dual-stream diffusion transformer and a scalable point-cloud-based data curation pipeline; it achieves higher long-term consistency and controllability than prior methods while maintaining high visual fidelity.", "motivation": "World-model video generators struggle with (1) maintaining content consistency when revisiting locations over long horizons and (2) enabling precise, user-specified camera control. Explicit 3D reconstruction limits flexibility in unbounded, fine-structured scenes, while frame-reuse methods lack explicit spatial correspondence, hurting controllability and consistency.", "method": "Introduce UCM: a time-aware positional encoding warping mechanism that ties spatial memory with camera control; an efficient dual-stream diffusion transformer for high-fidelity generation; and a scalable data curation strategy that uses point-cloud-based rendering to synthetically create scene revisits, enabling training on 500K+ monocular videos.", "result": "On real and synthetic benchmarks, UCM surpasses state-of-the-art baselines in long-term scene consistency and provides precise camera controllability while preserving high-fidelity video quality.", "conclusion": "Unifying memory and control via time-aware positional warping plus an efficient diffusion architecture and scalable revisit-centric data curation yields a controllable, consistent, and high-fidelity world-model video generator, advancing interactive environment simulation."}}
{"id": "2602.23013", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23013", "abs": "https://arxiv.org/abs/2602.23013", "authors": ["Camile Lendering", "Erkut Akdag", "Egor Bondarev"], "title": "SubspaceAD: Training-Free Few-Shot Anomaly Detection via Subspace Modeling", "comment": "Accepted to CVPR 2026", "summary": "Detecting visual anomalies in industrial inspection often requires training with only a few normal images per category. Recent few-shot methods achieve strong results employing foundation-model features, but typically rely on memory banks, auxiliary datasets, or multi-modal tuning of vision-language models. We therefore question whether such complexity is necessary given the feature representations of vision foundation models. To answer this question, we introduce SubspaceAD, a training-free method, that operates in two simple stages. First, patch-level features are extracted from a small set of normal images by a frozen DINOv2 backbone. Second, a Principal Component Analysis (PCA) model is fit to these features to estimate the low-dimensional subspace of normal variations. At inference, anomalies are detected via the reconstruction residual with respect to this subspace, producing interpretable and statistically grounded anomaly scores. Despite its simplicity, SubspaceAD achieves state-of-the-art performance across one-shot and few-shot settings without training, prompt tuning, or memory banks. In the one-shot anomaly detection setting, SubspaceAD achieves image-level and pixel-level AUROC of 98.0% and 97.6% on the MVTec-AD dataset, and 93.3% and 98.3% on the VisA dataset, respectively, surpassing prior state-of-the-art results. Code and demo are available at https://github.com/CLendering/SubspaceAD.", "AI": {"tldr": "SubspaceAD is a training-free few-shot visual anomaly detector that models normality as a low-dimensional PCA subspace over DINOv2 patch features and scores anomalies by reconstruction residuals, achieving state-of-the-art AUROC on MVTec-AD and VisA without memory banks or tuning.", "motivation": "Industrial anomaly detection often has only a handful of normal images per category. Many recent strong methods add complexity (memory banks, auxiliary data, or vision\u2013language tuning). The paper asks whether such complexity is needed given strong foundation-model features.", "method": "1) Extract patch-level features from a small set of normal images using a frozen DINOv2 backbone. 2) Fit PCA to those features to estimate the normal subspace. At inference, project test features onto this subspace and use the reconstruction residual as an interpretable, statistically grounded anomaly score; aggregate for pixel- and image-level detection.", "result": "In one-shot settings, achieves image-/pixel-level AUROC: 98.0%/97.6% on MVTec-AD and 93.3%/98.3% on VisA, outperforming prior SOTA. Works without training, prompt tuning, or memory banks; also strong in few-shot.", "conclusion": "A simple PCA-on-foundation-features approach suffices to reach or surpass SOTA in few-shot industrial anomaly detection while remaining training-free and interpretable; code and demo are publicly available."}}
{"id": "2602.23022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23022", "abs": "https://arxiv.org/abs/2602.23022", "authors": ["Xinglong Luo", "Ao Luo", "Zhengning Wang", "Yueqi Yang", "Chaoyu Feng", "Lei Lei", "Bing Zeng", "Shuaicheng Liu"], "title": "DMAligner: Enhancing Image Alignment via Diffusion Model Based View Synthesis", "comment": "Accepted by CVPR 2026", "summary": "Image alignment is a fundamental task in computer vision with broad applications. Existing methods predominantly employ optical flow-based image warping. However, this technique is susceptible to common challenges such as occlusions and illumination variations, leading to degraded alignment visual quality and compromised accuracy in downstream tasks. In this paper, we present DMAligner, a diffusion-based framework for image alignment through alignment-oriented view synthesis. DMAligner is crafted to tackle the challenges in image alignment from a new perspective, employing a generation-based solution that showcases strong capabilities and avoids the problems associated with flow-based image warping. Specifically, we propose a Dynamics-aware Diffusion Training approach for learning conditional image generation, synthesizing a novel view for image alignment. This incorporates a Dynamics-aware Mask Producing (DMP) module to adaptively distinguish dynamic foreground regions from static backgrounds, enabling the diffusion model to more effectively handle challenges that classical methods struggle to solve. Furthermore, we develop the Dynamic Scene Image Alignment (DSIA) dataset using Blender, which includes 1,033 indoor and outdoor scenes with over 30K image pairs tailored for image alignment. Extensive experimental results demonstrate the superiority of the proposed approach on DSIA benchmarks, as well as on a series of widely-used video datasets for qualitative comparisons. Our code is available at https://github.com/boomluo02/DMAligner.", "AI": {"tldr": "DMAligner replaces flow-based warping with a diffusion-driven, alignment-oriented view synthesis pipeline, using dynamics-aware training and masking to better handle occlusions and illumination changes; it introduces the synthetic DSIA dataset and reports superior results over baselines.", "motivation": "Optical-flow warping often breaks under occlusions, non-rigid motion, and illumination shifts, degrading alignment quality and downstream task performance. A more robust, generative alternative is sought to avoid warping artifacts.", "method": "A diffusion-based conditional image generation framework (DMAligner) that synthesizes a novel, alignment-targeted view. It uses Dynamics-aware Diffusion Training and a Dynamics-aware Mask Producing (DMP) module to separate dynamic foreground from static background so the model can treat them differently during training/inference. The authors also build the DSIA dataset in Blender with 1,033 scenes and >30K image pairs tailored for alignment.", "result": "Across DSIA benchmarks, DMAligner outperforms prior methods; qualitative evaluations on multiple popular video datasets also show better visual alignment and robustness compared to flow-based approaches. Code is publicly available.", "conclusion": "Diffusion-based, generation-centric alignment is an effective alternative to flow warping, especially in dynamic and challenging lighting scenarios; dynamics-aware masking is key, and the new DSIA dataset supports further research."}}
{"id": "2602.23029", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23029", "abs": "https://arxiv.org/abs/2602.23029", "authors": ["Tianyue Wang", "Leigang Qu", "Tianyu Yang", "Xiangzhao Hao", "Yifan Xu", "Haiyun Guo", "Jinqiao Wang"], "title": "WISER: Wider Search, Deeper Thinking, and Adaptive Fusion for Training-Free Zero-Shot Composed Image Retrieval", "comment": null, "summary": "Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images given a multimodal query (comprising a reference image and a modification text), without training on annotated triplets. Existing methods typically convert the multimodal query into a single modality-either as an edited caption for Text-to-Image retrieval (T2I) or as an edited image for Image-to-Image retrieval (I2I). However, each paradigm has inherent limitations: T2I often loses fine-grained visual details, while I2I struggles with complex semantic modifications. To effectively leverage their complementary strengths under diverse query intents, we propose WISER, a training-free framework that unifies T2I and I2I via a \"retrieve-verify-refine\" pipeline, explicitly modeling intent awareness and uncertainty awareness. Specifically, WISER first performs Wider Search by generating both edited captions and images for parallel retrieval to broaden the candidate pool. Then, it conducts Adaptive Fusion with a verifier to assess retrieval confidence, triggering refinement for uncertain retrievals, and dynamically fusing the dual-path for reliable ones. For uncertain retrievals, WISER generates refinement suggestions through structured self-reflection to guide the next retrieval round toward Deeper Thinking. Extensive experiments demonstrate that WISER significantly outperforms previous methods across multiple benchmarks, achieving relative improvements of 45% on CIRCO (mAP@5) and 57% on CIRR (Recall@1) over existing training-free methods. Notably, it even surpasses many training-dependent methods, highlighting its superiority and generalization under diverse scenarios. Code will be released at https://github.com/Physicsmile/WISER.", "AI": {"tldr": "WISER is a training\u2011free ZS-CIR framework that unifies text-to-image and image-to-image retrieval through a retrieve\u2011verify\u2011refine loop, broadening candidates, verifying/fusing results, and iteratively refining uncertain queries, yielding large gains over prior training\u2011free and many trained methods.", "motivation": "Zero-shot composed image retrieval requires matching a target image to a multimodal query (reference image + modification text) without annotated triplets. Existing approaches collapse the query to a single modality\u2014edited captions (T2I) lose fine visual details, while edited images (I2I) miss complex semantics\u2014failing to flexibly capture diverse user intents and to manage uncertainty in retrieval confidence.", "method": "A training-free pipeline with explicit intent and uncertainty awareness: (1) Wider Search: generate both edited captions and edited images and perform parallel T2I and I2I retrieval to enlarge the candidate set; (2) Adaptive Fusion: use a verifier to estimate retrieval confidence, dynamically fuse reliable dual-path results, and trigger refinement for uncertain cases; (3) Deeper Thinking: for uncertain retrievals, produce structured self-reflection suggestions to guide the next retrieval round, iterating until confidence improves.", "result": "Across multiple benchmarks, WISER outperforms prior training-free methods with relative gains of 45% mAP@5 on CIRCO and 57% Recall@1 on CIRR, and even surpasses many training-dependent methods, indicating strong effectiveness and generalization. Code to be released at https://github.com/Physicsmile/WISER.", "conclusion": "Unifying T2I and I2I within a retrieve\u2011verify\u2011refine loop and modeling intent/uncertainty enables robust, training-free ZS-CIR that leverages complementary strengths of both modalities, significantly improving performance and rivaling trained systems across diverse scenarios."}}
{"id": "2602.23031", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23031", "abs": "https://arxiv.org/abs/2602.23031", "authors": ["Zhangjian Ji", "Huijia Yan", "Shaotong Qiao", "Kai Feng", "Wei Wei"], "title": "Small Object Detection Model with Spatial Laplacian Pyramid Attention and Multi-Scale Features Enhancement in Aerial Images", "comment": null, "summary": "Detecting objects in aerial images confronts some significant challenges, including small size, dense and non-uniform distribution of objects over high-resolution images, which makes detection inefficient. Thus, in this paper, we proposed a small object detection algorithm based on a Spatial Laplacian Pyramid Attention and Multi-Scale Feature Enhancement in aerial images. Firstly, in order to improve the feature representation of ResNet-50 on small objects, we presented a novel Spatial Laplacian Pyramid Attention (SLPA) module, which is integrated after each stage of ResNet-50 to identify and emphasize important local regions. Secondly, to enhance the model's semantic understanding and features representation, we designed a Multi-Scale Feature Enhancement Module (MSFEM), which is incorporated into the lateral connections of C5 layer for building Feature Pyramid Network (FPN). Finally, the features representation quality of traditional feature pyramid network will be affected because the features are not aligned when the upper and lower layers are fused. In order to handle it, we utilized deformable convolutions to align the features in the fusion processing of the upper and lower levels of the Feature Pyramid Network, which can help enhance the model's ability to detect and recognize small objects. The extensive experimental results on two benchmark datasets: VisDrone and DOTA demonstrate that our improved model performs better for small object detection in aerial images compared to the original algorithm.", "AI": {"tldr": "They enhance a ResNet-50 + FPN detector for aerial images with a Spatial Laplacian Pyramid Attention module, a multi-scale feature enhancement at the C5 lateral path, and deformable-convolution-based feature alignment in FPN, yielding better small-object detection on VisDrone and DOTA.", "motivation": "Small, densely packed, and non-uniformly distributed objects in high-resolution aerial imagery are hard to detect; vanilla backbones under-represent small-object cues, FPN fusion can misalign features, and semantic richness at upper levels may be insufficient for tiny targets.", "method": "1) Insert a Spatial Laplacian Pyramid Attention (SLPA) after each ResNet-50 stage to emphasize salient local regions likely containing small objects. 2) Add a Multi-Scale Feature Enhancement Module (MSFEM) on the C5 lateral connection when constructing FPN to boost semantic/representational capacity. 3) Use deformable convolutions to align features during top-down/bottom-up FPN fusion, mitigating misalignment across scales.", "result": "On VisDrone and DOTA benchmarks, the improved model outperforms the original baseline for small-object detection (no exact metrics provided in the abstract).", "conclusion": "Combining spatial attention, targeted multi-scale enhancement, and deformable alignment improves small-object detectability in aerial images; the design is a drop-in augmentation to ResNet-50 + FPN and shows superior performance on standard benchmarks."}}
{"id": "2602.23040", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23040", "abs": "https://arxiv.org/abs/2602.23040", "authors": ["Aashish Rai", "Angela Xing", "Anushka Agarwal", "Xiaoyan Cong", "Zekun Li", "Tao Lu", "Aayush Prakash", "Srinath Sridhar"], "title": "PackUV: Packed Gaussian UV Maps for 4D Volumetric Video", "comment": "https://ivl.cs.brown.edu/packuv", "summary": "Volumetric videos offer immersive 4D experiences, but remain difficult to reconstruct, store, and stream at scale. Existing Gaussian Splatting based methods achieve high-quality reconstruction but break down on long sequences, temporal inconsistency, and fail under large motions and disocclusions. Moreover, their outputs are typically incompatible with conventional video coding pipelines, preventing practical applications.\n  We introduce PackUV, a novel 4D Gaussian representation that maps all Gaussian attributes into a sequence of structured, multi-scale UV atlas, enabling compact, image-native storage. To fit this representation from multi-view videos, we propose PackUV-GS, a temporally consistent fitting method that directly optimizes Gaussian parameters in the UV domain. A flow-guided Gaussian labeling and video keyframing module identifies dynamic Gaussians, stabilizes static regions, and preserves temporal coherence even under large motions and disocclusions. The resulting UV atlas format is the first unified volumetric video representation compatible with standard video codecs (e.g., FFV1) without losing quality, enabling efficient streaming within existing multimedia infrastructure.\n  To evaluate long-duration volumetric capture, we present PackUV-2B, the largest multi-view video dataset to date, featuring more than 50 synchronized cameras, substantial motion, and frequent disocclusions across 100 sequences and 2B (billion) frames. Extensive experiments demonstrate that our method surpasses existing baselines in rendering fidelity while scaling to sequences up to 30 minutes with consistent quality.", "AI": {"tldr": "PackUV maps 4D Gaussian splats into multi-scale UV atlases and fits them temporally consistently (PackUV-GS), yielding a codec-compatible, streamable volumetric video format that outperforms prior methods and scales to 30\u2011minute sequences; plus a new 2B\u2011frame, 50+ camera dataset (PackUV\u20112B).", "motivation": "Volumetric video is hard to reconstruct, store, and stream at scale. Existing Gaussian Splatting struggles with long sequences, temporal inconsistency, large motions, disocclusions, and is incompatible with standard video codecs\u2014hindering practical deployment.", "method": "Introduce a 4D Gaussian representation that packs all Gaussian attributes into structured, multi-scale UV atlases (image-native). Train via PackUV-GS: direct optimization of Gaussian parameters in UV space with flow-guided Gaussian labeling and video keyframing to separate dynamic/static regions, stabilize static content, and maintain temporal coherence under large motion/disocclusions. The UV atlas is designed to be directly encodable by standard video codecs (e.g., FFV1).", "result": "First unified volumetric video UV-atlas format that is compatible with standard codecs without quality loss, enabling efficient streaming. Demonstrates higher rendering fidelity than baselines and stable performance on long sequences (up to 30 minutes).", "conclusion": "PackUV provides a practical, temporally consistent, and scalable 4D Gaussian representation that integrates with existing video coding pipelines, enabling efficient storage/streaming of volumetric video; PackUV\u20112B establishes a large-scale benchmark for long-duration, high-motion, multi-view capture."}}
{"id": "2602.23043", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23043", "abs": "https://arxiv.org/abs/2602.23043", "authors": ["Argo Saakyan", "Dmitry Solntsev"], "title": "D-FINE-seg: Object Detection and Instance Segmentation Framework with multi-backend deployment", "comment": "6 pages, 4 figures, 5 tables", "summary": "Transformer-based real-time object detectors achieve strong accuracy-latency trade-offs, and D-FINE is among the top-performing recent architectures. However, real-time instance segmentation with transformers is still less common. We present D-FINE-seg, an instance segmentation extension of D-FINE that adds: a lightweight mask head, segmentation-aware training, including box cropped BCE and dice mask losses, auxiliary and denoising mask supervision, and adapted Hungarian matching cost. On the TACO dataset, D-FINE-seg improves F1-score over Ultralytics YOLO26 under a unified TensorRT FP16 end-to-end benchmarking protocol, while maintaining competitive latency. Second contribution is an end-to-end pipeline for training, exporting, and optimized inference across ONNX, TensorRT, OpenVINO for both object detection and instance segmentation tasks. This framework is released as open-source under the Apache-2.0 license. GitHub repository - https://github.com/ArgoHA/D-FINE-seg.", "AI": {"tldr": "D-FINE-seg extends the D-FINE transformer detector to real-time instance segmentation by adding a lightweight mask head, segmentation-aware training (cropped BCE + Dice losses), auxiliary/denoising mask supervision, and an adapted Hungarian matching cost; it outperforms a YOLO baseline on TACO in F1 while keeping competitive latency, and ships with an open-source, cross-runtime (ONNX/TensorRT/OpenVINO) training-to-inference pipeline.", "motivation": "Transformer detectors offer strong accuracy\u2013latency trade-offs, but real-time instance segmentation with transformers remains underexplored. There is a need for a method that preserves real-time performance while adding accurate masks, and for a practical deployment pipeline across common inference runtimes.", "method": "Build on D-FINE by: (1) adding a lightweight mask head; (2) using segmentation-aware training with box-cropped BCE and Dice mask losses; (3) adding auxiliary and denoising mask supervision; (4) adapting the Hungarian matching cost to include mask-related terms. Evaluate under a unified TensorRT FP16 end-to-end protocol, and provide an end-to-end framework for training, export, and optimized inference across ONNX, TensorRT, and OpenVINO.", "result": "On the TACO dataset, D-FINE-seg achieves higher F1-score than Ultralytics YOLO26 with competitive latency under TensorRT FP16. The authors also release an open-source pipeline supporting both detection and instance segmentation across multiple runtimes.", "conclusion": "Real-time transformer-based instance segmentation is practical: D-FINE-seg improves accuracy at similar latency versus a YOLO baseline and provides a deployable end-to-end solution via a cross-runtime, open-source framework."}}
{"id": "2602.23058", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23058", "abs": "https://arxiv.org/abs/2602.23058", "authors": ["Zeyu Zhang", "Danning Li", "Ian Reid", "Richard Hartley"], "title": "GeoWorld: Geometric World Models", "comment": "Accepted to CVPR 2026", "summary": "Energy-based predictive world models provide a powerful approach for multi-step visual planning by reasoning over latent energy landscapes rather than generating pixels. However, existing approaches face two major challenges: (i) their latent representations are typically learned in Euclidean space, neglecting the underlying geometric and hierarchical structure among states, and (ii) they struggle with long-horizon prediction, which leads to rapid degradation across extended rollouts. To address these challenges, we introduce GeoWorld, a geometric world model that preserves geometric structure and hierarchical relations through a Hyperbolic JEPA, which maps latent representations from Euclidean space onto hyperbolic manifolds. We further introduce Geometric Reinforcement Learning for energy-based optimization, enabling stable multi-step planning in hyperbolic latent space. Extensive experiments on CrossTask and COIN demonstrate around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to the state-of-the-art V-JEPA 2. Project website: https://steve-zeyu-zhang.github.io/GeoWorld.", "AI": {"tldr": "GeoWorld embeds world-model latents in hyperbolic space via a Hyperbolic JEPA and optimizes plans with geometric reinforcement learning over an energy landscape, yielding more stable multi-step visual planning and ~2\u20133% SR gains over V-JEPA 2 on CrossTask/COIN.", "motivation": "Energy-based predictive world models typically learn Euclidean latent representations that ignore hierarchical/geometric relations among states and degrade under long-horizon rollouts. The goal is to preserve such structure to stabilize and improve multi-step planning.", "method": "Introduce a geometric world model that (1) maps Euclidean latents to hyperbolic manifolds using a Hyperbolic JEPA to encode hierarchical/geometric structure; and (2) performs energy-based planning with a Geometric Reinforcement Learning procedure tailored to the hyperbolic latent space for stable multi-step optimization.", "result": "On CrossTask and COIN benchmarks, GeoWorld improves success rate by about 3% for 3-step planning and 2% for 4-step planning compared to the state-of-the-art V-JEPA 2, indicating better long-horizon stability.", "conclusion": "Preserving hierarchical geometry in hyperbolic latent spaces and using geometry-aware RL improves long-horizon energy-based planning, establishing new SOTA over V-JEPA 2 and suggesting geometry-aware representations are beneficial for visual world models."}}
{"id": "2602.23069", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23069", "abs": "https://arxiv.org/abs/2602.23069", "authors": ["Yiding Sun", "Jihua Zhu", "Haozhe Cheng", "Chaoyi Lu", "Zhichuan Yang", "Lin Chen", "Yaonan Wang"], "title": "Align then Adapt: Rethinking Parameter-Efficient Transfer Learning in 4D Perception", "comment": null, "summary": "Point cloud video understanding is critical for robotics as it accurately encodes motion and scene interaction. We recognize that 4D datasets are far scarcer than 3D ones, which hampers the scalability of self-supervised 4D models. A promising alternative is to transfer 3D pre-trained models to 4D perception tasks. However, rigorous empirical analysis reveals two critical limitations that impede transfer capability: overfitting and the modality gap. To overcome these challenges, we develop a novel \"Align then Adapt\" (PointATA) paradigm that decomposes parameter-efficient transfer learning into two sequential stages. Optimal-transport theory is employed to quantify the distributional discrepancy between 3D and 4D datasets, enabling our proposed point align embedder to be trained in Stage 1 to alleviate the underlying modality gap. To mitigate overfitting, an efficient point-video adapter and a spatial-context encoder are integrated into the frozen 3D backbone to enhance temporal modeling capacity in Stage 2. Notably, with the above engineering-oriented designs, PointATA enables a pre-trained 3D model without temporal knowledge to reason about dynamic video content at a smaller parameter cost compared to previous work. Extensive experiments show that PointATA can match or even outperform strong full fine-tuning models, whilst enjoying the advantage of parameter efficiency, e.g. 97.21 \\% accuracy on 3D action recognition, $+8.7 \\%$ on 4 D action segmentation, and 84.06\\% on 4D semantic segmentation.", "AI": {"tldr": "PointATA is a two-stage, parameter-efficient transfer approach that first aligns 3D and 4D point-cloud distributions via optimal transport, then adapts frozen 3D backbones with lightweight temporal adapters, achieving state-of-the-art 4D performance with far fewer trainable parameters.", "motivation": "4D (spatio-temporal) point-cloud datasets are scarce, limiting scalable self-supervised learning. Directly transferring 3D pre-trained models to 4D tasks suffers from overfitting on small 4D data and a modality gap between static 3D and dynamic 4D point clouds.", "method": "Align-then-Adapt paradigm: (1) Alignment stage uses optimal-transport to quantify distribution discrepancy and trains a point align embedder to reduce the 3D\u20134D modality gap. (2) Adaptation stage freezes the 3D backbone and inserts a point-video adapter plus a spatial-context encoder to endow temporal modeling while mitigating overfitting, keeping parameter costs low.", "result": "Across multiple benchmarks, PointATA matches or surpasses full fine-tuning with significantly fewer tunable parameters: 97.21% on 3D action recognition, +8.7% on 4D action segmentation, and 84.06% on 4D semantic segmentation.", "conclusion": "Decomposing 3D-to-4D transfer into alignment and lightweight adaptation effectively bridges modality gaps and curbs overfitting, enabling temporally naive 3D models to understand 4D dynamics efficiently and scalably."}}
{"id": "2602.23088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23088", "abs": "https://arxiv.org/abs/2602.23088", "authors": ["Matthew Sutton", "Katrin Amunts", "Timo Dickscheid", "Christian Schiffer"], "title": "Cytoarchitecture in Words: Weakly Supervised Vision-Language Modeling for Human Brain Microscopy", "comment": "8 pages, 3 figures, submitted for inclusion at a conference", "summary": "Foundation models increasingly offer potential to support interactive, agentic workflows that assist researchers during analysis and interpretation of image data. Such workflows often require coupling vision to language to provide a natural-language interface. However, paired image-text data needed to learn this coupling are scarce and difficult to obtain in many research and clinical settings. One such setting is microscopic analysis of cell-body-stained histological human brain sections, which enables the study of cytoarchitecture: cell density and morphology and their laminar and areal organization. Here, we propose a label-mediated method that generates meaningful captions from images by linking images and text only through a label, without requiring curated paired image-text data. Given the label, we automatically mine area descriptions from related literature and use them as synthetic captions reflecting canonical cytoarchitectonic attributes. An existing cytoarchitectonic vision foundation model (CytoNet) is then coupled to a large language model via an image-to-text training objective, enabling microscopy regions to be described in natural language. Across 57 brain areas, the resulting method produces plausible area-level descriptions and supports open-set use through explicit rejection of unseen areas. It matches the cytoarchitectonic reference label for in-scope patches with 90.6% accuracy and, with the area label masked, its descriptions remain discriminative enough to recover the area in an 8-way test with 68.6% accuracy. These results suggest that weak, label-mediated pairing can suffice to connect existing biomedical vision foundation models to language, providing a practical recipe for integrating natural-language in domains where fine-grained paired annotations are scarce.", "AI": {"tldr": "Weakly pair a cytoarchitectonic vision foundation model with language using labels instead of curated image\u2013text pairs: mine literature-based area descriptions as synthetic captions and train an image-to-text bridge. Achieves 90.6% in-scope label accuracy, 68.6% 8-way recovery without labels, generates plausible area descriptions, and rejects unseen areas across 57 brain regions.", "motivation": "Interactive, language-based assistance for microscopy analysis needs vision\u2013language coupling, but curated image\u2013text pairs are scarce in research/clinical histology. Cytoarchitectonic brain analysis especially lacks paired captions despite the need to describe cell density/morphology and laminar/areal organization.", "method": "Use label-mediated pairing: for each brain area label, automatically mine textual descriptions from literature to serve as synthetic captions capturing canonical cytoarchitectonic attributes. Couple an existing vision FM (CytoNet) to a large language model with an image-to-text training objective, enabling natural-language descriptions and explicit open-set rejection of unseen areas.", "result": "Across 57 areas, the system generates plausible area-level descriptions, supports open-set rejection, reaches 90.6% accuracy in matching the cytoarchitectonic reference label for in-scope patches, and\u2014when area labels are masked\u2014its generated descriptions remain discriminative enough to recover the area in an 8-way test with 68.6% accuracy.", "conclusion": "Label-mediated weak supervision can effectively bridge biomedical vision foundation models to language without curated image\u2013text pairs, offering a practical recipe for natural-language integration in data-scarce domains like histological cytoarchitecture analysis."}}
{"id": "2602.23101", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23101", "abs": "https://arxiv.org/abs/2602.23101", "authors": ["Paul Kielty", "Timothy Hanley", "Peter Corcoran"], "title": "Locally Adaptive Decay Surfaces for High-Speed Face and Landmark Detection with Event Cameras", "comment": null, "summary": "Event cameras record luminance changes with microsecond resolution, but converting their sparse, asynchronous output into dense tensors that neural networks can exploit remains a core challenge. Conventional histograms or globally-decayed time-surface representations apply fixed temporal parameters across the entire image plane, which in practice creates a trade-off between preserving spatial structure during still periods and retaining sharp edges during rapid motion. We introduce Locally Adaptive Decay Surfaces (LADS), a family of event representations in which the temporal decay at each location is modulated according to local signal dynamics. Three strategies are explored, based on event rate, Laplacian-of-Gaussian response, and high-frequency spectral energy. These adaptive schemes preserve detail in quiescent regions while reducing blur in regions of dense activity. Extensive experiments on the public data show that LADS consistently improves both face detection and facial landmark accuracy compared to standard non-adaptive representations. At 30 Hz, LADS achieves higher detection accuracy and lower landmark error than either baseline, and at 240 Hz it mitigates the accuracy decline typically observed at higher frequencies, sustaining 2.44 % normalized mean error for landmarks and 0.966 mAP50 in face detection. These high-frequency results even surpass the accuracy reported in prior works operating at 30 Hz, setting new benchmarks for event-based face analysis. Moreover, by preserving spatial structure at the representation stage, LADS supports the use of much lighter network architectures while still retaining real-time performance. These results highlight the importance of context-aware temporal integration for neuromorphic vision and point toward real-time, high-frequency human-computer interaction systems that exploit the unique advantages of event cameras.", "AI": {"tldr": "Proposes Locally Adaptive Decay Surfaces (LADS) that modulate temporal decay per-pixel using local dynamics (event rate, LoG, spectral energy), yielding sharper edges during motion and preserved structure during stillness, which significantly boosts event-based face detection and landmark accuracy\u2014even at 240 Hz\u2014while enabling lighter, real\u2011time networks.", "motivation": "Fixed, globally chosen temporal parameters in event representations force a trade-off between spatial fidelity in low-motion regions and edge sharpness during rapid motion, and performance typically degrades at higher frame aggregation rates. A representation that adapts integration time locally could overcome this and better exploit event cameras\u2019 microsecond timing.", "method": "Introduce LADS, where the temporal decay constant at each image location is modulated by local signal cues. Three adaptive schemes are explored: (1) event-rate\u2013based decay, (2) Laplacian-of-Gaussian (LoG) response\u2013based decay, and (3) high-frequency spectral energy\u2013based decay. These produce adaptive time surfaces that preserve detail in quiescent areas and reduce motion blur in active regions.", "result": "On public datasets for event-based face analysis, LADS consistently outperforms standard non-adaptive histograms/time-surfaces. At 30 Hz it achieves higher face detection accuracy and lower landmark error than baselines. At 240 Hz it mitigates the usual accuracy drop, sustaining 2.44% normalized mean error for landmarks and 0.966 mAP50 for detection. High-frequency (240 Hz) results surpass prior works reported at 30 Hz. LADS also permits much lighter networks while keeping real-time throughput.", "conclusion": "Context-aware, locally adaptive temporal integration is crucial for neuromorphic vision. LADS sets new benchmarks in event-based face detection/landmarking, maintains accuracy at high aggregation rates, and enables real-time, lightweight models\u2014promising for high-frequency human-computer interaction with event cameras."}}
{"id": "2602.23103", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23103", "abs": "https://arxiv.org/abs/2602.23103", "authors": ["Fuhao Zhang", "Lei Liu", "Jialin Zhang", "Ya-Nan Zhang", "Nan Mu"], "title": "SpectralMamba-UNet: Frequency-Disentangled State Space Modeling for Texture-Structure Consistent Medical Image Segmentation", "comment": null, "summary": "Accurate medical image segmentation requires effective modeling of both global anatomical structures and fine-grained boundary details. Recent state space models (e.g., Vision Mamba) offer efficient long-range dependency modeling. However, their one-dimensional serialization weakens local spatial continuity and high-frequency representation. To this end, we propose SpectralMamba-UNet, a novel frequency-disentangled framework to decouple the learning of structural and textural information in the spectral domain. Our Spectral Decomposition and Modeling (SDM) module applies discrete cosine transform to decompose low- and high-frequency features, where low frequency contributes to global contextual modeling via a frequency-domain Mamba and high frequency preserves boundary-sensitive details. To balance spectral contributions, we introduce a Spectral Channel Reweighting (SCR) mechanism to form channel-wise frequency-aware attention, and a Spectral-Guided Fusion (SGF) module to achieve adaptively multi-scale fusion in the decoder. Experiments on five public benchmarks demonstrate consistent improvements across diverse modalities and segmentation targets, validating the effectiveness and generalizability of our approach.", "AI": {"tldr": "SpectralMamba-UNet disentangles low- and high-frequency cues via DCT, using a frequency-domain Mamba for global context while preserving boundary details, and fuses them with frequency-aware attention\u2014yielding consistent segmentation gains on five medical benchmarks.", "motivation": "Medical segmentation needs both global anatomical context and sharp boundaries. Although state space models like Vision Mamba capture long-range dependencies efficiently, their 1D token serialization degrades local spatial continuity and high-frequency detail, harming boundary precision.", "method": "Introduce a frequency-disentangled UNet: (1) Spectral Decomposition and Modeling (SDM) applies DCT to split features into low (structure) and high (texture/boundary) bands; low-frequency features are modeled by a frequency-domain Mamba for global context, while high-frequency features preserve boundary-sensitive information. (2) Spectral Channel Reweighting (SCR) forms channel-wise frequency-aware attention to balance contributions of the two bands. (3) Spectral-Guided Fusion (SGF) adaptively fuses multi-scale features in the decoder. Overall within a UNet-like encoder\u2013decoder architecture.", "result": "On five public datasets spanning different imaging modalities and targets, the method shows consistent performance improvements over baselines, indicating stronger global modeling without sacrificing boundary accuracy (exact scores not provided in the abstract).", "conclusion": "Decoupling spectral components and assigning tailored modeling\u2014Mamba for low-frequency structure and dedicated handling for high-frequency details\u2014mitigates SSM limitations, improves boundary-aware segmentation, and generalizes across modalities and tasks."}}
{"id": "2602.23114", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23114", "abs": "https://arxiv.org/abs/2602.23114", "authors": ["Xudong Yan", "Songhe Feng", "Jiaxin Wang", "Xin Su", "Yi Jin"], "title": "WARM-CAT: : Warm-Started Test-Time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning", "comment": null, "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions based on the knowledge learned from seen ones. Existing methods suffer from performance degradation caused by the distribution shift of label space at test time, which stems from the inclusion of unseen compositions recombined from attributes and objects. To overcome the challenge, we propose a novel approach that accumulates comprehensive knowledge in both textual and visual modalities from unsupervised data to update multimodal prototypes at test time. Building on this, we further design an adaptive update weight to control the degree of prototype adjustment, enabling the model to flexibly adapt to distribution shift during testing. Moreover, a dynamic priority queue is introduced that stores high-confidence images to acquire visual prototypes from historical images for inference. Since the model tends to favor compositions already stored in the queue during testing, we warm-start the queue by initializing it with training images for visual prototypes of seen compositions and generating unseen visual prototypes using the mapping learned between seen and unseen textual prototypes. Considering the semantic consistency of multimodal knowledge, we align textual and visual prototypes by multimodal collaborative representation learning. To provide a more reliable evaluation for CZSL, we introduce a new benchmark dataset, C-Fashion, and refine the widely used but noisy MIT-States dataset. Extensive experiments indicate that our approach achieves state-of-the-art performance on four benchmark datasets under both closed-world and open-world settings. The source code and datasets are available at https://github.com/xud-yan/WARM-CAT .", "AI": {"tldr": "They introduce a test-time adaptive CZSL method that updates and aligns textual/visual prototypes using unlabeled data, with an adaptive weighting and a prioritized memory, achieving SOTA on multiple benchmarks and releasing a new dataset (C-Fashion) plus a refined MIT-States.", "motivation": "CZSL models face severe performance drops at test time because the label space shifts to include unseen attribute\u2013object compositions recombined from known attributes and objects. Current methods are not robust to this distribution shift and underuse readily available unlabeled data during inference.", "method": "- Accumulate multimodal knowledge from unsupervised test data to update textual and visual prototypes during inference.\n- Learn an adaptive update weight that controls how much each prototype is adjusted, enabling flexible test-time adaptation.\n- Maintain a dynamic priority queue of high-confidence images to compute visual prototypes from recent/history data; warm-start the queue with training images for seen compositions.\n- Generate unseen visual prototypes by mapping from seen to unseen textual prototypes, seeding the memory for unseen classes.\n- Align textual and visual prototypes via multimodal collaborative representation learning to ensure semantic consistency.", "result": "State-of-the-art performance on four CZSL benchmarks under both closed-world and open-world settings; introduction of the new C-Fashion benchmark and a refined MIT-States dataset; code and data released at the provided repository.", "conclusion": "Test-time multimodal prototype updating, guided by adaptive weighting and a prioritized memory, effectively mitigates label-space shift in CZSL. Seeding the memory and aligning modalities improves generalization to unseen compositions. The approach not only advances accuracy but also contributes improved benchmarks and resources for fairer evaluation."}}
{"id": "2602.23115", "categories": ["cs.CV", "cs.CG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23115", "abs": "https://arxiv.org/abs/2602.23115", "authors": ["David Dirnfeld", "Fabien Delattre", "Pedro Miraldo", "Erik Learned-Miller"], "title": "FLIGHT: Fibonacci Lattice-based Inference for Geometric Heading in real-Time", "comment": null, "summary": "Estimating camera motion from monocular video is a fundamental problem in computer vision, central to tasks such as SLAM, visual odometry, and structure-from-motion. Existing methods that recover the camera's heading under known rotation, whether from an IMU or an optimization algorithm, tend to perform well in low-noise, low-outlier conditions, but often decrease in accuracy or become computationally expensive as noise and outlier levels increase. To address these limitations, we propose a novel generalization of the Hough transform on the unit sphere (S(2)) to estimate the camera's heading. First, the method extracts correspondences between two frames and generates a great circle of directions compatible with each pair of correspondences. Then, by discretizing the unit sphere using a Fibonacci lattice as bin centers, each great circle casts votes for a range of directions, ensuring that features unaffected by noise or dynamic objects vote consistently for the correct motion direction. Experimental results on three datasets demonstrate that the proposed method is on the Pareto frontier of accuracy versus efficiency. Additionally, experiments on SLAM show that the proposed method reduces RMSE by correcting the heading during camera pose initialization.", "AI": {"tldr": "They estimate camera heading from monocular video with known rotation by casting great\u2011circle votes on the unit sphere using a generalized Hough transform discretized via a Fibonacci lattice, yielding robust and efficient performance and improving SLAM initialization.", "motivation": "Recovering motion direction from monocular data is crucial for SLAM/VO/SfM, but existing heading estimators degrade under noise/outliers or become slow as contamination rises. With rotation known (from IMU or optimization), there is an opportunity to design a faster, more outlier\u2011tolerant estimator for the translation direction.", "method": "From two-frame feature correspondences and a known rotation, each (pair of) correspondence(s) defines a great circle of translation directions consistent with the geometry. The unit sphere S^2 is discretized with near-uniform Fibonacci lattice bins. Each great circle accumulates votes across bins it passes through, so inlier/static features vote coherently for the true heading while noisy/dynamic ones spread their votes. The peak in the spherical accumulator gives the heading estimate.", "result": "Across three datasets, the method lies on the Pareto frontier of accuracy vs. efficiency relative to baselines. In SLAM, using the estimated heading to correct pose initialization lowers trajectory RMSE.", "conclusion": "A spherical Hough-voting scheme for heading provides a robust, computationally efficient alternative to conventional methods under known rotation, and it delivers practical gains when used to initialize or correct SLAM poses."}}
{"id": "2602.23117", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23117", "abs": "https://arxiv.org/abs/2602.23117", "authors": ["Xiaosen Wang", "Zhijin Ge", "Bohan Liu", "Zheng Fang", "Fengfan Zhou", "Ruixuan Zhang", "Shaokang Wang", "Yuyang Luo"], "title": "Devling into Adversarial Transferability on Image Classification: Review, Benchmark, and Evaluation", "comment": "Code is available at https://github.com/Trustworthy-AI-Group/TransferAttack", "summary": "Adversarial transferability refers to the capacity of adversarial examples generated on the surrogate model to deceive alternate, unexposed victim models. This property eliminates the need for direct access to the victim model during an attack, thereby raising considerable security concerns in practical applications and attracting substantial research attention recently. In this work, we discern a lack of a standardized framework and criteria for evaluating transfer-based attacks, leading to potentially biased assessments of existing approaches. To rectify this gap, we have conducted an exhaustive review of hundreds of related works, organizing various transfer-based attacks into six distinct categories. Subsequently, we propose a comprehensive framework designed to serve as a benchmark for evaluating these attacks. In addition, we delineate common strategies that enhance adversarial transferability and highlight prevalent issues that could lead to unfair comparisons. Finally, we provide a brief review of transfer-based attacks beyond image classification.", "AI": {"tldr": "A survey and benchmarking paper on adversarial transferability: it systematizes transfer-based attacks into six categories, proposes a standardized evaluation framework, distills common transfer-boosting strategies, flags unfair comparison pitfalls, and briefly extends the view beyond image classification.", "motivation": "Current research on transfer-based adversarial attacks lacks unified evaluation criteria and a common framework, leading to inconsistent and potentially biased comparisons across studies. Practitioners and researchers need a reliable benchmark and taxonomy to assess progress and identify best practices.", "method": "Conduct an exhaustive literature review of hundreds of works on transfer-based attacks; organize existing methods into six categories (a new taxonomy); design a comprehensive benchmarking framework with standardized criteria for evaluating transferability; summarize widely used strategies that improve transfer and enumerate pitfalls that cause unfair comparisons; provide a concise review of transfer attacks in tasks beyond image classification.", "result": "Deliverables include: (1) a six-way taxonomy of transfer-based attacks, (2) a standardized benchmarking framework and evaluation criteria, (3) a catalog of transferability-enhancing strategies, (4) a checklist of common pitfalls for fair comparisons, and (5) a brief survey of transfer attacks in non-classification settings.", "conclusion": "The paper offers a consolidated reference and benchmark to fairly evaluate and compare transfer-based attacks, guiding rigorous experimentation and future research while broadening the perspective beyond image classification."}}
{"id": "2602.23120", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23120", "abs": "https://arxiv.org/abs/2602.23120", "authors": ["Arian Sabaghi", "Jos\u00e9 Oramas"], "title": "TriLite: Efficient Weakly Supervised Object Localization with Universal Visual Features and Tri-Region Disentanglement", "comment": "This paper consists of 8 pages including 6 figures. Accepted at CVPR 2026", "summary": "Weakly supervised object localization (WSOL) aims to localize target objects in images using only image-level labels. Despite recent progress, many approaches still rely on multi-stage pipelines or full fine-tuning of large backbones, which increases training cost, while the broader WSOL community continues to face the challenge of partial object coverage. We present TriLite, a single-stage WSOL framework that leverages a frozen Vision Transformer with Dinov2 pre-training in a self-supervised manner, and introduces only a minimal number of trainable parameters (fewer than 800K on ImageNet-1K) for both classification and localization. At its core is the proposed TriHead module, which decomposes patch features into foreground, background, and ambiguous regions, thereby improving object coverage while suppressing spurious activations. By disentangling classification and localization objectives, TriLite effectively exploits the universal representations learned by self-supervised ViTs without requiring expensive end-to-end training. Extensive experiments on CUB-200-2011, ImageNet-1K, and OpenImages demonstrate that TriLite sets a new state of the art, while remaining significantly more parameter-efficient and easier to train than prior methods. The code will be released soon.", "AI": {"tldr": "TriLite is a single-stage, parameter-efficient WSOL method that freezes a DINOv2 ViT and adds a lightweight TriHead to decompose patches into foreground/background/ambiguous regions, improving object coverage and achieving SOTA on multiple benchmarks.", "motivation": "WSOL methods often cover only discriminative parts of objects and are costly to train due to multi-stage designs or full backbone fine-tuning. There is a need for a simple, efficient approach that leverages strong self-supervised ViT representations without expensive end-to-end training.", "method": "Freeze a self-supervised DINOv2 ViT backbone and introduce fewer than 800K trainable parameters for classification and localization. The core TriHead module factorizes patch features into three regions\u2014foreground, background, and ambiguous\u2014disentangling classification from localization to enhance coverage and suppress spurious activations.", "result": "On CUB-200-2011, ImageNet-1K, and OpenImages, TriLite achieves new state-of-the-art WSOL performance while being far more parameter-efficient and easier to train than prior methods.", "conclusion": "By separating objectives and exploiting universal self-supervised ViT features, TriLite delivers accurate, efficient WSOL with improved object coverage and reduced training cost; it establishes new benchmarks and promises practical training simplicity. Code will be released."}}
{"id": "2602.23133", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23133", "abs": "https://arxiv.org/abs/2602.23133", "authors": ["Xin Yuan", "Zhiyong Zhang", "Xin Xu", "Zheng Wang", "Chia-Wen Lin"], "title": "From Calibration to Refinement: Seeking Certainty via Probabilistic Evidence Propagation for Noisy-Label Person Re-Identification", "comment": "Accepted by IEEE TMM 2026", "summary": "With the increasing demand for robust person Re-ID in unconstrained environments, learning from datasets with noisy labels and sparse per-identity samples remains a critical challenge. Existing noise-robust person Re-ID methods primarily rely on loss-correction or sample-selection strategies using softmax outputs. However, these methods suffer from two key limitations: 1) Softmax exhibits translation invariance, leading to over-confident and unreliable predictions on corrupted labels. 2) Conventional sample selection based on small-loss criteria often discards valuable hard positives that are crucial for learning discriminative features. To overcome these issues, we propose the CAlibration-to-REfinement (CARE) method, a two-stage framework that seeks certainty through probabilistic evidence propagation from calibration to refinement. In the calibration stage, we propose the probabilistic evidence calibration (PEC) that dismantles softmax translation invariance by injecting adaptive learnable parameters into the similarity function, and employs an evidential calibration loss to mitigate overconfidence on mislabeled samples. In the refinement stage, we design the evidence propagation refinement (EPR) that can more accurately distinguish between clean and noisy samples. Specifically, the EPR contains two steps: Firstly, the composite angular margin (CAM) metric is proposed to precisely distinguish clean but hard-to-learn positive samples from mislabeled ones in a hyperspherical space; Secondly, the certainty-oriented sphere weighting (COSW) is developed to dynamically allocate the importance of samples according to CAM, ensuring clean instances drive model updates. Extensive experimental results on Market1501, DukeMTMC-ReID, and CUHK03 datasets under both random and patterned noises show that CARE achieves competitive performance.", "AI": {"tldr": "CARE is a two-stage, noise-robust person re-identification framework that first calibrates probabilistic evidence to curb softmax overconfidence, then refines learning by propagating certainty to keep clean (including hard) positives while down-weighting mislabeled data, achieving competitive results under various noise patterns.", "motivation": "Person Re-ID in the wild often suffers from noisy labels and few samples per identity. Existing approaches hinge on softmax-based losses and small-loss sample selection, which (1) produce over-confident, unreliable predictions due to softmax translation invariance and (2) mistakenly drop hard-but-correct positives that are crucial for discriminative learning.", "method": "CARE proceeds in two stages: (1) Probabilistic Evidence Calibration (PEC) injects adaptive, learnable parameters into the similarity function to break softmax translation invariance and employs an evidential calibration loss to reduce overconfidence on corrupted labels. (2) Evidence Propagation Refinement (EPR) improves clean/noisy separation via two components: Composite Angular Margin (CAM) to distinguish clean hard positives from mislabeled samples in a hyperspherical space, and Certainty-Oriented Sphere Weighting (COSW) to weight samples by certainty derived from CAM, ensuring clean instances drive updates.", "result": "Across Market1501, DukeMTMC-ReID, and CUHK03 with both random and structured label noise, CARE delivers competitive performance, outperforming or matching state-of-the-art noise-robust methods in typical metrics (e.g., mAP/Rank-1) according to the abstract.", "conclusion": "By coupling calibrated evidence modeling with certainty-driven refinement, CARE addresses overconfidence from softmax and avoids discarding hard positives, leading to robust feature learning and strong Re-ID accuracy under noisy-label conditions."}}
{"id": "2602.23141", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23141", "abs": "https://arxiv.org/abs/2602.23141", "authors": ["Tao Liu", "Gang Wan", "Kan Ren", "Shibo Wen"], "title": "No Labels, No Look-Ahead: Unsupervised Online Video Stabilization with Classical Priors", "comment": "CVPR2026", "summary": "We propose a new unsupervised framework for online video stabilization. Unlike methods based on deep learning that require paired stable and unstable datasets, our approach instantiates the classical stabilization pipeline with three stages and incorporates a multithreaded buffering mechanism. This design addresses three longstanding challenges in end-to-end learning: limited data, poor controllability, and inefficiency on hardware with constrained resources. Existing benchmarks focus mainly on handheld videos with a forward view in visible light, which restricts the applicability of stabilization to domains such as UAV nighttime remote sensing. To fill this gap, we introduce a new multimodal UAV aerial video dataset (UAV-Test). Experiments show that our method consistently outperforms state-of-the-art online stabilizers in both quantitative metrics and visual quality, while achieving performance comparable to offline methods.", "AI": {"tldr": "Unsupervised, online video stabilization using a classical three-stage pipeline augmented with multithreaded buffering; it outperforms state-of-the-art online stabilizers and approaches offline quality, and introduces a multimodal UAV aerial dataset (UAV-Test) including nighttime scenes.", "motivation": "End-to-end deep learning stabilizers need paired stable/unstable data, offer limited controllability, and are inefficient on resource-constrained hardware; existing benchmarks overemphasize handheld, daylight, visible-spectrum videos, limiting applicability to UAV nighttime/remote-sensing use cases.", "method": "Instantiate a classical three-stage stabilization pipeline (e.g., motion estimation, trajectory smoothing, and warping) in an unsupervised, online fashion, and add a multithreaded buffering mechanism to decouple stages and improve latency/throughput on constrained devices\u2014removing the need for paired supervision while improving controllability and efficiency.", "result": "Proposes the UAV-Test multimodal aerial dataset and shows consistent gains over state-of-the-art online stabilizers on quantitative metrics and visual quality, with performance comparable to offline methods.", "conclusion": "Demonstrates that controllable, efficient, unsupervised online stabilization is feasible beyond handheld/visible-light settings (e.g., UAV nighttime). The new dataset broadens evaluation coverage and supports adoption in underrepresented modalities."}}
{"id": "2602.23153", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23153", "abs": "https://arxiv.org/abs/2602.23153", "authors": ["Guofeng Mei", "Wei Lin", "Luigi Riz", "Yujiao Wu", "Yiming Wang", "Fabio Poiesi"], "title": "Efficient Encoder-Free Fourier-based 3D Large Multimodal Model", "comment": null, "summary": "Large Multimodal Models (LMMs) that process 3D data typically rely on heavy, pre-trained visual encoders to extract geometric features. While recent 2D LMMs have begun to eliminate such encoders for efficiency and scalability, extending this paradigm to 3D remains challenging due to the unordered and large-scale nature of point clouds. This leaves a critical unanswered question: How can we design an LMM that tokenizes unordered 3D data effectively and efficiently without a cumbersome encoder? We propose Fase3D, the first efficient encoder-free Fourier-based 3D scene LMM. Fase3D tackles the challenges of scalability and permutation invariance with a novel tokenizer that combines point cloud serialization and the Fast Fourier Transform (FFT) to approximate self-attention. This design enables an effective and computationally minimal architecture, built upon three key innovations: First, we represent large scenes compactly via structured superpoints. Second, our space-filling curve serialization followed by an FFT enables efficient global context modeling and graph-based token merging. Lastly, our Fourier-augmented LoRA adapters inject global frequency-aware interactions into the LLMs at a negligible cost. Fase3D achieves performance comparable to encoder-based 3D LMMs while being significantly more efficient in computation and parameters. Project website: https://tev-fbk.github.io/Fase3D.", "AI": {"tldr": "Fase3D is an encoder\u2011free, Fourier-based 3D large multimodal model that tokenizes point clouds via superpoints and space\u2011filling\u2011curve serialization, then uses FFT to approximate self\u2011attention and Fourier\u2011augmented LoRA for global frequency cues\u2014achieving encoder\u2011level performance with far lower compute and parameters.", "motivation": "Most 3D LMMs depend on heavy, pre\u2011trained visual encoders to extract geometry, which hurts scalability and efficiency. Unlike recent 2D LMMs that have shed such encoders, 3D point clouds are unordered and large\u2011scale, making efficient, permutation\u2011invariant tokenization and global context modeling difficult without an encoder.", "method": "Fase3D removes the 3D encoder and introduces: (1) compact scene representation via structured superpoints; (2) space\u2011filling curve\u2013based serialization of the point cloud, followed by FFT to approximate global self\u2011attention and enable graph\u2011based token merging; and (3) Fourier\u2011augmented LoRA adapters that inject global, frequency\u2011aware interactions into the LLM at negligible cost.", "result": "Fase3D matches the performance of encoder\u2011based 3D LMMs while being substantially more efficient in both computation and parameter count.", "conclusion": "An encoder\u2011free, FFT\u2011driven tokenizer with superpoints and frequency\u2011aware LoRA can deliver scalable, permutation\u2011invariant 3D LMMs that retain competitive accuracy at much lower resource cost."}}
{"id": "2602.23165", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23165", "abs": "https://arxiv.org/abs/2602.23165", "authors": ["Yichen Peng", "Jyun-Ting Song", "Siyeol Jung", "Ruofan Liu", "Haiyang Liu", "Xuangeng Chu", "Ruicong Liu", "Erwin Wu", "Hideki Koike", "Kris Kitani"], "title": "DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation", "comment": "13 pages, 9 figures", "summary": "Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.", "AI": {"tldr": "DyaDiT is a multi\u2011modal diffusion transformer that generates socially appropriate conversational gestures from dyadic audio by fusing both speakers\u2019 signals, optional social\u2011context tokens, and (optionally) the partner\u2019s gestures, outperforming single\u2011speaker baselines on objective metrics and user preference.", "motivation": "Existing gesture generators typically map one speaker\u2019s audio to that speaker\u2019s motion and ignore social context and mutual conversational dynamics, limiting realism and engagement in digital humans.", "method": "Train a diffusion\u2011based transformer (DyaDiT) on the Seamless Interaction Dataset to synthesize motion from two\u2011speaker audio. The model fuses both speakers\u2019 audio, accepts optional social\u2011context tokens, uses a motion dictionary to encode priors, and can condition on the partner\u2019s gestures for more responsive motion.", "result": "Across standard motion\u2011generation metrics and quantitative user studies, DyaDiT surpasses prior methods and is strongly preferred by users, indicating robust, socially favorable motion generation.", "conclusion": "Explicitly modeling dyadic interaction and social context with a diffusion transformer and motion priors produces more natural, context\u2011appropriate gestures; code and models will be released upon acceptance."}}
{"id": "2602.23166", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23166", "abs": "https://arxiv.org/abs/2602.23166", "authors": ["Zhaochen Su", "Jincheng Gao", "Hangyu Guo", "Zhenhua Liu", "Lueyang Zhang", "Xinyu Geng", "Shijue Huang", "Peng Xia", "Guanyu Jiang", "Cheng Wang", "Yue Zhang", "Yi R. Fung", "Junxian He"], "title": "AgentVista: Evaluating Multimodal Agents in Ultra-Challenging Realistic Visual Scenarios", "comment": "The project website is available at \\url{https://agentvista-bench.github.io/}, and the code is available at \\url{https://github.com/hkust-nlp/AgentVista}", "summary": "Real-world multimodal agents solve multi-step workflows grounded in visual evidence. For example, an agent can troubleshoot a device by linking a wiring photo to a schematic and validating the fix with online documentation, or plan a trip by interpreting a transit map and checking schedules under routing constraints. However, existing multimodal benchmarks mainly evaluate single-turn visual reasoning or specific tool skills, and they do not fully capture the realism, visual subtlety, and long-horizon tool use that practical agents require. We introduce AgentVista, a benchmark for generalist multimodal agents that spans 25 sub-domains across 7 categories, pairing realistic and detail-rich visual scenarios with natural hybrid tool use. Tasks require long-horizon tool interactions across modalities, including web search, image search, page navigation, and code-based operations for both image processing and general programming. Comprehensive evaluation of state-of-the-art models exposes significant gaps in their ability to carry out long-horizon multimodal tool use. Even the best model in our evaluation, Gemini-3-Pro with tools, achieves only 27.3% overall accuracy, and hard instances can require more than 25 tool-calling turns. We expect AgentVista to accelerate the development of more capable and reliable multimodal agents for realistic and ultra-challenging problem solving.", "AI": {"tldr": "AgentVista is a new benchmark for long-horizon, tool-using multimodal agents with realistic, detail-rich visual tasks across 25 sub-domains; current state-of-the-art models perform poorly (best 27.3% accuracy), revealing major gaps.", "motivation": "Real multimodal agents must execute multi-step, visually grounded workflows using diverse tools, but existing benchmarks mostly test single-turn reasoning or narrow tool skills and fail to capture realistic, long-horizon use cases.", "method": "Construct a benchmark (AgentVista) spanning 25 sub-domains in 7 categories that pairs complex visual scenarios with hybrid tool use. Tasks demand multi-turn interactions such as web and image search, page navigation, and code-based operations for image processing and general programming. Evaluate a range of state-of-the-art multimodal agents on these tasks.", "result": "Comprehensive evaluation shows substantial performance deficits on long-horizon multimodal tool use. The strongest tested model (Gemini-3-Pro with tools) attains only 27.3% overall accuracy; difficult instances can require more than 25 tool-calling turns.", "conclusion": "AgentVista exposes current limitations in multimodal agents\u2019 long-horizon tool use and is positioned to catalyze progress toward more capable, reliable agents for realistic, ultra-challenging problem solving."}}
{"id": "2602.23169", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23169", "abs": "https://arxiv.org/abs/2602.23169", "authors": ["Xiaole Tang", "Xiaoyi He", "Jiayi Xu", "Xiang Gu", "Jian Sun"], "title": "Learning Continuous Wasserstein Barycenter Space for Generalized All-in-One Image Restoration", "comment": null, "summary": "Despite substantial advances in all-in-one image restoration for addressing diverse degradations within a unified model, existing methods remain vulnerable to out-of-distribution degradations, thereby limiting their generalization in real-world scenarios. To tackle the challenge, this work is motivated by the intuition that multisource degraded feature distributions are induced by different degradation-specific shifts from an underlying degradation-agnostic distribution, and recovering such a shared distribution is thus crucial for achieving generalization across degradations. With this insight, we propose BaryIR, a representation learning framework that aligns multisource degraded features in the Wasserstein barycenter (WB) space, which models a degradation-agnostic distribution by minimizing the average of Wasserstein distances to multisource degraded distributions. We further introduce residual subspaces, whose embeddings are mutually contrasted while remaining orthogonal to the WB embeddings. Consequently, BaryIR explicitly decouples two orthogonal spaces: a WB space that encodes the degradation-agnostic invariant contents shared across degradations, and residual subspaces that adaptively preserve the degradation-specific knowledge. This disentanglement mitigates overfitting to in-distribution degradations and enables adaptive restoration grounded on the degradation-agnostic shared invariance. Extensive experiments demonstrate that BaryIR performs competitively against state-of-the-art all-in-one methods. Notably, BaryIR generalizes well to unseen degradations (\\textit{e.g.,} types and levels) and shows remarkable robustness in learning generalized features, even when trained on limited degradation types and evaluated on real-world data with mixed degradations.", "AI": {"tldr": "BaryIR learns a degradation-agnostic representation via Wasserstein barycenter alignment while preserving degradation-specific cues in orthogonal residual subspaces, yielding robust, all-in-one image restoration that generalizes to unseen degradations.", "motivation": "All-in-one image restoration models often overfit to the degradations seen during training and fail on out-of-distribution (OOD) degradations. The authors posit that diverse degradations are distributional shifts from a shared, degradation-agnostic content distribution; recovering this shared distribution should improve cross-degradation generalization.", "method": "Align multisource degraded feature distributions in a Wasserstein barycenter (WB) space to model the shared, degradation-agnostic distribution by minimizing average Wasserstein distances to each degraded distribution. Introduce residual subspaces whose embeddings are (i) orthogonal to the WB embeddings and (ii) mutually contrasted across degradation sources. This explicitly decouples invariant (WB) and specific (residual) features to guide adaptive restoration.", "result": "Across extensive experiments, BaryIR is competitive with state-of-the-art all-in-one restoration methods, generalizes to unseen degradation types and levels, and remains robust even when trained with limited degradation diversity and evaluated on real-world images with mixed degradations.", "conclusion": "Disentangling invariant content (via WB alignment) from degradation-specific information (via orthogonal residual subspaces) mitigates overfitting to in-distribution degradations and enables adaptive, generalizable image restoration in the wild."}}
{"id": "2602.23172", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23172", "abs": "https://arxiv.org/abs/2602.23172", "authors": ["Maximilian Luz", "Rohit Mohan", "Thomas N\u00fcrnberg", "Yakov Miron", "Daniele Cattaneo", "Abhinav Valada"], "title": "Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking", "comment": null, "summary": "Capturing 4D spatiotemporal surroundings is crucial for the safe and reliable operation of robots in dynamic environments. However, most existing methods address only one side of the problem: they either provide coarse geometric tracking via bounding boxes, or detailed 3D structures like voxel-based occupancy that lack explicit temporal association. In this work, we present Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking (LaGS) that advances spatiotemporal scene understanding in a holistic direction. Our approach incorporates camera-based end-to-end tracking with mask-based multi-view panoptic occupancy prediction, and addresses the key challenge of efficiently aggregating multi-view information into 3D voxel grids via a novel latent Gaussian splatting approach. Specifically, we first fuse observations into 3D Gaussians that serve as a sparse point-centric latent representation of the 3D scene, and then splat the aggregated features onto a 3D voxel grid that is decoded by a mask-based segmentation head. We evaluate LaGS on the Occ3D nuScenes and Waymo datasets, achieving state-of-the-art performance for 4D panoptic occupancy tracking. We make our code available at https://lags.cs.uni-freiburg.de/.", "AI": {"tldr": "LaGS unifies end-to-end camera-based tracking with multi-view panoptic occupancy by fusing image evidence into latent 3D Gaussians and splatting them into a voxel grid for mask-based decoding, achieving state-of-the-art 4D panoptic occupancy tracking on Occ3D nuScenes and Waymo.", "motivation": "Robots require detailed and temporally consistent 3D scene understanding. Existing methods either provide temporal association with coarse 3D bounding boxes or detailed occupancy without explicit temporal links, leaving a gap for holistic 4D panoptic occupancy tracking.", "method": "Aggregate multi-view camera features into sparse 3D Gaussians as a point-centric latent representation; splat these Gaussian features onto a 3D voxel grid; decode with a mask-based panoptic segmentation head to jointly predict occupancy and identities over time. The system performs camera-based end-to-end tracking while integrating multi-view panoptic occupancy prediction via a novel latent Gaussian splatting mechanism for efficient 3D aggregation.", "result": "LaGS attains state-of-the-art performance on the Occ3D nuScenes and Waymo datasets for 4D panoptic occupancy tracking (no specific metrics reported in the abstract).", "conclusion": "Latent Gaussian splatting enables efficient multi-view 3D feature aggregation and holistic 4D panoptic occupancy tracking, improving spatiotemporal accuracy and consistency; code is publicly released."}}
{"id": "2602.23177", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23177", "abs": "https://arxiv.org/abs/2602.23177", "authors": ["Bin Zeng", "Johannes K\u00fcnzel", "Anna Hilsmann", "Peter Eisert"], "title": "Phys-3D: Physics-Constrained Real-Time Crowd Tracking and Counting on Railway Platforms", "comment": "published at VISAPP 2026", "summary": "Accurate, real-time crowd counting on railway platforms is essential for safety and capacity management. We propose to use a single camera mounted in a train, scanning the platform while arriving. While hardware constraints are simple, counting remains challenging due to dense occlusions, camera motion, and perspective distortions during train arrivals. Most existing tracking-by-detection approaches assume static cameras or ignore physical consistency in motion modeling, leading to unreliable counting under dynamic conditions. We propose a physics-constrained tracking framework that unifies detection, appearance, and 3D motion reasoning in a real-time pipeline. Our approach integrates a transfer-learned YOLOv11m detector with EfficientNet-B0 appearance encoding within DeepSORT, while introducing a physics-constrained Kalman model (Phys-3D) that enforces physically plausible 3D motion dynamics through pinhole geometry. To address counting brittleness under occlusions, we implement a virtual counting band with persistence. On our platform benchmark, MOT-RailwayPlatformCrowdHead Dataset(MOT-RPCH), our method reduces counting error to 2.97%, demonstrating robust performance despite motion and occlusions. Our results show that incorporating first-principles geometry and motion priors enables reliable crowd counting in safety-critical transportation scenarios, facilitating effective train scheduling and platform safety management.", "AI": {"tldr": "Train-mounted single-camera, real-time crowd counting via DeepSORT with YOLOv11m + EfficientNet-B0 and a physics-constrained 3D Kalman filter grounded in pinhole geometry; a virtual counting band with persistence mitigates occlusions; achieves 2.97% counting error on the MOT-RPCH platform benchmark.", "motivation": "Platform crowd estimation is critical for safety and capacity planning, but moving-camera footage during train arrival introduces occlusions, perspective changes, and ego-motion that break assumptions of static-camera tracking-by-detection, degrading count reliability.", "method": "Unifies detection (transfer-learned YOLOv11m), appearance embedding (EfficientNet-B0) and tracking (DeepSORT) with a new physics-constrained Kalman model (Phys-3D) enforcing plausible 3D kinematics via pinhole projection. Adds a virtual counting band with persistence to stabilize counts through occlusions; runs in real time from a train-mounted camera scanning the platform.", "result": "On the MOT-RailwayPlatformCrowdHead (MOT-RPCH) benchmark, the approach reduces counting error to 2.97% and remains robust under motion and occlusion, meeting real-time constraints.", "conclusion": "Embedding first-principles geometry and motion priors into the tracking pipeline enables reliable crowd counting under dynamic, safety-critical conditions, supporting platform safety management and train scheduling."}}
{"id": "2602.23191", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23191", "abs": "https://arxiv.org/abs/2602.23191", "authors": ["Xinyuan Chen", "Yao Xu", "Shaowen Wang", "Pengjie Song", "Bowen Deng"], "title": "Uni-Animator: Towards Unified Visual Colorization", "comment": "10 pages, 8 figures. Submitted to CVPR 2026", "summary": "We propose Uni-Animator, a novel Diffusion Transformer (DiT)-based framework for unified image and video sketch colorization. Existing sketch colorization methods struggle to unify image and video tasks, suffering from imprecise color transfer with single or multiple references, inadequate preservation of high-frequency physical details, and compromised temporal coherence with motion artifacts in large-motion scenes. To tackle imprecise color transfer, we introduce visual reference enhancement via instance patch embedding, enabling precise alignment and fusion of reference color information. To resolve insufficient physical detail preservation, we design physical detail reinforcement using physical features that effectively capture and retain high-frequency textures. To mitigate motion-induced temporal inconsistency, we propose sketch-based dynamic RoPE encoding that adaptively models motion-aware spatial-temporal dependencies. Extensive experimental results demonstrate that Uni-Animator achieves competitive performance on both image and video sketch colorization, matching that of task-specific methods while unlocking unified cross-domain capabilities with high detail fidelity and robust temporal consistency.", "AI": {"tldr": "Uni-Animator is a Diffusion Transformer that unifies image and video sketch colorization, improving reference color transfer, high-frequency detail fidelity, and temporal coherence, achieving competitive results with task-specific methods.", "motivation": "Current sketch colorization methods handle images and videos separately and suffer from (i) inaccurate color transfer from one or multiple references, (ii) loss of fine, high-frequency textures, and (iii) temporal artifacts\u2014especially under large motion\u2014leading to incoherent videos.", "method": "A DiT-based framework with three core designs: (1) visual reference enhancement via instance patch embedding to precisely align and fuse color cues from single/multiple references; (2) physical detail reinforcement that injects physical features to capture and preserve high-frequency textures; (3) sketch-based dynamic RoPE encoding that adaptively models motion-aware spatiotemporal dependencies to maintain temporal consistency.", "result": "Extensive experiments show competitive performance on both image and video sketch colorization, with precise color transfer, strong detail preservation, and robust temporal consistency, matching specialized baselines while enabling a single unified model across domains.", "conclusion": "Uni-Animator provides a unified, DiT-based solution for sketch colorization across images and videos, delivering high-detail fidelity and temporally coherent results and narrowing the gap with task-specific approaches while adding cross-domain flexibility."}}
{"id": "2602.23192", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23192", "abs": "https://arxiv.org/abs/2602.23192", "authors": ["Thomas Woergaard", "Raghavendra Selvan"], "title": "FairQuant: Fairness-Aware Mixed-Precision Quantization for Medical Image Classification", "comment": "Source code available at https://github.com/saintslab/FairQuant", "summary": "Compressing neural networks by quantizing model parameters offers useful trade-off between performance and efficiency. Methods like quantization-aware training and post-training quantization strive to maintain the downstream performance of compressed models compared to the full precision models. However, these techniques do not explicitly consider the impact on algorithmic fairness. In this work, we study fairness-aware mixed-precision quantization schemes for medical image classification under explicit bit budgets. We introduce FairQuant, a framework that combines group-aware importance analysis, budgeted mixed-precision allocation, and a learnable Bit-Aware Quantization (BAQ) mode that jointly optimizes weights and per-unit bit allocations under bitrate and fairness regularization. We evaluate the method on Fitzpatrick17k and ISIC2019 across ResNet18/50, DeiT-Tiny, and TinyViT. Results show that FairQuant configurations with average precision near 4-6 bits recover much of the Uniform 8-bit accuracy while improving worst-group performance relative to Uniform 4- and 8-bit baselines, with comparable fairness metrics under shared budgets.", "AI": {"tldr": "FairQuant is a fairness-aware mixed\u2011precision quantization framework that jointly optimizes weights and per\u2011unit bit allocations under bitrate and fairness regularization. On dermatology datasets and multiple CNN/ViT backbones, it recovers much of 8\u2011bit accuracy at ~4\u20136 average bits while improving worst\u2011group performance over uniform 4\u2011/8\u2011bit baselines.", "motivation": "Existing quantization methods focus on efficiency\u2013accuracy trade\u2011offs but largely ignore algorithmic fairness, which is critical in medical imaging where performance disparities across demographic groups (e.g., skin tones) can have clinical consequences. The goal is to compress models under strict bit budgets without sacrificing, and ideally improving, worst\u2011group performance.", "method": "Introduce FairQuant with three components: (1) group\u2011aware importance analysis to identify layers/units whose precision most affects subgroup performance; (2) budgeted mixed\u2011precision allocation to distribute bits across units under an explicit bitrate constraint; and (3) a learnable Bit\u2011Aware Quantization (BAQ) mode that jointly optimizes weights and per\u2011unit bitwidths using fairness and bitrate regularizers. Evaluated on Fitzpatrick17k and ISIC2019 across ResNet18/50, DeiT\u2011Tiny, and TinyViT, against uniform 4\u2011 and 8\u2011bit baselines and fairness metrics under shared budgets.", "result": "Configurations with ~4\u20136 average bits retain much of the Uniform 8\u2011bit accuracy and yield higher worst\u2011group performance than both Uniform 4\u2011bit and 8\u2011bit baselines, achieving comparable fairness metrics given the same bitrate budgets.", "conclusion": "Fairness\u2011aware mixed\u2011precision quantization can deliver substantial compression while maintaining near\u20118\u2011bit accuracy and improving worst\u2011group outcomes in medical image classification. Incorporating group\u2011aware importance and learnable bit allocation under fairness regularization is an effective path to fairer, budget\u2011constrained model deployment."}}
{"id": "2602.23203", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23203", "abs": "https://arxiv.org/abs/2602.23203", "authors": ["Junhu Fu", "Shuyu Liang", "Wutong Li", "Chen Ma", "Peng Huang", "Kehao Wang", "Ke Chen", "Shengli Lin", "Pinghong Zhou", "Zeju Li", "Yuanyuan Wang", "Yi Guo"], "title": "ColoDiff: Integrating Dynamic Consistency With Content Awareness for Colonoscopy Video Generation", "comment": null, "summary": "Colonoscopy video generation delivers dynamic, information-rich data critical for diagnosing intestinal diseases, particularly in data-scarce scenarios. High-quality video generation demands temporal consistency and precise control over clinical attributes, but faces challenges from irregular intestinal structures, diverse disease representations, and various imaging modalities. To this end, we propose ColoDiff, a diffusion-based framework that generates dynamic-consistent and content-aware colonoscopy videos, aiming to alleviate data shortage and assist clinical analysis. At the inter-frame level, our TimeStream module decouples temporal dependency from video sequences through a cross-frame tokenization mechanism, enabling intricate dynamic modeling despite irregular intestinal structures. At the intra-frame level, our Content-Aware module incorporates noise-injected embeddings and learnable prototypes to realize precise control over clinical attributes, breaking through the coarse guidance of diffusion models. Additionally, ColoDiff employs a non-Markovian sampling strategy that cuts steps by over 90% for real-time generation. ColoDiff is evaluated across three public datasets and one hospital database, based on both generation metrics and downstream tasks including disease diagnosis, modality discrimination, bowel preparation scoring, and lesion segmentation. Extensive experiments show ColoDiff generates videos with smooth transitions and rich dynamics. ColoDiff presents an effort in controllable colonoscopy video generation, revealing the potential of synthetic videos in complementing authentic representation and mitigating data scarcity in clinical settings.", "AI": {"tldr": "ColoDiff is a diffusion-based system for generating temporally consistent, clinically controllable colonoscopy videos, combining cross-frame tokenization for dynamics, content-aware controls for attributes, and fast non-Markovian sampling; validated on multiple datasets and downstream tasks.", "motivation": "Real colonoscopy video data are limited and heterogeneous; high-quality synthetic videos could mitigate data scarcity, but irregular anatomy, diverse pathologies, multiple modalities, and the need for temporal consistency and precise clinical control make generation hard.", "method": "Introduce ColoDiff: (1) TimeStream module decouples temporal dependencies via cross-frame tokenization to model complex inter-frame dynamics; (2) Content-Aware module uses noise-injected embeddings and learnable prototypes to precisely control clinical attributes within frames; (3) a non-Markovian sampler accelerates inference (>90% fewer steps) for near\u2013real-time generation; evaluated across public datasets and a hospital database using generation metrics and downstream tasks.", "result": "Produces videos with smooth transitions and rich dynamics; enables precise attribute control; achieves large sampling speedups (>90% step reduction); shows utility in downstream tasks such as disease diagnosis, modality discrimination, bowel prep scoring, and lesion segmentation across multiple datasets.", "conclusion": "ColoDiff advances controllable colonoscopy video synthesis, indicating synthetic videos can complement real data and help address clinical data scarcity while enabling fast, temporally coherent generation."}}
{"id": "2602.23204", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23204", "abs": "https://arxiv.org/abs/2602.23204", "authors": ["Roberto Pellerito", "Nico Messikommer", "Giovanni Cioffi", "Marco Cannici", "Davide Scaramuzza"], "title": "Motion-aware Event Suppression for Event Cameras", "comment": null, "summary": "In this work, we introduce the first framework for Motion-aware Event Suppression, which learns to filter events triggered by IMOs and ego-motion in real time. Our model jointly segments IMOs in the current event stream while predicting their future motion, enabling anticipatory suppression of dynamic events before they occur. Our lightweight architecture achieves 173 Hz inference on consumer-grade GPUs with less than 1 GB of memory usage, outperforming previous state-of-the-art methods on the challenging EVIMO benchmark by 67\\% in segmentation accuracy while operating at a 53\\% higher inference rate. Moreover, we demonstrate significant benefits for downstream applications: our method accelerates Vision Transformer inference by 83\\% via token pruning and improves event-based visual odometry accuracy, reducing Absolute Trajectory Error (ATE) by 13\\%.", "AI": {"tldr": "First real-time, motion-aware event-suppression framework for event cameras that jointly segments independently moving objects (IMOs) and predicts their motion to proactively filter dynamic events, achieving 173 Hz on consumer GPUs with <1 GB memory, +67% segmentation accuracy and +53% higher inference rate over SOTA on EVIMO, while speeding up ViT by 83% and improving VO ATE by 13%.", "motivation": "Event cameras produce vast, redundant, and motion-induced events from ego-motion and independently moving objects that can degrade perception and downstream tasks. Existing methods lack anticipatory, motion-aware filtering that runs in real time and benefits broader pipelines.", "method": "A lightweight architecture that (1) segments IMOs in the incoming event stream and (2) predicts their future motion, enabling proactive suppression of dynamic events before they occur. Trained/evaluated on EVIMO; designed for high throughput and low memory, with downstream integration for token pruning in ViTs and improved event-based VO.", "result": "Operates at 173 Hz with <1 GB memory. On EVIMO, achieves 67% higher segmentation accuracy and 53% higher inference rate than prior SOTA. Downstream: accelerates Vision Transformer inference by 83% via token pruning and reduces visual-odometry Absolute Trajectory Error by 13%.", "conclusion": "Motion-aware, anticipatory suppression of dynamic events is both feasible and practical, delivering state-of-the-art segmentation performance and speed while substantially improving efficiency and accuracy in downstream perception tasks on commodity hardware."}}
{"id": "2602.23205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23205", "abs": "https://arxiv.org/abs/2602.23205", "authors": ["Wenjia Wang", "Liang Pan", "Huaijin Pi", "Yuke Lou", "Xuqian Ren", "Yifan Wu", "Zhouyingcheng Liao", "Lei Yang", "Rishabh Dabral", "Christian Theobalt", "Taku Komura"], "title": "EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents", "comment": null, "summary": "Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research.", "AI": {"tldr": "EmbodMocap is a portable, low-cost dual\u2013iPhone RGB\u2011D pipeline that jointly calibrates two moving views to a unified metric world frame, enabling scene-consistent human motion capture in everyday environments and improving reconstruction over single-view/monocular setups while powering three embodied-AI tasks (monocular human\u2013scene reconstruction, physics-based character animation, and robot motion imitation via sim-to-real RL).", "motivation": "High-quality, scene-aware human motion data in the wild is hard to obtain because studio mocap and wearables are costly and constrained, and monocular video suffers from depth ambiguity and lacks reliable metric/scene alignment. A convenient, affordable approach is needed to capture human motion together with scene geometry at metric scale outside studios.", "method": "Use two moving iPhones capturing synchronized RGB\u2011D. Jointly calibrate both sequences into a single metric world coordinate frame to reconstruct humans and surrounding scenes with consistent scale and alignment, without static cameras or markers. Validate against optical mocap ground truth, then use the collected data to (1) fine-tune feedforward models for monocular human\u2013scene reconstruction producing metric, world-aligned outputs, (2) train physics-based character animation for human\u2013object interaction and scene-aware motion tracking, and (3) train humanoid robot control via sim-to-real RL to replicate human motions from videos.", "result": "Dual-view capture mitigates depth ambiguity and achieves better alignment and reconstruction than single\u2011iPhone or monocular baselines when compared to optical mocap ground truth. The dataset and pipeline improve performance across the three downstream tasks, demonstrating practical utility for embodied AI.", "conclusion": "A simple, affordable dual\u2011phone RGB\u2011D pipeline makes metric, scene-consistent human capture feasible in everyday settings, effectively bridging human motion and scene geometry and accelerating embodied AI research through improved reconstruction quality and successful downstream applications."}}
{"id": "2602.23212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23212", "abs": "https://arxiv.org/abs/2602.23212", "authors": ["Prottay Kumar Adhikary"], "title": "Through BrokenEyes: How Eye Disorders Impact Face Detection?", "comment": null, "summary": "Vision disorders significantly impact millions of lives, altering how visual information is processed and perceived. In this work, a computational framework was developed using the BrokenEyes system to simulate five common eye disorders: Age-related macular degeneration, cataract, glaucoma, refractive errors, and diabetic retinopathy and analyze their effects on neural-like feature representations in deep learning models. Leveraging a combination of human and non-human datasets, models trained under normal and disorder-specific conditions revealed critical disruptions in feature maps, particularly for cataract and glaucoma, which align with known neural processing challenges in these conditions. Evaluation metrics such as activation energy and cosine similarity quantified the severity of these distortions, providing insights into the interplay between degraded visual inputs and learned representations.", "AI": {"tldr": "They simulate five common ocular disorders with a \u201cBrokenEyes\u201d framework and show, via feature-map analyses in deep models, that degraded inputs\u2014especially from cataract and glaucoma\u2014cause pronounced representational distortions quantified by activation energy and cosine similarity.", "motivation": "To understand how real-world ocular degradations alter neural-like visual representations, bridging clinical vision science and computational models, and to quantify which disorders most disrupt learned features.", "method": "Develop the BrokenEyes simulation of five disorders (AMD, cataract, glaucoma, refractive errors, diabetic retinopathy); generate degraded datasets from human and non-human images; train deep models under healthy vs disorder-specific conditions; compare internal feature maps using activation energy and cosine similarity.", "result": "Feature representations are disrupted under disorder-specific inputs, with the strongest, clinically consistent distortions for cataract and glaucoma; metrics capture severity and differences across conditions.", "conclusion": "Simulating ocular pathologies provides a measurable link between input degradations and changes in learned visual features; the framework and metrics can guide analysis of robustness, aid interpretation of clinical deficits, and inform assistive or training strategies."}}
{"id": "2602.23214", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.23214", "abs": "https://arxiv.org/abs/2602.23214", "authors": ["Chenhe Du", "Xuanyu Tian", "Qing Wu", "Muyu Liu", "Jingyi Yu", "Hongjiang Wei", "Yuyao Zhang"], "title": "Plug-and-Play Diffusion Meets ADMM: Dual-Variable Coupling for Robust Medical Image Reconstruction", "comment": null, "summary": "Plug-and-Play diffusion prior (PnPDP) frameworks have emerged as a powerful paradigm for solving imaging inverse problems by treating pretrained generative models as modular priors. However, we identify a critical flaw in prevailing PnP solvers (e.g., based on HQS or Proximal Gradient): they function as memoryless operators, updating estimates solely based on instantaneous gradients. This lack of historical tracking inevitably leads to non-vanishing steady-state bias, where the reconstruction fails to strictly satisfy physical measurements under heavy corruption. To resolve this, we propose Dual-Coupled PnP Diffusion, which restores the classical dual variable to provide integral feedback, theoretically guaranteeing asymptotic convergence to the exact data manifold. However, this rigorous geometric coupling introduces a secondary challenge: the accumulated dual residuals exhibit spectrally colored, structured artifacts that violate the Additive White Gaussian Noise (AWGN) assumption of diffusion priors, causing severe hallucinations. To bridge this gap, we introduce Spectral Homogenization (SH), a frequency-domain adaptation mechanism that modulates these structured residuals into statistically compliant pseudo-AWGN inputs. This effectively aligns the solver's rigorous optimization trajectory with the denoiser's valid statistical manifold. Extensive experiments on CT and MRI reconstruction demonstrate that our approach resolves the bias-hallucination trade-off, achieving state-of-the-art fidelity with significantly accelerated convergence.", "AI": {"tldr": "Introduce dual-variable feedback and spectral homogenization into PnP diffusion to remove steady-state bias and prevent hallucinations, yielding data-consistent, SOTA CT/MRI reconstructions with faster convergence.", "motivation": "Prevailing PnP solvers (HQS/Prox-Grad) are memoryless, updating only from current gradients. This causes non-vanishing steady-state bias\u2014reconstructions fail to strictly satisfy measurement physics under heavy corruption. Moreover, diffusion priors assume AWGN, but accumulated residuals from rigorous data consistency are spectrally colored, inducing hallucinations.", "method": "Dual-Coupled PnP Diffusion reintroduces a classical dual variable to supply integral feedback, theoretically enforcing asymptotic convergence to the exact data manifold. To reconcile this with diffusion priors, Spectral Homogenization (frequency-domain modulation) transforms structured, colored residuals into pseudo-AWGN, aligning the optimization trajectory with the denoiser\u2019s statistical assumptions.", "result": "Theory: asymptotic convergence to the data manifold (eliminating steady-state bias). Practice: on CT and MRI, the approach resolves the bias\u2013hallucination trade-off, achieves state-of-the-art fidelity, and converges significantly faster than baselines.", "conclusion": "Coupling dual-variable feedback with spectral homogenization harmonizes strict data consistency and diffusion prior assumptions, delivering accurate, hallucination-resistant reconstructions and improved efficiency for challenging inverse problems."}}
{"id": "2602.23217", "categories": ["cs.CV", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.23217", "abs": "https://arxiv.org/abs/2602.23217", "authors": ["Alaa El Ichi", "Khalide Jbilou"], "title": "Multidimensional Task Learning: A Unified Tensor Framework for Computer Vision Tasks", "comment": null, "summary": "This paper introduces Multidimensional Task Learning (MTL), a unified mathematical framework based on Generalized Einstein MLPs (GE-MLPs) that operate directly on tensors via the Einstein product. We argue that current computer vision task formulations are inherently constrained by matrix-based thinking: standard architectures rely on matrix-valued weights and vectorvalued biases, requiring structural flattening that restricts the space of naturally expressible tasks. GE-MLPs lift this constraint by operating with tensor-valued parameters, enabling explicit control over which dimensions are preserved or contracted without information loss. Through rigorous mathematical derivations, we demonstrate that classification, segmentation, and detection are special cases of MTL, differing only in their dimensional configuration within a formally defined task space. We further prove that this task space is strictly larger than what matrix-based formulations can natively express, enabling principled task configurations such as spatiotemporal or cross modal predictions that require destructive flattening under conventional approaches. This work provides a mathematical foundation for understanding, comparing, and designing computer vision tasks through the lens of tensor algebra.", "AI": {"tldr": "Proposes Multidimensional Task Learning (MTL), a tensor-native framework via Generalized Einstein MLPs that treats CV tasks as dimensional configurations, subsuming classification/segmentation/detection and enabling richer spatiotemporal and cross-modal predictions without destructive flattening.", "motivation": "Matrix-centric architectures force vector/matrix parameters and flattening, which constrains what tasks can be expressed and may lose structural information. A unified, mathematically precise way to preserve and manipulate dimensions is needed to better model naturally multidimensional vision tasks.", "method": "Introduce GE-MLPs that operate directly on tensors using the Einstein product with tensor-valued weights/biases, allowing explicit control over which dimensions are preserved or contracted. Formally define a task space in this tensor setting and derive mappings showing common CV tasks as specific dimensional configurations. Provide proofs that this tensor task space strictly exceeds the expressivity of matrix-based formulations.", "result": "Mathematically shows classification, segmentation, and detection are special cases within the MTL task space; proves that the MTL space is strictly larger than matrix-native spaces, thus supporting principled configurations like spatiotemporal and cross-modal predictions that would otherwise require destructive flattening.", "conclusion": "A tensor-algebraic foundation unifies and generalizes vision task design, removing flattening-induced constraints and enabling explicit dimensional control. This offers a principled basis for understanding, comparing, and constructing multidimensional tasks; claims are primarily theoretical, setting groundwork for future empirical validation."}}
{"id": "2602.23224", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23224", "abs": "https://arxiv.org/abs/2602.23224", "authors": ["Mohammad Mahdavian", "Gordon Tan", "Binbin Xu", "Yuan Ren", "Dongfeng Bai", "Bingbing Liu"], "title": "UniScale: Unified Scale-Aware 3D Reconstruction for Multi-View Understanding via Prior Injection for Robotic Perception", "comment": null, "summary": "We present UniScale, a unified, scale-aware multi-view 3D reconstruction framework for robotic applications that flexibly integrates geometric priors through a modular, semantically informed design. In vision-based robotic navigation, the accurate extraction of environmental structure from raw image sequences is critical for downstream tasks. UniScale addresses this challenge with a single feed-forward network that jointly estimates camera intrinsics and extrinsics, scale-invariant depth and point maps, and the metric scale of a scene from multi-view images, while optionally incorporating auxiliary geometric priors when available. By combining global contextual reasoning with camera-aware feature representations, UniScale is able to recover the metric-scale of the scene. In robotic settings where camera intrinsics are known, they can be easily incorporated to improve performance, with additional gains obtained when camera poses are also available. This co-design enables robust, metric-aware 3D reconstruction within a single unified model. Importantly, UniScale does not require training from scratch, and leverages world priors exhibited in pre-existing models without geometric encoding strategies, making it particularly suitable for resource-constrained robotic teams. We evaluate UniScale on multiple benchmarks, demonstrating strong generalization and consistent performance across diverse environments. We will release our implementation upon acceptance.", "AI": {"tldr": "UniScale is a single, feed\u2011forward, multi\u2011view 3D reconstruction model that estimates intrinsics, extrinsics, scale\u2011invariant depth/point maps, and recovers metric scale, while optionally leveraging known camera priors. It generalizes well across benchmarks and can reuse existing pretrained models without retraining from scratch.", "motivation": "Robotic navigation needs accurate, metric\u2011aware scene structure from image sequences. Existing methods often lack metric scale recovery, require careful calibration, or are brittle across environments\u2014posing issues for resource\u2011constrained robotic teams.", "method": "A unified, modular, semantically informed network that performs joint estimation of camera intrinsics/extrinsics, scale\u2011invariant depth and point maps, and the metric scale of a scene from multi\u2011view inputs. It uses global contextual reasoning with camera\u2011aware feature representations, and can seamlessly incorporate auxiliary priors (known intrinsics, poses). It leverages world priors from existing models without specialized geometric encodings and avoids training from scratch.", "result": "On multiple benchmarks, UniScale shows strong generalization and consistent performance. Performance improves further when camera intrinsics (and optionally poses) are provided. The approach yields robust, metric\u2011aware 3D reconstruction within a single model.", "conclusion": "A practical, scale\u2011aware, unified 3D reconstruction framework for robotics that flexibly fuses geometric priors, recovers metric scale, and exploits pretrained models\u2014promising for deployment in varied, resource\u2011limited settings; code to be released upon acceptance."}}
{"id": "2602.23228", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23228", "abs": "https://arxiv.org/abs/2602.23228", "authors": ["Yizhi Li", "Xiaohan Chen", "Miao Jiang", "Wentao Tang", "Gaoang Wang"], "title": "MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction", "comment": "6 pages, CSCWD 2026", "summary": "With the explosive growth of digital entertainment, automated video summarization has become indispensable for applications such as content indexing, personalized recommendation, and efficient media archiving. Automatic synopsis generation for long-form videos, such as movies and TV series, presents a significant challenge for existing Vision-Language Models (VLMs). While proficient at single-image captioning, these general-purpose models often exhibit critical failures in long-duration contexts, primarily a lack of ID-consistent character identification and a fractured narrative coherence. To overcome these limitations, we propose MovieTeller, a novel framework for generating movie synopses via tool-augmented progressive abstraction. Our core contribution is a training-free, tool-augmented, fact-grounded generation process. Instead of requiring costly model fine-tuning, our framework directly leverages off-the-shelf models in a plug-and-play manner. We first invoke a specialized face recognition model as an external \"tool\" to establish Factual Groundings--precise character identities and their corresponding bounding boxes. These groundings are then injected into the prompt to steer the VLM's reasoning, ensuring the generated scene descriptions are anchored to verifiable facts. Furthermore, our progressive abstraction pipeline decomposes the summarization of a full-length movie into a multi-stage process, effectively mitigating the context length limitations of current VLMs. Experiments demonstrate that our approach yields significant improvements in factual accuracy, character consistency, and overall narrative coherence compared to end-to-end baselines.", "AI": {"tldr": "MovieTeller is a training-free, tool-augmented framework that generates coherent, character-consistent synopses for long-form videos by grounding VLM outputs with face-recognition identities and summarizing via multi-stage progressive abstraction.", "motivation": "General-purpose VLMs struggle with long-duration video summarization: they lack consistent character identification, suffer narrative incoherence, and are constrained by context length. Fine-tuning is costly; a plug-and-play solution is desirable for indexing, recommendation, and archiving.", "method": "Use an external face-recognition model to obtain factual groundings (character IDs and bounding boxes) and inject these into prompts to steer a VLM\u2019s scene descriptions. Decompose full-length movie summarization into multi-stage progressive abstraction to fit VLM context limits. No model fine-tuning; relies on off-the-shelf components in a plug-and-play manner.", "result": "Experiments report substantial gains over end-to-end baselines in factual accuracy, character ID consistency, and overall narrative coherence for movie synopsis generation.", "conclusion": "Tool-augmented, fact-grounded prompting plus progressive abstraction can overcome key VLM weaknesses for long-form video synopsis without training, improving faithfulness and coherence while remaining practical to deploy."}}
{"id": "2602.23229", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23229", "abs": "https://arxiv.org/abs/2602.23229", "authors": ["Marco Garosi", "Matteo Farina", "Alessandro Conti", "Massimiliano Mancini", "Elisa Ricci"], "title": "Large Multimodal Models as General In-Context Classifiers", "comment": "CVPR Findings 2026. Project website at https://circle-lmm.github.io/", "summary": "Which multimodal model should we use for classification? Previous studies suggest that the answer lies in CLIP-like contrastive Vision-Language Models (VLMs), due to their remarkable performance in zero-shot classification. In contrast, Large Multimodal Models (LMM) are more suitable for complex tasks. In this work, we argue that this answer overlooks an important capability of LMMs: in-context learning. We benchmark state-of-the-art LMMs on diverse datasets for closed-world classification and find that, although their zero-shot performance is lower than CLIP's, LMMs with a few in-context examples can match or even surpass contrastive VLMs with cache-based adapters, their \"in-context\" equivalent. We extend this analysis to the open-world setting, where the generative nature of LMMs makes them more suitable for the task. In this challenging scenario, LMMs struggle whenever provided with imperfect context information. To address this issue, we propose CIRCLE, a simple training-free method that assigns pseudo-labels to in-context examples, iteratively refining them with the available context itself. Through extensive experiments, we show that CIRCLE establishes a robust baseline for open-world classification, surpassing VLM counterparts and highlighting the potential of LMMs to serve as unified classifiers, and a flexible alternative to specialized models.", "AI": {"tldr": "LMMs, though weaker than CLIP-like VLMs in zero-shot, become strong classifiers with a few in-context examples; a new training-free method (CIRCLE) fixes context noise for open-world classification, letting LMMs outperform contrastive VLM baselines.", "motivation": "Prevailing wisdom says use contrastive VLMs (e.g., CLIP) for classification and reserve LMMs for complex reasoning. This overlooks LMMs\u2019 in-context learning ability and their generative advantages in open-world settings. The work seeks to reassess which multimodal models to use for classification across closed- and open-world scenarios.", "method": "1) Benchmark state-of-the-art LMMs on diverse closed-world classification datasets, comparing zero-shot and few-shot in-context performance to contrastive VLMs and cache-based adapters (their in-context analog). 2) Extend to open-world classification, analyzing failures under imperfect context. 3) Propose CIRCLE, a training-free procedure that assigns pseudo-labels to in-context examples and iteratively refines them using the available context.", "result": "In closed-world classification, few-shot in-context LMMs match or surpass contrastive VLMs with cache-based adapters despite weaker zero-shot scores. In open-world classification, vanilla LMMs degrade with noisy context, but CIRCLE yields a robust baseline that surpasses VLM counterparts.", "conclusion": "LMMs, empowered by in-context learning and CIRCLE for handling imperfect context, can act as unified, flexible classifiers across closed- and open-world tasks, offering a compelling alternative to specialized contrastive models."}}
{"id": "2602.23231", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23231", "abs": "https://arxiv.org/abs/2602.23231", "authors": ["Daniel Bermuth", "Alexander Poeppel", "Wolfgang Reif"], "title": "Skarimva: Skeleton-based Action Recognition is a Multi-view Application", "comment": null, "summary": "Human action recognition plays an important role when developing intelligent interactions between humans and machines. While there is a lot of active research on improving the machine learning algorithms for skeleton-based action recognition, not much attention has been given to the quality of the input skeleton data itself. This work demonstrates that by making use of multiple camera views to triangulate more accurate 3D~skeletons, the performance of state-of-the-art action recognition models can be improved significantly. This suggests that the quality of the input data is currently a limiting factor for the performance of these models. Based on these results, it is argued that the cost-benefit ratio of using multiple cameras is very favorable in most practical use-cases, therefore future research in skeleton-based action recognition should consider multi-view applications as the standard setup.", "AI": {"tldr": "Using multiple cameras to triangulate higher-quality 3D skeletons markedly boosts the accuracy of state-of-the-art skeleton-based action recognition models, indicating input data quality is a key bottleneck and advocating multi-view setups as a standard.", "motivation": "Most work optimizes recognition algorithms while neglecting the quality of skeleton inputs; the authors suspect imperfect single-view skeletons cap performance and that better 3D capture could unlock latent model capacity.", "method": "Triangulate 3D skeletons from synchronized multi-view cameras and feed these higher-fidelity skeletons into existing SOTA action recognition models; compare performance against standard single-view/monocular skeleton inputs and assess practical cost-benefit.", "result": "Significant accuracy gains are observed across SOTA models when supplied with multi-view triangulated skeletons, empirically showing that data quality, not just model architecture, limits current performance.", "conclusion": "Given the sizable gains and manageable overhead, multi-view capture offers a favorable cost-benefit and should become the default setup for skeleton-based action recognition; future work should prioritize multi-view data quality and pipelines."}}
{"id": "2602.23235", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23235", "abs": "https://arxiv.org/abs/2602.23235", "authors": ["Zhou Xu", "Bowen Zhou", "Qi Wang", "Shuwen Feng", "Jingyu Xiao"], "title": "Spatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents", "comment": null, "summary": "Pure-vision GUI agents provide universal interaction capabilities but suffer from severe efficiency bottlenecks due to the massive spatiotemporal redundancy inherent in high-resolution screenshots and historical trajectories. We identify two critical misalignments in existing compression paradigms: the temporal mismatch, where uniform history encoding diverges from the agent's \"fading memory\" attention pattern, and the spatial topology conflict, where unstructured pruning compromises the grid integrity required for precise coordinate grounding, inducing spatial hallucinations. To address these challenges, we introduce GUIPruner, a training-free framework tailored for high-resolution GUI navigation. It synergizes Temporal-Adaptive Resolution (TAR), which eliminates historical redundancy via decay-based resizing, and Stratified Structure-aware Pruning (SSP), which prioritizes interactive foregrounds and semantic anchors while safeguarding global layout. Extensive evaluations across diverse benchmarks demonstrate that GUIPruner consistently achieves state-of-the-art performance, effectively preventing the collapse observed in large-scale models under high compression. Notably, on Qwen2-VL-2B, our method delivers a 3.4x reduction in FLOPs and a 3.3x speedup in vision encoding latency while retaining over 94% of the original performance, enabling real-time, high-precision navigation with minimal resource consumption.", "AI": {"tldr": "GUIPruner is a training-free compression framework for pure-vision GUI agents that cuts spatiotemporal redundancy while preserving coordinate fidelity, yielding large speedups with minimal accuracy loss.", "motivation": "Pure-vision GUI agents are slow because high-res screenshots and long histories contain massive redundant information. Existing pruning compresses history uniformly (misaligned with agents\u2019 fading-memory attention) and removes pixels/patches without respecting screen-grid topology, harming coordinate grounding and causing spatial hallucinations.", "method": "Two components: (1) Temporal-Adaptive Resolution (TAR) downsamples past frames progressively via decay-based resizing to match the agent\u2019s diminishing need for old context; (2) Stratified Structure-aware Pruning (SSP) prioritizes interactive foreground elements and semantic anchors while preserving global layout/grid integrity, avoiding unstructured pruning that breaks spatial topology. The framework is training-free and slots into vision encoders for high-res GUI navigation.", "result": "Across multiple benchmarks, GUIPruner achieves state-of-the-art under compression, prevents model collapse at high pruning rates, and on Qwen2-VL-2B yields ~3.4x FLOPs reduction and 3.3x vision-encoder latency speedup while retaining >94% of original performance, enabling near real-time operation.", "conclusion": "Aligning temporal compression with fading memory and preserving screen topology enables efficient, accurate GUI navigation without retraining. GUIPruner materially reduces compute while maintaining precision, making lightweight, real-time pure-vision agents practical."}}
{"id": "2602.23259", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23259", "abs": "https://arxiv.org/abs/2602.23259", "authors": ["Jiangxin Sun", "Feng Xue", "Teng Long", "Chang Liu", "Jian-Fang Hu", "Wei-Shi Zheng", "Nicu Sebe"], "title": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving", "comment": null, "summary": "With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of \"only driving like the expert\" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.", "AI": {"tldr": "RaWMPC replaces expert-supervised imitation with a risk-aware, model-based MPC that predicts outcomes of candidate actions and selects the lowest-risk ones, achieving SOTA performance in- and out-of-distribution while improving interpretability.", "motivation": "Imitation learning for end-to-end driving overfits to expert behaviors and fails on rare, long-tail scenarios where no expert demonstrations exist. The goal is to enable reliable decision-making without relying on expert action supervision.", "method": "Risk-aware World Model Predictive Control (RaWMPC): (1) learn a world model to forecast consequences for multiple candidate actions; (2) perform explicit risk evaluation over predicted rollouts and choose low-risk actions; (3) train the world model with a risk-aware interaction strategy that exposes it to hazardous behaviors so catastrophic outcomes become predictable; (4) at test time, use self-evaluation distillation to transfer the world model\u2019s risk-avoidance knowledge into a generative action proposal network, all without expert demonstrations.", "result": "Extensive experiments show RaWMPC outperforms state-of-the-art methods on both in-distribution and out-of-distribution scenarios and yields better decision interpretability.", "conclusion": "Reliable E2E autonomous driving can be achieved without expert action supervision by combining risk-aware world models with MPC and distillation, improving robustness to long-tail cases and interpretability."}}
{"id": "2602.23262", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.23262", "abs": "https://arxiv.org/abs/2602.23262", "authors": ["Jasmine Bayrooti", "Weiwei Kong", "Natalia Ponomareva", "Carlos Esteves", "Ameesh Makadia", "Amanda Prorok"], "title": "Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling", "comment": null, "summary": "Generative models trained on sensitive image datasets risk memorizing and reproducing individual training examples, making strong privacy guarantees essential. While differential privacy (DP) provides a principled framework for such guarantees, standard DP finetuning (e.g., with DP-SGD) often results in severe degradation of image quality, particularly in high-frequency textures, due to the indiscriminate addition of noise across all model parameters. In this work, we propose a spectral DP framework based on the hypothesis that the most privacy-sensitive portions of an image are often low-frequency components in the wavelet space (e.g., facial features and object shapes) while high-frequency components are largely generic and public. Based on this hypothesis, we propose the following two-stage framework for DP image generation with coarse image intermediaries: (1) DP finetune an autoregressive spectral image tokenizer model on the low-resolution wavelet coefficients of the sensitive images, and (2) perform high-resolution upsampling using a publicly pretrained super-resolution model. By restricting the privacy budget to the global structures of the image in the first stage, and leveraging the post-processing property of DP for detail refinement, we achieve promising trade-offs between privacy and utility. Experiments on the MS-COCO and MM-CelebA-HQ datasets show that our method generates images with improved quality and style capture relative to other leading DP image frameworks.", "AI": {"tldr": "A two-stage spectral DP approach for image generation: apply differential privacy only to low-frequency wavelet coefficients via a DP-finetuned autoregressive tokenizer, then use a public super-resolution model to restore high-frequency details\u2014yielding better privacy\u2013utility trade-offs and image quality than prior DP methods.", "motivation": "Standard DP fine-tuning (e.g., DP-SGD) injects uniform noise across all parameters, severely degrading image fidelity\u2014especially textures\u2014while sensitive identity/structure often resides in low-frequency components. The authors hypothesize high-frequency details are largely generic/public, motivating a frequency-selective privacy strategy.", "method": "1) Convert images to wavelet domain and retain low-resolution/low-frequency coefficients. Train/finetune an autoregressive spectral tokenizer under DP on sensitive data. 2) Upsample/reconstruct high-frequency details using a publicly pretrained super-resolution model, leveraging DP\u2019s post-processing property so this step consumes no privacy budget.", "result": "On MS-COCO and MM-CelebA-HQ, the approach produces higher-quality, more stylistically faithful images than leading DP image-generation frameworks (qualitative and metric improvements implied; specifics not given in the abstract).", "conclusion": "Concentrating the privacy budget on global, low-frequency structure and outsourcing fine detail to a public super-resolution model can materially mitigate DP\u2019s quality penalty in image generation, offering a promising path for practical DP generative modeling."}}
{"id": "2602.23290", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23290", "abs": "https://arxiv.org/abs/2602.23290", "authors": ["Zhengyang Wei", "Renzhi Jing", "Yiyi He", "Jenny Suckale"], "title": "LineGraph2Road: Structural Graph Reasoning on Line Graphs for Road Network Extraction", "comment": null, "summary": "The accurate and automatic extraction of roads from satellite imagery is critical for applications in navigation and urban planning, significantly reducing the need for manual annotation. Many existing methods decompose this task into keypoint extraction and connectedness prediction, but often struggle to capture long-range dependencies and complex topologies. Here, we propose LineGraph2Road, a framework that improves connectedness prediction by formulating it as binary classification over edges in a constructed global but sparse Euclidean graph, where nodes are keypoints extracted from segmentation masks and edges connect node pairs within a predefined distance threshold, representing potential road segments. To better learn structural link representation, we transform the original graph into its corresponding line graph and apply a Graph Transformer on it for connectedness prediction. This formulation overcomes the limitations of endpoint-embedding fusion on set-isomorphic links, enabling rich link representations and effective relational reasoning over the global structure. Additionally, we introduce an overpass/underpass head to resolve multi-level crossings and a coupled NMS strategy to preserve critical connections. We evaluate LineGraph2Road on three benchmarks: City-scale, SpaceNet, and Global-scale, and show that it achieves state-of-the-art results on two key metrics, TOPO-F1 and APLS. It also captures fine visual details critical for real-world deployment. We will make our code publicly available.", "AI": {"tldr": "LineGraph2Road reframes road-network extraction as edge-level connectivity prediction on a global sparse graph, then reasons over its line graph with a Graph Transformer to capture long-range structure, achieving SOTA topology metrics on multiple benchmarks.", "motivation": "Automatic road extraction from satellite imagery is vital for navigation and urban planning but existing keypoint+connectivity pipelines struggle with long-range dependencies, complex topologies (e.g., multi-level crossings), and limited link representations.", "method": "1) Generate keypoints from segmentation masks. 2) Build a sparse Euclidean graph by connecting keypoint pairs within a distance threshold. 3) Formulate connectivity as binary classification on edges. 4) Convert this graph to its line graph and apply a Graph Transformer to learn rich link representations and perform global relational reasoning. 5) Add an overpass/underpass head to disambiguate multi-level crossings. 6) Use a coupled NMS to retain critical connections.", "result": "On City-scale, SpaceNet, and Global-scale datasets, the method attains state-of-the-art performance on TOPO-F1 and APLS and recovers fine visual/topological details important for deployment.", "conclusion": "Transforming edge connectivity into line-graph reasoning with a Graph Transformer overcomes endpoint-embedding limitations, better models global structure and complex crossings, and delivers SOTA topology accuracy; code will be released."}}
{"id": "2602.23292", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23292", "abs": "https://arxiv.org/abs/2602.23292", "authors": ["Fuqiang Chen", "Ranran Zhang", "Wanming Hu", "Deboch Eyob Abera", "Yue Peng", "Boyun Zheng", "Yiwen Sun", "Jing Cai", "Wenjian Qin"], "title": "PGVMS: A Prompt-Guided Unified Framework for Virtual Multiplex IHC Staining with Pathological Semantic Learning", "comment": "Accepted by TMI", "summary": "Immunohistochemical (IHC) staining enables precise molecular profiling of protein expression, with over 200 clinically available antibody-based tests in modern pathology. However, comprehensive IHC analysis is frequently limited by insufficient tissue quantities in small biopsies. Therefore, virtual multiplex staining emerges as an innovative solution to digitally transform H&E images into multiple IHC representations, yet current methods still face three critical challenges: (1) inadequate semantic guidance for multi-staining, (2) inconsistent distribution of immunochemistry staining, and (3) spatial misalignment across different stain modalities. To overcome these limitations, we present a prompt-guided framework for virtual multiplex IHC staining using only uniplex training data (PGVMS). Our framework introduces three key innovations corresponding to each challenge: First, an adaptive prompt guidance mechanism employing a pathological visual language model dynamically adjusts staining prompts to resolve semantic guidance limitations (Challenge 1). Second, our protein-aware learning strategy (PALS) maintains precise protein expression patterns by direct quantification and constraint of protein distributions (Challenge 2). Third, the prototype-consistent learning strategy (PCLS) establishes cross-image semantic interaction to correct spatial misalignments (Challenge 3).", "AI": {"tldr": "PGVMS is a prompt-guided framework that virtually generates multiplex IHC stains from H&E using only uniplex training data, combining adaptive VLM prompts, protein-aware constraints, and prototype-consistent learning to tackle semantic, distributional, and spatial misalignment issues.", "motivation": "IHC enables rich protein profiling but is limited by scarce tissue in small biopsies. Virtual multiplex staining can expand analyses from H&E, yet existing methods lack robust semantic guidance, produce inconsistent protein distributions, and suffer cross-stain spatial misalignment.", "method": "PGVMS introduces: (1) adaptive prompt guidance via a pathology-focused visual language model to refine stain-specific semantics; (2) Protein-Aware Learning Strategy (PALS) to directly quantify and constrain protein expression distributions; and (3) Prototype-Consistent Learning Strategy (PCLS) to enforce cross-image semantic interactions that correct spatial misalignments. Training uses only uniplex data while enabling multi-stain synthesis.", "result": "The abstract emphasizes methodological innovations; specific quantitative performance metrics or benchmarks are not provided.", "conclusion": "By uniting VLM-driven prompts with protein-aware and prototype-consistent objectives, PGVMS aims to deliver semantically accurate, distribution-consistent, and spatially aligned virtual multiplex IHC from H&E, potentially enabling comprehensive molecular profiling when tissue is limited."}}
{"id": "2602.23294", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23294", "abs": "https://arxiv.org/abs/2602.23294", "authors": ["Xin Gu", "Bing Fan", "Jiali Yao", "Zhipeng Zhang", "Yan Huang", "Cheng Han", "Heng Fan", "Libo Zhang"], "title": "Towards Long-Form Spatio-Temporal Video Grounding", "comment": null, "summary": "In real scenarios, videos can span several minutes or even hours. However, existing research on spatio-temporal video grounding (STVG), given a textual query, mainly focuses on localizing targets in short videos of tens of seconds, typically less than one minute, which limits real-world applications. In this paper, we explore Long-Form STVG (LF-STVG), which aims to locate targets in long-term videos. Compared with short videos, long-term videos contain much longer temporal spans and more irrelevant information, making it difficult for existing STVG methods that process all frames at once. To address this challenge, we propose an AutoRegressive Transformer architecture for LF-STVG, termed ART-STVG. Unlike conventional STVG methods that require the entire video sequence to make predictions at once, ART-STVG treats the video as streaming input and processes frames sequentially, enabling efficient handling of long videos. To model spatio-temporal context, we design spatial and temporal memory banks and apply them to the decoders. Since memories from different moments are not always relevant to the current frame, we introduce simple yet effective memory selection strategies to provide more relevant information to the decoders, significantly improving performance. Furthermore, instead of parallel spatial and temporal localization, we propose a cascaded spatio-temporal design that connects the spatial decoder to the temporal decoder, allowing fine-grained spatial cues to assist complex temporal localization in long videos. Experiments on newly extended LF-STVG datasets show that ART-STVG significantly outperforms state-of-the-art methods, while achieving competitive performance on conventional short-form STVG.", "AI": {"tldr": "ART-STVG is an autoregressive transformer for long-form spatio-temporal video grounding that processes videos as streams, uses spatial/temporal memory banks with relevance-based selection, and cascades spatial-to-temporal decoding, achieving SOTA on new long-form datasets and competitive short-form results.", "motivation": "Real-world videos are minutes to hours long, but most STVG methods only work on short clips and process all frames at once, making them inefficient and easily distracted by irrelevant content in long videos.", "method": "Treat the video as a streaming sequence and process frames autoregressively. Introduce spatial and temporal memory banks inside decoders to carry historical context, equip them with simple relevance-based memory selection to filter out unhelpful states, and adopt a cascaded design where spatial localization feeds fine-grained cues into temporal localization for long-horizon reasoning.", "result": "On newly extended long-form STVG benchmarks, ART-STVG significantly outperforms prior state of the art, and it remains competitive on conventional short-form STVG, while enabling efficient handling of long videos.", "conclusion": "Sequential (streaming) processing with curated memory and a cascaded spatial-to-temporal pipeline is a practical and superior solution for long-form STVG, improving accuracy on long videos without sacrificing performance on short ones."}}
{"id": "2602.23295", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23295", "abs": "https://arxiv.org/abs/2602.23295", "authors": ["Ayush Roy", "Wei-Yang Alex Lee", "Rudrasis Chakraborty", "Vishnu Suresh Lokhande"], "title": "ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation", "comment": "CVPE 2026", "summary": "In recent times, large datasets hinder efficient model training while also containing redundant concepts. Dataset distillation aims to synthesize compact datasets that preserve the knowledge of large-scale training sets while drastically reducing storage and computation. Recent advances in diffusion models have enabled training-free distillation by leveraging pre-trained generative priors; however, existing guidance strategies remain limited. Current score-based methods either perform unguided denoising or rely on simple mode-based guidance toward instance prototype centroids (IPC centroids), which often are rudimentary and suboptimal. We propose Manifold-Guided Distillation (ManifoldGD), a training-free diffusion-based framework that integrates manifold consistent guidance at every denoising timestep. Our method employs IPCs computed via a hierarchical, divisive clustering of VAE latent features, yielding a multi-scale coreset of IPCs that captures both coarse semantic modes and fine intra-class variability. Using a local neighborhood of the extracted IPC centroids, we create the latent manifold for each diffusion denoising timestep. At each denoising step, we project the mode-alignment vector onto the local tangent space of the estimated latent manifold, thus constraining the generation trajectory to remain manifold-faithful while preserving semantic consistency. This formulation improves representativeness, diversity, and image fidelity without requiring any model retraining. Empirical results demonstrate consistent gains over existing training-free and training-based baselines in terms of FID, l2 distance among real and synthetic dataset embeddings, and classification accuracy, establishing ManifoldGD as the first geometry-aware training-free data distillation framework.", "AI": {"tldr": "ManifoldGD is a training-free, diffusion-based dataset distillation method that guides denoising along a geometry-aware latent manifold built from multi-scale instance prototype centroids (IPCs), yielding more representative, diverse, and faithful synthetic data and outperforming prior training-free and training-based baselines on FID, embedding distance, and downstream accuracy.", "motivation": "Large datasets are redundant and expensive to train on. Training-free diffusion-based distillation exists but current guidance is weak\u2014either unguided or using crude, single-mode IPC centroid guidance\u2014failing to capture class diversity and latent geometry. The paper seeks a principled, geometry-aware guidance to improve fidelity and diversity without retraining models.", "method": "Compute IPCs via hierarchical divisive clustering in a VAE latent space to obtain a multi-scale coreset capturing both coarse class modes and fine intra-class variability. At each diffusion denoising timestep, form a local manifold from the neighborhood of relevant IPC centroids; compute a mode-alignment guidance vector and project it onto the local tangent space of this manifold to keep the denoising trajectory manifold-consistent while preserving semantic alignment. Entire pipeline is training-free and plugs into pre-trained diffusion priors.", "result": "Across benchmarks, ManifoldGD achieves consistent improvements over training-free and training-based distillation methods on FID, L2 distance between real and synthetic embeddings, and classification accuracy, with qualitative gains in representativeness, diversity, and image fidelity.", "conclusion": "Geometry-aware, manifold-consistent guidance enables more effective training-free dataset distillation. ManifoldGD establishes a new paradigm for diffusion-guided distillation that preserves semantics and diversity without retraining, serving as the first geometry-aware training-free framework in this space."}}
{"id": "2602.23297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23297", "abs": "https://arxiv.org/abs/2602.23297", "authors": ["Yiqing Wang", "Chunming He", "Ming-Chen Lu", "Mercy Pawar", "Leslie Niziol", "Maria Woodward", "Sina Farsiu"], "title": "PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM", "comment": null, "summary": "Medical diagnosis requires the effective synthesis of visual manifestations and clinical metadata. However, existing methods often treat metadata as isolated tags, failing to exploit the rich semantic knowledge embedded in clinical descriptions. We propose PRIMA (Pre-training with Risk-integrated Image-Metadata Alignment), a framework that integrates domain-specific knowledge into multi-modal representation learning. We first curate an expert corpus of risk-disease correlations via Retrieval-Augmented Generation (RAG) to refine Clinical ModernBERT, embedding diagnostic priors into the text encoder. To bridge the modality gap, we introduce a dual-encoder pre-training strategy utilizing DINOv3 and our refined BERT, optimized by a suite of four complementary loss functions. These losses are designed to capture multi-granular semantic alignment and handle the ambiguity of clinical correlations through soft labels. Finally, we leverage Qwen-3 to fuse these aligned features for precise disease classification. Extensive experiments demonstrate that PRIMA effectively harmonizes pixel-level features with abstract clinical expertise, significantly outperforming other state-of-the-art methods. Notably, our framework achieves superior robustness without the need for massive data collection or exhaustive computational resources. Our code will be made public upon acceptance.", "AI": {"tldr": "PRIMA is a multimodal medical diagnosis framework that injects curated clinical risk\u2013disease knowledge into a BERT text encoder, aligns it with DINOv3 image features via multi-granular, soft-label losses, and fuses them with Qwen-3 for classification, claiming SOTA accuracy and robustness without massive data/compute.", "motivation": "Current medical vision\u2013language systems underuse clinical metadata by treating it as flat tags, missing rich semantic priors and nuanced risk\u2013disease relationships needed to synthesize imaging with clinical context efficiently and robustly.", "method": "(1) Build an expert corpus of risk\u2013disease correlations via Retrieval-Augmented Generation and use it to refine Clinical ModernBERT, embedding diagnostic priors. (2) Pretrain a dual encoder with DINOv3 (vision) and the refined BERT (text), optimized by four complementary losses that target multi-granular semantic alignment and ambiguity through soft labels. (3) Fuse aligned features using Qwen-3 for downstream disease classification.", "result": "Extensive experiments reportedly show consistent, significant gains over SOTA baselines and superior robustness, achieved without massive datasets or heavy compute.", "conclusion": "Encoding domain knowledge into the text encoder and enforcing fine-grained cross-modal alignment yields more faithful image\u2013metadata synergy and robust clinical classification; the approach is efficient and will be released upon acceptance."}}
{"id": "2602.23306", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23306", "abs": "https://arxiv.org/abs/2602.23306", "authors": ["Yiran Guan", "Sifan Tu", "Dingkang Liang", "Linghao Zhu", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan", "Yuliang Liu", "Xiang Bai"], "title": "ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding", "comment": "Accept by ICLR 2026", "summary": "Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (LRM). However, enhancing the reasoning ability of OLLMs through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computational costs. To address these limitations, we propose ThinkOmni, a training-free and data-free framework that lifts textual reasoning to omni-modal scenarios. ThinkOmni introduces two key components: 1) LRM-as-a-Guide, which leverages off-the-shelf LRMs to guide the OLLM decoding process; 2) Stepwise Contrastive Scaling, which adaptively balances perception and reasoning signals without manual hyperparameter tuning. Experiments on six multi-modal reasoning benchmarks demonstrate that ThinkOmni consistently delivers performance improvements, with main results achieving 70.2 on MathVista and 75.5 on MMAU. Overall, ThinkOmni offers a flexible and generalizable solution for omni-modal reasoning and provides new insights into the generalization and application of reasoning capabilities.", "AI": {"tldr": "ThinkOmni is a training-free, data-free framework that plugs a large reasoning model (LRM) into an omni-modal LLM\u2019s decoding and adaptively balances perception and reasoning signals, yielding consistent gains on six multimodal reasoning benchmarks (e.g., 70.2 on MathVista, 75.5 on MMAU).", "motivation": "Omni-modal LLMs perceive diverse modalities but lag in complex stepwise reasoning. Improving them via additional training is costly, data-hungry, and often task-specific, motivating a plug-and-play alternative that transfers the reasoning strengths of text LRMs to multimodal settings without extra data or finetuning.", "method": "Two components: (1) LRM-as-a-Guide\u2014use an off-the-shelf text-only large reasoning model to guide the OLLM\u2019s token-level decoding, lifting chain-of-thought/style reasoning into multimodal answers; (2) Stepwise Contrastive Scaling\u2014adaptively balance/contrast perception logits from the OLLM with reasoning guidance from the LRM at each decoding step, avoiding manual hyperparameter tuning.", "result": "Across six multimodal reasoning benchmarks, ThinkOmni consistently improves OLLM performance; headline results include 70.2 on MathVista and 75.5 on MMAU.", "conclusion": "A training- and data-free integration of LRM guidance with adaptive stepwise scaling offers a flexible, generalizable path to enhance omni-modal reasoning, transferring textual reasoning capabilities to multimodal tasks and yielding practical performance gains."}}
{"id": "2602.23339", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23339", "abs": "https://arxiv.org/abs/2602.23339", "authors": ["Tilemachos Aravanis", "Vladan Stojni\u0107", "Bill Psomas", "Nikos Komodakis", "Giorgos Tolias"], "title": "Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?", "comment": null, "summary": "Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.", "AI": {"tldr": "They introduce a few-shot, retrieval-augmented test-time adapter for open-vocabulary segmentation that learns a lightweight per-image classifier by jointly fusing textual prompts with pixel-annotated visual supports via learned, per-query fusion, substantially reducing the gap to fully supervised segmentation while keeping open-vocabulary flexibility.", "motivation": "Open-vocabulary segmentation underperforms supervised methods because VLMs are trained with coarse image-level labels and because natural-language prompts can be semantically ambiguous. Adding minimal pixel-level supervision at test time can resolve ambiguity and provide finer guidance without giving up open-vocabulary generality.", "method": "Adopt a few-shot setting that augments text prompts with a support set of pixel-annotated images. At test time, a retrieval-augmented adapter learns a lightweight, per-image classifier by fusing textual and visual support features using learned, per-query fusion (rather than late, hand-crafted fusion). The approach supports continual expansion of the support set and targets fine-grained/personalized segmentation.", "result": "Experiments show the method significantly narrows the performance gap between zero-shot and fully supervised segmentation, outperforms prior late-fusion baselines, and preserves open-vocabulary capabilities, including on fine-grained/personalized tasks.", "conclusion": "Few-shot pixel-level support combined with learned, per-query multimodal fusion at test time is an effective and scalable way to boost open-vocabulary segmentation, maintaining flexibility while approaching supervised performance and enabling continual support growth and personalization."}}
{"id": "2602.23359", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23359", "abs": "https://arxiv.org/abs/2602.23359", "authors": ["Vaibhav Agrawal", "Rishubh Parihar", "Pradhaan Bhat", "Ravi Kiran Sarvadevabhatla", "R. Venkatesh Babu"], "title": "SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation", "comment": "Project page: https://seethrough3d.github.io. Accepted at CVPR 2026", "summary": "We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they often fail to model precise inter-object occlusions. We propose SeeThrough3D, a model for 3D layout conditioned generation that explicitly models occlusions. We introduce an occlusion-aware 3D scene representation (OSCR), where objects are depicted as translucent 3D boxes placed within a virtual environment and rendered from desired camera viewpoint. The transparency encodes hidden object regions, enabling the model to reason about occlusions, while the rendered viewpoint provides explicit camera control during generation. We condition a pretrained flow based text-to-image image generation model by introducing a set of visual tokens derived from our rendered 3D representation. Furthermore, we apply masked self-attention to accurately bind each object bounding box to its corresponding textual description, enabling accurate generation of multiple objects without object attribute mixing. To train the model, we construct a synthetic dataset with diverse multi-object scenes with strong inter-object occlusions. SeeThrough3D generalizes effectively to unseen object categories and enables precise 3D layout control with realistic occlusions and consistent camera control.", "AI": {"tldr": "SeeThrough3D introduces an occlusion-aware 3D representation and conditioning scheme to generate images that strictly follow 3D layouts while producing realistic, depth-consistent occlusions and controllable camera viewpoints.", "motivation": "3D layout-conditioned generation often ignores precise inter-object occlusions, leading to incorrect depth, scale, and object interactions. The paper targets explicit occlusion reasoning to achieve geometrically consistent multi-object synthesis.", "method": "Propose OSCR: objects as translucent 3D boxes rendered from a specified camera; transparency encodes hidden regions for occlusion reasoning. Derive visual tokens from the render to condition a pretrained flow-based text-to-image model. Use masked self-attention to bind each 3D box to its textual description, preventing attribute mixing. Train on a synthetic dataset with diverse multi-object scenes exhibiting strong occlusions.", "result": "The model produces images with realistic inter-object occlusions, accurate multi-object binding without attribute mixing, precise 3D layout adherence, and explicit, consistent camera control. It generalizes to unseen object categories (per abstract, primarily qualitative claims).", "conclusion": "Explicitly modeling occlusion via OSCR and token-based conditioning enables faithful 3D layout control with realistic occlusions and robust generalization, addressing a key gap in prior layout-conditioned generation systems."}}
{"id": "2602.23357", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23357", "abs": "https://arxiv.org/abs/2602.23357", "authors": ["Aheli Saha", "Ren\u00e9 Schuster", "Didier Stricker"], "title": "Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training", "comment": "12 pages, International Conference on Pattern Recognition Applications and Methods", "summary": "Bio-inspired event cameras have recently attracted significant research due to their asynchronous and low-latency capabilities. These features provide a high dynamic range and significantly reduce motion blur. However, because of the novelty in the nature of their output signals, there is a gap in the variability of available data and a lack of extensive analysis of the parameters characterizing their signals. This paper addresses these issues by providing readers with an in-depth understanding of how intrinsic parameters affect the performance of a model trained on event data, specifically for object detection. We also use our findings to expand the capabilities of the downstream model towards sensor-agnostic robustness.", "AI": {"tldr": "They systematically study how intrinsic event\u2011camera parameters influence object\u2011detection performance and use the insights to make models more robust across different sensors.", "motivation": "Event cameras offer high dynamic range, low latency, and minimal motion blur, but there\u2019s limited data diversity and little understanding of how sensor parameters shape the resulting event signals and downstream model performance. This hinders generalization and cross\u2011sensor deployment.", "method": "Conduct an in\u2011depth analysis of intrinsic event\u2011camera parameters by training and evaluating an object\u2011detection model on event data while varying these parameters; leverage the empirical findings to adjust training/processing so the detector becomes sensor\u2011agnostic (e.g., via parameter\u2011aware design or robustness strategies).", "result": "Identify which intrinsic parameters most affect detection and show that incorporating the derived insights improves robustness when models face data from different event sensors (i.e., better cross\u2011sensor performance).", "conclusion": "Understanding and accounting for intrinsic event\u2011camera parameters is key to reliable object detection; the paper provides analysis and practical guidance that extends detectors toward sensor\u2011agnostic robustness."}}
{"id": "2602.23361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23361", "abs": "https://arxiv.org/abs/2602.23361", "authors": ["Sven Elflein", "Ruilong Li", "S\u00e9rgio Agostinho", "Zan Gojcic", "Laura Leal-Taix\u00e9", "Qunjie Zhou", "Aljosa Osep"], "title": "VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale", "comment": "CVPR 2026, Project page: https://research.nvidia.com/labs/dvl/projects/vgg-ttt", "summary": "We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T$^3$ (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a $1k$ image collection in just $54$ seconds, achieving a $11.6\\times$ speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images.", "AI": {"tldr": "VGG-T^3 compresses the variable-length attention KV memory used in multi-view 3D reconstruction into a fixed-size MLP via test-time training, yielding linear scaling with input images, fast reconstruction (1k images in 54 s), and better accuracy than other linear-time methods, while enabling visual localization from unseen views.", "motivation": "Offline feed-forward 3D reconstruction methods that aggregate information across many views often rely on softmax attention over a growing set of keys/values, causing compute and memory to scale quadratically with the number of input images. This makes large-scale scene reconstruction impractical. The goal is to retain global scene aggregation while achieving linear-time scaling comparable to online methods.", "method": "Introduce Visual Geometry Grounded Test-Time Training (VGG-T^3): during inference, distill the scene\u2019s variable-length Key-Value (KV) representation into a fixed-size MLP that serves as a compact, global scene memory. Queries from images are answered by this MLP instead of a large attention map, preserving global context but capping memory/compute. The procedure performs test-time optimization to fit the MLP to the scene\u2019s KV space, then uses it for fast feed-forward reconstruction and for querying with novel images.", "result": "- Linear scaling with number of input views.\n- Reconstructs a 1k-image collection in 54 seconds, achieving 11.6\u00d7 speed-up over softmax-attention baselines.\n- Lower point-map reconstruction error than other linear-time methods by a large margin.\n- Supports visual localization by querying the learned scene representation with unseen images.", "conclusion": "Distilling variable-length attention memory into a fixed-size MLP at test time provides an effective, scalable scene representation for 3D reconstruction: it preserves global aggregation, delivers substantial speed-ups with linear scaling, improves accuracy over other linear-time approaches, and extends naturally to visual localization."}}
{"id": "2602.23363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23363", "abs": "https://arxiv.org/abs/2602.23363", "authors": ["Sahal Shaji Mullappilly", "Mohammed Irfan Kurpath", "Omair Mohamed", "Mohamed Zidan", "Fahad Khan", "Salman Khan", "Rao Anwer", "Hisham Cholakkal"], "title": "MediX-R1: Open Ended Medical Reinforcement Learning", "comment": null, "summary": "We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com", "AI": {"tldr": "MediX-R1 uses open-ended reinforcement learning with composite rewards and an LLM-as-judge evaluation to train medical multimodal LLMs that generate clinically grounded free-form answers, achieving strong gains over open-source baselines on both text-only and image+text tasks with ~51K instructions.", "motivation": "Medical QA often needs free-form, clinically grounded reasoning rather than multiple-choice. Existing RL signals and string-overlap metrics are brittle for open-ended outputs, especially across text and vision modalities. There is a need for stable training signals and a unified, semantically aware evaluation for medical MLLMs.", "method": "Fine-tune a vision-language backbone via Group Based RL with a composite reward: (1) LLM-based accuracy reward that outputs a strict YES/NO on semantic correctness; (2) medical embedding\u2013based semantic reward capturing paraphrase/terminology variants; (3) lightweight format and modality rewards to enforce interpretable reasoning and correct modality handling. Propose a unified evaluation using a reference-based LLM-as-judge for text-only and image+text tasks.", "result": "With ~51K instruction examples, MediX-R1 outperforms strong open-source baselines across standard medical LLM and VLM benchmarks, showing especially large improvements on open-ended clinical tasks. The multi-signal reward yields stable, informative feedback for free-form outputs.", "conclusion": "Comprehensive, multi-signal open-ended RL combined with LLM-based evaluation is an effective, practical route to improving reliability and clinical reasoning quality in multimodal medical models; code, models, and datasets are released."}}
