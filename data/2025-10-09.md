<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 88]
- [cs.CL](#cs.CL) [Total: 105]
- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Milestone Determination for Autonomous Railway Operation](https://arxiv.org/abs/2510.06229)
*Josh Hunter,John McDermid,Simon Burton,Poppy Fynes,Mia Dempster*

Main category: cs.CV

TL;DR: Proposes a route-specific, milestone-based framework to build sequential, context-rich datasets and targeted rule-based vision models for railway automation, simplifying perception at critical decision points to improve safety and efficiency.


<details>
  <summary>Details</summary>
Motivation: Railway vision systems lack high-quality, spatiotemporal, sequential data. Existing datasets miss real-time context, and synthetic/alternative sources suffer from realism and applicability gaps, hindering reliable automation.

Method: Focus on route-specific, contextually relevant cues to generate rich sequential datasets aligned with operational logic. Introduce milestone determination to identify critical decision points and train targeted, rule-based models that avoid broad, generalized recognition of dynamic components.

Result: Conceptual demonstration/argument that milestone-centric, route-aware datasets and rule-based models reduce learning complexity, better reflect operational realities, and are poised to enable safer, more efficient decision-making compared to generic approaches (no quantitative results reported).

Conclusion: A practical framework for training vision agents in controlled, predictable railway environments: prioritize milestone determination along routes to reduce perception scope, enhance realism, and facilitate safer, more efficient machine learning—pending empirical validation.

Abstract: In the field of railway automation, one of the key challenges has been the
development of effective computer vision systems due to the limited
availability of high-quality, sequential data. Traditional datasets are
restricted in scope, lacking the spatio temporal context necessary for
real-time decision-making, while alternative solutions introduce issues related
to realism and applicability. By focusing on route-specific, contextually
relevant cues, we can generate rich, sequential datasets that align more
closely with real-world operational logic. The concept of milestone
determination allows for the development of targeted, rule-based models that
simplify the learning process by eliminating the need for generalized
recognition of dynamic components, focusing instead on the critical decision
points along a route. We argue that this approach provides a practical
framework for training vision agents in controlled, predictable environments,
facilitating safer and more efficient machine learning systems for railway
automation.

</details>


### [2] [CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation](https://arxiv.org/abs/2510.06231)
*Mingzhe Zheng,Dingjie Song,Guanyu Zhou,Jun You,Jiahao Zhan,Xuran Ma,Xinyuan Song,Ser-Nam Lim,Qifeng Chen,Harry Yang*

Main category: cs.CV

TL;DR: They build a domain-specific dataset and benchmark (CML-Dataset/CML-Bench) to evaluate LLM-written movie scripts on dialogue coherence, character consistency, and plot reasonableness, and propose an instruction strategy (CML-Instruction) that measurably improves script quality and aligns with human preferences.


<details>
  <summary>Details</summary>
Motivation: LLMs can produce structurally formatted scripts but lack nuanced storytelling—the continuity, consistent characterization, and logical plot progression needed for compelling cinema. The field lacks targeted metrics and guidance to both assess and improve these aspects.

Method: 1) Curate CML-Dataset of (summary, content) pairs in Cinematic Markup Language from high-quality scripts. 2) Analyze multi-shot continuity and narrative structure to define three evaluation dimensions: Dialogue Coherence (DC), Character Consistency (CC), Plot Reasonableness (PR). 3) Propose CML-Bench with quantitative metrics for these dimensions. 4) Introduce CML-Instruction, a detailed prompting strategy focusing on character dialogue and event logic to guide LLM generation. 5) Conduct experiments comparing human scripts, baseline LLM outputs, and LLMs guided by CML-Instruction, with human preference checks.

Result: CML-Bench assigns high scores to human-written scripts and highlights weaknesses in LLM-generated ones. Using CML-Instruction, LLMs produce more structured, cinematically sound scripts, with improvements reflected in benchmark metrics and aligning with human judgments.

Conclusion: A focused benchmark (CML-Bench) effectively quantifies critical cinematic storytelling qualities, and targeted prompting (CML-Instruction) materially improves LLM screenplay generation. This provides a foundation for evaluating and steering LLMs toward higher-quality narrative outputs.

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
generating highly structured texts. However, while exhibiting a high degree of
structural organization, movie scripts demand an additional layer of nuanced
storytelling and emotional depth-the 'soul' of compelling cinema-that LLMs
often fail to capture. To investigate this deficiency, we first curated
CML-Dataset, a dataset comprising (summary, content) pairs for Cinematic Markup
Language (CML), where 'content' consists of segments from esteemed,
high-quality movie scripts and 'summary' is a concise description of the
content. Through an in-depth analysis of the intrinsic multi-shot continuity
and narrative structures within these authentic scripts, we identified three
pivotal dimensions for quality assessment: Dialogue Coherence (DC), Character
Consistency (CC), and Plot Reasonableness (PR). Informed by these findings, we
propose the CML-Bench, featuring quantitative metrics across these dimensions.
CML-Bench effectively assigns high scores to well-crafted, human-written
scripts while concurrently pinpointing the weaknesses in screenplays generated
by LLMs. To further validate our benchmark, we introduce CML-Instruction, a
prompting strategy with detailed instructions on character dialogue and event
logic, to guide LLMs to generate more structured and cinematically sound
scripts. Extensive experiments validate the effectiveness of our benchmark and
demonstrate that LLMs guided by CML-Instruction generate higher-quality
screenplays, with results aligned with human preferences.

</details>


### [3] [User to Video: A Model for Spammer Detection Inspired by Video Classification Technology](https://arxiv.org/abs/2510.06233)
*Haoyang Zhang,Zhou Yang,Yucai Pang*

Main category: cs.CV

TL;DR: UVSD reframes user behavior as a sequence of images (a “video”)—users become pixels, stances map to RGB, behavior subspaces become frames—and applies video classification to detect spammers, outperforming prior methods on WEIBO and TWITTER.


<details>
  <summary>Details</summary>
Motivation: Conventional spammer detectors struggle to capture evolving, adversarial user behaviors and rich temporal dependencies. By leveraging video models’ strength in spatiotemporal pattern learning, the authors aim to create a representation that preserves temporal and relational dynamics for robust spam detection.

Method: Pipeline: (1) user2pixel maps each user to a pixel, encoding stance as RGB to reflect adversarial behavior. (2) behavior2image turns the user behavior subspace into frame images via representation learning for low‑rank dense embeddings of user relations, followed by cutting and diffusion to complete imageization. (3) Temporal stacking builds per‑user behavior videos. (4) A video classification model is trained to label spammers vs. non‑spammers.

Result: On public WEIBO and TWITTER datasets, UVSD outperforms state‑of‑the‑art baselines (exact metrics not provided in the abstract).

Conclusion: Transforming behavioral data into a video domain and leveraging video classifiers provides an effective approach for spammer detection, suggesting that spatiotemporal representations can improve robustness to adversarial behavior and capture dynamics better than prior methods.

Abstract: This article is inspired by video classification technology. If the user
behavior subspace is viewed as a frame image, consecutive frame images are
viewed as a video. Following this novel idea, a model for spammer detection
based on user videoization, called UVSD, is proposed. Firstly, a user2piexl
algorithm for user pixelization is proposed. Considering the adversarial
behavior of user stances, the user is viewed as a pixel, and the stance is
quantified as the pixel's RGB. Secondly, a behavior2image algorithm is proposed
for transforming user behavior subspace into frame images. Low-rank dense
vectorization of subspace user relations is performed using representation
learning, while cutting and diffusion algorithms are introduced to complete the
frame imageization. Finally, user behavior videos are constructed based on
temporal features. Subsequently, a video classification algorithm is combined
to identify the spammers. Experiments using publicly available datasets, i.e.,
WEIBO and TWITTER, show an advantage of the UVSD model over state-of-the-art
methods.

</details>


### [4] [Uncertainty Quantification In Surface Landmines and UXO Classification Using MC Dropout](https://arxiv.org/abs/2510.06238)
*Sagar Lekhak,Emmett J. Ientilucci,Dimah Dera,Susmita Ghosh*

Main category: cs.CV

TL;DR: Adds Monte Carlo Dropout to a fine-tuned ResNet-50 to quantify epistemic uncertainty in classifying simulated images of surface landmines/UXOs, enabling the system to flag unreliable predictions, especially under noise and adversarial perturbations.


<details>
  <summary>Details</summary>
Motivation: Deterministic neural networks used for demining can be brittle in the presence of noise and adversarial attacks, risking missed detections or misclassifications. Reliable, uncertainty-aware predictions are needed to support safe decision-making in the field.

Method: Integrate MC Dropout into a fine-tuned ResNet-50 and perform multiple stochastic forward passes at inference to estimate epistemic uncertainty. Evaluate on simulated datasets with clean, noisy, and adversarially perturbed images; use uncertainty as an auxiliary reliability metric to identify questionable predictions.

Result: The MC Dropout-enhanced model successfully flags higher uncertainty on challenging (noisy/adversarial) inputs while maintaining reliable confidence on clean data, demonstrating practical usefulness of uncertainty estimates. Specific performance metrics are not reported in the abstract.

Conclusion: Uncertainty quantification is valuable for demining models, both to expose vulnerabilities of current deterministic approaches and to guide more robust, safety-critical deployment. The study is a proof of concept and calls for developing stronger, uncertainty-aware models for real-world use.

Abstract: Detecting surface landmines and unexploded ordnances (UXOs) using deep
learning has shown promise in humanitarian demining. However, deterministic
neural networks can be vulnerable to noisy conditions and adversarial attacks,
leading to missed detection or misclassification. This study introduces the
idea of uncertainty quantification through Monte Carlo (MC) Dropout, integrated
into a fine-tuned ResNet-50 architecture for surface landmine and UXO
classification, which was tested on a simulated dataset. Integrating the MC
Dropout approach helps quantify epistemic uncertainty, providing an additional
metric for prediction reliability, which could be helpful to make more informed
decisions in demining operations. Experimental results on clean, adversarially
perturbed, and noisy test images demonstrate the model's ability to flag
unreliable predictions under challenging conditions. This proof-of-concept
study highlights the need for uncertainty quantification in demining, raises
awareness about the vulnerability of existing neural networks in demining to
adversarial threats, and emphasizes the importance of developing more robust
and reliable models for practical applications.

</details>


### [5] [multimodars: A Rust-powered toolkit for multi-modality cardiac image fusion and registration](https://arxiv.org/abs/2510.06241)
*Anselm W. Stark,Marc Ilic,Ali Mokhtari,Pooya Mohammadi Kazaj,Christoph Graeni,Isaac Shiri*

Main category: cs.CV

TL;DR: multimodars is an open, deterministic, high‑performance toolkit to fuse intravascular imaging with CCTA for multi‑state coronary 3D modeling, built on a NumPy‑centric data model with an optimized Rust backend and support for CSV/NumPy (incl. AIVUS‑CAA) inputs.


<details>
  <summary>Details</summary>
Motivation: High‑fidelity coronary models require combining complementary modalities: intravascular imaging offers sub‑millimeter detail but poor global context, while CCTA provides full‑vessel geometry with lower resolution and artefacts. Existing fusion efforts lack an open, flexible, deterministic, and easily integrable toolkit that supports multi‑state (rest/stress, pre/post‑stent) analysis at scale.

Method: Provide deterministic alignment algorithms within a compact NumPy‑centered data model, backed by an optimized Rust core for speed and reproducibility. Accept standard CSV/NumPy inputs, including formats from AIVUS‑CAA, to ease pipeline integration and enable scalable experiments.

Result: Introduces a software package that operationalizes intravascular/CCTA fusion with deterministic behavior and high performance, suitable for multi‑state analyses and reproducible large‑scale experiments. No quantitative benchmarks are reported in the abstract.

Conclusion: The toolkit fills a practical gap by enabling reproducible, scalable, and integrable multimodal coronary reconstruction workflows across multiple physiological or interventional states, potentially standardizing and accelerating research and pipeline deployment.

Abstract: Combining complementary imaging modalities is critical to build reliable 3D
coronary models: intravascular imaging gives sub-millimetre resolution but
limited whole-vessel context, while CCTA supplies 3D geometry but suffers from
limited spatial resolution and artefacts (e.g., blooming). Prior work
demonstrated intravascular/CCTA fusion, yet no open, flexible toolkit is
tailored for multi-state analysis (rest/stress, pre-/post-stenting) while
offering deterministic behaviour, high performance, and easy pipeline
integration. multimodars addresses this gap with deterministic alignment
algorithms, a compact NumPy-centred data model, and an optimised Rust backend
suitable for scalable, reproducible experiments. The package accepts CSV/NumPy
inputs including data formats produced by the AIVUS-CAA software

</details>


### [6] [Does Physics Knowledge Emerge in Frontier Models?](https://arxiv.org/abs/2510.06251)
*Ieva Bagdonaviciute,Vibhav Vineet*

Main category: cs.CV

TL;DR: They benchmark six top vision-language models on physical reasoning tasks (CLEVRER, Physion, Physion++) and find that good perception or physics subskills don’t translate into accurate predictions/counterfactuals, revealing a gap in causal understanding.


<details>
  <summary>Details</summary>
Motivation: Assess whether current VLMs truly understand and can predict physical dynamics, not just recognize visual elements or perform generic reasoning; identify why strong perceptual or isolated physics skills may fail on causal prediction tasks.

Method: Evaluate six frontier VLMs on predictive and counterfactual tasks across three physical simulation datasets. Create diagnostic subtests that separately measure perception (objects, colors, occlusion) and physics reasoning (motion prediction, spatial relations). Analyze correlations between diagnostic scores and end-task performance.

Result: Weak correlations between diagnostic subskills and evaluation accuracy: models that excel at perception or isolated physics reasoning do not consistently perform better on predictive/counterfactual tasks.

Conclusion: Current VLMs exhibit fragmented perceptual and physics abilities that do not integrate into causal understanding. Progress likely requires architectures that tightly bind perception with physics reasoning/causal modeling.

Abstract: Leading Vision-Language Models (VLMs) show strong results in visual
perception and general reasoning, but their ability to understand and predict
physical dynamics remains unclear. We benchmark six frontier VLMs on three
physical simulation datasets - CLEVRER, Physion, and Physion++ - where the
evaluation tasks test whether a model can predict outcomes or hypothesize about
alternative situations. To probe deeper, we design diagnostic subtests that
isolate perception (objects, colors, occluders) from physics reasoning (motion
prediction, spatial relations). Intuitively, stronger diagnostic performance
should support higher evaluation accuracy. Yet our analysis reveals weak
correlations: models that excel at perception or physics reasoning do not
consistently perform better on predictive or counterfactual evaluation. This
counterintuitive gap exposes a central limitation of current VLMs: perceptual
and physics skills remain fragmented and fail to combine into causal
understanding, underscoring the need for architectures that bind perception and
reasoning more tightly.

</details>


### [7] [Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training](https://arxiv.org/abs/2510.06254)
*Xiaochen Zhao,Chengting Yu,Kairong Yu,Lei Liu,Aili Wang*

Main category: cs.CV

TL;DR: They train spiking neural networks efficiently by adding lightweight ANN side-branches and performing reliable self-distillation on firing-rate signals, cutting BPTT-style cost while maintaining strong accuracy across CIFAR, CIFAR-DVS, and ImageNet.


<details>
  <summary>Details</summary>
Motivation: SNNs are energy-efficient but standard surrogate/BPTT training is memory- and compute-heavy and still lags ANNs in accuracy. There is a need for a training approach that scales better with time and resources without sacrificing performance.

Method: Project intermediate SNN firing rates to lightweight ANN branches and jointly optimize with a rate-based backpropagation objective. Use self-distillation where the model’s own signals supervise submodules, but split the teacher signal into reliable vs. unreliable components and train only with the reliable part to avoid harmful guidance.

Result: Across CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet, the approach reduces training complexity and achieves high-performance SNN models (abstract implies competitive or improved accuracy; specific numbers not provided).

Conclusion: Decoupled, reliability-aware self-distillation via ANN side-branches enables compute- and memory-efficient SNN training that maintains high accuracy, offering a practical alternative to BPTT-based methods.

Abstract: Spiking Neural Networks (SNNs) exhibit exceptional energy efficiency on
neuromorphic hardware due to their sparse activation patterns. However,
conventional training methods based on surrogate gradients and Backpropagation
Through Time (BPTT) not only lag behind Artificial Neural Networks (ANNs) in
performance, but also incur significant computational and memory overheads that
grow linearly with the temporal dimension. To enable high-performance SNN
training under limited computational resources, we propose an enhanced
self-distillation framework, jointly optimized with rate-based backpropagation.
Specifically, the firing rates of intermediate SNN layers are projected onto
lightweight ANN branches, and high-quality knowledge generated by the model
itself is used to optimize substructures through the ANN pathways. Unlike
traditional self-distillation paradigms, we observe that low-quality
self-generated knowledge may hinder convergence. To address this, we decouple
the teacher signal into reliable and unreliable components, ensuring that only
reliable knowledge is used to guide the optimization of the model. Extensive
experiments on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate that
our method reduces training complexity while achieving high-performance SNN
training. Our code is available at
https://github.com/Intelli-Chip-Lab/enhanced-self-distillation-framework-for-snn.

</details>


### [8] [Ensemble Deep Learning and LLM-Assisted Reporting for Automated Skin Lesion Diagnosis](https://arxiv.org/abs/2510.06260)
*Sher Khan,Raz Muhammad,Adil Hussain,Muhammad Sajjad,Muhammad Rashid*

Main category: cs.CV

TL;DR: A unified dermatology AI that fuses a diverse CNN ensemble with built‑in uncertainty triage and an embedded LLM to produce structured clinical reports and patient education, aiming to boost diagnostic reliability and close communication gaps.


<details>
  <summary>Details</summary>
Motivation: Early melanoma detection is critical, but current practice suffers from inter‑observer variability and unequal access. Existing AI tools underperform due to homogeneous model architectures, dataset bias across skin tones, and treating language explanations as an afterthought rather than part of the clinical workflow.

Method: Two-part framework: (1) a deliberately heterogeneous ensemble of CNNs provides complementary visual diagnoses and an intrinsic uncertainty mechanism that flags discordant cases for specialist review; (2) an LLM is integrated into the diagnostic pipeline to convert model outputs into structured, clinically meaningful documentation with lesion characterization, reasoning, and patient‑centered education and monitoring guidance.

Result: The system purportedly increases diagnostic reliability via ensemble agreement/triage and improves clinical usability by generating ready-to-use reports and patient guidance. However, no quantitative performance, fairness metrics, or prospective validation results are reported in the abstract.

Conclusion: The approach is positioned as a step toward deployable dermatology AI that unifies perception and communication, potentially improving early intervention rates. Real-world impact depends on rigorous, bias-sensitive evaluation across diverse skin tones and care settings, and on clinical workflow integration.

Abstract: Cutaneous malignancies demand early detection for favorable outcomes, yet
current diagnostics suffer from inter-observer variability and access
disparities. While AI shows promise, existing dermatological systems are
limited by homogeneous architectures, dataset biases across skin tones, and
fragmented approaches that treat natural language processing as separate
post-hoc explanations rather than integral to clinical decision-making. We
introduce a unified framework that fundamentally reimagines AI integration for
dermatological diagnostics through two synergistic innovations. First, a
purposefully heterogeneous ensemble of architecturally diverse convolutional
neural networks provides complementary diagnostic perspectives, with an
intrinsic uncertainty mechanism flagging discordant cases for specialist review
-- mimicking clinical best practices. Second, we embed large language model
capabilities directly into the diagnostic workflow, transforming classification
outputs into clinically meaningful assessments that simultaneously fulfill
medical documentation requirements and deliver patient-centered education. This
seamless integration generates structured reports featuring precise lesion
characterization, accessible diagnostic reasoning, and actionable monitoring
guidance -- empowering patients to recognize early warning signs between
visits. By addressing both diagnostic reliability and communication barriers
within a single cohesive system, our approach bridges the critical
translational gap that has prevented previous AI implementations from achieving
clinical impact. The framework represents a significant advancement toward
deployable dermatological AI that enhances diagnostic precision while actively
supporting the continuum of care from initial detection through patient
education, ultimately improving early intervention rates for skin lesions.

</details>


### [9] [Vision Transformer for Transient Noise Classification](https://arxiv.org/abs/2510.06273)
*Divyansh Srivastava,Andrzej Niedzielski*

Main category: cs.CV

TL;DR: Fine-tunes a pre-trained Vision Transformer (ViT-B/32) to classify 24 LIGO glitch classes (22 Gravity Spy + 2 new O3a classes), reaching 92.26% accuracy, suggesting improved transient-noise discrimination for gravitational-wave searches.


<details>
  <summary>Details</summary>
Motivation: Glitch transients in LIGO data impede gravitational-wave detection. The O3 run introduced two new noise classes, creating a gap in existing classifiers trained on earlier runs and motivating an updated, more capable model to maintain/boost data quality and detection reliability.

Method: Transfer learning with a pre-trained ViT-B/32, fine-tuned on a combined dataset comprising the Gravity Spy glitch classes plus two additional O3a classes. The model outputs multi-class predictions across 24 noise categories.

Result: Reported classification efficiency of 92.26% on the combined dataset, indicating strong performance of ViT for multi-class glitch categorization.

Conclusion: Vision Transformers are effective for LIGO glitch classification and can help improve gravitational-wave detection by better separating transient noise from true signals; expanding/updating class coverage enables adaptation to newer observing runs.

Abstract: Transient noise (glitches) in LIGO data hinders the detection of
gravitational waves (GW). The Gravity Spy project has categorized these noise
events into various classes. With the O3 run, there is the inclusion of two
additional noise classes and thus a need to train new models for effective
classification. We aim to classify glitches in LIGO data into 22 existing
classes from the first run plus 2 additional noise classes from O3a using the
Vision Transformer (ViT) model. We train a pre-trained Vision Transformer
(ViT-B/32) model on a combined dataset consisting of the Gravity Spy dataset
with the additional two classes from the LIGO O3a run. We achieve a
classification efficiency of 92.26%, demonstrating the potential of Vision
Transformer to improve the accuracy of gravitational wave detection by
effectively distinguishing transient noise.
  Key words: gravitational waves --vision transformer --machine learning

</details>


### [10] [General and Efficient Visual Goal-Conditioned Reinforcement Learning using Object-Agnostic Masks](https://arxiv.org/abs/2510.06277)
*Fahim Shahriar,Cheryl Wang,Alireza Azimi,Gautham Vasan,Hany Hamed Elanwar,A. Rupam Mahmood,Colin Bellinger*

Main category: cs.CV

TL;DR: Proposes a mask-based goal representation for goal-conditioned RL that supplies object-agnostic visual goals and dense rewards, yielding fast learning, strong generalization to unseen objects, and successful sim-to-real transfer.


<details>
  <summary>Details</summary>
Motivation: Existing goal encodings (goal images, 3D coordinates, one-hot IDs) often generalize poorly to unseen objects, converge slowly, or require special sensing. The authors seek a representation that is simple, object-agnostic, supports dense rewards without fragile distance metrics, and works in both simulation and the real world.

Method: Represent the goal as a segmentation mask highlighting the target object/region. Use the mask both as an input cue to the policy and to compute dense reward signals by comparing the robot’s effect relative to the masked region—avoiding explicit geometric distance calculations. Train in simulation with ground-truth masks; for real robots, obtain masks from pretrained open-vocabulary object detectors. Evaluate on reaching and pick-up tasks.

Result: In simulation with ground-truth masks, the method achieves 99.9% reaching accuracy on both training and unseen objects. It performs high-accuracy pick-up without using any target positional coordinates. It learns policies from scratch and transfers from simulation to two different physical robots using detector-generated masks.

Conclusion: Mask-based goal representations provide efficient learning and superior generalization for GCRL, eliminating dependence on precise position measurements or special cameras, and enabling practical sim-to-real applications across diverse objects.

Abstract: Goal-conditioned reinforcement learning (GCRL) allows agents to learn diverse
objectives using a unified policy. The success of GCRL, however, is contingent
on the choice of goal representation. In this work, we propose a mask-based
goal representation system that provides object-agnostic visual cues to the
agent, enabling efficient learning and superior generalization. In contrast,
existing goal representation methods, such as target state images, 3D
coordinates, and one-hot vectors, face issues of poor generalization to unseen
objects, slow convergence, and the need for special cameras. Masks can be
processed to generate dense rewards without requiring error-prone distance
calculations. Learning with ground truth masks in simulation, we achieved 99.9%
reaching accuracy on training and unseen test objects. Our proposed method can
be utilized to perform pick-up tasks with high accuracy, without using any
positional information of the target. Moreover, we demonstrate learning from
scratch and sim-to-real transfer applications using two different physical
robots, utilizing pretrained open vocabulary object detection models for mask
generation.

</details>


### [11] [Improving the Spatial Resolution of GONG Solar Images to GST Quality Using Deep Learning](https://arxiv.org/abs/2510.06281)
*Chenyang Li,Qin Li,Haimin Wang,Bo Shen*

Main category: cs.CV

TL;DR: Uses a GAN-based super-resolution (Real-ESRGAN with RRDB and a relativistic discriminator) to enhance low-resolution full-disk GONG Hα images toward high-resolution GST-like quality, recovering fine filament/fibril details; performance limited by residual pair misalignments.


<details>
  <summary>Details</summary>
Motivation: Full-disk Hα imagery lacks the spatial resolution needed to study fine solar features (filaments, fibrils, sunspot penumbrae), while HR instruments like GST provide detail but not full-disk coverage. A super-resolution method could bridge this gap by upgrading ubiquitous LR observations to HR-like quality.

Method: Curated and carefully aligned paired GONG (LR) and GST (HR) Hα images; trained a Real-ESRGAN model using Residual-in-Residual Dense Blocks and a relativistic discriminator to learn LR→HR mapping; evaluated with pixel-wise and similarity metrics.

Result: Qualitatively, the model restores fine penumbral textures and resolves filament/fibril structures. Quantitatively, it attains MSE 467.15, RMSE 21.59, and cross-correlation 0.7794 on test pairs. Slight residual misalignments between training/validation pairs reduce numerical scores.

Conclusion: GAN-based super-resolution can substantially enhance GONG full-disk Hα images toward GST-like detail, enabling better study of small-scale solar structures at global scales. Improving image co-registration and enlarging/diversifying the dataset are planned to further boost reconstruction fidelity and metrics.

Abstract: High-resolution (HR) solar imaging is crucial for capturing fine-scale
dynamic features such as filaments and fibrils. However, the spatial resolution
of the full-disk H$\alpha$ images is limited and insufficient to resolve these
small-scale structures. To address this, we propose a GAN-based superresolution
approach to enhance low-resolution (LR) full-disk H$\alpha$ images from the
Global Oscillation Network Group (GONG) to a quality comparable with HR
observations from the Big Bear Solar Observatory/Goode Solar Telescope
(BBSO/GST). We employ Real-ESRGAN with Residual-in-Residual Dense Blocks and a
relativistic discriminator. We carefully aligned GONG-GST pairs. The model
effectively recovers fine details within sunspot penumbrae and resolves fine
details in filaments and fibrils, achieving an average mean squared error (MSE)
of 467.15, root mean squared error (RMSE) of 21.59, and cross-correlation (CC)
of 0.7794. Slight misalignments between image pairs limit quantitative
performance, which we plan to address in future work alongside dataset
expansion to further improve reconstruction quality.

</details>


### [12] [ChainMPQ: Interleaved Text-Image Reasoning Chains for Mitigating Relation Hallucinations](https://arxiv.org/abs/2510.06292)
*Yike Wu,Yiwei Wang,Yujun Cai*

Main category: cs.CV

TL;DR: ChainMPQ is a training-free prompting framework that reduces relation hallucinations in LVLMs by interleaving multi-perspective questions with accumulated visual and textual memories, yielding substantial improvements across models and benchmarks.


<details>
  <summary>Details</summary>
Motivation: Among object, attribute, and relation hallucinations in LVLMs, relation errors are the most prevalent yet least studied. Improving reliability in relational reasoning is crucial, ideally without model retraining.

Method: ChainMPQ (Multi-Perspective Questions guided Interleaved Chain of Image and Text) operates in three steps: (1) extract subject/object keywords from the question to enhance corresponding image regions (visual focus/memory); (2) build multi-perspective questions targeting subject, object, and the linking relation; (3) sequentially query the LVLM so that textual and visual outputs from earlier steps are reused as context for later steps, forming an interleaved chain that supports progressive relational inference. The approach is training-free and model-agnostic.

Result: Across multiple LVLMs and benchmarks, ChainMPQ substantially lowers relation hallucination rates. Ablation studies show that each core module—region enhancement, multi-perspective questioning, and interleaved memory—contributes meaningfully to the gains.

Conclusion: Interleaving visual and textual memories with multi-perspective questioning improves relational inference in LVLMs without retraining, making ChainMPQ a practical, general method for mitigating relation hallucinations.

Abstract: While Large Vision-Language Models (LVLMs) achieve strong performance in
multimodal tasks, hallucinations continue to hinder their reliability. Among
the three categories of hallucinations, which include object, attribute, and
relation, relation hallucinations account for the largest proportion but have
received the least attention. To address this issue, we propose ChainMPQ
(Multi-Perspective Questions guided Interleaved Chain of Image and Text), a
training-free method that improves relational inference in LVLMs by utilizing
accumulated textual and visual memories. ChainMPQ first extracts subject and
object keywords from the question to enhance the corresponding image regions.
It then constructs multi-perspective questions that focus on the three core
components of a relationship: the subject, the object, and the relation that
links them. These questions are sequentially input to the model, with textual
and visual memories from earlier steps providing supporting context for
subsequent ones, thereby forming an interleaved chain of images and text that
guides progressive relational reasoning. Experiments on multiple LVLMs and
benchmarks show that ChainMPQ substantially reduces relation hallucinations,
while ablation studies further validate the effectiveness of its three core
modules.

</details>


### [13] [Efficient High-Resolution Image Editing with Hallucination-Aware Loss and Adaptive Tiling](https://arxiv.org/abs/2510.06295)
*Young D. Kwon,Abhinav Mehrotra,Malcolm Chadwick,Alberto Gil Ramos,Sourav Bhattacharya*

Main category: cs.CV

TL;DR: MobilePicasso is a three-stage diffusion-based pipeline that enables 4K on-device image editing by editing at standard resolution with hallucination-aware loss, staying in latent space via projection, and upscaling with context-preserving tiling—yielding large latency gains (up to 55.8×), better quality (18–48%), fewer hallucinations (14–51%), and only ~9% more memory than prior work, even outperforming a server A100 setup.


<details>
  <summary>Details</summary>
Motivation: High-resolution (4K) image-to-image editing is increasingly needed on mobile, but existing diffusion models are too memory-hungry and prone to quality degradation/hallucinations on resource-constrained devices.

Method: A three-stage system: (i) perform image editing at a standard resolution with a hallucination-aware loss to penalize spurious content; (ii) apply latent projection so the pipeline avoids expensive pixel-space operations; (iii) upscale the edited latent to the target resolution using adaptive, context-preserving tiling to maintain coherence across tiles.

Result: User study with 46 participants shows 18–48% image-quality improvement and 14–51% hallucination reduction vs existing methods. Runtime latency is much lower—up to 55.8× speed-up—with only a ~9% runtime memory increase over prior work. On-device runtime is reported faster than a server-based high-res model on an A100 GPU.

Conclusion: MobilePicasso makes high-res on-device image editing practical, improving quality and robustness while significantly reducing latency with modest memory overhead, and can outperform powerful server-side baselines.

Abstract: High-resolution (4K) image-to-image synthesis has become increasingly
important for mobile applications. Existing diffusion models for image editing
face significant challenges, in terms of memory and image quality, when
deployed on resource-constrained devices. In this paper, we present
MobilePicasso, a novel system that enables efficient image editing at high
resolutions, while minimising computational cost and memory usage.
MobilePicasso comprises three stages: (i) performing image editing at a
standard resolution with hallucination-aware loss, (ii) applying latent
projection to overcome going to the pixel space, and (iii) upscaling the edited
image latent to a higher resolution with adaptive context-preserving tiling.
Our user study with 46 participants reveals that MobilePicasso not only
improves image quality by 18-48% but reduces hallucinations by 14-51% over
existing methods. MobilePicasso demonstrates significantly lower latency, e.g.,
up to 55.8$\times$ speed-up, yet with a small increase in runtime memory, e.g.,
a mere 9% increase over prior work. Surprisingly, the on-device runtime of
MobilePicasso is observed to be faster than a server-based high-resolution
image editing model running on an A100 GPU.

</details>


### [14] [RGBD Gaze Tracking Using Transformer for Feature Fusion](https://arxiv.org/abs/2510.06298)
*Tobias J. Bauer*

Main category: cs.CV

TL;DR: RGBD-based gaze tracking thesis evaluates Transformer vs simpler fusion on multiple datasets; finds that removing a pre-trained GAN and using an MLP for feature fusion outperforms a Transformer, though results still trail state-of-the-art on ETH-XGaze; also introduces a new RGBD dataset and a real-time pipeline.


<details>
  <summary>Details</summary>
Motivation: Explore an unstudied combination—RGBD inputs with Transformer-based feature fusion—for gaze angle/point estimation. Address dataset gaps: existing sets often lack depth or only provide gaze point labels, not angles, motivating the creation of a new RGBD dataset for training and evaluation.

Method: Build on Lian et al.’s RGBD gaze architecture that uses a GAN to denoise depth maps and extract head-pose features. Propose a Transformer module for RGB and depth feature fusion; train multiple configurations with/without the pre-trained GAN and with Transformer vs MLP fusion. Evaluate across three datasets (ShanghaiTechGaze+, ETH-XGaze, and a newly created OTH-Gaze-Estimation) and integrate the best model into a real-time gaze estimation pipeline.

Result: On ShanghaiTechGaze+: Transformer fusion yields 55.3 mm mean Euclidean error; removing the pre-trained GAN gives 30.1 mm; replacing Transformer with an MLP achieves 26.9 mm, beating Lian et al.’s 38.7 mm. On ETH-XGaze: Transformer 3.59°, without Transformer 3.26°, while Zhang et al.’s distinct architecture reports 2.04°. Results are consistent on the new OTH dataset (details truncated). Overall, the Transformer fusion and pre-trained GAN underperform relative to simpler alternatives.

Conclusion: For RGBD gaze angle estimation, simpler fusion (MLP) and avoiding a misaligned pre-trained GAN can outperform a Transformer-based fusion in this setting, though the approach still lags state-of-the-art on ETH-XGaze. Contributions include a new RGBD dataset and a real-time pipeline; future gains may come from better-suited fusion strategies and pretraining tailored to RGBD gaze.

Abstract: Subject of this thesis is the implementation of an AI-based Gaze Tracking
system using RGBD images that contain both color (RGB) and depth (D)
information. To fuse the features extracted from the images, a module based on
the Transformer architecture is used. The combination of RGBD input images and
Transformers was chosen because it has not yet been investigated. Furthermore,
a new dataset is created for training the AI models as existing datasets either
do not contain depth information or only contain labels for Gaze Point
Estimation that are not suitable for the task of Gaze Angle Estimation. Various
model configurations are trained, validated and evaluated on a total of three
different datasets. The trained models are then to be used in a real-time
pipeline to estimate the gaze direction and thus the gaze point of a person in
front of a computer screen. The AI model architecture used in this thesis is
based on an earlier work by Lian et al. It uses a Generative Adversarial
Network (GAN) to simultaneously remove depth map artifacts and extract head
pose features. Lian et al. achieve a mean Euclidean error of 38.7mm on their
own dataset ShanghaiTechGaze+. In this thesis, a model architecture with a
Transformer module for feature fusion achieves a mean Euclidean error of 55.3mm
on the same dataset, but we show that using no pre-trained GAN module leads to
a mean Euclidean error of 30.1mm. Replacing the Transformer module with a
Multilayer Perceptron (MLP) improves the error to 26.9mm. These results are
coherent with the ones on the other two datasets. On the ETH-XGaze dataset, the
model with Transformer module achieves a mean angular error of 3.59{\deg} and
without Transformer module 3.26{\deg}, whereas the fundamentally different
model architecture used by the dataset authors Zhang et al. achieves a mean
angular error of 2.04{\deg}. On the OTH-Gaze-Estimation dataset created for...

</details>


### [15] [Scalable deep fusion of spaceborne lidar and synthetic aperture radar for global forest structural complexity mapping](https://arxiv.org/abs/2510.06299)
*Tiago de Conto,John Armston,Ralph Dubayah*

Main category: cs.CV

TL;DR: They fuse sparse GEDI spaceborne lidar with multimodal SAR using a compact EfficientNetV2-based model to generate global, 25 m, multi-temporal maps (2015–2022) of forest structural complexity with high accuracy and calibrated uncertainty, enabling scalable monitoring and conservation applications.


<details>
  <summary>Details</summary>
Motivation: Forest structural complexity underpins habitat quality and ecosystem function, but GEDI’s sparse sampling prevents continuous, high-resolution mapping needed for monitoring, biodiversity, and management decisions.

Method: A scalable deep learning framework that integrates GEDI lidar observations with multimodal SAR inputs. An adapted, lightweight EfficientNetV2 (≤400k parameters) is trained on >130 million GEDI footprints to predict structural complexity at 25 m resolution, providing calibrated uncertainty and supporting transfer learning to other forest structure variables.

Result: Global predictive performance R² = 0.82; accurate, uncertainty-calibrated predictions across biomes and time, preserving fine spatial patterns. Produced a global, wall-to-wall, multi-temporal dataset of structural complexity from 2015–2022; efficient enough to run without specialized computing.

Conclusion: The approach enables continuous, high-resolution, multi-temporal monitoring of forest structural dynamics globally, is scalable and extensible via transfer learning, and provides practical tools for biodiversity conservation and ecosystem management under climate change.

Abstract: Forest structural complexity metrics integrate multiple canopy attributes
into a single value that reflects habitat quality and ecosystem function.
Spaceborne lidar from the Global Ecosystem Dynamics Investigation (GEDI) has
enabled mapping of structural complexity in temperate and tropical forests, but
its sparse sampling limits continuous high-resolution mapping. We present a
scalable, deep learning framework fusing GEDI observations with multimodal
Synthetic Aperture Radar (SAR) datasets to produce global, high-resolution (25
m) wall-to-wall maps of forest structural complexity. Our adapted
EfficientNetV2 architecture, trained on over 130 million GEDI footprints,
achieves high performance (global R2 = 0.82) with fewer than 400,000
parameters, making it an accessible tool that enables researchers to process
datasets at any scale without requiring specialized computing infrastructure.
The model produces accurate predictions with calibrated uncertainty estimates
across biomes and time periods, preserving fine-scale spatial patterns. It has
been used to generate a global, multi-temporal dataset of forest structural
complexity from 2015 to 2022. Through transfer learning, this framework can be
extended to predict additional forest structural variables with minimal
computational cost. This approach supports continuous, multi-temporal
monitoring of global forest structural dynamics and provides tools for
biodiversity conservation and ecosystem management efforts in a changing
climate.

</details>


### [16] [Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding](https://arxiv.org/abs/2510.06308)
*Yi Xin,Qi Qin,Siqi Luo,Kaiwen Zhu,Juncheng Yan,Yan Tai,Jiayi Lei,Yuewen Cao,Keqi Wang,Yibin Wang,Jinbin Bai,Qian Yu,Dengyang Jiang,Yuandong Pu,Haoxing Chen,Le Zhuo,Junjun He,Gen Luo,Tianbin Li,Ming Hu,Jin Ye,Shenglong Ye,Bo Zhang,Chang Xu,Wenhai Wang,Hongsheng Li,Guangtao Zhai,Tianfan Xue,Bin Fu,Xiaohong Liu,Yu Qiao,Yihao Liu*

Main category: cs.CV

TL;DR: Lumina-DiMOO is an open-source unified multi-modal model that uses fully discrete diffusion to handle both inputs and outputs across modalities, delivering faster sampling and state-of-the-art results for text-to-image, image-to-image, and image understanding tasks.


<details>
  <summary>Details</summary>
Motivation: Create a single foundation model that can both generate and understand across modalities while overcoming sampling inefficiency and modeling limitations of autoregressive and hybrid AR-diffusion approaches.

Method: Adopt a fully discrete diffusion framework that represents multi-modal inputs/outputs as discrete variables, enabling a unified architecture for text-to-image and image-to-image generation (editing, subject-driven, inpainting) as well as image understanding, with an emphasis on improved sampling efficiency; release code and checkpoints for reproducibility.

Result: Reports state-of-the-art performance on multiple benchmarks and higher sampling efficiency compared to existing open-source unified multi-modal models.

Conclusion: Fully discrete diffusion is an effective, efficient backbone for unified multi-modal generation and understanding; Lumina-DiMOO advances the state of the art and is released to support further research.

Abstract: We introduce Lumina-DiMOO, an open-source foundational model for seamless
multi-modal generation and understanding. Lumina-DiMOO sets itself apart from
prior unified models by utilizing a fully discrete diffusion modeling to handle
inputs and outputs across various modalities. This innovative approach allows
Lumina-DiMOO to achieve higher sampling efficiency compared to previous
autoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a
broad spectrum of multi-modal tasks, including text-to-image generation,
image-to-image generation (e.g., image editing, subject-driven generation, and
image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves
state-of-the-art performance on multiple benchmarks, surpassing existing
open-source unified multi-modal models. To foster further advancements in
multi-modal and discrete diffusion model research, we release our code and
checkpoints to the community. Project Page:
https://synbol.github.io/Lumina-DiMOO.

</details>


### [17] [TransFIRA: Transfer Learning for Face Image Recognizability Assessment](https://arxiv.org/abs/2510.06353)
*Allen Tu,Kartik Narayan,Joshua Gleason,Jennifer Xu,Matthew Meyn,Tom Goldstein,Vishal M. Patel*

Main category: cs.CV

TL;DR: TransFIRA is an annotation-free, encoder-grounded FIQA framework that defines recognizability in embedding space (via CCS/CCAS), enabling effective filtering/weighting and recognizability-aware aggregation to achieve SOTA verification on BRIAR and IJB-C, higher correlation with true recognizability, interpretability, cross-dataset robustness, and extensions to body recognition.


<details>
  <summary>Details</summary>
Motivation: Face recognition in the wild breaks conventional image-quality heuristics: pose, blur, illumination, and occlusion can make faces unrecognizable to a specific encoder even when visuals look “good.” Existing FIQA relies on heuristics, curated labels, or heavy generative models and is detached from the recognition model’s decision geometry. The goal is to align quality assessment with the deployed encoder’s actual decision boundaries, without extra labels or costly pipelines.

Method: Ground recognizability directly in the embedding space using two metrics: (1) class-center similarity (CCS) and (2) class-center angular separation (CCAS). These provide a decision-boundary–aligned criterion for filtering and weighting samples. Build a recognizability-informed aggregation strategy for verification without external labels or backbone-specific training. Provide encoder-grounded explainability to localize degradations/subject factors affecting recognizability and extend the approach to body recognition assessment.

Result: The approach delivers state-of-the-art verification accuracy on BRIAR and IJB-C, nearly doubles correlation with true recognizability, shows strong performance for body recognition, and remains robust under cross-dataset shifts—all achieved without extra annotations, heuristics, or backbone-specific tuning.

Conclusion: TransFIRA offers a unified, geometry-driven, annotation-free FIQA that is encoder-specific, accurate, interpretable, and extensible across modalities, substantially advancing the accuracy, explainability, and scope of recognizability assessment.

Abstract: Face recognition in unconstrained environments such as surveillance, video,
and web imagery must contend with extreme variation in pose, blur,
illumination, and occlusion, where conventional visual quality metrics fail to
predict whether inputs are truly recognizable to the deployed encoder. Existing
FIQA methods typically rely on visual heuristics, curated annotations, or
computationally intensive generative pipelines, leaving their predictions
detached from the encoder's decision geometry. We introduce TransFIRA (Transfer
Learning for Face Image Recognizability Assessment), a lightweight and
annotation-free framework that grounds recognizability directly in embedding
space. TransFIRA delivers three advances: (i) a definition of recognizability
via class-center similarity (CCS) and class-center angular separation (CCAS),
yielding the first natural, decision-boundary--aligned criterion for filtering
and weighting; (ii) a recognizability-informed aggregation strategy that
achieves state-of-the-art verification accuracy on BRIAR and IJB-C while nearly
doubling correlation with true recognizability, all without external labels,
heuristics, or backbone-specific training; and (iii) new extensions beyond
faces, including encoder-grounded explainability that reveals how degradations
and subject-specific factors affect recognizability, and the first
recognizability-aware body recognition assessment. Experiments confirm
state-of-the-art results on faces, strong performance on body recognition, and
robustness under cross-dataset shifts. Together, these contributions establish
TransFIRA as a unified, geometry-driven framework for recognizability
assessment -- encoder-specific, accurate, interpretable, and extensible across
modalities -- significantly advancing FIQA in accuracy, explainability, and
scope.

</details>


### [18] [Road Surface Condition Detection with Machine Learning using New York State Department of Transportation Camera Images and Weather Forecast Data](https://arxiv.org/abs/2510.06440)
*Carly Sutter,Kara J. Sulia,Nick P. Bassill,Christopher D. Wirz,Christopher D. Thorncroft,Jay C. Rothenberger,Vanessa Przybylo,Mariana G. Cains,Jacob Radford,David Aaron Evans*

Main category: cs.CV

TL;DR: Trains CNNs (images) and random forests (weather features) on ~22k labeled NYSDOT traffic-camera images to auto-classify six road surface conditions; emphasizes generalization and attains 81.5% accuracy on completely unseen cameras.


<details>
  <summary>Details</summary>
Motivation: Manual road condition assessment via patrols and live camera monitoring is labor-intensive and time-critical during winter weather. An automated, scalable classifier could provide consistent, near-real-time statewide situational awareness to support NYSDOT operational decisions.

Method: Assemble a hand-labeled dataset (~22,000 images) from NYSDOT roadside cameras with six classes (severe snow, snow, wet, dry, poor visibility, obstructed). Train convolutional neural networks on images and random forest models using weather data; combine modalities to predict current road surface condition. Evaluate with a camera-level holdout to test generalization to unseen cameras, prioritizing operational robustness.

Result: The weather-related road surface condition model achieves 81.5% accuracy when evaluated on entirely unseen cameras, indicating cross-camera generalization. The system distinguishes six classes relevant to winter operations.

Conclusion: Automated classification of road surface conditions from public cameras, augmented by weather data, can reliably support NYSDOT decision-making during winter events at scale. The achieved accuracy on unseen cameras suggests deployability; further gains may come from more data, handling edge cases (visibility/obstruction), temporal modeling, and uncertainty calibration.

Abstract: The New York State Department of Transportation (NYSDOT) has a network of
roadside traffic cameras that are used by both the NYSDOT and the public to
observe road conditions. The NYSDOT evaluates road conditions by driving on
roads and observing live cameras, tasks which are labor-intensive but necessary
for making critical operational decisions during winter weather events.
However, machine learning models can provide additional support for the NYSDOT
by automatically classifying current road conditions across the state. In this
study, convolutional neural networks and random forests are trained on camera
images and weather data to predict road surface conditions. Models are trained
on a hand-labeled dataset of ~22,000 camera images, each classified by human
labelers into one of six road surface conditions: severe snow, snow, wet, dry,
poor visibility, or obstructed. Model generalizability is prioritized to meet
the operational needs of the NYSDOT decision makers, and the weather-related
road surface condition model in this study achieves an accuracy of 81.5% on
completely unseen cameras.

</details>


### [19] [TDiff: Thermal Plug-And-Play Prior with Patch-Based Diffusion](https://arxiv.org/abs/2510.06460)
*Piyush Dashpute,Niki Nezakati,Wolfgang Heidrich,Vishwanath Saragadam*

Main category: cs.CV

TL;DR: TDiff is a patch-based diffusion framework that restores thermal images by denoising overlapping patches and smoothly blending them, achieving strong performance across denoising, super-resolution, and deblurring on simulated and real data.


<details>
  <summary>Details</summary>
Motivation: Low-cost thermal cameras produce low-resolution images with fixed pattern noise and localized degradations, and available thermal datasets are small and limited. A method that leverages the locality of distortions and can be trained with limited data is needed to provide a unified restoration solution.

Method: Train a diffusion model on small thermal patches to learn a prior tailored to local thermal distortions. At inference, process full-resolution images by denoising overlapping patches and blending them via smooth spatial windowing. This patch-based diffusion prior is applied uniformly to multiple tasks: denoising, super-resolution, and deblurring.

Result: Across denoising, super-resolution, and deblurring tasks, the approach delivers strong results on both simulated and real thermal datasets, indicating robustness and generality. The abstract implies state-of-the-art or highly competitive performance, though no metrics are provided.

Conclusion: A unified, patch-based diffusion prior effectively restores thermal images across multiple tasks by exploiting the local nature of thermal degradations, offering a practical pipeline especially suited to limited-data regimes.

Abstract: Thermal images from low-cost cameras often suffer from low resolution, fixed
pattern noise, and other localized degradations. Available datasets for thermal
imaging are also limited in both size and diversity. To address these
challenges, we propose a patch-based diffusion framework (TDiff) that leverages
the local nature of these distortions by training on small thermal patches. In
this approach, full-resolution images are restored by denoising overlapping
patches and blending them using smooth spatial windowing. To our knowledge,
this is the first patch-based diffusion framework that models a learned prior
for thermal image restoration across multiple tasks. Experiments on denoising,
super-resolution, and deblurring demonstrate strong results on both simulated
and real thermal data, establishing our method as a unified restoration
pipeline.

</details>


### [20] [SIGMA-GEN: Structure and Identity Guided Multi-subject Assembly for Image Generation](https://arxiv.org/abs/2510.06469)
*Oindrila Saha,Vojtech Krs,Radomir Mech,Subhransu Maji,Kevin Blackburn-Matzen,Matheus Gadelha*

Main category: cs.CV

TL;DR: SIGMA-GEN is a single-pass, multi-subject image generator that preserves individual identities while following flexible structural/spatial guidance, trained with a new synthetic dataset (SIGMA-SET27K). It reports state-of-the-art identity fidelity, quality, and speed.


<details>
  <summary>Details</summary>
Motivation: Existing image generators struggle to compose multiple distinct subjects in one pass while reliably preserving each identity, especially when users provide diverse forms of spatial or structural guidance. A unified approach that handles coarse-to-fine constraints (boxes, depth, segments) without separate models is needed.

Method: Introduce SIGMA-GEN, a unified conditioning framework that accepts varied guidance signals—from 2D/3D boxes to pixel-level segmentation and depth—alongside identity cues to generate multi-subject images in a single pass. They also create SIGMA-SET27K, a synthetic dataset with identity, structure, and spatial annotations for 100k+ subjects across 27k images to train and evaluate the model.

Result: Across extensive evaluations, SIGMA-GEN attains state-of-the-art performance in identity preservation, overall image quality, and inference speed for multi-subject generation tasks. The new SIGMA-SET27K dataset supports these results and enables rigorous benchmarking.

Conclusion: A single model can robustly compose multiple identities under diverse structural/spatial constraints, advancing controllable multi-subject image generation. The introduced dataset and framework set a new bar for identity fidelity, quality, and efficiency.

Abstract: We present SIGMA-GEN, a unified framework for multi-identity preserving image
generation. Unlike prior approaches, SIGMA-GEN is the first to enable
single-pass multi-subject identity-preserved generation guided by both
structural and spatial constraints. A key strength of our method is its ability
to support user guidance at various levels of precision -- from coarse 2D or 3D
boxes to pixel-level segmentations and depth -- with a single model. To enable
this, we introduce SIGMA-SET27K, a novel synthetic dataset that provides
identity, structure, and spatial information for over 100k unique subjects
across 27k images. Through extensive evaluation we demonstrate that SIGMA-GEN
achieves state-of-the-art performance in identity preservation, image
generation quality, and speed. Code and visualizations at
https://oindrilasaha.github.io/SIGMA-Gen/

</details>


### [21] [Superpixel Integrated Grids for Fast Image Segmentation](https://arxiv.org/abs/2510.06487)
*Jack Roberts,Jeova Farias Sales Rocha Neto*

Main category: cs.CV

TL;DR: They propose SIGRID, a superpixel-integrated grid representation that encodes color and shape descriptors into a regular, lower-dimensional input for CNN segmentation, matching or surpassing pixel-level accuracy while training substantially faster on four benchmarks and two standard architectures.


<details>
  <summary>Details</summary>
Motivation: Superpixels can reduce data size, but their irregular layout forces custom deep-learning pipelines, negating simplicity and efficiency benefits. The goal is to keep superpixel efficiency while enabling use of off-the-shelf convolutional architectures without specialized training.

Method: Construct SIGRID, a regular grid that aggregates superpixel information. Each cell encodes both color and classical shape descriptors of the corresponding superpixel, yielding a compact representation compatible with standard convolutional segmentation models. Evaluate on four benchmark datasets with two popular CNN segmentation architectures.

Result: Despite compressed inputs, SIGRID achieves segmentation performance that matches or even surpasses pixel-level baselines and significantly accelerates model training across four datasets and two architectures.

Conclusion: SIGRID offers a practical trade-off between accuracy and computational efficiency, enabling standard CNNs to leverage superpixel-based compression without bespoke architectures or training procedures.

Abstract: Superpixels have long been used in image simplification to enable more
efficient data processing and storage. However, despite their computational
potential, their irregular spatial distribution has often forced deep learning
approaches to rely on specialized training algorithms and architectures,
undermining the original motivation for superpixelations. In this work, we
introduce a new superpixel-based data structure, SIGRID (Superpixel-Integrated
Grid), as an alternative to full-resolution images in segmentation tasks. By
leveraging classical shape descriptors, SIGRID encodes both color and shape
information of superpixels while substantially reducing input dimensionality.
We evaluate SIGRIDs on four benchmark datasets using two popular convolutional
segmentation architectures. Our results show that, despite compressing the
original data, SIGRIDs not only match but in some cases surpass the performance
of pixel-level representations, all while significantly accelerating model
training. This demonstrates that SIGRIDs achieve a favorable balance between
accuracy and computational efficiency.

</details>


### [22] [Text2Interact: High-Fidelity and Diverse Text-to-Two-Person Interaction Generation](https://arxiv.org/abs/2510.06504)
*Qingxuan Wu,Zhiyang Dou,Chuan Guo,Yiming Huang,Qiao Feng,Bing Zhou,Jian Wang,Lingjie Liu*

Main category: cs.CV

TL;DR: Text2Interact is a two-person, text-conditioned motion framework that pairs a scalable synthetic data pipeline (InterCompose) with a fine-grained text-to-interaction model (InterActor), yielding more diverse, realistic, and text-aligned human-human interactions than prior work.


<details>
  <summary>Details</summary>
Motivation: Two-person interaction generation needs precise spatiotemporal coupling and adherence to rich textual cues, but current progress is limited by scarce two-person datasets and coarse sentence-level language conditioning that discards structured interaction details.

Method: 1) InterCompose: a synthesis-by-composition pipeline that, given a prompt and a motion for one agent, retrieves single-person motion candidates, trains a conditional reaction generator for the second agent, and filters outputs with a neural motion evaluator to scale high-fidelity interaction data without new motion capture. 2) InterActor: a text-to-interaction model with word-level conditioning to preserve token-level signals (e.g., initiation, response, contact ordering) and an adaptive interaction loss that emphasizes context-relevant inter-person joint pairs for better coupling and physical plausibility.

Result: Experiments report consistent improvements in motion diversity, fidelity, and generalization—including out-of-distribution settings—and favorable user study outcomes, indicating stronger spatiotemporal coupling and text consistency than baselines.

Conclusion: Combining scalable interaction synthesis with token-aware conditioning and adaptive inter-person losses enables realistic, text-aligned two-person motion generation. The approach promises better fine-grained coupling and broader coverage, with planned code/model release to support reproducibility.

Abstract: Modeling human-human interactions from text remains challenging because it
requires not only realistic individual dynamics but also precise,
text-consistent spatiotemporal coupling between agents. Currently, progress is
hindered by 1) limited two-person training data, inadequate to capture the
diverse intricacies of two-person interactions; and 2) insufficiently
fine-grained text-to-interaction modeling, where language conditioning
collapses rich, structured prompts into a single sentence embedding. To address
these limitations, we propose our Text2Interact framework, designed to generate
realistic, text-aligned human-human interactions through a scalable
high-fidelity interaction data synthesizer and an effective spatiotemporal
coordination pipeline. First, we present InterCompose, a scalable
synthesis-by-composition pipeline that aligns LLM-generated interaction
descriptions with strong single-person motion priors. Given a prompt and a
motion for an agent, InterCompose retrieves candidate single-person motions,
trains a conditional reaction generator for another agent, and uses a neural
motion evaluator to filter weak or misaligned samples-expanding interaction
coverage without extra capture. Second, we propose InterActor, a
text-to-interaction model with word-level conditioning that preserves
token-level cues (initiation, response, contact ordering) and an adaptive
interaction loss that emphasizes contextually relevant inter-person joint
pairs, improving coupling and physical plausibility for fine-grained
interaction modeling. Extensive experiments show consistent gains in motion
diversity, fidelity, and generalization, including out-of-distribution
scenarios and user studies. We will release code and models to facilitate
reproducibility.

</details>


### [23] [From Captions to Keyframes: Efficient Video Summarization via Caption- and Context-Aware Frame Scoring](https://arxiv.org/abs/2510.06509)
*Shih-Yao Lin,Sibendu Paul,Caren Chen*

Main category: cs.CV

TL;DR: KeyScore + STACFP select a tiny, diverse, and text-aligned set of video frames, yielding up to 99% frame reduction while improving or surpassing 8-frame baselines on standard video-language benchmarks.


<details>
  <summary>Details</summary>
Motivation: Long videos make video-language understanding expensive; fixed, sparse sampling can miss semantically crucial moments and lacks grounding to the textual query/caption. The goal is to pick few frames that maximally retain multimodal (vision–language) semantics for downstream tasks.

Method: KeyScore scores frame importance by jointly using captions and visual context, combining (1) semantic similarity to text, (2) temporal diversity, and (3) contextual drop impact (how removal affects understanding). STACFP (Spatio-Temporal Adaptive Clustering for Frame Proposals) proposes compact, diverse frame candidates in long videos; KeyScore then selects the best frames for tasks like retrieval, captioning, and reasoning.

Result: Using KeyScore+STACFP achieves up to 99% fewer frames than full-frame inference and substantially outperforms standard 8-frame encoders on MSRVTT, MSVD, and DiDeMo across retrieval, captioning, and reasoning tasks.

Conclusion: Emphasizing multimodal alignment and adaptive, diverse frame selection enables scalable, efficient, caption-grounded video understanding without explicit video summarization, improving accuracy while drastically reducing computation.

Abstract: Efficient video-language understanding requires selecting a small set of
frames that retain semantic and contextual information from long videos. We
propose KeyScore, a multimodal frame scoring framework that jointly leverages
captions and visual context to estimate frame-level importance. By combining
semantic similarity, temporal diversity, and contextual drop impact, KeyScore
identifies the most informative frames for downstream tasks such as retrieval,
captioning, and video-language reasoning. To complement KeyScore, we introduce
STACFP (Spatio-Temporal Adaptive Clustering for Frame Proposals), which
generates compact and diverse frame candidates for long-form videos. Together,
these modules achieve up to 99\% frame reduction compared to full-frame
inference and substantially outperform standard 8-frame encoders on MSRVTT,
MSVD, and DiDeMo. Our results demonstrate that emphasizing multimodal alignment
between visual and textual signals enables scalable, efficient, and
caption-grounded video understanding -- without explicit video summarization.

</details>


### [24] [LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval](https://arxiv.org/abs/2510.06512)
*Avishree Khare,Hideki Okamoto,Bardh Hoxha,Georgios Fainekos,Rajeev Alur*

Main category: cs.CV

TL;DR: They introduce LogSTOP, an efficient scoring function that lifts frame-level detector scores (e.g., YOLO, HuBERT) into scores for Linear Temporal Logic (LTL) properties over sequences, enabling temporal query matching and ranked retrieval; it yields sizable gains over LVLMs and prior temporal-logic baselines.


<details>
  <summary>Details</summary>
Motivation: Frame- or segment-level neural detectors provide probabilistic scores for local properties (objects, emotions), but many applications require reasoning about how these properties evolve over time (e.g., eventually, until). There is a need to robustly aggregate noisy local scores into reliable scores for temporal properties to support tasks like query matching and retrieval.

Method: Formalize Scores for TempOral Properties (STOPs) over sequences and propose LogSTOP, a computationally efficient scoring function for evaluating LTL-encoded temporal queries using noisy detector outputs. They instantiate it with detectors such as YOLO, HuBERT, Grounding DINO, and SlowR50 to compute temporal-property scores over videos and audio.

Result: On temporal query matching, LogSTOP combined with YOLO/HuBERT surpasses Large Vision/Audio Language Models and other LTL baselines by at least 16%. For ranked retrieval of temporal patterns in videos, LogSTOP with Grounding DINO and SlowR50 improves mean average precision by at least 19% and recall by at least 16% over zero-shot text-to-video baselines.

Conclusion: LogSTOP effectively turns local detection scores into reliable scores for LTL temporal properties, enabling better temporal reasoning for multimedia search and analysis. It achieves substantial empirical gains across audio and video tasks, suggesting it is a strong, efficient alternative to LVLMs for temporally-structured queries.

Abstract: Neural models such as YOLO and HuBERT can be used to detect local properties
such as objects ("car") and emotions ("angry") in individual frames of videos
and audio clips respectively. The likelihood of these detections is indicated
by scores in [0, 1]. Lifting these scores to temporal properties over sequences
can be useful for several downstream applications such as query matching (e.g.,
"does the speaker eventually sound happy in this audio clip?"), and ranked
retrieval (e.g., "retrieve top 5 videos with a 10 second scene where a car is
detected until a pedestrian is detected"). In this work, we formalize this
problem of assigning Scores for TempOral Properties (STOPs) over sequences,
given potentially noisy score predictors for local properties. We then propose
a scoring function called LogSTOP that can efficiently compute these scores for
temporal properties represented in Linear Temporal Logic. Empirically, LogSTOP,
with YOLO and HuBERT, outperforms Large Vision / Audio Language Models and
other Temporal Logic-based baselines by at least 16% on query matching with
temporal properties over objects-in-videos and emotions-in-speech respectively.
Similarly, on ranked retrieval with temporal properties over objects and
actions in videos, LogSTOP with Grounding DINO and SlowR50 reports at least a
19% and 16% increase in mean average precision and recall over zero-shot
text-to-video retrieval baselines respectively.

</details>


### [25] [Limited-Angle Tomography Reconstruction via Projector Guided 3D Diffusion](https://arxiv.org/abs/2510.06516)
*Zhantao Deng,Mériem Er-Rafik,Anna Sushko,Cécile Hébert,Pascal Fua*

Main category: cs.CV

TL;DR: TEMDiff is a 3D diffusion-based, data-consistent reconstruction framework for limited-angle TEM tomography trained on FIB-SEM volumes via a TEM simulator, reducing missing-wedge artifacts and outperforming prior methods on simulated data while generalizing to real tilts, even from extremely narrow ranges (≈8° with 2° steps) without retraining.


<details>
  <summary>Details</summary>
Motivation: Limited-angle TEM tomography suffers from severe missing-wedge artifacts and lacks abundant ground-truth 3D TEM datasets for supervised learning. The goal is to leverage accessible volumetric data to learn strong structural priors and enforce 3D consistency without requiring clean TEM ground truth.

Method: Train a 3D diffusion model on volumetric FIB-SEM data that are mapped to simulated TEM tilt series. Use an iterative reconstruction scheme that alternates data-consistency with diffusion-based 3D prior refinement, operating directly on volumes to implicitly maintain inter-slice consistency and avoiding extra regularizers.

Result: On simulated limited-angle ET datasets, TEMDiff surpasses state-of-the-art in reconstruction quality. A single trained model generalizes to real TEM tilts under different conditions, recovering accurate structures from very narrow tilt ranges (≈8° total, 2° increments) without retraining or fine-tuning.

Conclusion: Diffusion-based 3D priors learned from FIB-SEM with a TEM simulator can effectively mitigate missing-wedge artifacts in limited-angle TEM tomography, providing data-efficient, generalizable reconstructions and potentially reducing acquisition requirements while maintaining cross-slice consistency.

Abstract: Limited-angle electron tomography aims to reconstruct 3D shapes from 2D
projections of Transmission Electron Microscopy (TEM) within a restricted range
and number of tilting angles, but it suffers from the missing-wedge problem
that causes severe reconstruction artifacts. Deep learning approaches have
shown promising results in alleviating these artifacts, yet they typically
require large high-quality training datasets with known 3D ground truth which
are difficult to obtain in electron microscopy. To address these challenges, we
propose TEMDiff, a novel 3D diffusion-based iterative reconstruction framework.
Our method is trained on readily available volumetric FIB-SEM data using a
simulator that maps them to TEM tilt series, enabling the model to learn
realistic structural priors without requiring clean TEM ground truth. By
operating directly on 3D volumes, TEMDiff implicitly enforces consistency
across slices without the need for additional regularization. On simulated
electron tomography datasets with limited angular coverage, TEMDiff outperforms
state-of-the-art methods in reconstruction quality. We further demonstrate that
a trained TEMDiff model generalizes well to real-world TEM tilts obtained under
different conditions and can recover accurate structures from tilt ranges as
narrow as 8 degrees, with 2-degree increments, without any retraining or
fine-tuning.

</details>


### [26] [VUGEN: Visual Understanding priors for GENeration](https://arxiv.org/abs/2510.06529)
*Xiangyi Chen,Théophane Vallaeys,Maha Elbayad,John Nguyen,Jakob Verbeek*

Main category: cs.CV

TL;DR: VUGEN aligns a pretrained vision-language model’s visual understanding with image generation by sampling in a compact, information-preserving latent derived from the VLM’s own encoder and decoding to pixels with a VAE-free diffusion decoder, yielding better COCO FID and DPG Bench scores while retaining understanding ability.


<details>
  <summary>Details</summary>
Motivation: Current VLMs unify text–image understanding but struggle to generate images well. Prior methods depend on reconstruction-oriented autoencoders or complex bridges between understanding and generation spaces, causing misalignment or architectural overhead. The goal is to exploit the VLM’s native visual prior for generation without added complexity or loss of understanding.

Method: 1) Map the VLM vision encoder’s high-dimensional latent to a lower-dimensional, tractable distribution that preserves maximal visual information. 2) Train the VLM to sample within this reduced latent space, ensuring consistency with its understanding representation. 3) Use a dedicated pixel decoder to convert generated latents to images; specifically, a VAE-free pixel diffusion decoder instead of common latent diffusion decoders that rely on VAE latents.

Result: On COCO, FID improves from 11.86 to 9.06; DPG Bench improves from 71.17 to 74.32. The method maintains the original understanding capabilities of the VLM. The pixel diffusion decoder performs on par with or better than latent diffusion decoders that depend on VAE latents.

Conclusion: Leveraging the VLM’s own visual latent and aligning generation directly to it, then decoding with a pixel diffusion model, reduces complexity and misalignment while improving image quality. This indicates VAE-free pixel diffusion can be a competitive, simpler alternative to latent diffusion for VLM-based generation, preserving understanding while enhancing generation.

Abstract: Recent advances in Vision-Language Models (VLMs) have enabled unified
understanding across text and images, yet equipping these models with robust
image generation capabilities remains challenging. Existing approaches often
rely on reconstruction-oriented autoencoders or complex bridging mechanisms,
leading to misalignment between understanding and generation representations,
or architectural complexity. In this work, we propose VUGEN, a novel framework
that explicitly leverages VLM's pretrained visual understanding priors for
efficient and high-quality image generation. Our approach first transforms the
high-dimensional latent space of the VLM's native vision encoder into a
lower-dimensional, tractable distribution that maximally preserves visual
information. The VLM is then trained to sample within this reduced latent
space, ensuring alignment with its visual understanding capabilities. Finally,
a dedicated pixel decoder maps these generated latents back to the image space.
We find that a VAE-free pixel diffusion decoder to be on par or better than
commonly used complex latent diffusion decoders that internally rely on VAE
latents. Extensive experiments demonstrate that VUGEN achieves superior image
generation performance, improving DPG Bench from 71.17 to 74.32 and FID from
11.86 to 9.06 on COCO, while fully preserving the VLM's original understanding
capabilities.

</details>


### [27] [Cluster Paths: Navigating Interpretability in Neural Networks](https://arxiv.org/abs/2510.06541)
*Nicholas M. Kroeger,Vincent Bindschaedler*

Main category: cs.CV

TL;DR: Introduces “cluster paths,” a post-hoc interpretability method that clusters intermediate activations into discrete paths, supplies four evaluation metrics, scales to large vision models (ViTs) and LLM-derived concept paths, exposes spurious cues, stays faithful/stable, and doubles as an OOD detector.


<details>
  <summary>Details</summary>
Motivation: Deep vision models are accurate but opaque, which can lead to misplaced trust, hidden biases (e.g., reliance on spurious cues), and brittle failures. The work seeks interpretable, human-understandable explanations that are faithful to the model’s decision process and robust across inputs and perturbations.

Method: Cluster activations at selected layers and represent each input by the sequence of cluster IDs (a “cluster path”). Evaluate interpretability with four metrics: (1) path complexity (cognitive load), (2) weighted-path purity (class alignment), (3) decision-alignment faithfulness (predictive fidelity), and (4) path agreement (stability under perturbations). Extend to ViTs and derive “concept paths” by prompting a large language model on minimal path divergences to map discrete paths to human concepts. Use paths for tasks like OOD detection.

Result: On a spurious-cue CIFAR-10 setup, cluster paths reveal color-based shortcuts and collapse when the cue is removed. On a 5-class CelebA hair-color task, they reach ~90% faithfulness and ~96% agreement under Gaussian noise without accuracy loss. Scaled to an ImageNet-pretrained ViT, the approach yields concept paths via LLM prompting. Cluster paths serve as a strong OOD detector, flagging anomalies before overconfident predictions. They uncover visual concepts (color palettes, textures, contexts) across depths.

Conclusion: Cluster paths provide concise, human-readable, and scalable explanations of model behavior, help diagnose spurious cues, maintain high faithfulness and stability, and enable practical utilities like OOD detection. The approach bridges activation-level structure with semantic concepts in large vision models.

Abstract: While modern deep neural networks achieve impressive performance in vision
tasks, they remain opaque in their decision processes, risking unwarranted
trust, undetected biases and unexpected failures. We propose cluster paths, a
post-hoc interpretability method that clusters activations at selected layers
and represents each input as its sequence of cluster IDs. To assess these
cluster paths, we introduce four metrics: path complexity (cognitive load),
weighted-path purity (class alignment), decision-alignment faithfulness
(predictive fidelity), and path agreement (stability under perturbations). In a
spurious-cue CIFAR-10 experiment, cluster paths identify color-based shortcuts
and collapse when the cue is removed. On a five-class CelebA hair-color task,
they achieve 90% faithfulness and maintain 96% agreement under Gaussian noise
without sacrificing accuracy. Scaling to a Vision Transformer pretrained on
ImageNet, we extend cluster paths to concept paths derived from prompting a
large language model on minimal path divergences. Finally, we show that cluster
paths can serve as an effective out-of-distribution (OOD) detector, reliably
flagging anomalous samples before the model generates over-confident
predictions. Cluster paths uncover visual concepts, such as color palettes,
textures, or object contexts, at multiple network depths, demonstrating that
cluster paths scale to large vision models while generating concise and
human-readable explanations.

</details>


### [28] [HSNet: Heterogeneous Subgraph Network for Single Image Super-resolution](https://arxiv.org/abs/2510.06564)
*Qiongyang Hu,Wenyang Liu,Wenbin Zou,Yuejiao Su,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: HSNet is a graph-based image super-resolution framework that decomposes a global graph into heterogeneous subgraphs, aggregates them adaptively, and samples salient nodes to achieve state-of-the-art quality with lower computational cost.


<details>
  <summary>Details</summary>
Motivation: CNN- and attention-based SR models can be structurally rigid, while graph-based models are more flexible but often too computationally expensive. The paper aims to retain the adaptability of graph modeling while making it efficient and scalable.

Method: 1) Constructive Subgraph Set Block (CSSB) generates a set of complementary subgraphs that capture diverse local/global relations and feature interactions; 2) Subgraph Aggregation Block (SAB) fuses multi-graph representations via adaptive weighting to form a comprehensive representation; 3) Node Sampling Strategy (NSS) retains salient nodes to reduce computation without sacrificing accuracy. Overall, the approach replaces a single monolithic graph with a heterogeneous ensemble plus adaptive fusion and sampling.

Result: Across extensive experiments (datasets and metrics not specified in the abstract), HSNet achieves state-of-the-art reconstruction accuracy while reducing computational overhead compared to prior CNN/attention and graph-based SR methods.

Conclusion: Decomposing global graphs into heterogeneous subgraphs and aggregating them with adaptive fusion, plus selective node sampling, provides an effective and efficient SR model. HSNet balances representational flexibility and computational feasibility, and code will be released.

Abstract: Existing deep learning approaches for image super-resolution, particularly
those based on CNNs and attention mechanisms, often suffer from structural
inflexibility. Although graph-based methods offer greater representational
adaptability, they are frequently impeded by excessive computational
complexity. To overcome these limitations, this paper proposes the
Heterogeneous Subgraph Network (HSNet), a novel framework that efficiently
leverages graph modeling while maintaining computational feasibility. The core
idea of HSNet is to decompose the global graph into manageable sub-components.
First, we introduce the Constructive Subgraph Set Block (CSSB), which generates
a diverse set of complementary subgraphs. Rather than relying on a single
monolithic graph, CSSB captures heterogeneous characteristics of the image by
modeling different relational patterns and feature interactions, producing a
rich ensemble of both local and global graph structures. Subsequently, the
Subgraph Aggregation Block (SAB) integrates the representations embedded across
these subgraphs. Through adaptive weighting and fusion of multi-graph features,
SAB constructs a comprehensive and discriminative representation that captures
intricate interdependencies. Furthermore, a Node Sampling Strategy (NSS) is
designed to selectively retain the most salient features, thereby enhancing
accuracy while reducing computational overhead. Extensive experiments
demonstrate that HSNet achieves state-of-the-art performance, effectively
balancing reconstruction quality with computational efficiency. The code will
be made publicly available.

</details>


### [29] [Through the Perspective of LiDAR: A Feature-Enriched and Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud Segmentation](https://arxiv.org/abs/2510.06582)
*Fei Zhang,Rob Chancia,Josie Clapp,Amirhossein Hassanzadeh,Dimah Dera,Richard MacKenzie,Jan van Aardt*

Main category: cs.CV

TL;DR: A semi-automated, uncertainty-aware pipeline projects TLS point clouds to 2D, enriches features, ensembles segmenters for pseudo-labels and uncertainty, guides targeted annotation, and back-projects to 3D—yielding accurate labels with less manual effort. It produces the Mangrove3D dataset and shows strong data efficiency (≈12 scans), with geometric features dominating and ~0.76 mIoU using compact 9-channel features; the approach generalizes across datasets.


<details>
  <summary>Details</summary>
Motivation: High-quality semantic segmentation of TLS point clouds requires expensive manual annotation. The authors aim to reduce labeling cost while maintaining accuracy, provide a new mangrove-specific TLS dataset, and offer empirical guidance on how much data and which features matter most.

Method: 1) Project 3D TLS points onto a 2D spherical grid. 2) Enrich pixels with multi-source features (notably geometric). 3) Train an ensemble of segmentation networks to produce pseudo-labels and uncertainty maps. 4) Use uncertainty to prioritize human annotation of ambiguous regions. 5) Back-project 2D outputs to 3D. 6) Provide a three-tier visualization suite (2D feature maps, 3D colorized point clouds, virtual spheres) to accelerate review. 7) Build Mangrove3D and conduct data-efficiency and feature-importance studies; validate generalization on ForestSemantic and Semantic3D.

Result: Performance saturates after about 12 annotated scans; geometric features contribute most; a compact 9-channel feature set captures nearly all discriminative power. Mean IoU plateaus around 0.76. Cross-dataset tests support generalization of the feature-enrichment strategy. The pipeline reduces labeling effort while sustaining high accuracy (qualitatively stated).

Conclusion: Contributions are: (i) a robust, uncertainty-aware TLS annotation pipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii) practical guidance on data efficiency and feature importance. These enable scalable, high-quality segmentation for ecological monitoring and related domains; data and code are publicly available.

Abstract: Accurate semantic segmentation of terrestrial laser scanning (TLS) point
clouds is limited by costly manual annotation. We propose a semi-automated,
uncertainty-aware pipeline that integrates spherical projection, feature
enrichment, ensemble learning, and targeted annotation to reduce labeling
effort, while sustaining high accuracy. Our approach projects 3D points to a 2D
spherical grid, enriches pixels with multi-source features, and trains an
ensemble of segmentation networks to produce pseudo-labels and uncertainty
maps, the latter guiding annotation of ambiguous regions. The 2D outputs are
back-projected to 3D, yielding densely annotated point clouds supported by a
three-tier visualization suite (2D feature maps, 3D colorized point clouds, and
compact virtual spheres) for rapid triage and reviewer guidance. Using this
pipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove
forests. We further evaluate data efficiency and feature importance to address
two key questions: (1) how much annotated data are needed and (2) which
features matter most. Results show that performance saturates after ~12
annotated scans, geometric features contribute the most, and compact
nine-channel stacks capture nearly all discriminative power, with the mean
Intersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm
the generalization of our feature-enrichment strategy through cross-dataset
tests on ForestSemantic and Semantic3D.
  Our contributions include: (i) a robust, uncertainty-aware TLS annotation
pipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii)
empirical guidance on data efficiency and feature importance, thus enabling
scalable, high-quality segmentation of TLS point clouds for ecological
monitoring and beyond. The dataset and processing scripts are publicly
available at https://fz-rit.github.io/through-the-lidars-eye/.

</details>


### [30] [Improving Artifact Robustness for CT Deep Learning Models Without Labeled Artifact Images via Domain Adaptation](https://arxiv.org/abs/2510.06584)
*Justin Cheung,Samuel Savine,Calvin Nguyen,Lin Lu,Alhassan S. Yasin*

Main category: cs.CV

TL;DR: Using domain adversarial neural networks (DANN) with only unlabeled, artifacted CT images, the authors maintain classification accuracy under ring-artifact distribution shifts, outperforming clean-trained and augmentation-based baselines and matching models trained with labeled artifacts, with some spillover robustness to uniform noise.


<details>
  <summary>Details</summary>
Motivation: CT images can suffer from scanner-specific artifacts that are absent during training, causing deep models to fail. Obtaining expert labels for every new artifact domain is costly. The study explores whether unsupervised domain adaptation can deliver robustness to novel artifacts without requiring new labels.

Method: Simulate ring artifacts via detector gain error in sinogram space on the OrganAMNIST abdominal CT dataset. Compare (i) a baseline trained on clean images, (ii) traditional augmentation with other distortions, and (iii) a Domain Adversarial Neural Network (DANN) trained with labeled clean source data and unlabeled artifacted target data.

Result: Baseline models trained only on clean images perform poorly on ring-artifact images; augmentations with other distortions do not help on unseen ring-artifact domains. DANN maintains high accuracy on ring-artifact test data using only unlabeled artifact data and performs comparably to fully supervised models trained with labeled artifact images. It also shows unexpected generalization to uniform noise.

Conclusion: Unsupervised domain adaptation, specifically DANN, can mitigate distribution shift from CT artifacts without additional labeling, making it a practical route toward artifact-robust medical imaging models suitable for clinical settings where new artifacts may arise.

Abstract: Deep learning models which perform well on images from their training
distribution can degrade substantially when applied to new distributions. If a
CT scanner introduces a new artifact not present in the training labels, the
model may misclassify the images. Although modern CT scanners include design
features which mitigate these artifacts, unanticipated or difficult-to-mitigate
artifacts can still appear in practice. The direct solution of labeling images
from this new distribution can be costly. As a more accessible alternative,
this study evaluates domain adaptation as an approach for training models that
maintain classification performance despite new artifacts, even without
corresponding labels. We simulate ring artifacts from detector gain error in
sinogram space and evaluate domain adversarial neural networks (DANN) against
baseline and augmentation-based approaches on the OrganAMNIST abdominal CT
dataset. Our results demonstrate that baseline models trained only on clean
images fail to generalize to images with ring artifacts, and traditional
augmentation with other distortion types provides no improvement on unseen
artifact domains. In contrast, the DANN approach successfully maintains high
classification accuracy on ring artifact images using only unlabeled artifact
data during training, demonstrating the viability of domain adaptation for
artifact robustness. The domain-adapted model achieved classification
performance on ring artifact test data comparable to models explicitly trained
with labeled artifact images, while also showing unexpected generalization to
uniform noise. These findings provide empirical evidence that domain adaptation
can effectively address distribution shift in medical imaging without requiring
expensive expert labeling of new artifact distributions, suggesting promise for
deployment in clinical settings where novel artifacts may emerge.

</details>


### [31] [Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer](https://arxiv.org/abs/2510.06590)
*Ziyuan Huang,DanDan Zheng,Cheng Zou,Rui Liu,Xiaolong Wang,Kaixiang Ji,Weilong Chai,Jianxin Sun,Libin Wang,Yongjie Lv,Taozhi Huang,Jiajia Liu,Qingpei Guo,Ming Yang,Jingdong Chen,Jun Zhou*

Main category: cs.CV

TL;DR: MingTok introduces a continuous visual tokenizer and a unified autoregressive framework (Ming-UniVision) that formulates both understanding and generation as next-token prediction in a shared continuous space, achieving state-of-the-art performance across both domains.


<details>
  <summary>Details</summary>
Motivation: Discrete latent tokenizers aligned to LLM tokens suffer from quantization errors, reducing semantic expressiveness and harming vision-language understanding. Moreover, understanding tasks prefer high-dimensional discriminative features, while generation favors compact low-level codes—creating a tension that current tokenizers cannot reconcile.

Method: Propose MingTok, a family of continuous latent visual tokenizers with a three-stage pipeline: (1) low-level encoding, (2) semantic expansion, and (3) visual reconstruction. Build Ming-UniVision on top to eliminate task-specific visual representations and unify diverse vision-language tasks under a single autoregressive next-token prediction paradigm in the shared continuous space, supporting multi-round in-context understanding, generation, and editing.

Result: Using unified continuous visual representations reconciles the competing requirements of understanding and generation, yielding state-of-the-art-level results across both domains. The approach supports iterative, multi-round tasks. Inference code and model weights are released.

Conclusion: Continuous visual tokenization can unify visual understanding and generation within an autoregressive framework, mitigating quantization-induced limitations of discrete tokenizers and enabling seamless, iterative, in-context multimodal tasks. The work positions continuous token spaces as a promising direction for unified vision-language modeling.

Abstract: Visual tokenization remains a core challenge in unifying visual understanding
and generation within the autoregressive paradigm. Existing methods typically
employ tokenizers in discrete latent spaces to align with the tokens from large
language models, where the quantization errors can limit semantic
expressiveness and degrade the capability of vision-language understanding. To
address this, we introduce MingTok, a new family of visual tokenizers with a
continuous latent space, for unified autoregressive generation and
understanding. While understanding tasks favor discriminative high-dimensional
features, generation tasks prefer compact low-level codes. Thus, to reconcile
these competing demands, MingTok adopts a three-stage sequential architecture
involving low-level encoding, semantic expansion, and visual reconstruction.
Built on top of it, Ming-UniVision eliminates the need for task-specific visual
representations, and unifies diverse vision-language tasks under a single
autoregrsssive prediction paradigm. By formulating both understanding and
generation as next-token prediction in a shared continuous space, it seamlessly
supports multi-round, in-context tasks such as iterative understanding,
generation and editing. Empirically, we find that using a unified continuous
visual representation reconciles the competing requirements on the tokenizers
by the understanding and generation tasks, thereby leading to state-of-the-art
level performance across both domains. We hope our findings will facilitate
unified visual tokenization in the continuous domain. Inference code and model
weights are released to benefit community.

</details>


### [32] [Adaptive Stain Normalization for Cross-Domain Medical Histology](https://arxiv.org/abs/2510.06592)
*Tianyue Xu,Yanlin Wu,Abhai K. Tripathi,Matthew M. Ippolito,Benjamin D. Haeffele*

Main category: cs.CV

TL;DR: BeerLaNet: a physics-guided, trainable color-normalization module—derived from the Beer–Lambert law via unrolled NMF—plugged before standard backbones to produce stain-invariant representations, boosting cross-domain pathology detection/classification and outperforming SOTA normalization methods.


<details>
  <summary>Details</summary>
Motivation: Color/stain and imaging-condition variability cause domain shift in digital pathology, degrading model performance when test data differ from training data. Existing color normalization often introduces artifacts and depends on carefully chosen templates.

Method: Design a trainable color normalization model grounded in the Beer–Lambert imaging physics. Use algorithmic unrolling of a nonnegative matrix factorization (NMF) to separate stain components and extract stain-invariant structural information. Insert this module ahead of arbitrary backbones for downstream tasks (detection/classification).

Result: On public pathology datasets and an internal malaria blood-smear set for cross-domain detection and classification, the proposed method surpasses many state-of-the-art stain normalization approaches.

Conclusion: A physics-informed, unrolled NMF color-normalization module effectively mitigates stain variability, improves robustness across domains, and is broadly compatible with existing deep learning pipelines. Code is publicly available (https://github.com/xutianyue/BeerLaNet).

Abstract: Deep learning advances have revolutionized automated digital pathology
analysis. However, differences in staining protocols and imaging conditions can
introduce significant color variability. In deep learning, such color
inconsistency often reduces performance when deploying models on data acquired
under different conditions from the training data, a challenge known as domain
shift. Many existing methods attempt to address this problem via color
normalization but suffer from several notable drawbacks such as introducing
artifacts or requiring careful choice of a template image for stain mapping. To
address these limitations, we propose a trainable color normalization model
that can be integrated with any backbone network for downstream tasks such as
object detection and classification. Based on the physics of the imaging
process per the Beer-Lambert law, our model architecture is derived via
algorithmic unrolling of a nonnegative matrix factorization (NMF) model to
extract stain-invariant structural information from the original pathology
images, which serves as input for further processing. Experimentally, we
evaluate the method on publicly available pathology datasets and an internally
curated collection of malaria blood smears for cross-domain object detection
and classification, where our method outperforms many state-of-the-art stain
normalization methods. Our code is available at
https://github.com/xutianyue/BeerLaNet.

</details>


### [33] [SDQM: Synthetic Data Quality Metric for Object Detection Dataset Evaluation](https://arxiv.org/abs/2510.06596)
*Ayush Zenith,Arnold Zumbrun,Neel Raut,Jing Lin*

Main category: cs.CV

TL;DR: Proposes SDQM, a fast, training-free (or early-training) metric to evaluate synthetic datasets for object detection; shows strong correlation with YOLOv11 mAP, outperforming prior metrics; aims to guide dataset generation/selection and cut costly iterative training; code released.


<details>
  <summary>Details</summary>
Motivation: High dependence of ML models on data quality and scarcity of large, annotated datasets. Synthetic data helps, but there is no reliable, efficient way to assess its utility for object detection without expensive full training runs.

Method: Introduce Synthetic Dataset Quality Metric (SDQM) to score synthetic datasets for object detection without requiring a model to train to convergence. Validate SDQM by correlating it with downstream performance (mAP) of YOLOv11 and compare against existing metrics.

Result: SDQM correlates strongly with YOLOv11 mAP, whereas prior metrics show only moderate/weak correlation. The metric provides actionable signals for improving datasets and reduces the need for iterative, compute-heavy training cycles.

Conclusion: SDQM offers a scalable, efficient standard for evaluating synthetic datasets in object detection, enabling better dataset selection and generation under resource constraints. Public code is available for adoption and verification.

Abstract: The performance of machine learning models depends heavily on training data.
The scarcity of large-scale, well-annotated datasets poses significant
challenges in creating robust models. To address this, synthetic data generated
through simulations and generative models has emerged as a promising solution,
enhancing dataset diversity and improving the performance, reliability, and
resilience of models. However, evaluating the quality of this generated data
requires an effective metric. This paper introduces the Synthetic Dataset
Quality Metric (SDQM) to assess data quality for object detection tasks without
requiring model training to converge. This metric enables more efficient
generation and selection of synthetic datasets, addressing a key challenge in
resource-constrained object detection tasks. In our experiments, SDQM
demonstrated a strong correlation with the mean Average Precision (mAP) scores
of YOLOv11, a leading object detection model, while previous metrics only
exhibited moderate or weak correlations. Additionally, it provides actionable
insights for improving dataset quality, minimizing the need for costly
iterative training. This scalable and efficient metric sets a new standard for
evaluating synthetic data. The code for SDQM is available at
https://github.com/ayushzenith/SDQM

</details>


### [34] [AIM 2025 Challenge on Real-World RAW Image Denoising](https://arxiv.org/abs/2510.06601)
*Feiran Li,Jiacheng Li,Marcos V. Conde,Beril Besbinar,Vlad Hosu,Daisuke Iso,Radu Timofte*

Main category: cs.CV

TL;DR: AIM 2025 launches a real‑world low‑light RAW image denoising challenge with a new 5‑camera benchmark; teams must design synthetic‑noise pipelines and denoising models, ranked by a mix of full‑reference (PSNR, SSIM, LPIPS) and no‑reference (ARNIQA, TOPIQ) metrics to foster robust, camera‑agnostic performance.


<details>
  <summary>Details</summary>
Motivation: Real‑world low‑light RAW denoising is difficult and data‑hungry; collecting paired clean/noisy data across diverse cameras is costly and impractical. A standardized benchmark and competition leveraging synthetic data can catalyze progress toward practical, generalizable denoisers.

Method: Establish a benchmark of challenging in‑the‑wild low‑light RAW images from five DSLR cameras. Task participants with building noise‑synthesis pipelines, network architectures, and training recipes. Evaluate submissions with combined full‑reference (PSNR, SSIM, LPIPS) and no‑reference (ARNIQA, TOPIQ) metrics, aggregating them for final rankings.

Result: No empirical model results are reported; the paper defines the dataset, task, and evaluation protocol that will surface high‑performing, camera‑agnostic denoisers trained on synthetic data.

Conclusion: Formalizing this challenge should drive efficient, robust low‑light RAW denoising methods trained on synthetic data, with expected impact from general image restoration to night‑time autonomous driving.

Abstract: We introduce the AIM 2025 Real-World RAW Image Denoising Challenge, aiming to
advance efficient and effective denoising techniques grounded in data
synthesis. The competition is built upon a newly established evaluation
benchmark featuring challenging low-light noisy images captured in the wild
using five different DSLR cameras. Participants are tasked with developing
novel noise synthesis pipelines, network architectures, and training
methodologies to achieve high performance across different camera models.
Winners are determined based on a combination of performance metrics, including
full-reference measures (PSNR, SSIM, LPIPS), and non-reference ones (ARNIQA,
TOPIQ). By pushing the boundaries of camera-agnostic low-light RAW image
denoising trained on synthetic data, the competition promotes the development
of robust and practical models aligned with the rapid progress in digital
photography. We expect the competition outcomes to influence multiple domains,
from image restoration to night-time autonomous driving.

</details>


### [35] [Self-supervised Physics-guided Model with Implicit Representation Regularization for Fast MRI Reconstruction](https://arxiv.org/abs/2510.06611)
*Jingran Xu,Yuanyuan Liu,Yanjie Zhu*

Main category: cs.CV

TL;DR: UnrollINR is a zero-shot, scan-specific MRI reconstruction method that unrolls physics-guided optimization and uses an implicit neural representation prior, outperforming supervised baselines even at 10× acceleration without external training data.


<details>
  <summary>Details</summary>
Motivation: MRI’s long acquisition time motivates reconstruction from undersampled k-space. However, fully sampled data for supervised training are scarce, pushing the need for self/unsupervised, scan-specific, and interpretable methods that leverage physics and strong priors.

Method: A zero-shot self-supervised framework (UnrollINR) that performs physics-guided unrolled iterative reconstruction while using an Implicit Neural Representation (INR) as a regularization prior to constrain the solution space. The model is optimized per scan (no external training data), combining data-consistency steps from the MR forward model with the expressive INR to improve interpretability and reconstruction quality.

Result: On experiments with high undersampling (e.g., 10× acceleration), UnrollINR delivers superior reconstruction performance compared to a supervised learning baseline, yielding higher-fidelity images from undersampled k-space.

Conclusion: Marrying unrolled, physics-consistent optimization with INR priors enables effective zero-shot, self-supervised, scan-specific MRI reconstruction, improving both performance and interpretability, especially when fully sampled training data are unavailable.

Abstract: Magnetic Resonance Imaging (MRI) is a vital clinical diagnostic tool, yet its
widespread application is limited by prolonged scan times. Fast MRI
reconstruction techniques effectively reduce acquisition duration by
reconstructing high-fidelity MR images from undersampled k-space data. In
recent years, deep learning-based methods have demonstrated remarkable progress
in this field, with self-supervised and unsupervised learning approaches
proving particularly valuable in scenarios where fully sampled data are
difficult to obtain. This paper proposes a novel zero-shot self-supervised
reconstruction framework named UnrollINR, which enables scan-specific MRI
reconstruction without relying on external training data. The method adopts a
physics-guided unrolled iterative reconstruction architecture and introduces
Implicit Neural Representation (INR) as a regularization prior to effectively
constrain the solution space. By combining a deep unrolled structure with the
powerful implicit representation capability of INR, the model's
interpretability and reconstruction performance are enhanced. Experimental
results demonstrate that even at a high acceleration rate of 10, UnrollINR
achieves superior reconstruction performance compared to the supervised
learning method, validating the superiority of the proposed method.

</details>


### [36] [A Bridge from Audio to Video: Phoneme-Viseme Alignment Allows Every Face to Speak Multiple Languages](https://arxiv.org/abs/2510.06612)
*Zibo Su,Kun Wei,Jiahua Li,Xu Yang,Cheng Deng*

Main category: cs.CV

TL;DR: MuEx is a multilingual talking-face synthesis framework that uses phoneme/viseme intermediates with a phoneme-guided Mixture-of-Experts and a phoneme–viseme alignment mechanism, plus a new 12-language dataset, to achieve better lip-sync and expressions across languages and zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Existing speech-driven talking-face systems trained mostly on English fail on non-English inputs, yielding incorrect mouth shapes and stiff expressions due to dataset bias and weak cross-language generalization. A language-agnostic bridge and robust audio–visual alignment are needed.

Method: Introduce Multilingual Experts (MuEx) with a Phoneme-Guided Mixture-of-Experts (PG-MoE) that maps audio to phonemes and video to visemes as universal units, then aligns them via a Phoneme–Viseme Alignment (PV-Align) mechanism. Construct a Multilingual Talking Face Benchmark (MTFB) covering 12 languages (95.04 hours) for training/evaluation.

Result: Across all languages in MTFB, MuEx outperforms baselines and demonstrates effective zero-shot transfer to unseen languages without extra training, producing more lifelike, synchronized facial animations.

Conclusion: Using phonemes/visemes as universal intermediaries within a guided MoE and enforcing cross-modal alignment reduces language bias and improves synchronization, enabling robust multilingual talking-face synthesis; the new benchmark supports standardized evaluation.

Abstract: Speech-driven talking face synthesis (TFS) focuses on generating lifelike
facial animations from audio input. Current TFS models perform well in English
but unsatisfactorily in non-English languages, producing wrong mouth shapes and
rigid facial expressions. The terrible performance is caused by the
English-dominated training datasets and the lack of cross-language
generalization abilities. Thus, we propose Multilingual Experts (MuEx), a novel
framework featuring a Phoneme-Guided Mixture-of-Experts (PG-MoE) architecture
that employs phonemes and visemes as universal intermediaries to bridge audio
and video modalities, achieving lifelike multilingual TFS. To alleviate the
influence of linguistic differences and dataset bias, we extract audio and
video features as phonemes and visemes respectively, which are the basic units
of speech sounds and mouth movements. To address audiovisual synchronization
issues, we introduce the Phoneme-Viseme Alignment Mechanism (PV-Align), which
establishes robust cross-modal correspondences between phonemes and visemes. In
addition, we build a Multilingual Talking Face Benchmark (MTFB) comprising 12
diverse languages with 95.04 hours of high-quality videos for training and
evaluating multilingual TFS performance. Extensive experiments demonstrate that
MuEx achieves superior performance across all languages in MTFB and exhibits
effective zero-shot generalization to unseen languages without additional
training.

</details>


### [37] [MSITrack: A Challenging Benchmark for Multispectral Single Object Tracking](https://arxiv.org/abs/2510.06619)
*Tao Feng,Tingfa Xu,Haolin Qin,Tianhao Li,Shuaihao Han,Xuyang Zou,Zhan Lv,Jianan Li*

Main category: cs.CV

TL;DR: MSITrack introduces the largest, most diverse multispectral single-object tracking dataset, showing that multispectral cues markedly outperform RGB-only tracking in challenging real-world conditions.


<details>
  <summary>Details</summary>
Motivation: RGB-based tracking suffers in real-world scenarios with occlusion, look-alike distractors, and complex backgrounds. Although multispectral imagery can improve target discriminability by leveraging spectral reflectance, progress is hampered by the scarcity of large, diverse, well-annotated multispectral tracking datasets.

Method: Construct a large-scale, diverse multispectral tracking dataset (MSITrack) with: 300 videos, >129k frames, 55 object categories, and 300 natural scenes. Curate challenging attributes (similar-object interference, target–background similarity, common tracking difficulties). Ensure high-quality annotations via meticulous processing, manual labeling, and multi-stage verification. Evaluate representative trackers to compare multispectral versus RGB-only performance.

Result: Across extensive evaluations, multispectral data from MSITrack yields significant performance improvements over RGB-only baselines, establishing stronger benchmarks and demonstrating the practical value of spectral information for tracking.

Conclusion: MSITrack fills a critical data gap for multispectral tracking, provides a challenging and comprehensive benchmark, and is poised to drive future advances in robust object tracking. The dataset is publicly released, enabling broad community use.

Abstract: Visual object tracking in real-world scenarios presents numerous challenges
including occlusion, interference from similar objects and complex
backgrounds-all of which limit the effectiveness of RGB-based trackers.
Multispectral imagery, which captures pixel-level spectral reflectance,
enhances target discriminability. However, the availability of multispectral
tracking datasets remains limited. To bridge this gap, we introduce MSITrack,
the largest and most diverse multispectral single object tracking dataset to
date. MSITrack offers the following key features: (i) More Challenging
Attributes-including interference from similar objects and similarity in color
and texture between targets and backgrounds in natural scenarios, along with a
wide range of real-world tracking challenges; (ii) Richer and More Natural
Scenes-spanning 55 object categories and 300 distinct natural scenes, MSITrack
far exceeds the scope of existing benchmarks. Many of these scenes and
categories are introduced to the multispectral tracking domain for the first
time; (iii) Larger Scale-300 videos comprising over 129k frames of
multispectral imagery. To ensure annotation precision, each frame has undergone
meticulous processing, manual labeling and multi-stage verification. Extensive
evaluations using representative trackers demonstrate that the multispectral
data in MSITrack significantly improves performance over RGB-only baselines,
highlighting its potential to drive future advancements in the field. The
MSITrack dataset is publicly available at:
https://github.com/Fengtao191/MSITrack.

</details>


### [38] [StaR-KVQA: Structured Reasoning Traces for Implicit-Knowledge Visual Question Answering](https://arxiv.org/abs/2510.06638)
*Zhihao Wen,Wenkang Wei,Yuan Fang,Xingtong Yu,Hui Zhang,Weicheng Zhu,Xin Zhang*

Main category: cs.CV

TL;DR: StaR-KVQA supervises multimodal LLMs with structured, path-grounded reasoning traces (symbolic relation paths plus natural-language explanations) to perform knowledge-based VQA without external retrieval, improving both accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Implicit-knowledge KVQA relies on the MLLM as the sole knowledge source, but standard SFT yields poor generalization and inconsistent, non-verifiable justifications due to lack of explicit reasoning supervision.

Method: Construct offline, with one open-source MLLM, dual symbolic relation paths that connect image-grounded entities to answers and pair them with path-grounded natural-language explanations; select high-quality traces to build a trace-enriched dataset; fine-tune the same MLLM via structured self-distillation to align generation with these traces; perform single-pass autoregressive inference without external retrievers, verifiers, or curated KBs.

Result: Across benchmarks, StaR-KVQA raises both answer accuracy and interpretability, achieving up to +11.3% accuracy gain on OK-VQA over the strongest baseline and showing robust cross-domain generalization.

Conclusion: Supervising transparent, verifiable reasoning traces enables MLLMs to perform IK-KVQA more accurately and interpretably without external knowledge sources, indicating a scalable path to better grounded reasoning and transfer.

Abstract: Knowledge-based Visual Question Answering (KVQA) requires models to ground
entities in images and reason over factual knowledge. We study its
implicit-knowledge variant, IK-KVQA, where a multimodal large language model
(MLLM) is the sole knowledge source, without external retrieval. Yet, MLLMs
lack explicit reasoning supervision and produce inconsistent justifications,
and generalize poorly after standard supervised fine-tuning (SFT). We present
StaR-KVQA (Structured Reasoning Traces for IK-KVQA), which supervises
structured traces - dual symbolic relation paths plus path-grounded
natural-language explanations - so that reasoning becomes transparent and
verifiable. With one open-source MLLM, StaR-KVQA constructs and selects
path-grounded reasoning traces to form a trace-enriched dataset, then
fine-tunes via structured self-distillation to align generation with
supervision; no external retrievers, verifiers, or curated knowledge bases
(KBs) are used, traces are built offline, and inference is a single
autoregressive pass. Across benchmarks, StaR-KVQA improves both accuracy and
interpretability, achieving up to +11.3% higher answer accuracy on OK-VQA over
the strongest baseline while exhibiting robust cross-domain generalization.

</details>


### [39] [Automated Neural Architecture Design for Industrial Defect Detection](https://arxiv.org/abs/2510.06669)
*Yuxi Liu,Yunfeng Ma,Yi Tang,Min Liu,Shuai Jiang,Yaonan Wang*

Main category: cs.CV

TL;DR: AutoNAD is an automated neural architecture design framework for surface defect detection that jointly searches convolution, transformer, and MLP components, adds cross weight sharing and a searchable multi-level feature aggregation module, and uses a latency-aware prior to balance accuracy and speed; it shows strong results on three industrial datasets and is deployed in a defect detection platform.


<details>
  <summary>Details</summary>
Motivation: Industrial surface defect detection suffers from large intra-class variation and high inter-class similarity. Manually designed models require heavy trial-and-error and struggle to capture both fine local details and long-range context while remaining efficient. An automated, accuracy–efficiency-aware architecture search is needed.

Method: A NAS framework (AutoNAD) that searches a hybrid space spanning convolutions, transformers, and MLPs. It introduces: (1) cross weight sharing to accelerate supernet convergence and boost subnet quality; (2) a searchable multi-level feature aggregation module (MFAM) for multi-scale feature learning; and (3) a latency-aware prior to steer the search toward deployable architectures.

Result: Validated on three industrial defect datasets with improved detection accuracy and runtime efficiency (exact metrics not provided in the abstract) and integrated into a practical defect imaging/detection platform. Code to be released at the provided repository.

Conclusion: AutoNAD reduces manual design cost and effectively addresses intra-class variation and inter-class similarity by combining local and global modeling within a latency-aware NAS. Its hybrid, efficiency-guided architectures make it suitable for real-world industrial deployment.

Abstract: Industrial surface defect detection (SDD) is critical for ensuring product
quality and manufacturing reliability. Due to the diverse shapes and sizes of
surface defects, SDD faces two main challenges: intraclass difference and
interclass similarity. Existing methods primarily utilize manually designed
models, which require extensive trial and error and often struggle to address
both challenges effectively. To overcome this, we propose AutoNAD, an automated
neural architecture design framework for SDD that jointly searches over
convolutions, transformers, and multi-layer perceptrons. This hybrid design
enables the model to capture both fine-grained local variations and long-range
semantic context, addressing the two key challenges while reducing the cost of
manual network design. To support efficient training of such a diverse search
space, AutoNAD introduces a cross weight sharing strategy, which accelerates
supernet convergence and improves subnet performance. Additionally, a
searchable multi-level feature aggregation module (MFAM) is integrated to
enhance multi-scale feature learning. Beyond detection accuracy, runtime
efficiency is essential for industrial deployment. To this end, AutoNAD
incorporates a latency-aware prior to guide the selection of efficient
architectures. The effectiveness of AutoNAD is validated on three industrial
defect datasets and further applied within a defect imaging and detection
platform. Code will be available at https://github.com/Yuxi104/AutoNAD.

</details>


### [40] [Heptapod: Language Modeling on Visual Signals](https://arxiv.org/abs/2510.06673)
*Yongxin Zhu,Jiawei Chen,Yuanzhe Chen,Zhuo Chen,Dongya Jia,Jian Cong,Xiaobin Zhuang,Yuping Wang,Yuxuan Wang*

Main category: cs.CV

TL;DR: Heptapod is a causal Transformer for image generation that predicts a full 2D token distribution at each step using a reconstruction-focused visual tokenizer, avoiding CFG and semantic tokenizers; it achieves FID 2.70 on ImageNet, surpassing prior causal autoregressive methods.


<details>
  <summary>Details</summary>
Motivation: Bring the principled simplicity of language modeling to vision while overcoming autoregressive image models’ difficulty with global semantics and their reliance on classifier-free guidance or semantic tokenizers. Unify sequential AR training with holistic self-supervised objectives to better capture image semantics.

Method: Use causal attention in a Transformer with a reconstruction-focused (non-semantic) visual tokenizer. Train with a next-2D distribution prediction objective: at each timestep the model predicts distributions over the entire 2D spatial grid, marrying AR sequencing with MAE-like holistic learning, and dispensing with CFG and semantic tokenizers.

Result: On ImageNet generation, Heptapod attains FID 2.70, substantially outperforming previous causal autoregressive approaches, indicating improved semantic fidelity and sample quality.

Conclusion: Next-2D distribution prediction provides a principled bridge between AR language modeling and masked autoencoding for vision, achieving strong generative performance without CFG or semantic tokenizers and motivating a reevaluation of language-modeling paradigms for visual signals.

Abstract: We introduce Heptapod, an image autoregressive model that adheres to the
foundational principles of language modeling. Heptapod employs \textbf{causal
attention}, \textbf{eliminates reliance on CFG}, and \textbf{eschews the trend
of semantic tokenizers}. Our key innovation is \textit{next 2D distribution
prediction}: a causal Transformer with reconstruction-focused visual tokenizer,
learns to predict the distribution over the entire 2D spatial grid of images at
each timestep. This learning objective unifies the sequential modeling of
autoregressive framework with the holistic self-supervised learning of masked
autoencoding, enabling the model to capture comprehensive image semantics via
generative training. On the ImageNet generation benchmark, Heptapod achieves an
FID of $2.70$, significantly outperforming previous causal autoregressive
approaches. We hope our work inspires a principled rethinking of language
modeling on visual signals and beyond.

</details>


### [41] [DreamOmni2: Multimodal Instruction-based Editing and Generation](https://arxiv.org/abs/2510.06679)
*Bin Xia,Bohao Peng,Yuechen Zhang,Junjia Huang,Jiyang Liu,Jingyao Li,Haoru Tan,Sitong Wu,Chengyao Wang,Yitong Wang,Xinglong Wu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: DreamOmni2 introduces multimodal (text+image) instruction-based editing and generation that handle both concrete and abstract concepts, providing a data pipeline, a multi-image encoding scheme, joint VLM training, and new benchmarks, achieving strong experimental results.


<details>
  <summary>Details</summary>
Motivation: Current instruction-based image editing lacks precise control from text alone and often needs reference images; subject-driven generation focuses on concrete entities and ignores abstract concepts. There is a practical need for systems that accept multimodal instructions and reason over both concrete and abstract ideas.

Method: They define two new tasks—multimodal instruction-based editing and generation—and build DreamOmni2. Data: (1) feature mixing to synthesize extraction data for abstract and concrete concepts; (2) generate multimodal editing training data using editing and extraction models; (3) further use the extraction model to create additional multimodal training data. Framework: introduce index encoding and position-encoding shifts to disambiguate multiple input images and avoid pixel confusion; perform joint training with a vision-language model (VLM) and the gen/edit model to better follow complex instructions. They also construct comprehensive benchmarks for these tasks.

Result: On the proposed benchmarks, DreamOmni2 achieves impressive performance (reported qualitatively as strong/improved results). Models and code will be released.

Conclusion: By unifying multimodal instructions and supporting abstract and concrete concept manipulation, DreamOmni2 expands the scope and practicality of image editing and generation. Its data synthesis pipeline, multi-image encoding strategy, joint VLM training, and new benchmarks collectively deliver strong performance and lay groundwork for future research.

Abstract: Recent advancements in instruction-based image editing and subject-driven
generation have garnered significant attention, yet both tasks still face
limitations in meeting practical user needs. Instruction-based editing relies
solely on language instructions, which often fail to capture specific editing
details, making reference images necessary. Meanwhile, subject-driven
generation is limited to combining concrete objects or people, overlooking
broader, abstract concepts. To address these challenges, we propose two novel
tasks: multimodal instruction-based editing and generation. These tasks support
both text and image instructions and extend the scope to include both concrete
and abstract concepts, greatly enhancing their practical applications. We
introduce DreamOmni2, tackling two primary challenges: data creation and model
framework design. Our data synthesis pipeline consists of three steps: (1)
using a feature mixing method to create extraction data for both abstract and
concrete concepts, (2) generating multimodal instruction-based editing training
data using the editing and extraction models, and (3) further applying the
extraction model to create training data for multimodal instruction-based
editing. For the framework, to handle multi-image input, we propose an index
encoding and position encoding shift scheme, which helps the model distinguish
images and avoid pixel confusion. Additionally, we introduce joint training
with the VLM and our generation/editing model to better process complex
instructions. In addition, we have proposed comprehensive benchmarks for these
two new tasks to drive their development. Experiments show that DreamOmni2 has
achieved impressive results. Models and codes will be released.

</details>


### [42] [Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion](https://arxiv.org/abs/2510.06687)
*Jie Luo,Yuxuan Jiang,Xin Jin,Mingyu Liu,Yihui Fan*

Main category: cs.CV

TL;DR: They introduce the first light field (LF) + LiDAR multimodal semantic segmentation dataset and a fusion network (Mlpfseg) with feature completion and depth-aware attention, yielding modest but consistent mIoU gains over image-only (+1.71) and LiDAR-only (+2.38), especially under occlusion.


<details>
  <summary>Details</summary>
Motivation: Semantic segmentation in autonomous driving struggles with occlusion. LF images and LiDAR provide complementary angular/depth cues, yet are hard to fuse due to limited LF viewpoint diversity and modality gaps (e.g., density mismatch between sparse points and dense pixels).

Method: 1) Build a new dataset pairing LF imagery with LiDAR point clouds. 2) Propose Mlpfseg, a joint segmentation model for both images and point clouds. It includes: (a) a feature completion module that differentially reconstructs point-cloud feature maps to mitigate density mismatch and improve cross-modal alignment; (b) a depth perception module that reinforces attention to enhance occlusion awareness and depth reasoning; 3) Fuse features to segment both modalities simultaneously.

Result: On their dataset, Mlpfseg surpasses image-only segmentation by +1.71 mIoU and point cloud-only by +2.38 mIoU, indicating better robustness, particularly for occluded objects.

Conclusion: Combining LF and LiDAR via feature completion and depth-aware attention improves multimodal fusion and occlusion handling. The new dataset and model provide a foundation for more robust autonomous-driving segmentation under challenging visibility.

Abstract: Semantic segmentation serves as a cornerstone of scene understanding in
autonomous driving but continues to face significant challenges under complex
conditions such as occlusion. Light field and LiDAR modalities provide
complementary visual and spatial cues that are beneficial for robust
perception; how- ever, their effective integration is hindered by limited
viewpoint diversity and inherent modality discrepancies. To address these
challenges, the first multimodal semantic segmentation dataset integrating
light field data and point cloud data is proposed. Based on this dataset, we
proposed a multi-modal light field point-cloud fusion segmentation
network(Mlpfseg), incorporating feature completion and depth perception to
segment both camera images and LiDAR point clouds simultaneously. The feature
completion module addresses the density mismatch between point clouds and image
pixels by performing differential re- construction of point-cloud feature maps,
enhancing the fusion of these modalities. The depth perception module improves
the segmentation of occluded objects by reinforcing attention scores for better
occlusion awareness. Our method outperforms image- only segmentation by 1.71
Mean Intersection over Union(mIoU) and point cloud-only segmentation by 2.38
mIoU, demonstrating its effectiveness.

</details>


### [43] [SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis](https://arxiv.org/abs/2510.06694)
*Jipeng Lyu,Jiahua Dong,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: SCas4D introduces a hierarchical, cascaded deformation optimization on top of 3D Gaussian Splatting, refining from parts to points to achieve fast, accurate dynamic scene modeling for tracking and novel-view synthesis.


<details>
  <summary>Details</summary>
Motivation: Dynamic scene modeling needs both accurate deformation capture and high computational efficiency; existing methods struggle to balance precision with training cost for tasks like tracking and novel-view synthesis.

Method: A cascaded optimization framework (SCas4D) that exploits structural regularities in dynamic scenes: groups of Gaussians share similar transformations. It progressively optimizes deformations from coarse part-level to fine point-level within a 3D Gaussian Splatting representation.

Result: Converges within ~100 iterations per time frame; matches prior methods’ quality using only about 1/20 of their training iterations; demonstrates strong performance on self-supervised articulated object segmentation, novel view synthesis, and dense point tracking.

Conclusion: Leveraging hierarchical deformation structure via cascaded refinements yields efficient and accurate dynamic scene modeling, substantially cutting training iterations while maintaining competitive visual and tracking performance.

Abstract: Persistent dynamic scene modeling for tracking and novel-view synthesis
remains challenging due to the difficulty of capturing accurate deformations
while maintaining computational efficiency. We propose SCas4D, a cascaded
optimization framework that leverages structural patterns in 3D Gaussian
Splatting for dynamic scenes. The key idea is that real-world deformations
often exhibit hierarchical patterns, where groups of Gaussians share similar
transformations. By progressively refining deformations from coarse part-level
to fine point-level, SCas4D achieves convergence within 100 iterations per time
frame and produces results comparable to existing methods with only
one-twentieth of the training iterations. The approach also demonstrates
effectiveness in self-supervised articulated object segmentation, novel view
synthesis, and dense point tracking tasks.

</details>


### [44] [Evaluating LLMs for Historical Document OCR: A Methodological Framework for Digital Humanities](https://arxiv.org/abs/2510.06743)
*Maria Levchenko*

Main category: cs.CV

TL;DR: Proposes and validates a tailored evaluation framework for LLM-based historical OCR, introducing HCPR and AIR metrics, contamination/stability protocols, and shows top LLMs beat traditional OCR yet anachronistically insert archaic characters; naïve post-correction worsens outcomes.


<details>
  <summary>Details</summary>
Motivation: Digital humanists increasingly rely on LLMs for digitizing historical texts, but existing OCR metrics miss temporally specific errors and biases (e.g., anachronistic character use) and do not control for training-data contamination—issues that threaten the reliability of historical corpora.

Method: Design an evaluation methodology for LLM-based historical OCR focused on diplomatic transcription. Use 18th‑century Russian Civil font materials; define Historical Character Preservation Rate (HCPR) and Archaic Insertion Rate (AIR); implement contamination-control and stability-testing protocols; benchmark 12 multimodal LLMs against conventional OCR systems.

Result: Gemini and Qwen multimodal models outperform traditional OCR on overall transcription quality but exhibit over-historicization by inserting characters from incorrect periods; post-OCR correction pipelines reduce, rather than improve, accuracy; the proposed metrics and protocols reveal systematic temporal biases not captured by standard measures.

Conclusion: The framework and metrics enable nuanced assessment of LLM-based historical OCR, highlighting risks of anachronistic outputs and the counterproductive effects of generic post-correction. Practitioners should use the proposed protocols for model selection and quality control when building historical corpora.

Abstract: Digital humanities scholars increasingly use Large Language Models for
historical document digitization, yet lack appropriate evaluation frameworks
for LLM-based OCR. Traditional metrics fail to capture temporal biases and
period-specific errors crucial for historical corpus creation. We present an
evaluation methodology for LLM-based historical OCR, addressing contamination
risks and systematic biases in diplomatic transcription. Using 18th-century
Russian Civil font texts, we introduce novel metrics including Historical
Character Preservation Rate (HCPR) and Archaic Insertion Rate (AIR), alongside
protocols for contamination control and stability testing. We evaluate 12
multimodal LLMs, finding that Gemini and Qwen models outperform traditional OCR
while exhibiting over-historicization: inserting archaic characters from
incorrect historical periods. Post-OCR correction degrades rather than improves
performance. Our methodology provides digital humanities practitioners with
guidelines for model selection and quality assessment in historical corpus
digitization.

</details>


### [45] [DeRainMamba: A Frequency-Aware State Space Model with Detail Enhancement for Image Deraining](https://arxiv.org/abs/2510.06746)
*Zhiliang Zhu,Tao Zeng,Tao Yang,Guoliang Luo,Jiyong Zeng*

Main category: cs.CV

TL;DR: DeRainMamba augments Mamba-based state-space models with a frequency-aware module and multi-directional convolutions to better separate rain streaks from true image details, achieving state-of-the-art deraining quality with fewer parameters and lower compute.


<details>
  <summary>Details</summary>
Motivation: Mamba-based models are efficient for sequence modeling but struggle with fine-grained spatial details and lack frequency-domain awareness. Single-image deraining critically relies on distinguishing rain streaks (often high-frequency) from genuine high-frequency textures; existing methods trade off removal vs. detail preservation.

Method: Introduce two components within a state-space framework: (1) Frequency-Aware State-Space Module (FASSM) that uses Fourier-domain processing to differentiate and balance rain removal and detail retention; (2) Multi-Directional Perception Convolution (MDPConv) to capture anisotropic gradient features and fuse multiple convolutional branches for local structure restoration. Combined into DeRainMamba.

Result: Across four public deraining benchmarks, DeRainMamba consistently surpasses prior methods in PSNR and SSIM while using fewer parameters and lower computational cost (no exact numbers provided in the abstract).

Conclusion: Coupling frequency-domain modeling with spatial detail enhancement inside a state-space (Mamba) architecture is an effective recipe for single-image deraining, improving quality-efficiency trade-offs and supporting more reliable downstream vision tasks.

Abstract: Image deraining is crucial for improving visual quality and supporting
reliable downstream vision tasks. Although Mamba-based models provide efficient
sequence modeling, their limited ability to capture fine-grained details and
lack of frequency-domain awareness restrict further improvements. To address
these issues, we propose DeRainMamba, which integrates a Frequency-Aware
State-Space Module (FASSM) and Multi-Directional Perception Convolution
(MDPConv). FASSM leverages Fourier transform to distinguish rain streaks from
high-frequency image details, balancing rain removal and detail preservation.
MDPConv further restores local structures by capturing anisotropic gradient
features and efficiently fusing multiple convolution branches. Extensive
experiments on four public benchmarks demonstrate that DeRainMamba consistently
outperforms state-of-the-art methods in PSNR and SSIM, while requiring fewer
parameters and lower computational costs. These results validate the
effectiveness of combining frequency-domain modeling and spatial detail
enhancement within a state-space framework for single image deraining.

</details>


### [46] [OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot](https://arxiv.org/abs/2510.06751)
*Junhan Zhu,Hesong Wang,Mingluo Su,Zefang Wang,Huan Wang*

Main category: cs.CV

TL;DR: OBS-Diff is a training-free, one-shot pruning framework that adapts Optimal Brain Surgeon to large text-to-image diffusion models with timestep-aware Hessian weighting and group-wise sequential pruning, achieving faster inference with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: Large text-to-image diffusion models are computationally expensive. Existing one-shot pruning methods for standard networks fail to account for the iterative denoising process where pruning errors can accumulate across timesteps. There is a need for an accurate, training-free compression method tailored to diffusion dynamics and compatible with multiple sparsity types.

Method: (1) Adapt Optimal Brain Surgeon (OBS) to diffusion architectures and multiple pruning granularities (unstructured, N:M semi-structured, and structured pruning of MHA heads and FFN neurons). (2) Introduce a timestep-aware Hessian that weights earlier timesteps more (logarithmic decrease) to align pruning importance with error accumulation dynamics. (3) Use a computationally efficient group-wise sequential pruning strategy to amortize calibration costs. One-shot, training-free pruning across the model.

Result: Experiments demonstrate state-of-the-art one-shot pruning for diffusion models, providing inference acceleration while maintaining visual quality with minimal degradation.

Conclusion: By aligning OBS-based pruning with the iterative nature of diffusion via timestep-aware Hessians and efficient group-wise pruning, OBS-Diff delivers flexible, accurate, training-free compression that preserves image quality and speeds up inference.

Abstract: Large-scale text-to-image diffusion models, while powerful, suffer from
prohibitive computational cost. Existing one-shot network pruning methods can
hardly be directly applied to them due to the iterative denoising nature of
diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel
one-shot pruning framework that enables accurate and training-free compression
of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff
revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex
architectures of modern diffusion models and supporting diverse pruning
granularity, including unstructured, N:M semi-structured, and structured (MHA
heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the
iterative dynamics of the diffusion process, by examining the problem from an
error-accumulation perspective, we propose a novel timestep-aware Hessian
construction that incorporates a logarithmic-decrease weighting scheme,
assigning greater importance to earlier timesteps to mitigate potential error
accumulation; (iii) Furthermore, a computationally efficient group-wise
sequential pruning strategy is proposed to amortize the expensive calibration
process. Extensive experiments show that OBS-Diff achieves state-of-the-art
one-shot pruning for diffusion models, delivering inference acceleration with
minimal degradation in visual quality.

</details>


### [47] [Transforming Noise Distributions with Histogram Matching: Towards a Single Denoiser for All](https://arxiv.org/abs/2510.06757)
*Sheng Fu,Junchao Zhang,Kailun Yang*

Main category: cs.CV

TL;DR: Turn arbitrary noise into Gaussian via histogram matching and iterate this with a Gaussian denoiser, enabling one supervised Gaussian denoiser to handle many out-of-distribution noise types and real-world noise.


<details>
  <summary>Details</summary>
Motivation: Supervised Gaussian denoisers fail on out-of-distribution noise because different noise sources have distinct distributions (signal dependence, channel correlations, spatial correlations). A universal, training-free way to map diverse noises into a known Gaussian distribution would unlock reuse of existing denoisers.

Method: Preprocess noisy images to morph the noise toward a target Gaussian with known variance using: (1) local histogram matching for signal-dependent noise, (2) intrapatch permutation to address channel-related correlations, and (3) frequency-domain histogram matching with pixel-shuffle downsampling to break spatial correlation. Establish an iterative loop where denoising output informs and refines the noise transformation, progressively aligning the residual noise with the true noise statistics before feeding it again to the Gaussian denoiser.

Result: A single supervised Gaussian denoiser, combined with the proposed transformations, effectively handles diverse synthetic noises (Poisson, salt-and-pepper, repeating pattern) and complex real-world noise, showing superior generalization in extensive experiments.

Conclusion: Iteratively transforming heterogeneous noise to a canonical Gaussian form is an effective, general strategy that upgrades off-the-shelf Gaussian denoisers to robust, broad-spectrum denoisers without retraining.

Abstract: Supervised Gaussian denoisers exhibit limited generalization when confronted
with out-of-distribution noise, due to the diverse distributional
characteristics of different noise types. To bridge this gap, we propose a
histogram matching approach that transforms arbitrary noise towards a target
Gaussian distribution with known intensity. Moreover, a mutually reinforcing
cycle is established between noise transformation and subsequent denoising.
This cycle progressively refines the noise to be converted, making it
approximate the real noise, thereby enhancing the noise transformation effect
and further improving the denoising performance. We tackle specific noise
complexities: local histogram matching handles signal-dependent noise,
intrapatch permutation processes channel-related noise, and frequency-domain
histogram matching coupled with pixel-shuffle down-sampling breaks spatial
correlation. By applying these transformations, a single Gaussian denoiser
gains remarkable capability to handle various out-of-distribution noises,
including synthetic noises such as Poisson, salt-and-pepper and repeating
pattern noises, as well as complex real-world noises. Extensive experiments
demonstrate the superior generalization and effectiveness of our method.

</details>


### [48] [A deep multiple instance learning approach based on coarse labels for high-resolution land-cover mapping](https://arxiv.org/abs/2510.06769)
*Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: Train pixel-level land-cover classifiers from high-resolution imagery using only weak, low-resolution labels by framing the task as Deep Multiple Instance Learning with flexible pooling, including a Positive-Unlabeled variant; achieves better results than standard weak-label training on the GRSS DFC 2020 dataset.


<details>
  <summary>Details</summary>
Motivation: High-resolution land-cover mapping needs many accurate labels, which are costly. However, abundant weak labels exist from low-resolution or outdated products. The goal is to leverage these weak labels to learn accurate pixel-level classifiers for high-resolution imagery.

Method: Use Sentinel-2 high-resolution images with weak MODIS-derived patch labels. Model pixels as instances within patches and employ Deep Multiple Instance Learning. Introduce flexible pooling layers to map pixel-level predictions to patch-level supervision. Reframe MIL in two settings: (1) multi-class, where the patch label corresponds to the majority class within the patch; (2) multi-label presence-only, where the provided patch label indicates at least one pixel belongs to that class while others remain unlabeled, trained via Positive-Unlabeled Learning. The network learns pixel-level multi-class predictions implicitly through patch-level losses.

Result: On the 2020 IEEE GRSS Data Fusion Contest dataset, the proposed framework outperforms standard training strategies that rely on weak labels, demonstrating improved effectiveness in learning from low-resolution supervision.

Conclusion: Weak low-resolution labels can effectively supervise high-resolution pixel-level land-cover mapping when cast as DMIL with flexible pooling and PUL for presence-only labels. This approach learns fine-grained semantics without direct pixel annotations and yields superior performance to baseline weak-label training.

Abstract: The quantity and the quality of the training labels are central problems in
high-resolution land-cover mapping with machine-learning-based solutions. In
this context, weak labels can be gathered in large quantities by leveraging on
existing low-resolution or obsolete products. In this paper, we address the
problem of training land-cover classifiers using high-resolution imagery (e.g.,
Sentinel-2) and weak low-resolution reference data (e.g., MODIS -derived
land-cover maps). Inspired by recent works in Deep Multiple Instance Learning
(DMIL), we propose a method that trains pixel-level multi-class classifiers and
predicts low-resolution labels (i.e., patch-level classification), where the
actual high-resolution labels are learned implicitly without direct
supervision. This is achieved with flexible pooling layers that are able to
link the semantics of the pixels in the high-resolution imagery to the
low-resolution reference labels. Then, the Multiple Instance Learning (MIL)
problem is re-framed in a multi-class and in a multi-label setting. In the
former, the low-resolution annotation represents the majority of the pixels in
the patch. In the latter, the annotation only provides us information on the
presence of one of the land-cover classes in the patch and thus multiple labels
can be considered valid for a patch at a time, whereas the low-resolution
labels provide us only one label. Therefore, the classifier is trained with a
Positive-Unlabeled Learning (PUL) strategy. Experimental results on the 2020
IEEE GRSS Data Fusion Contest dataset show the effectiveness of the proposed
framework compared to standard training strategies.

</details>


### [49] [TTRV: Test-Time Reinforcement Learning for Vision Language Models](https://arxiv.org/abs/2510.06783)
*Akshit Singh,Shyam Marjit,Wei Lin,Paul Gavrikov,Serena Yeung-Levy,Hilde Kuehne,Rogerio Feris,Sivan Doveh,James Glass,M. Jehanzeb Mirza*

Main category: cs.CV

TL;DR: TTRV is a label-free, test-time reinforcement learning approach that adapts a vision-language model on each inference by rewarding frequent and low-entropy outputs via an enhanced GRPO scheme, yielding large gains on recognition and VQA and even surpassing GPT-4o on image recognition benchmarks.


<details>
  <summary>Details</summary>
Motivation: Most RL-based reward extraction for VLMs depends on labeled data and predefined splits, unlike human learning from the environment. The goal is to remove this reliance and boost performance by adapting at inference without labels.

Method: Extend Group Relative Policy Optimization to test time: for each test sample, generate multiple outputs, compute rewards based on the frequency of the base model’s outputs (mode-seeking) and encourage low entropy in the empirical output distribution to control diversity/consistency, then update the model on-the-fly without labeled supervision.

Result: Consistent improvements across 16 datasets: up to 52.4% (object recognition) and 29.8% (VQA); average gains of 24.6% and 10.0%, respectively. With InternVL 8B on image recognition, TTRV averages 2.3% above GPT-4o over 8 benchmarks. Even adapting on a single unlabeled test example yields up to 5.5% gains in recognition.

Conclusion: Label-free test-time RL with frequency- and entropy-based rewards can substantially enhance VLMs, matching or surpassing top proprietary systems and remaining effective even under extreme data scarcity.

Abstract: Existing methods for extracting reward signals in Reinforcement Learning
typically rely on labeled data and dedicated training splits, a setup that
contrasts with how humans learn directly from their environment. In this work,
we propose TTRV to enhance vision language understanding by adapting the model
on the fly at inference time, without the need for any labeled data.
Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework
by designing rewards based on the frequency of the base model's output, while
inferring on each test sample multiple times. Further, we also propose to
control the diversity of the model's output by simultaneously rewarding the
model for obtaining low entropy of the output empirical distribution. Our
approach delivers consistent gains across both object recognition and visual
question answering (VQA), with improvements of up to 52.4% and 29.8%,
respectively, and average boosts of 24.6% and 10.0% across 16
datasets.Remarkably, on image recognition, TTRV applied to InternVL 8B
surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining
highly competitive on VQA, demonstrating that test-time reinforcement learning
can match or exceed the strongest proprietary models. Finally, we find many
interesting properties of test-time RL for VLMs: for example, even in extremely
data-constrained scenarios, where adaptation is performed on a single randomly
chosen unlabeled test example, TTRV still yields non-trivial improvements of up
to 5.5% in recognition tasks.

</details>


### [50] [Extreme Amodal Face Detection](https://arxiv.org/abs/2510.06791)
*Changlin Song,Yunzhong Hou,Michael Randall Barnes,Rahul Shome,Dylan Campbell*

Main category: cs.CV

TL;DR: They introduce a single-image, sample-free, heatmap-based detector that uses contextual cues with a selective coarse-to-fine decoder to locate faces (and objects) outside the image frame, outperforming heavier generative approaches.


<details>
  <summary>Details</summary>
Motivation: Detecting objects that are entirely outside the current field-of-view (but would be visible with a broader view) matters for safety and privacy (e.g., anticipating people just off-camera). Prior work needs video sequences for interpolation or slow, sampling-based generative models. There is a need for an efficient, single-image solution that leverages scene context.

Method: Formulate “extreme amodal detection” and instantiate it for faces without class-specific tailoring. Use a heatmap-based predictor with a selective coarse-to-fine decoder to infer the 2D locations of out-of-frame objects from a single image by exploiting contextual cues, avoiding sampling/generative completion.

Result: On this newly defined task, their method achieves strong performance and surpasses less efficient generative baselines while being computationally more efficient.

Conclusion: Context from a single image is sufficient to infer likely locations of unseen, out-of-frame objects. A heatmap-based, coarse-to-fine, sample-free detector provides an effective and efficient baseline for extreme amodal detection, with demonstrated gains over generative approaches.

Abstract: Extreme amodal detection is the task of inferring the 2D location of objects
that are not fully visible in the input image but are visible within an
expanded field-of-view. This differs from amodal detection, where the object is
partially visible within the input image, but is occluded. In this paper, we
consider the sub-problem of face detection, since this class provides
motivating applications involving safety and privacy, but do not tailor our
method specifically to this class. Existing approaches rely on image sequences
so that missing detections may be interpolated from surrounding frames or make
use of generative models to sample possible completions. In contrast, we
consider the single-image task and propose a more efficient, sample-free
approach that makes use of the contextual cues from the image to infer the
presence of unseen faces. We design a heatmap-based extreme amodal object
detector that addresses the problem of efficiently predicting a lot (the
out-of-frame region) from a little (the image) with a selective coarse-to-fine
decoder. Our method establishes strong results for this new task, even
outperforming less efficient generative approaches.

</details>


### [51] [VA-Adapter: Adapting Ultrasound Foundation Model to Echocardiography Probe Guidance](https://arxiv.org/abs/2510.06809)
*Teng Wang,Haojun Jiang,Yuxuan Wang,Zhenguo Sun,Shiji Song,Gao Huang*

Main category: cs.CV

TL;DR: They propose a compact, parameter‑efficient Vision‑Action Adapter that plugs into an ultrasound foundation model to turn it into a real‑time probe guidance system, encoding vision–action sequences and outperforming strong baselines with minimal fine‑tuning.


<details>
  <summary>Details</summary>
Motivation: High‑quality cardiac ultrasound imaging is essential for diagnosis, but skilled operators are scarce and probe manipulation is difficult. Foundation models encode broad medical knowledge, yet are not directly suited for sequential, action‑conditioned guidance. The work aims to transfer this knowledge to real‑time probe control to help junior sonographers acquire diagnostic views.

Method: Introduce a Vision‑Action Adapter (VA‑Adapter) that augments a pre‑trained ultrasound foundation model’s image encoder to process vision–action sequences and perform sequential reasoning. The adapter is parameter‑efficient, enabling learning of probe adjustment strategies via fine‑tuning only a small subset of parameters.

Result: Across extensive experiments, the VA‑Adapter reportedly surpasses strong probe guidance models in performance (details not provided in the abstract). Code will be released upon acceptance.

Conclusion: Adapting foundation models with a lightweight, sequence‑aware adapter enables accurate, real‑time probe guidance while keeping training efficient. This approach can help mitigate operator shortages by improving the acquisition of high‑quality cardiac ultrasound images.

Abstract: Echocardiography is a critical tool for detecting heart diseases. Recently,
ultrasound foundation models have demonstrated remarkable capabilities in
cardiac ultrasound image analysis. However, obtaining high-quality ultrasound
images is a prerequisite for accurate diagnosis. Due to the exceptionally high
operational difficulty of cardiac ultrasound, there is a shortage of highly
skilled personnel, which hinders patients from receiving timely examination
services. In this paper, we aim to adapt the medical knowledge learned by
foundation models from vast datasets to the probe guidance task, which is
designed to provide real-time operational recommendations for junior
sonographers to acquire high-quality ultrasound images. Moreover, inspired by
the practice where experts optimize action decisions based on past
explorations, we meticulously design a parameter-efficient Vision-Action
Adapter (VA-Adapter) to enable foundation model's image encoder to encode
vision-action sequences, thereby enhancing guidance performance. With built-in
sequential reasoning capabilities in a compact design, the VA-Adapter enables a
pre-trained ultrasound foundation model to learn precise probe adjustment
strategies by fine-tuning only a small subset of parameters. Extensive
experiments demonstrate that the VA-Adapter can surpass strong probe guidance
models. Our code will be released after acceptance.

</details>


### [52] [Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking](https://arxiv.org/abs/2510.06820)
*Mitchell Keren Taraday,Shahaf Wagner,Chaim Baskin*

Main category: cs.CV

TL;DR: EDJE is an efficient joint vision–language reranker that precomputes and compresses image tokens offline, allowing a compact online joint encoder to rerank image–text pairs at high throughput while matching prior retrieval accuracy.


<details>
  <summary>Details</summary>
Motivation: Embedding-based models (e.g., CLIP) enable fast candidate retrieval but lack the accuracy of joint-encoder rerankers. Existing joint encoders like BLIP are impractical at scale due to the heavy, online visual feature-extraction step. The goal is to make joint reranking feasible at web scale without sacrificing quality.

Method: Precompute per-image vision tokens offline and compress them with a lightweight attention-based adapter. At query time, run a small joint encoder only over the compressed visual tokens plus the text, drastically cutting compute and memory. This preserves joint reasoning while avoiding expensive per-query visual backbone passes.

Result: EDJE achieves 50k image–text pair evaluations per second and requires 49 kB storage per image, while matching prior state-of-the-art on Flickr (zero-shot) and COCO (fine-tuned) retrieval benchmarks. It reduces online compute and storage requirements and enables high-throughput inference.

Conclusion: Decoupling heavy visual feature extraction from online inference makes joint-encoder reranking practical at scale. EDJE maintains strong retrieval performance with minimal storage and compute, offering a deployable alternative to traditional joint encoders. Code and checkpoints will be released.

Abstract: Multimodal retrieval still leans on embedding-based models like CLIP for fast
vector search over pre-computed image embeddings. Yet, unlike text retrieval,
where joint-encoder rerankers are standard, comparable vision--language
rerankers are largely absent. We find that seminal joint encoders such as BLIP
are severely bottlenecked by an expensive visual feature-extraction stage,
preventing practical deployment at scale. Motivated by this bottleneck, we
introduce EDJE, an Efficient Discriminative Joint Encoder that precomputes
vision tokens offline and compresses them via a lightweight attention-based
adapter, so online inference runs only a compact joint encoder over a small set
of visual tokens plus the text. EDJE preserves strong retrieval performance
while drastically reducing storage and online compute, enabling high-throughput
inference. Specifically, EDJE processes 50k image--text pairs/second while
requiring 49kB of disk storage per image, matching prior art on Flickr
(zero-shot) and COCO (fine-tuned) retrieval. The implementation and checkpoints
will be made publicly available shortly.

</details>


### [53] [StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance](https://arxiv.org/abs/2510.06827)
*Jaeseok Jeong,Junho Kim,Gayoung Lee,Yunjey Choi,Youngjung Uh*

Main category: cs.CV

TL;DR: StyleKeeper reduces content leakage in visual prompting for diffusion-based text-to-image generation by extending classifier-free guidance with swapped self-attention and introducing Negative Visual Query Guidance (NVQG), yielding better style transfer while preserving text-specified content.


<details>
  <summary>Details</summary>
Motivation: Visual style prompts often leak unwanted content (objects/layout) into generated images, undermining control: users want only style transferred, not scene/content from the reference image.

Method: (1) Extend classifier-free guidance (CFG) to operate with swapping self-attention signals. (2) Negative Visual Query Guidance (NVQG): construct a negative guidance term by simulating content leakage—swap queries (Q) in self-attention using the visual style prompt (instead of swapping keys/values)—and use its negative score to suppress leakage. Provide practical procedures to use real images as style prompts.

Result: Across diverse styles and text prompts, the approach significantly lowers content leakage, better matches the text description, and more faithfully reflects the reference style than prior visual prompting baselines; qualitative and quantitative evaluations show superiority. Code released.

Conclusion: A simple, effective, and easily integrable attention-guided negative guidance method improves style control without carrying over unwanted content from style images; practical for real-image prompts and ready for adoption via open-source code.

Abstract: In the domain of text-to-image generation, diffusion models have emerged as
powerful tools. Recently, studies on visual prompting, where images are used as
prompts, have enabled more precise control over style and content. However,
existing methods often suffer from content leakage, where undesired elements of
the visual style prompt are transferred along with the intended style. To
address this issue, we 1) extend classifier-free guidance (CFG) to utilize
swapping self-attention and propose 2) negative visual query guidance (NVQG) to
reduce the transfer of unwanted contents. NVQG employs negative score by
intentionally simulating content leakage scenarios that swap queries instead of
key and values of self-attention layers from visual style prompts. This simple
yet effective method significantly reduces content leakage. Furthermore, we
provide careful solutions for using a real image as visual style prompts.
Through extensive evaluation across various styles and text prompts, our method
demonstrates superiority over existing approaches, reflecting the style of the
references, and ensuring that resulting images match the text prompts. Our code
is available \href{https://github.com/naver-ai/StyleKeeper}{here}.

</details>


### [54] [Lattice-allocated Real-time Line Segment Feature Detection and Tracking Using Only an Event-based Camera](https://arxiv.org/abs/2510.06829)
*Mikihiro Ikura,Arren Glover,Masayoshi Mizuno,Chiara Bartolozzi*

Main category: cs.CV

TL;DR: Real-time, event-only line segment detection and tracking that remains robust at high event rates via a lattice-allocated pipeline, outperforming event-only and hybrid baselines on public and custom datasets.


<details>
  <summary>Details</summary>
Motivation: Line segments compactly encode man-made geometry, but existing event-based methods often require additional frame cameras or break down at modern high-resolution (high event-rate) settings. A standalone, real-time event-only solution is needed for efficiency and deployment simplicity.

Method: A three-stage, lattice-allocated pipeline: (i) velocity-invariant event representation to normalize motion effects, (ii) line segment detection using a fitting score to select candidates, and (iii) tracking by perturbing endpoints to maintain segments over time, all operating on event streams without frames.

Result: On both ad-hoc and public datasets, the method runs in real time and achieves higher accuracy than state-of-the-art event-only and event-frame hybrid approaches, handling high event rates from modern sensors.

Conclusion: The proposed pipeline enables fully stand-alone event camera operation for reliable, real-time line segment extraction and tracking in real-world environments, advancing the practicality of event-based vision.

Abstract: Line segment extraction is effective for capturing geometric features of
human-made environments. Event-based cameras, which asynchronously respond to
contrast changes along edges, enable efficient extraction by reducing redundant
data. However, recent methods often rely on additional frame cameras or
struggle with high event rates. This research addresses real-time line segment
detection and tracking using only a modern, high-resolution (i.e., high event
rate) event-based camera. Our lattice-allocated pipeline consists of (i)
velocity-invariant event representation, (ii) line segment detection based on a
fitting score, (iii) and line segment tracking by perturbating endpoints.
Evaluation using ad-hoc recorded dataset and public datasets demonstrates
real-time performance and higher accuracy compared to state-of-the-art
event-only and event-frame hybrid baselines, enabling fully stand-alone event
camera operation in real-world settings.

</details>


### [55] [Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph Regularization](https://arxiv.org/abs/2510.06842)
*Kanglei Zhou,Qingyi Pan,Xingxing Zhang,Hubert P. H. Shum,Frederick W. B. Li,Xiaohui Liang,Liyuan Wang*

Main category: cs.CV

TL;DR: They define a Continual Action Quality Assessment setting and propose MAGR++, a controlled full-parameter fine-tuning framework that stabilizes shallow layers, adapts deeper layers, projects historical features onto the current manifold, and aligns distributions via graph regularization, achieving state-of-the-art gains (+3.6% offline, +12.2% online correlation).


<details>
  <summary>Details</summary>
Motivation: AQA models face non-stationary quality distributions in real-world deployments, leading to poor generalization and catastrophic forgetting. Prior continual learning successes with parameter-efficient tuning in classification do not transfer well to AQA; effective representation learning seems to require full-parameter tuning without incurring overfitting or feature manifold shift.

Method: Introduce the CAQA setting and analyze why PEFT underperforms. Show that full-parameter fine-tuning is needed but risky. Propose MAGR++: (1) backbone fine-tuning with layer-wise control—stabilize shallow layers and adapt deeper ones; (2) a two-step feature rectification pipeline—manifold projector to map historical features into the current representation space; (3) a graph regularizer to align local and global feature distributions over time. Build four CAQA benchmarks across three datasets with tailored protocols and strong baselines for systematic comparison.

Result: MAGR++ consistently outperforms strong baselines, yielding average correlation improvements of 3.6% in offline and 12.2% in online continual settings, demonstrating robustness to evolving distributions. Code is released for reproducibility.

Conclusion: Full-parameter tuning is necessary for CAQA but must be controlled to avoid overfitting and manifold drift. MAGR++ effectively mitigates forgetting by rectifying historical features and aligning feature distributions, setting a new state of the art on newly established CAQA benchmarks.

Abstract: Action Quality Assessment (AQA) quantifies human actions in videos,
supporting applications in sports scoring, rehabilitation, and skill
evaluation. A major challenge lies in the non-stationary nature of quality
distributions in real-world scenarios, which limits the generalization ability
of conventional methods. We introduce Continual AQA (CAQA), which equips AQA
with Continual Learning (CL) capabilities to handle evolving distributions
while mitigating catastrophic forgetting. Although parameter-efficient
fine-tuning of pretrained models has shown promise in CL for image
classification, we find it insufficient for CAQA. Our empirical and theoretical
analyses reveal two insights: (i) Full-Parameter Fine-Tuning (FPFT) is
necessary for effective representation learning; yet (ii) uncontrolled FPFT
induces overfitting and feature manifold shift, thereby aggravating forgetting.
To address this, we propose Adaptive Manifold-Aligned Graph Regularization
(MAGR++), which couples backbone fine-tuning that stabilizes shallow layers
while adapting deeper ones with a two-step feature rectification pipeline: a
manifold projector to translate deviated historical features into the current
representation space, and a graph regularizer to align local and global
distributions. We construct four CAQA benchmarks from three datasets with
tailored evaluation protocols and strong baselines, enabling systematic
cross-dataset comparison. Extensive experiments show that MAGR++ achieves
state-of-the-art performance, with average correlation gains of 3.6% offline
and 12.2% online over the strongest baseline, confirming its robustness and
effectiveness. Our code is available at https://github.com/ZhouKanglei/MAGRPP.

</details>


### [56] [Online Generic Event Boundary Detection](https://arxiv.org/abs/2510.06855)
*Hyungrok Jung,Daneul Kim,Seunggyun Lim,Jeany Son,Jonghyun Choi*

Main category: cs.CV

TL;DR: Defines and tackles Online Generic Event Boundary Detection by predicting the next frame and using adaptive error-based tests to flag boundaries in real time, achieving SOTA among online methods and near-offline performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Current GEBD models need full video access, unlike humans who segment events online. There is a need to detect taxonomy-free, subtle event boundaries immediately in streaming video without peeking at future frames.

Method: Proposes Estimator, inspired by Event Segmentation Theory. It has: (1) Consistent Event Anticipator (CEA) that predicts the next frame from past frames; (2) Online Boundary Discriminator (OBD) that computes prediction errors and applies adaptive thresholds via statistical tests over past errors to decide boundaries.

Result: On Kinetics-GEBD and TAPOS, Estimator outperforms online baselines adapted from recent online video understanding models and reaches performance comparable to prior offline GEBD methods.

Conclusion: Prediction–actual discrepancy with adaptive statistical testing is effective for real-time, taxonomy-free event boundary detection. The work establishes On-GEBD as a viable task and sets a strong baseline for future research.

Abstract: Generic Event Boundary Detection (GEBD) aims to interpret long-form videos
through the lens of human perception. However, current GEBD methods require
processing complete video frames to make predictions, unlike humans processing
data online and in real-time. To bridge this gap, we introduce a new task,
Online Generic Event Boundary Detection (On-GEBD), aiming to detect boundaries
of generic events immediately in streaming videos. This task faces unique
challenges of identifying subtle, taxonomy-free event changes in real-time,
without the access to future frames. To tackle these challenges, we propose a
novel On-GEBD framework, Estimator, inspired by Event Segmentation Theory (EST)
which explains how humans segment ongoing activity into events by leveraging
the discrepancies between predicted and actual information. Our framework
consists of two key components: the Consistent Event Anticipator (CEA), and the
Online Boundary Discriminator (OBD). Specifically, the CEA generates a
prediction of the future frame reflecting current event dynamics based solely
on prior frames. Then, the OBD measures the prediction error and adaptively
adjusts the threshold using statistical tests on past errors to capture
diverse, subtle event transitions. Experimental results demonstrate that
Estimator outperforms all baselines adapted from recent online video
understanding models and achieves performance comparable to prior offline-GEBD
methods on the Kinetics-GEBD and TAPOS datasets.

</details>


### [57] [Explaining raw data complexity to improve satellite onboard processing](https://arxiv.org/abs/2510.06858)
*Adrien Dorise,Marjorie Bellizzi,Adrien Girard,Benjamin Francesconi,Stéphane May*

Main category: cs.CV

TL;DR: Benchmarks object detection on simulated raw satellite data versus L1 imagery; performance is similar at low/medium thresholds, but raw-trained models lose boundary precision at high confidence, implying the need for contour-aware architectures for reliable onboard AI.


<details>
  <summary>Details</summary>
Motivation: Onboard inference can cut latency and downlink needs, but most models assume preprocessed (L1) images. Raw sensor data onboard has different noise, radiometry, and geometry, and it’s unclear how standard detectors behave, blocking practical deployment.

Method: Build a workflow that converts high-resolution L1 images into raw-like data; train YOLOv11s and YOLOX-S on both raw-like and L1 datasets; compare with standard detection metrics across confidence thresholds and analyze with explainability tools.

Result: Detectors trained on raw-like data match L1-trained counterparts at low-to-medium confidence but underperform at high confidence due to poorer object boundary localization; explainability indicates boundary uncertainty/contouring issues.

Conclusion: Raw data is viable for onboard detection with small average losses, but achieving robust high-confidence outputs requires adapting architectures and losses to better model edges/contours; such changes could materially improve onboard remote-sensing AI.

Abstract: With increasing processing power, deploying AI models for remote sensing
directly onboard satellites is becoming feasible. However, new constraints
arise, mainly when using raw, unprocessed sensor data instead of preprocessed
ground-based products. While current solutions primarily rely on preprocessed
sensor images, few approaches directly leverage raw data. This study
investigates the effects of utilising raw data on deep learning models for
object detection and classification tasks. We introduce a simulation workflow
to generate raw-like products from high-resolution L1 imagery, enabling
systemic evaluation. Two object detection models (YOLOv11s and YOLOX-S) are
trained on both raw and L1 datasets, and their performance is compared using
standard detection metrics and explainability tools. Results indicate that
while both models perform similarly at low to medium confidence thresholds, the
model trained on raw data struggles with object boundary identification at high
confidence levels. It suggests that adapting AI architectures with improved
contouring methods can enhance object detection on raw images, improving
onboard AI for remote sensing.

</details>


### [58] [HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation](https://arxiv.org/abs/2510.06876)
*Samir Abou Haidar,Alexandre Chariot,Mehdi Darouich,Cyril Joly,Jean-Emmanuel Deschaud*

Main category: cs.CV

TL;DR: HARP-NeXt is a LiDAR semantic segmentation network that balances accuracy and real-time speed on embedded platforms via lightweight preprocessing, efficient Conv-SE-NeXt blocks, and multi-scale range–point fusion—matching top accuracy while being dramatically faster and TTA-free.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles and robots need highly accurate, real-time LiDAR segmentation on resource-constrained hardware. Existing methods either are accurate but slow (point/sparse conv due to neighbor search and 3D conv), or fast but lose geometry (2D projection). Heavy preprocessing and reliance on test-time augmentation further hurt deployment speed.

Method: 1) Novel lightweight preprocessing to cut computational overhead. 2) Conv-SE-NeXt feature extraction block to capture rich representations without deep stacks. 3) Multi-scale range–point fusion backbone to preserve geometric details across abstraction levels. No ensembles or TTA required.

Result: On nuScenes and SemanticKITTI, HARP-NeXt yields a superior speed–accuracy trade-off versus state-of-the-art methods; it is comparable in accuracy to top-ranked PTv3 while running 24× faster.

Conclusion: Efficient design choices (lean preprocessing, tailored feature blocks, multi-scale fusion) enable practical, accurate, real-time LiDAR segmentation suitable for embedded platforms; code is publicly available.

Abstract: LiDAR semantic segmentation is crucial for autonomous vehicles and mobile
robots, requiring high accuracy and real-time processing, especially on
resource-constrained embedded systems. Previous state-of-the-art methods often
face a trade-off between accuracy and speed. Point-based and sparse
convolution-based methods are accurate but slow due to the complexity of
neighbor searching and 3D convolutions. Projection-based methods are faster but
lose critical geometric information during the 2D projection. Additionally,
many recent methods rely on test-time augmentation (TTA) to improve
performance, which further slows the inference. Moreover, the pre-processing
phase across all methods increases execution time and is demanding on embedded
platforms. Therefore, we introduce HARP-NeXt, a high-speed and accurate LiDAR
semantic segmentation network. We first propose a novel pre-processing
methodology that significantly reduces computational overhead. Then, we design
the Conv-SE-NeXt feature extraction block to efficiently capture
representations without deep layer stacking per network stage. We also employ a
multi-scale range-point fusion backbone that leverages information at multiple
abstraction levels to preserve essential geometric details, thereby enhancing
accuracy. Experiments on the nuScenes and SemanticKITTI benchmarks show that
HARP-NeXt achieves a superior speed-accuracy trade-off compared to all
state-of-the-art methods, and, without relying on ensemble models or TTA, is
comparable to the top-ranked PTv3, while running 24$\times$ faster. The code is
available at https://github.com/SamirAbouHaidar/HARP-NeXt

</details>


### [59] [Lung Infection Severity Prediction Using Transformers with Conditional TransMix Augmentation and Cross-Attention](https://arxiv.org/abs/2510.06887)
*Bouthaina Slika,Fadi Dornaika,Fares Bougourzi,Karim Hammoudi*

Main category: cs.CV

TL;DR: They propose a transformer-based model (QCross-Att-PVT) plus a conditional online TransMix augmentation to predict lung infection severity from both chest X-rays and CTs, achieving state-of-the-art results on two benchmarks and highlighting the value of gated attention and targeted augmentation.


<details>
  <summary>Details</summary>
Motivation: Rapid, accurate severity assessment of pneumonia-like lung infections is critical for timely clinical decisions, especially during pandemics, but datasets are imbalanced and existing models may not capture multi-scale features robustly across modalities (CXR and CT).

Method: 1) QCross-Att-PVT: a Transformer with parallel encoders, a cross-gated attention module, and a feature aggregator to capture rich multi-scale representations; 2) Conditional Online TransMix: an on-the-fly, mixed-label patch augmentation tailored to mitigate class imbalance. The framework is modality-agnostic, working with both CXR and CT.

Result: On RALO CXR and Per-COVID-19 CT benchmarks, the approach consistently outperforms multiple contemporary deep learning baselines, improving robustness and predictive accuracy. The study identifies data augmentation and gated attention as key contributors to performance gains.

Conclusion: The method provides a reliable, adaptable tool for clinical diagnosis support, monitoring, and personalized treatment planning, with open-source code available for reproducibility and further research.

Abstract: Lung infections, particularly pneumonia, pose serious health risks that can
escalate rapidly, especially during pandemics. Accurate AI-based severity
prediction from medical imaging is essential to support timely clinical
decisions and optimize patient outcomes. In this work, we present a novel
method applicable to both CT scans and chest X-rays for assessing lung
infection severity. Our contributions are twofold: (i) QCross-Att-PVT, a
Transformer-based architecture that integrates parallel encoders, a cross-gated
attention mechanism, and a feature aggregator to capture rich multi-scale
features; and (ii) Conditional Online TransMix, a custom data augmentation
strategy designed to address dataset imbalance by generating mixed-label image
patches during training. Evaluated on two benchmark datasets, RALO CXR and
Per-COVID-19 CT, our method consistently outperforms several state-of-the-art
deep learning models. The results emphasize the critical role of data
augmentation and gated attention in improving both robustness and predictive
accuracy. This approach offers a reliable, adaptable tool to support clinical
diagnosis, disease monitoring, and personalized treatment planning. The source
code of this work is available at https://github.com/bouthainas/QCross-Att-PVT.

</details>


### [60] [Label-frugal satellite image change detection with generative virtual exemplar learning](https://arxiv.org/abs/2510.06926)
*Hichem Sahbi*

Main category: cs.CV

TL;DR: They propose a label-efficient active learning framework for remote-sensing change detection that generates and queries the most informative “virtual exemplars” via an invertible graph convnet optimized with an adversarial loss, improving performance with fewer annotations.


<details>
  <summary>Details</summary>
Motivation: Change detection in multi-temporal satellite/aerial imagery relies heavily on large, hand-labeled datasets that reflect acquisition conditions and annotator subjectivity. Collecting such labels is costly and often impractical, limiting deep models’ effectiveness. A more label-efficient approach is needed.

Method: An active learning scheme ranks unlabeled samples by importance and queries only the most critical ones. Instead of selecting raw samples, it generates virtual exemplars with an invertible graph convolutional network. These exemplars are obtained by optimizing an adversarial loss that jointly accounts for representativity, diversity, and ambiguity, thereby producing queries that maximally challenge the current change-detection criteria. The model is iteratively updated with oracle-labeled exemplars.

Result: Extensive experiments indicate the proposed approach outperforms comparative methods under limited labeling budgets, achieving better change-detection performance with fewer annotated samples.

Conclusion: Generating adversarially informative virtual exemplars within an active learning loop reduces annotation needs and leads to improved change detection. The approach offers a practical, label-efficient alternative to standard supervised methods and can be iteratively refined to adapt to data and annotator bias.

Abstract: Change detection is a major task in remote sensing which consists in finding
all the occurrences of changes in multi-temporal satellite or aerial images.
The success of existing methods, and particularly deep learning ones, is
tributary to the availability of hand-labeled training data that capture the
acquisition conditions and the subjectivity of the user (oracle). In this
paper, we devise a novel change detection algorithm, based on active learning.
The main contribution of our work resides in a new model that measures how
important is each unlabeled sample, and provides an oracle with only the most
critical samples (also referred to as virtual exemplars) for further labeling.
These exemplars are generated, using an invertible graph convnet, as the
optimum of an adversarial loss that (i) measures representativity, diversity
and ambiguity of the data, and thereby (ii) challenges (the most) the current
change detection criteria, leading to a better re-estimate of these criteria in
the subsequent iterations of active learning. Extensive experiments show the
positive impact of our label-efficient learning model against comparative
methods.

</details>


### [61] [IAR2: Improving Autoregressive Visual Generation with Semantic-Detail Associated Token Prediction](https://arxiv.org/abs/2510.06928)
*Ran Yi,Teng Hu,Zihan Su,Lizhuang Ma*

Main category: cs.CV

TL;DR: IAR2 is a structured, coarse-to-fine autoregressive image generator that uses a dual codebook (semantic + detail), hierarchical token prediction with local context, and adaptive per-token CFG, achieving state-of-the-art FID 1.50 on ImageNet with improved efficiency.


<details>
  <summary>Details</summary>
Motivation: Standard autoregressive (AR) image models often ignore visual structure, and prior IAR’s codebook reorganization was limited by fixed codebooks and hard, uniform clustering. The goal is to better capture global semantics and fine details while overcoming rigidity and improving robustness and conditional alignment.

Method: Introduce a Semantic-Detail Associated Dual Codebook that decouples global semantics from fine details, expanding quantization capacity polynomially (via combinatorial semantic-detail pairs). Use a Semantic-Detail Autoregressive Prediction scheme with a Local-Context Enhanced AR head to predict semantic tokens first, then detail tokens, leveraging a local spatial window for coherence. For conditional generation, apply a Progressive Attention-Guided Adaptive CFG that modulates guidance per token based on attention-derived relevance and its position in the sequence.

Result: Extensive experiments show new SOTA for AR image generation: FID 1.50 on ImageNet. The model surpasses previous methods in quality and is computationally more efficient, indicating better expressiveness and robustness.

Conclusion: Decoupling semantics and details and predicting them hierarchically yields more expressive, robust, and efficient AR image generation with stronger conditional alignment. IAR2’s structured, coarse-to-fine strategy advances the state of the art in AR image synthesis.

Abstract: Autoregressive models have emerged as a powerful paradigm for visual content
creation, but often overlook the intrinsic structural properties of visual
data. Our prior work, IAR, initiated a direction to address this by
reorganizing the visual codebook based on embedding similarity, thereby
improving generation robustness. However, it is constrained by the rigidity of
pre-trained codebooks and the inaccuracies of hard, uniform clustering. To
overcome these limitations, we propose IAR2, an advanced autoregressive
framework that enables a hierarchical semantic-detail synthesis process. At the
core of IAR2 is a novel Semantic-Detail Associated Dual Codebook, which
decouples image representations into a semantic codebook for global semantic
information and a detail codebook for fine-grained refinements. It expands the
quantization capacity from a linear to a polynomial scale, significantly
enhancing expressiveness. To accommodate this dual representation, we propose a
Semantic-Detail Autoregressive Prediction scheme coupled with a Local-Context
Enhanced Autoregressive Head, which performs hierarchical prediction-first the
semantic token, then the detail token-while leveraging a local context window
to enhance spatial coherence. Furthermore, for conditional generation, we
introduce a Progressive Attention-Guided Adaptive CFG mechanism that
dynamically modulates the guidance scale for each token based on its relevance
to the condition and its temporal position in the generation sequence,
improving conditional alignment without sacrificing realism. Extensive
experiments demonstrate that IAR2 sets a new state-of-the-art for
autoregressive image generation, achieving a FID of 1.50 on ImageNet. Our model
not only surpasses previous methods in performance but also demonstrates
superior computational efficiency, highlighting the effectiveness of our
structured, coarse-to-fine generation strategy.

</details>


### [62] [OBJVanish: Physically Realizable Text-to-3D Adv. Generation of LiDAR-Invisible Objects](https://arxiv.org/abs/2510.06952)
*Bing Li,Wuqi Wang,Yanan Zhang,Jingzheng Li,Haigen Min,Wei Feng,Xingyu Zhao,Jie Zhang,Qing Guo*

Main category: cs.CV

TL;DR: They propose Phy3DAdvGen, a physically informed text-to-3D attack that generates pedestrian 3D models which become effectively invisible to LiDAR detectors, validated in both CARLA simulation and real-world tests against six SOTA detectors.


<details>
  <summary>Details</summary>
Motivation: LiDAR object detectors are safety-critical, yet existing point-perturbation attacks rarely cause full object disappearance and are hard to realize physically. There is a need for practical, high-impact adversarial tests to expose weaknesses before deployment.

Method: (1) Empirical study in CARLA probing vulnerability factors by manipulating pedestrian model topology, connectivity, intensity, and by composing pedestrians with other objects. (2) A text-to-3D adversarial generation pipeline (Phy3DAdvGen) that iteratively optimizes prompts—verbs, objects, and poses—to yield LiDAR-invisible pedestrians. (3) Physical realizability enforced via a curated pool of 13 real-object 3D models; generation is constrained to combinations from this pool. Evaluations target six SOTA LiDAR 3D detectors in simulation and physical environments.

Result: The generated 3D pedestrian models evade detection across six SOTA LiDAR detectors, achieving practical object disappearance in both simulated CARLA scenarios and real-world setups; experiments are described as extensive, implying consistent cross-model effectiveness.

Conclusion: Physically realizable, prompt-optimized text-to-3D generation can craft LiDAR-invisible objects, revealing significant vulnerabilities in current 3D detection systems and underscoring the need for stronger defenses and robustness evaluations.

Abstract: LiDAR-based 3D object detectors are fundamental to autonomous driving, where
failing to detect objects poses severe safety risks. Developing effective 3D
adversarial attacks is essential for thoroughly testing these detection systems
and exposing their vulnerabilities before real-world deployment. However,
existing adversarial attacks that add optimized perturbations to 3D points have
two critical limitations: they rarely cause complete object disappearance and
prove difficult to implement in physical environments. We introduce the
text-to-3D adversarial generation method, a novel approach enabling physically
realizable attacks that can generate 3D models of objects truly invisible to
LiDAR detectors and be easily realized in the real world. Specifically, we
present the first empirical study that systematically investigates the factors
influencing detection vulnerability by manipulating the topology, connectivity,
and intensity of individual pedestrian 3D models and combining pedestrians with
multiple objects within the CARLA simulation environment. Building on the
insights, we propose the physically-informed text-to-3D adversarial generation
(Phy3DAdvGen) that systematically optimizes text prompts by iteratively
refining verbs, objects, and poses to produce LiDAR-invisible pedestrians. To
ensure physical realizability, we construct a comprehensive object pool
containing 13 3D models of real objects and constrain Phy3DAdvGen to generate
3D objects based on combinations of objects in this set. Extensive experiments
demonstrate that our approach can generate 3D pedestrians that evade six
state-of-the-art (SOTA) LiDAR 3D detectors in both CARLA simulation and
physical environments, thereby highlighting vulnerabilities in safety-critical
applications.

</details>


### [63] [Generating Surface for Text-to-3D using 2D Gaussian Splatting](https://arxiv.org/abs/2510.06967)
*Huanning Dong,Fan Li,Ping Kuang,Jianwen Min*

Main category: cs.CV

TL;DR: DirectGaussian is a text-to-3D method that directly generates surfel-based object surfaces using 2D Gaussian splatting, guided by multi-view normal/texture priors and curvature constraints to ensure geometric consistency, yielding diverse, high-fidelity 3D assets.


<details>
  <summary>Details</summary>
Motivation: Text-to-3D remains hard due to complex real-world geometries and the limitations of relying solely on 2D diffusion priors or committing to rigid 3D representations. There is a need for a method that produces geometrically consistent, detailed surfaces from text prompts.

Method: Represent object surfaces as surfels and render them with 2D Gaussian splatting. Use conditional text generation to obtain multi-view normal and texture priors for guidance. During optimization, add curvature-based constraints to enforce multi-view geometric consistency of the generated surface.

Result: Extensive experiments (details not provided in the abstract) show the framework can create diverse 3D content with high fidelity and improved geometric consistency.

Conclusion: A surfel-based, Gaussian-splatting approach with curvature constraints is an effective path for text-driven 3D surface generation, producing diverse and high-quality results while addressing multi-view consistency challenges.

Abstract: Recent advancements in Text-to-3D modeling have shown significant potential
for the creation of 3D content. However, due to the complex geometric shapes of
objects in the natural world, generating 3D content remains a challenging task.
Current methods either leverage 2D diffusion priors to recover 3D geometry, or
train the model directly based on specific 3D representations. In this paper,
we propose a novel method named DirectGaussian, which focuses on generating the
surfaces of 3D objects represented by surfels. In DirectGaussian, we utilize
conditional text generation models and the surface of a 3D object is rendered
by 2D Gaussian splatting with multi-view normal and texture priors. For
multi-view geometric consistency problems, DirectGaussian incorporates
curvature constraints on the generated surface during optimization process.
Through extensive experiments, we demonstrate that our framework is capable of
achieving diverse and high-fidelity 3D content creation.

</details>


### [64] [Learning Global Representation from Queries for Vectorized HD Map Construction](https://arxiv.org/abs/2510.06969)
*Shoumeng Qiu,Xinrun Li,Yang Long,Xiangyang Xue,Varun Ojha,Jian Pu*

Main category: cs.CV

TL;DR: MapGR augments DETR-style vectorized HD map construction with learned global representations, using a holistic segmentation-driven learning signal and global guidance to queries, yielding higher mAP on nuScenes and Argoverse2.


<details>
  <summary>Details</summary>
Motivation: DETR-based HD map methods treat vector elements as independent instances via learnable queries, which biases learning toward local features and misses the global structure and context inherent to maps. This can hurt coherence and accuracy of vectorized outputs. The motivation is to inject a global map representation that regularizes and informs per-query predictions.

Method: Introduce two modules: (1) Global Representation Learning (GRL), which learns a global map representation by supervising the collective query distribution with a holistic segmentation task, aligning queries to the overall map layout; (2) Global Representation Guidance (GRG), which feeds the learned global representation back to each query, providing explicit global context during optimization within a DETR-like framework.

Result: On nuScenes and Argoverse2, MapGR achieves substantial gains in mean Average Precision over state-of-the-art baselines (exact numbers not provided in the abstract), demonstrating improved vectorized map detection performance.

Conclusion: Embedding a learned global map representation into query-based HD map construction both regularizes the query set and enriches per-query context, leading to better accuracy and likely more coherent map outputs across diverse datasets.

Abstract: The online construction of vectorized high-definition (HD) maps is a
cornerstone of modern autonomous driving systems. State-of-the-art approaches,
particularly those based on the DETR framework, formulate this as an instance
detection problem. However, their reliance on independent, learnable object
queries results in a predominantly local query perspective, neglecting the
inherent global representation within HD maps. In this work, we propose
\textbf{MapGR} (\textbf{G}lobal \textbf{R}epresentation learning for HD
\textbf{Map} construction), an architecture designed to learn and utilize a
global representations from queries. Our method introduces two synergistic
modules: a Global Representation Learning (GRL) module, which encourages the
distribution of all queries to better align with the global map through a
carefully designed holistic segmentation task, and a Global Representation
Guidance (GRG) module, which endows each individual query with explicit,
global-level contextual information to facilitate its optimization. Evaluations
on the nuScenes and Argoverse2 datasets validate the efficacy of our approach,
demonstrating substantial improvements in mean Average Precision (mAP) compared
to leading baselines.

</details>


### [65] [Addressing the ID-Matching Challenge in Long Video Captioning](https://arxiv.org/abs/2510.06973)
*Zhantao Yang,Huangji Wang,Ruili Feng,Han Zhang,Yuting Hu,Shangwen Zhu,Junyan Li,Yu Liu,Fan Cheng*

Main category: cs.CV

TL;DR: They identify identity matching (consistent recognition of the same people across frames) as the key bottleneck in long‑video captioning, build a benchmark to measure it, study LVLMs (incl. GPT‑4o) to derive two practical levers—use image evidence more and enrich per‑person descriptions—and propose RICE, a method that exploits LVLM priors to track individuals in captions, boosting ID‑matching precision from 50→90% and recall from 15→80%.


<details>
  <summary>Details</summary>
Motivation: Long, complex videos require captions that consistently refer to the same individuals across time. Existing methods rarely target this ID‑matching need; those that do rely on point‑wise matching and don’t generalize well, hurting both caption quality and downstream multimodal/text‑to‑video tasks.

Method: 1) Introduce a benchmark to assess ID‑matching in video captions. 2) Probe LVLMs (e.g., GPT‑4o) to discover that ID‑matching improves by (a) leveraging more image information and (b) providing richer per‑individual descriptions. 3) Propose RICE (Recognizing Identities for Captioning Effectively), a captioning approach that unlocks LVLMs’ inherent ID‑matching priors via enhanced image usage and increased identity‑description granularity, enabling continuous tracking of people across long videos.

Result: Across experiments, RICE yields superior caption quality and ID‑matching. On GPT‑4o, precision improves from 50% to 90% and recall from 15% to 80% relative to baseline.

Conclusion: LVLMs contain latent capacity for identity consistency that can be activated with the right strategy. RICE operationalizes this, enabling stable per‑person tracking in long‑video captions and setting a benchmark for evaluating ID‑matching, with implications for text‑to‑video generation and broader multimodal understanding.

Abstract: Generating captions for long and complex videos is both critical and
challenging, with significant implications for the growing fields of
text-to-video generation and multi-modal understanding. One key challenge in
long video captioning is accurately recognizing the same individuals who appear
in different frames, which we refer to as the ID-Matching problem. Few prior
works have focused on this important issue. Those that have, usually suffer
from limited generalization and depend on point-wise matching, which limits
their overall effectiveness. In this paper, unlike previous approaches, we
build upon LVLMs to leverage their powerful priors. We aim to unlock the
inherent ID-Matching capabilities within LVLMs themselves to enhance the
ID-Matching performance of captions. Specifically, we first introduce a new
benchmark for assessing the ID-Matching capabilities of video captions. Using
this benchmark, we investigate LVLMs containing GPT-4o, revealing key insights
that the performance of ID-Matching can be improved through two methods: 1)
enhancing the usage of image information and 2) increasing the quantity of
information of individual descriptions. Based on these insights, we propose a
novel video captioning method called Recognizing Identities for Captioning
Effectively (RICE). Extensive experiments including assessments of caption
quality and ID-Matching performance, demonstrate the superiority of our
approach. Notably, when implemented on GPT-4o, our RICE improves the precision
of ID-Matching from 50% to 90% and improves the recall of ID-Matching from 15%
to 80% compared to baseline. RICE makes it possible to continuously track
different individuals in the captions of long videos.

</details>


### [66] [No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts](https://arxiv.org/abs/2510.06988)
*Girolamo Macaluso,Lorenzo Mandelli,Mirko Bicchierai,Stefano Berretti,Andrew D. Bagdanov*

Main category: cs.CV

TL;DR: They fine-tune pretrained motion diffusion models to new actions/styles using only text prompts by optimizing with an RL objective driven by a text–motion retrieval reward, improving motion quality/diversity without needing new motion capture data.


<details>
  <summary>Details</summary>
Motivation: Adapting motion diffusion models to unseen actions or styles typically needs extra mocap data and full retraining, which is expensive, hard to scale, and can raise data-access/privacy issues. A method that adapts using only textual descriptions would be far more data-efficient and practical.

Method: Post-training reinforcement learning on top of pretrained motion diffusion models. A frozen text–motion retrieval network provides a reward measuring text–motion alignment. They optimize the diffusion generator with Denoising Diffusion Policy Optimization (DDPO) to shift the generative distribution toward the target domain using only prompts, with no paired motion ground truth. Applicable to both latent- and joint-space diffusion architectures.

Result: On HumanML3D and KIT-ML, across cross-dataset and leave-one-out settings, quantitative metrics and user studies indicate consistent gains in motion quality and diversity while maintaining performance on the original distribution.

Conclusion: A flexible, data-efficient, and privacy-preserving post-training framework for adapting motion diffusion models to new domains using text-only supervision, avoiding costly data collection and retraining while preserving base-model capabilities.

Abstract: Diffusion models have recently advanced human motion generation, producing
realistic and diverse animations from textual prompts. However, adapting these
models to unseen actions or styles typically requires additional motion capture
data and full retraining, which is costly and difficult to scale. We propose a
post-training framework based on Reinforcement Learning that fine-tunes
pretrained motion diffusion models using only textual prompts, without
requiring any motion ground truth. Our approach employs a pretrained
text-motion retrieval network as a reward signal and optimizes the diffusion
policy with Denoising Diffusion Policy Optimization, effectively shifting the
model's generative distribution toward the target domain without relying on
paired motion data. We evaluate our method on cross-dataset adaptation and
leave-one-out motion experiments using the HumanML3D and KIT-ML datasets across
both latent- and joint-space diffusion architectures. Results from quantitative
metrics and user studies show that our approach consistently improves the
quality and diversity of generated motions, while preserving performance on the
original distribution. Our approach is a flexible, data-efficient, and
privacy-preserving solution for motion adaptation.

</details>


### [67] [Bayesian Modelling of Multi-Year Crop Type Classification Using Deep Neural Networks and Hidden Markov Models](https://arxiv.org/abs/2510.07008)
*Gianmarco Perantoni,Giulio Weikmann,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: HMMs stacked on top of a Transformer Encoder enforce temporally consistent multi‑year crop‑type labels in satellite image time series, improving overall performance and F1 on a 6‑year, 47‑class Sentinel‑2 dataset.


<details>
  <summary>Details</summary>
Motivation: Yearly land‑cover maps should evolve coherently over time, but per‑year classifiers often ignore inter‑annual dependencies and valid crop rotation patterns, leading to temporally inconsistent labels. The work aims to model these temporal consistencies explicitly to better capture land‑cover change and crop sequences.

Method: A cascade model: a Transformer Encoder–based DNN captures complex temporal signatures within yearly SITS and produces per‑year predictions, while a Hidden Markov Model on top constrains and smooths the label sequence by modeling inter‑year transitions, yielding the most probable, temporally consistent crop‑type sequence.

Result: On a multi‑year crop classification benchmark (47 crop types across six years of Sentinel‑2), integrating the HMM with the Transformer improves overall performance and F1 compared to using the deep model alone, highlighting gains from modeling temporal label consistency.

Conclusion: Coupling deep temporal feature learning (Transformer Encoder) with probabilistic sequence modeling (HMM) produces more coherent and accurate multi‑year land‑cover classifications, underscoring the value of enforcing temporal consistency in operational mapping pipelines.

Abstract: The temporal consistency of yearly land-cover maps is of great importance to
model the evolution and change of the land cover over the years. In this paper,
we focus the attention on a novel approach to classification of yearly
satellite image time series (SITS) that combines deep learning with Bayesian
modelling, using Hidden Markov Models (HMMs) integrated with Transformer
Encoder (TE) based DNNs. The proposed approach aims to capture both i)
intricate temporal correlations in yearly SITS and ii) specific patterns in
multiyear crop type sequences. It leverages the cascade classification of an
HMM layer built on top of the TE, discerning consistent yearly crop-type
sequences. Validation on a multiyear crop type classification dataset spanning
47 crop types and six years of Sentinel-2 acquisitions demonstrates the
importance of modelling temporal consistency in the predicted labels. HMMs
enhance the overall performance and F1 scores, emphasising the effectiveness of
the proposed approach.

</details>


### [68] [U-Bench: A Comprehensive Understanding of U-Net through 100-Variant Benchmarking](https://arxiv.org/abs/2510.07041)
*Fenghe Tang,Chengqi Dong,Wenxin Ma,Zikang Xu,Heqin Zhu,Zihang Jiang,Rongsheng Wang,Yuhao Wang,Chenxu Wu,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: U-Bench is a large-scale, statistically rigorous benchmark evaluating 100 U-Net variants on 28 datasets across 10 modalities, introducing a deployment-oriented U-Score that balances segmentation performance and efficiency, plus a model-selection advisor and full open resources.


<details>
  <summary>Details</summary>
Motivation: Despite thousands of U-Net variants in medical image segmentation, prior comparisons lack statistical rigor, ignore efficiency, and rarely test cross-dataset generalization, leaving practitioners without reliable, deployment-focused guidance.

Method: Construct a unified benchmark (U-Bench) that: (1) evaluates models on statistical robustness, zero-shot generalization, and computational efficiency; (2) introduces U-Score to jointly assess performance–efficiency trade-offs; (3) performs systematic analyses of dataset/architecture effects; (4) offers a model advisor agent; and (5) releases code, models, protocols, and weights for reproducibility and extensibility.

Result: Benchmarked 100 U-Net variants across 28 datasets/10 modalities; produced statistically validated comparisons, deployment-oriented rankings via U-Score, and empirical insights into how dataset characteristics and architectural paradigms affect outcomes; packaged a model-selection advisor for practical use.

Conclusion: U-Bench exposes shortcomings in prior evaluations and establishes a fair, reproducible, and deployment-relevant foundation for comparing U-Net-based segmentation models, providing tools and resources likely to shape model selection and development in the coming years.

Abstract: Over the past decade, U-Net has been the dominant architecture in medical
image segmentation, leading to the development of thousands of U-shaped
variants. Despite its widespread adoption, there is still no comprehensive
benchmark to systematically evaluate their performance and utility, largely
because of insufficient statistical validation and limited consideration of
efficiency and generalization across diverse datasets. To bridge this gap, we
present U-Bench, the first large-scale, statistically rigorous benchmark that
evaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our
contributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates
models along three key dimensions: statistical robustness, zero-shot
generalization, and computational efficiency. We introduce a novel metric,
U-Score, which jointly captures the performance-efficiency trade-off, offering
a deployment-oriented perspective on model progress. (2) Systematic Analysis
and Model Selection Guidance: We summarize key findings from the large-scale
evaluation and systematically analyze the impact of dataset characteristics and
architectural paradigms on model performance. Based on these insights, we
propose a model advisor agent to guide researchers in selecting the most
suitable models for specific datasets and tasks. (3) Public Availability: We
provide all code, models, protocols, and weights, enabling the community to
reproduce our results and extend the benchmark with future methods. In summary,
U-Bench not only exposes gaps in previous evaluations but also establishes a
foundation for fair, reproducible, and practically relevant benchmarking in the
next decade of U-Net-based segmentation models. The project can be accessed at:
https://fenghetan9.github.io/ubench. Code is available at:
https://github.com/FengheTan9/U-Bench.

</details>


### [69] [Concept Retrieval - What and How?](https://arxiv.org/abs/2510.07058)
*Ori nizan,Oren Shrout,Ayellet Tal*

Main category: cs.CV

TL;DR: Concept-oriented image retrieval method that models a query image’s embedding-space neighborhood with a bimodal Gaussian to disentangle and identify shared concepts, enabling retrieval beyond surface visual/semantic similarity; includes formal problem definition, requirements, and metrics; validated via qualitative, quantitative, and human studies; released as the coret package.


<details>
  <summary>Details</summary>
Motivation: Standard retrieval/clustering focuses on overall visual or semantic similarity and often misses the central concepts or narrative that make images related at a deeper level. The paper aims to retrieve images sharing a query’s core concepts, addressing this gap and formalizing the task with requirements and evaluation metrics.

Method: Define the concept-oriented retrieval problem and its evaluation criteria. For a query image, collect nearest neighbors in an embedding space. Observe that each neighbor may share at least one concept with the query, but neighbors may not share the same concept among themselves. Fit a bimodal Gaussian to the neighbor distribution to reveal meaningful substructure corresponding to different concepts, then use this structure to identify and rank images aligned with the query’s central concepts.

Result: Qualitative case studies, quantitative experiments, and human evaluations indicate the approach more effectively retrieves images sharing the query’s central concepts than conventional methods, with the bimodal modeling uncovering useful conceptual groupings.

Conclusion: Modeling a query’s neighborhood with a bimodal Gaussian enables robust identification of core concepts, yielding improved concept-oriented retrieval relative to standard similarity-based approaches. The work also contributes a formal problem framing, metrics, and a practical implementation (coret).

Abstract: A concept may reflect either a concrete or abstract idea. Given an input
image, this paper seeks to retrieve other images that share its central
concepts, capturing aspects of the underlying narrative. This goes beyond
conventional retrieval or clustering methods, which emphasize visual or
semantic similarity. We formally define the problem, outline key requirements,
and introduce appropriate evaluation metrics. We propose a novel approach
grounded in two key observations: (1) While each neighbor in the embedding
space typically shares at least one concept with the query, not all neighbors
necessarily share the same concept with one another. (2) Modeling this
neighborhood with a bimodal Gaussian distribution uncovers meaningful structure
that facilitates concept identification. Qualitative, quantitative, and human
evaluations confirm the effectiveness of our approach. See the package on PyPI:
https://pypi.org/project/coret/

</details>


### [70] [DADO: A Depth-Attention framework for Object Discovery](https://arxiv.org/abs/2510.07089)
*Federico Gonzalez,Estefania Talavera,Petia Radeva*

Main category: cs.CV

TL;DR: DADO is a self-supervised method that fuses image attention with monocular depth via dynamic weighting to discover and localize objects without labels, outperforming prior methods on standard benchmarks without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Unsupervised object discovery is hard due to lack of labels and failure modes like noisy attention maps and complex, multi-depth scenes. The work aims to leverage complementary cues (attention and depth) to improve robustness and accuracy in identifying object regions without supervision.

Method: Introduce DADO (Depth-Attention self-supervised technique for Discovering unseen Objects): combine an attention mechanism with a depth estimation model; apply dynamic, image-level weighting that adaptively prioritizes attention or depth features depending on global image characteristics to mitigate noise and handle varying depth planes. Entirely self-supervised; no fine-tuning on benchmarks.

Result: On standard object-discovery benchmarks, DADO achieves state-of-the-art accuracy and robustness, outperforming existing methods while requiring no fine-tuning.

Conclusion: Adaptive fusion of depth and attention provides a strong, label-free signal for object discovery, yielding better robustness across diverse scenes and validating the utility of dynamic weighting. Results suggest depth-attention synergy is an effective direction for unsupervised object localization and discovery.

Abstract: Unsupervised object discovery, the task of identifying and localizing objects
in images without human-annotated labels, remains a significant challenge and a
growing focus in computer vision. In this work, we introduce a novel model,
DADO (Depth-Attention self-supervised technique for Discovering unseen
Objects), which combines an attention mechanism and a depth model to identify
potential objects in images. To address challenges such as noisy attention maps
or complex scenes with varying depth planes, DADO employs dynamic weighting to
adaptively emphasize attention or depth features based on the global
characteristics of each image. We evaluated DADO on standard benchmarks, where
it outperforms state-of-the-art methods in object discovery accuracy and
robustness without the need for fine-tuning.

</details>


### [71] [Enhancing Concept Localization in CLIP-based Concept Bottleneck Models](https://arxiv.org/abs/2510.07115)
*Rémi Kazmierczak,Steve Azzolin,Eloïse Berthier,Goran Frehse,Gianni Franchi*

Main category: cs.CV

TL;DR: CLIP-driven concept bottleneck models can hallucinate concepts, harming explanation faithfulness; the paper proposes CHILI to disentangle and localize concept evidence, yielding more accurate and interpretable saliency-based explanations.


<details>
  <summary>Details</summary>
Motivation: Concept Bottleneck Models promise interpretable predictions via intermediate human concepts, but explicit concept labels are costly. Recent works use CLIP zero-shot concepts instead—however, if CLIP misidentifies concept presence, the resulting explanations are unfaithful. The authors aim to diagnose and mitigate such concept hallucination while improving the interpretability of concept evidence.

Method: CHILI (Concept Hallucination Inhibition via Localized Interpretability): a technique that disentangles image embeddings and localizes pixels corresponding to each target concept, replacing/globalizing CLIP’s similarity with spatially grounded evidence and producing saliency-based explanations for concepts.

Result: They show CLIP often hallucinates concept presence/absence in CBM settings and demonstrate that CHILI reduces these hallucinations while producing more interpretable, saliency-based explanations. (No quantitative metrics are provided in the abstract.)

Conclusion: Grounding concepts via localized, disentangled evidence improves the faithfulness and interpretability of CLIP-based CBMs; CHILI is presented as a practical approach to inhibit hallucination and yield clearer concept-level saliency maps.

Abstract: This paper addresses explainable AI (XAI) through the lens of Concept
Bottleneck Models (CBMs) that do not require explicit concept annotations,
relying instead on concepts extracted using CLIP in a zero-shot manner. We show
that CLIP, which is central in these techniques, is prone to concept
hallucination, incorrectly predicting the presence or absence of concepts
within an image in scenarios used in numerous CBMs, hence undermining the
faithfulness of explanations. To mitigate this issue, we introduce Concept
Hallucination Inhibition via Localized Interpretability (CHILI), a technique
that disentangles image embeddings and localizes pixels corresponding to target
concepts. Furthermore, our approach supports the generation of saliency-based
explanations that are more interpretable.

</details>


### [72] [MoRe: Monocular Geometry Refinement via Graph Optimization for Cross-View Consistency](https://arxiv.org/abs/2510.07119)
*Dongki Jung,Jaehoon Choi,Yonghan Lee,Sungmin Eum,Heesung Kwon,Dinesh Manocha*

Main category: cs.CV

TL;DR: MoRe is a training‑free refinement that enforces cross‑view consistency and resolves scale ambiguity for monocular 3D foundation model outputs via feature-matched correspondences and a graph-based, locally planar optimization, improving both 3D reconstruction and sparse‑view novel view synthesis.


<details>
  <summary>Details</summary>
Motivation: Monocular 3D foundation models are attractive for general 3D perception but suffer from cross-view inconsistency and inherent scale ambiguity, which limit accurate multi‑view reconstruction and novel view synthesis without additional training.

Method: Establish inter-frame correspondences using feature matching. Build a graph-based optimization that uses estimated 3D points and surface normals (from monocular foundation models) to perform local planar approximations across views. This jointly enforces cross-view geometric consistency and aligns scales while preserving underlying 3D structure, avoiding simple least-squares that can distort geometry.

Result: Compared with direct monocular outputs, MoRe yields more consistent multi-view geometry, better scale alignment, and improved 3D reconstruction quality. It also enhances novel view synthesis, especially in sparse-view rendering settings.

Conclusion: A practical, training-free refinement layer for monocular 3D foundation models can effectively mitigate scale ambiguity and inter-frame inconsistencies through graph-based, locally planar constraints, boosting both reconstruction fidelity and sparse-view NVS performance.

Abstract: Monocular 3D foundation models offer an extensible solution for perception
tasks, making them attractive for broader 3D vision applications. In this
paper, we propose MoRe, a training-free Monocular Geometry Refinement method
designed to improve cross-view consistency and achieve scale alignment. To
induce inter-frame relationships, our method employs feature matching between
frames to establish correspondences. Rather than applying simple least squares
optimization on these matched points, we formulate a graph-based optimization
framework that performs local planar approximation using the estimated 3D
points and surface normals estimated by monocular foundation models. This
formulation addresses the scale ambiguity inherent in monocular geometric
priors while preserving the underlying 3D structure. We further demonstrate
that MoRe not only enhances 3D reconstruction but also improves novel view
synthesis, particularly in sparse view rendering scenarios.

</details>


### [73] [Validation of Various Normalization Methods for Brain Tumor Segmentation: Can Federated Learning Overcome This Heterogeneity?](https://arxiv.org/abs/2510.07126)
*Jan Fiszer,Dominika Ciupek,Maciej Malawski*

Main category: cs.CV

TL;DR: Federated learning for brain tumor segmentation remains robust when clients use different MRI intensity normalization methods, achieving a 3D Dice of ~92% comparable to centralized training, suggesting FL can address privacy concerns without sacrificing performance under this type of non-IID heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Deep learning in medical imaging needs large, pooled datasets, but privacy, storage, and transfer barriers prevent centralization. Federated learning mitigates these issues, yet its performance can degrade with non-IID data. A common, realistic source of non-IID distribution across sites is differing MRI intensity normalization. The study aims to quantify how such normalization heterogeneity affects segmentation training and inference, and whether FL remains effective.

Method: Simulate non-IID conditions by splitting data into subsets and applying different MRI intensity normalization techniques to each subset (mimicking site-specific preprocessing). Train brain tumor segmentation models under federated learning across these subsets and compare to a centralized model trained on all data. Evaluate performance (3D Dice) and analyze the influence of normalization differences on both training and inference.

Result: Federated learning methods showed resilience to inconsistently normalized client data, reaching a 3D Dice score of approximately 92%, comparable to the centralized baseline trained with all data. The study provides empirical insights into how normalization choices impact segmentation performance in both training and inference settings.

Conclusion: Federated learning can effectively train high-performing brain tumor segmentation models even when clients use different MRI intensity normalization techniques, thereby enabling privacy-preserving collaboration without notable loss of accuracy. The released code supports reproducibility and further investigation.

Abstract: Deep learning (DL) has been increasingly applied in medical imaging, however,
it requires large amounts of data, which raises many challenges related to data
privacy, storage, and transfer. Federated learning (FL) is a training paradigm
that overcomes these issues, though its effectiveness may be reduced when
dealing with non-independent and identically distributed (non-IID) data. This
study simulates non-IID conditions by applying different MRI intensity
normalization techniques to separate data subsets, reflecting a common cause of
heterogeneity. These subsets are then used for training and testing models for
brain tumor segmentation. The findings provide insights into the influence of
the MRI intensity normalization methods on segmentation models, both training
and inference. Notably, the FL methods demonstrated resilience to
inconsistently normalized data across clients, achieving the 3D Dice score of
92%, which is comparable to a centralized model (trained using all data). These
results indicate that FL is a solution to effectively train high-performing
models without violating data privacy, a crucial concern in medical
applications. The code is available at:
https://github.com/SanoScience/fl-varying-normalization.

</details>


### [74] [Graph Conditioned Diffusion for Controllable Histopathology Image Generation](https://arxiv.org/abs/2510.07129)
*Sarah Cechnicka,Matthew Baugh,Weitong Zhang,Mischa Dombrowski,Zhe Li,Johannes C. Paetzold,Candice Roufosse,Bernhard Kainz*

Main category: cs.CV

TL;DR: Introduce graph-conditioned diffusion that uses object-level graphs (nodes for major structures + relations) processed by a transformer and injected via text-conditioning to enable fine-grained control in medical image synthesis; validated on histopathology where synthetic data can replace annotated data for segmentation.


<details>
  <summary>Details</summary>
Motivation: High-quality diffusion models lack semantic control because their latent spaces are noisy and unstructured, which is problematic in medical imaging where spatial arrangement, shape, and texture carry diagnostic significance.

Method: Construct a graph with nodes representing key anatomical/structural entities and their relationships; encode these graphs with a transformer; integrate the resulting representation into a diffusion model through the text-conditioning mechanism to steer generation at object/structure level.

Result: In a real-world histopathology scenario, the method produces images whose use in downstream segmentation tasks can reliably substitute for annotated patient data (suggesting comparable utility to real labels).

Conclusion: Graph-based, object-level conditioning restores semantic structure and controllability to diffusion generation in medical imaging, enabling fine-grained control and viable synthetic data for training; implementation is available (code link referenced).

Abstract: Recent advances in Diffusion Probabilistic Models (DPMs) have set new
standards in high-quality image synthesis. Yet, controlled generation remains
challenging, particularly in sensitive areas such as medical imaging. Medical
images feature inherent structure such as consistent spatial arrangement, shape
or texture, all of which are critical for diagnosis. However, existing DPMs
operate in noisy latent spaces that lack semantic structure and strong priors,
making it difficult to ensure meaningful control over generated content. To
address this, we propose graph-based object-level representations for
Graph-Conditioned-Diffusion. Our approach generates graph nodes corresponding
to each major structure in the image, encapsulating their individual features
and relationships. These graph representations are processed by a transformer
module and integrated into a diffusion model via the text-conditioning
mechanism, enabling fine-grained control over generation. We evaluate this
approach using a real-world histopathology use case, demonstrating that our
generated data can reliably substitute for annotated patient data in downstream
segmentation tasks. The code is available here.

</details>


### [75] [Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models](https://arxiv.org/abs/2510.07135)
*Karim El Khoury,Maxime Zanella,Christophe De Vleeschouwer,Benoit Macq*

Main category: cs.CV

TL;DR: They introduce the first structured benchmark for few-shot adaptation of remote sensing vision-language models (RSVLMs), evaluating 5 adaptation strategies on 3 SOTA models across 10 scene classification datasets. Results show large variability—models with similar zero-shot scores adapt very differently—and no single method dominates. They release a reproducible framework and code to spur research.


<details>
  <summary>Details</summary>
Motivation: RSVLMs perform well in zero-shot settings due to large-scale pretraining, but their behavior when only a few labeled examples are available (few-shot adaptation) is underexplored. Practitioners in remote sensing often face low-label regimes, so understanding and improving few-shot adaptation is important.

Method: Build a structured, reproducible benchmark for few-shot adaptation in RS: test five widely used adaptation strategies on three state-of-the-art RSVLMs with different backbones, evaluated across ten remote sensing scene classification datasets.

Result: Despite similar zero-shot performance, RSVLMs show markedly different few-shot adaptation behavior; some models are inherently more adaptable. Performance varies widely across methods and datasets, with no clear overall winner among existing strategies.

Conclusion: Current few-shot adaptation methods are inconsistent for RSVLMs, indicating a need for more robust, RS-tailored approaches. The authors provide an open-source, reproducible benchmarking framework to facilitate systematic evaluation and future method development (code: https://github.com/elkhouryk/fewshot_RSVLMs).

Abstract: Remote Sensing Vision-Language Models (RSVLMs) have shown remarkable
potential thanks to large-scale pretraining, achieving strong zero-shot
performance on various tasks. However, their ability to generalize in low-data
regimes, such as few-shot learning, remains insufficiently explored. In this
work, we present the first structured benchmark for evaluating few-shot
adaptation methods on RSVLMs. We conduct comprehensive experiments across ten
remote sensing scene classification datasets, applying five widely used
few-shot adaptation strategies to three state-of-the-art RSVLMs with varying
backbones. Our findings reveal that models with similar zero-shot performance
can exhibit markedly different behavior under few-shot adaptation, with some
RSVLMs being inherently more amenable to such adaptation than others. The
variability of performance and the absence of a clear winner among existing
methods highlight the need for the development of more robust methods for
few-shot adaptation tailored to RS. To facilitate future research, we provide a
reproducible benchmarking framework and open-source code to systematically
evaluate RSVLMs under few-shot conditions. The source code is publicly
available on Github: https://github.com/elkhouryk/fewshot_RSVLMs

</details>


### [76] [Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods](https://arxiv.org/abs/2510.07143)
*Chenfei Liao,Wensong Wang,Zichen Wen,Xu Zheng,Yiyu Wang,Haocong He,Yuanhuiyi Lyu,Lutao Jiang,Xin Zou,Yuqian Fu,Bin Ren,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: Simple image downsampling beats many sophisticated visual token compression methods on current MLLM benchmarks, revealing a task–benchmark mismatch; the authors propose VTC-Bench, an evaluation framework with data filtering to denoise benchmarks and fairly assess compression methods.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM benchmarks were designed to test perception/reasoning, not the quality of visual token compression. Using them directly to judge compression methods can be misleading. The surprising strength of naive downsampling suggests that current evaluations are noisy and misaligned with the compression task, motivating a purpose-built assessment approach.

Method: Run extensive experiments comparing naive image downsampling with advanced visual token compression techniques across multiple commonly used MLLM benchmarks. Analyze performance sensitivity to compression and use downsampling as a proxy data filter to gauge sample difficulty. Build VTC-Bench, an evaluation framework that integrates a data filtering mechanism to denoise existing benchmarks for the compression task.

Result: Across several widely used benchmarks, naive downsampling consistently outperforms many advanced compression approaches. Two key observations emerge: (i) current benchmarks are noisy for evaluating visual token compression; (ii) downsampling can act as a data filter to assess sample difficulty in this task. Code and data are released publicly.

Conclusion: There is a fundamental mismatch between current benchmarks and the visual token compression objective, leading to unreliable comparisons. VTC-Bench addresses this by filtering/denoising data to provide fairer, more accurate evaluation of visual token compression methods.

Abstract: Recent endeavors to accelerate inference in Multimodal Large Language Models
(MLLMs) have primarily focused on visual token compression. The effectiveness
of these methods is typically assessed by measuring the accuracy drop on
established benchmarks, comparing model performance before and after
compression. However, these benchmarks are originally designed to assess the
perception and reasoning capabilities of MLLMs, rather than to evaluate
compression techniques. As a result, directly applying them to visual token
compression introduces a task mismatch. Strikingly, our investigation reveals
that simple image downsampling consistently outperforms many advanced
compression methods across multiple widely used benchmarks. Through extensive
experiments, we make the following observations: (i) Current benchmarks are
noisy for the visual token compression task. (ii) Down-sampling is able to
serve as a data filter to evaluate the difficulty of samples in the visual
token compression task. Motivated by these findings, we introduce VTC-Bench, an
evaluation framework that incorporates a data filtering mechanism to denoise
existing benchmarks, thereby enabling fairer and more accurate assessment of
visual token compression methods. All data and code are available at
https://github.com/Chenfei-Liao/VTC-Bench.

</details>


### [77] [MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis](https://arxiv.org/abs/2510.07190)
*Yihao Zhi,Chenghong Li,Hongjie Liao,Xihe Yang,Zhengwentai Sun,Jiahao Chang,Xiaodong Cun,Wensen Feng,Xiaoguang Han*

Main category: cs.CV

TL;DR: MV-Performer is a multi-view human-centric video diffusion framework that generates synchronized 360° novel-view videos from a single monocular full-body capture by conditioning on camera-dependent normal maps derived from oriented partial point clouds, achieving state-of-the-art quality and robustness across datasets.


<details>
  <summary>Details</summary>
Motivation: Video diffusion models can implicitly perform 4D novel view synthesis, but existing methods mainly handle small front-view redirections and fail to produce coherent 360° viewpoint changes, especially for humans and in-the-wild videos where monocular depth is noisy. A solution is needed to disambiguate seen vs. unseen surfaces and keep multi-view videos temporally synchronized.

Method: Leverage the MVHumanNet dataset and introduce an informative conditioning signal: camera-dependent normal maps rendered from oriented partial point clouds to guide view synthesis. Propose a multi-view human-centric video diffusion model that fuses the reference monocular video, the partial renderings, and target viewpoints to maintain cross-view temporal synchronization. Add a robust inference procedure tailored for in-the-wild inputs to mitigate artifacts from imperfect monocular depth estimation.

Result: Across three datasets, MV-Performer achieves state-of-the-art performance for human-centric 4D novel view synthesis, producing synchronized, high-quality 360° novel-view videos and demonstrating robustness to real-world inputs with noisy depth.

Conclusion: Conditioning video diffusion on camera-dependent normal maps and fusing multi-source signals enables reliable, synchronized 360° human novel-view video generation from monocular inputs. MV-Performer establishes a strong, robust SOTA baseline for human-centric 4D novel view synthesis.

Abstract: Recent breakthroughs in video generation, powered by large-scale datasets and
diffusion techniques, have shown that video diffusion models can function as
implicit 4D novel view synthesizers. Nevertheless, current methods primarily
concentrate on redirecting camera trajectory within the front view while
struggling to generate 360-degree viewpoint changes. In this paper, we focus on
human-centric subdomain and present MV-Performer, an innovative framework for
creating synchronized novel view videos from monocular full-body captures. To
achieve a 360-degree synthesis, we extensively leverage the MVHumanNet dataset
and incorporate an informative condition signal. Specifically, we use the
camera-dependent normal maps rendered from oriented partial point clouds, which
effectively alleviate the ambiguity between seen and unseen observations. To
maintain synchronization in the generated videos, we propose a multi-view
human-centric video diffusion model that fuses information from the reference
video, partial rendering, and different viewpoints. Additionally, we provide a
robust inference procedure for in-the-wild video cases, which greatly mitigates
the artifacts induced by imperfect monocular depth estimation. Extensive
experiments on three datasets demonstrate our MV-Performer's state-of-the-art
effectiveness and robustness, setting a strong model for human-centric 4D novel
view synthesis.

</details>


### [78] [Resolution scaling governs DINOv3 transfer performance in chest radiograph classification](https://arxiv.org/abs/2510.07191)
*Soroosh Tayebi Arasteh,Mina Shaigan,Christiane Kuhl,Jakob Nikolas Kather,Sven Nebelung,Daniel Truhn*

Main category: cs.CV

TL;DR: Large-scale benchmark (>814k images) shows that for chest radiography, DINOv3 self-supervised initialization paired with ConvNeXt-B and 512×512 inputs outperforms DINOv2 and ImageNet baselines; 1024×1024 brings no gains, pediatric results show no differences, and frozen 7B features underperform finetuning.


<details>
  <summary>Details</summary>
Motivation: Clarify whether modern SSL—specifically DINOv3 with Gram-anchored self-distillation—offers tangible benefits for chest radiograph transfer learning, an imaging domain with fine-grained, boundary-centric abnormalities where SSL’s value is uncertain.

Method: Compare DINOv3 vs DINOv2 vs ImageNet initialization across seven chest X-ray datasets (>814k studies). Evaluate two backbones (ViT-B/16, ConvNeXt-B) at three input resolutions (224, 512, 1024). Also test models using frozen features from a DINOv3-7B model. Primary metric: mean AUROC across labels; examine lesion-type sensitivity (small/boundary-dependent).

Result: At 224×224, DINOv3 ≈ DINOv2 on adult datasets. Moving to 512×512 consistently improves DINOv3 over DINOv2 and ImageNet. No meaningful differences across initializations in pediatric cohort. ConvNeXt-B > ViT-B/16 in all settings. Frozen DINOv3-7B features underperform fully finetuned 86–89M backbones, indicating need for domain adaptation. Scaling to 1024×1024 yields no further accuracy gains. Resolution gains are strongest for boundary-dependent and small focal findings.

Conclusion: In chest radiography, higher but moderate resolution (512×512) is key to realizing benefits of modern SSL. Best-performing recipe: finetuned, mid-sized ConvNeXt-B with DINOv3 initialization at 512×512. Larger inputs have minimal ROI, and frozen foundation features are inferior to task-specific finetuning. Gains are most pronounced for subtle or boundary-centered lesions relevant to acute care.

Abstract: Self-supervised learning (SSL) has advanced visual representation learning,
but its value in chest radiography, a high-volume imaging modality with
fine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL
models through Gram-anchored self-distillation. Whether these design choices
improve transfer learning for chest radiography has not been systematically
tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across
seven datasets (n>814,000). Two representative backbones were evaluated:
ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and
1024x1024 pixels. We additionally assessed frozen features from a 7B model. The
primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2
achieved comparable performance on adult datasets. Increasing resolution to
512x512 yielded consistent improvements for DINOv3 over both DINOv2 and
ImageNet. In contrast, results in pediatric cohort showed no differences across
initializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models
using frozen DINOv3-7B features underperformed relative to fully finetuned
86-89M-parameter backbones, highlighting the importance of domain adaptation.
Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains
were most evident for boundary-dependent and small focal abnormalities. In
chest radiography, higher input resolution is critical for leveraging the
benefits of modern self-supervised models. 512x512 pixels represent a practical
upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest
performance, while larger inputs offer minimal return on cost. Clinically,
these findings support use of finetuned, mid-sized backbones at 512x512 for
chest radiograph interpretation, with the greatest gains expected in detecting
subtle or boundary-centered lesions relevant to emergency and critical care
settings.

</details>


### [79] [EigenScore: OOD Detection using Covariance in Diffusion Models](https://arxiv.org/abs/2510.07206)
*Shirin Shoushtari,Yi Wang,Xiao Shi,M. Salman Asif,Ulugbek S. Kamilov*

Main category: cs.CV

TL;DR: EigenScore uses the eigenvalue spectrum (trace and top eigenvalues) of the posterior covariance from a diffusion model as an OOD score, estimated efficiently without Jacobians; it yields state-of-the-art OOD detection, including challenging near-OOD cases.


<details>
  <summary>Details</summary>
Motivation: OOD detection is essential for safety-critical deployment, yet existing diffusion-based detectors often fail on near-distribution shifts. A more principled and reliable signal of distribution mismatch is needed, along with a tractable way to compute it on top of diffusion models.

Method: Define an OOD score from the posterior covariance induced by a diffusion model’s denoiser; use its spectral properties—particularly the trace and leading eigenvalues—which are argued to inflate under distribution shift. Provide analysis linking posterior covariance magnitude to distribution mismatch. For efficiency, estimate leading eigenvalues via a Jacobian-free subspace iteration using only forward passes of the denoiser.

Result: Across benchmarks, EigenScore achieves state-of-the-art AUROC, improving up to 5% over the best baselines, and remains robust in near-OOD scenarios such as CIFAR-10 vs CIFAR-100 where prior diffusion-based methods underperform.

Conclusion: The posterior covariance spectrum is a reliable, theoretically grounded, and practical signal for OOD detection in diffusion models. EigenScore delivers strong and robust performance while remaining computationally tractable.

Abstract: Out-of-distribution (OOD) detection is critical for the safe deployment of
machine learning systems in safety-sensitive domains. Diffusion models have
recently emerged as powerful generative models, capable of capturing complex
data distributions through iterative denoising. Building on this progress,
recent work has explored their potential for OOD detection. We propose
EigenScore, a new OOD detection method that leverages the eigenvalue spectrum
of the posterior covariance induced by a diffusion model. We argue that
posterior covariance provides a consistent signal of distribution shift,
leading to larger trace and leading eigenvalues on OOD inputs, yielding a clear
spectral signature. We further provide analysis explicitly linking posterior
covariance to distribution mismatch, establishing it as a reliable signal for
OOD detection. To ensure tractability, we adopt a Jacobian-free subspace
iteration method to estimate the leading eigenvalues using only forward
evaluations of the denoiser. Empirically, EigenScore achieves SOTA performance,
with up to 5% AUROC improvement over the best baseline. Notably, it remains
robust in near-OOD settings such as CIFAR-10 vs CIFAR-100, where existing
diffusion-based methods often fail.

</details>


### [80] [GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation](https://arxiv.org/abs/2510.07217)
*Wen Ye,Zhaocheng Liu,Yuwei Gui,Tingyu Yuan,Yunyue Su,Bowen Fang,Chaoyang Zhao,Qiang Liu,Liang Wang*

Main category: cs.CV

TL;DR: GenPilot is a model-agnostic, test-time prompt optimization system for text-to-image generation that iteratively rewrites input prompts via a multi-agent process (error analysis, adaptive exploration, verification, and memory), improving text–image alignment and structural coherence with up to 16.9% and 5.7% gains on DPG-bench and Geneval.


<details>
  <summary>Details</summary>
Motivation: Modern text-to-image models often misinterpret long, complex prompts, causing semantic omissions and inconsistencies. Existing fixes—model fine-tuning or prior automatic prompt optimization—either require training and are model-specific, or lack systematic diagnosis and refinement. Test-time scaling methods tune sampling or noise but leave prompts unchanged, limiting interpretability and adaptability. A model-agnostic, interpretable, test-time method that operates directly on the prompt is needed.

Method: Introduce GenPilot, a plug-and-play multi-agent framework that, at test time, iteratively edits the input prompt. Components include: (1) error analysis to detect semantic/structural failures; (2) clustering-based adaptive exploration to diversify and focus candidate prompt edits; (3) fine-grained verification to assess candidates; and (4) a memory module to accumulate findings across iterations. The system summarizes common error patterns and guides refinements accordingly. It is designed to work across different T2I models without retraining.

Result: On DPG-bench and Geneval, GenPilot yields improvements up to 16.9% and 5.7%, respectively, enhancing text–image consistency and structural coherence. These results indicate reliable gains over fixed-prompt baselines and prior APO/test-time scaling strategies.

Conclusion: Test-time prompt optimization via GenPilot is an effective, interpretable, and model-agnostic approach for complex prompts in text-to-image synthesis. It systematically diagnoses and refines prompts, improves alignment and structure, and provides reusable error/refinement patterns. Open-source code is provided for reproducibility.

Abstract: Text-to-image synthesis has made remarkable progress, yet accurately
interpreting complex and lengthy prompts remains challenging, often resulting
in semantic inconsistencies and missing details. Existing solutions, such as
fine-tuning, are model-specific and require training, while prior automatic
prompt optimization (APO) approaches typically lack systematic error analysis
and refinement strategies, resulting in limited reliability and effectiveness.
Meanwhile, test-time scaling methods operate on fixed prompts and on noise or
sample numbers, limiting their interpretability and adaptability. To solve
these, we introduce a flexible and efficient test-time prompt optimization
strategy that operates directly on the input text. We propose a plug-and-play
multi-agent system called GenPilot, integrating error analysis,
clustering-based adaptive exploration, fine-grained verification, and a memory
module for iterative optimization. Our approach is model-agnostic,
interpretable, and well-suited for handling long and complex prompts.
Simultaneously, we summarize the common patterns of errors and the refinement
strategy, offering more experience and encouraging further exploration.
Experiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7%
demonstrate the strong capability of our methods in enhancing the text and
image consistency and structural coherence of generated images, revealing the
effectiveness of our test-time prompt optimization strategy. The code is
available at https://github.com/27yw/GenPilot.

</details>


### [81] [TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation](https://arxiv.org/abs/2510.07249)
*Jiaben Chen,Zixin Wang,Ailing Zeng,Yang Fu,Xueyang Yu,Siyuan Cen,Julian Tanke,Yihang Chen,Koichi Saito,Yuki Mitsufuji,Chuang Gan*

Main category: cs.CV

TL;DR: TalkCuts is a large-scale, richly annotated dataset for multi-shot human speech video generation, paired with an LLM-directed baseline (Orator). Training on TalkCuts improves cinematographic coherence and visual appeal of generated multi-shot speech videos.


<details>
  <summary>Details</summary>
Motivation: Existing resources emphasize single-shot, static viewpoints, limiting progress on generating coherent, controllable multi-shot speech videos. The field needs data and tools that capture realistic camera transitions, body configurations, and multimodal cues for evaluation and learning.

Method: Build TalkCuts: 164k clips (>500 hours) of human speech with diverse camera shots (close-up, half-, full-body), textual descriptions, 2D keypoints, and 3D SMPL-X motion across 10k+ identities. Propose Orator, an LLM-guided multimodal generation framework where the LLM acts as a director specifying camera transitions, gestures, and vocal modulation; an integrated module synthesizes long-form videos. Evaluate in pose-guided and audio-driven settings.

Result: Models trained on TalkCuts produce multi-shot speech videos with significantly better cinematographic coherence and visual appeal. Orator demonstrates the dataset’s utility, yielding coherent long-form outputs in both pose-guided and audio-driven experiments.

Conclusion: TalkCuts offers a robust foundation for controllable, multi-shot speech video generation and broader multimodal learning; the dataset and baseline enable future research on coherent, directive, and multimodal video synthesis.

Abstract: In this work, we present TalkCuts, a large-scale dataset designed to
facilitate the study of multi-shot human speech video generation. Unlike
existing datasets that focus on single-shot, static viewpoints, TalkCuts offers
164k clips totaling over 500 hours of high-quality human speech videos with
diverse camera shots, including close-up, half-body, and full-body views. The
dataset includes detailed textual descriptions, 2D keypoints and 3D SMPL-X
motion annotations, covering over 10k identities, enabling multimodal learning
and evaluation. As a first attempt to showcase the value of the dataset, we
present Orator, an LLM-guided multi-modal generation framework as a simple
baseline, where the language model functions as a multi-faceted director,
orchestrating detailed specifications for camera transitions, speaker
gesticulations, and vocal modulation. This architecture enables the synthesis
of coherent long-form videos through our integrated multi-modal video
generation module. Extensive experiments in both pose-guided and audio-driven
settings show that training on TalkCuts significantly enhances the
cinematographic coherence and visual appeal of generated multi-shot speech
videos. We believe TalkCuts provides a strong foundation for future work in
controllable, multi-shot speech video generation and broader multimodal
learning.

</details>


### [82] [Evaluating Fundus-Specific Foundation Models for Diabetic Macular Edema Detection](https://arxiv.org/abs/2510.07277)
*Franco Javier Arellano,José Ignacio Orlando*

Main category: cs.CV

TL;DR: Benchmarking foundation models (RETFound, FLAIR) against a fine-tuned EfficientNet-B0 for DME detection on multiple fundus datasets shows FMs do not consistently outperform a lightweight CNN; EfficientNet often ranks best, while FLAIR offers competitive zero-shot results with careful prompting.


<details>
  <summary>Details</summary>
Motivation: DME is a major cause of vision loss in DR, but annotated fundus data are scarce. Foundation models promise strong performance with limited labels; it is unclear whether they provide advantages for fine-grained ophthalmic tasks like DME detection.

Method: Systematic comparison of RETFound and FLAIR (as foundation models) versus an EfficientNet-B0 backbone under different training regimes (including zero-shot and fine-tuning) and evaluation settings on IDRiD, MESSIDOR-2, and OEFI. Performance measured mainly via AUROC and AUC-PR; FLAIR evaluated with prompt-based zero-shot inference.

Result: EfficientNet-B0 ranks first or second in most evaluation settings by AUROC and AUC-PR. RETFound shows promising results primarily on OEFI. FLAIR exhibits competitive zero-shot performance, achieving strong AUC-PR when prompted appropriately. Overall, FMs do not consistently surpass a well-tuned CNN.

Conclusion: For DME detection from fundus images, foundation models may not offer consistent gains over fine-tuned lightweight CNNs, even after fine-tuning. In data-scarce environments, EfficientNet-like models remain strong baselines, while FM zero-shot capabilities can be useful but are not broadly superior.

Abstract: Diabetic Macular Edema (DME) is a leading cause of vision loss among patients
with Diabetic Retinopathy (DR). While deep learning has shown promising results
for automatically detecting this condition from fundus images, its application
remains challenging due the limited availability of annotated data. Foundation
Models (FM) have emerged as an alternative solution. However, it is unclear if
they can cope with DME detection in particular. In this paper, we
systematically compare different FM and standard transfer learning approaches
for this task. Specifically, we compare the two most popular FM for retinal
images--RETFound and FLAIR--and an EfficientNet-B0 backbone, across different
training regimes and evaluation settings in IDRiD, MESSIDOR-2 and
OCT-and-Eye-Fundus-Images (OEFI). Results show that despite their scale, FM do
not consistently outperform fine-tuned CNNs in this task. In particular, an
EfficientNet-B0 ranked first or second in terms of area under the ROC and
precision/recall curves in most evaluation settings, with RETFound only showing
promising results in OEFI. FLAIR, on the other hand, demonstrated competitive
zero-shot performance, achieving notable AUC-PR scores when prompted
appropriately. These findings reveal that FM might not be a good tool for
fine-grained ophthalmic tasks such as DME detection even after fine-tuning,
suggesting that lightweight CNNs remain strong baselines in data-scarce
environments.

</details>


### [83] [SpecGuard: Spectral Projection-based Advanced Invisible Watermarking](https://arxiv.org/abs/2510.07302)
*Inzamamul Alam,Md Tanvir Islam,Khan Muhammad,Simon S. Woo*

Main category: cs.CV

TL;DR: SpecGuard embeds watermarks in high-frequency spectral components of hidden convolutional features using wavelet-based decomposition and FFT projection, with a strength-controlled encoder and a Parseval’s-theorem-guided decoder, achieving robust, invisible, and higher-capacity watermarking that outperforms prior methods under diverse attacks.


<details>
  <summary>Details</summary>
Motivation: Existing image watermarking methods struggle to remain both imperceptible and robust against real-world transformations, including geometric distortions, regeneration (e.g., model-based re-synthesis), and adversarial perturbations. The paper aims to close this robustness gap without sacrificing invisibility or capacity.

Method: Convert feature representations from spatial to frequency domain via an efficient FFT-based spectral projection; decompose via wavelets to isolate higher-frequency bands; embed message bits into these bands within hidden convolution layers using a tunable strength factor to balance invisibility and robustness; train a decoder that leverages Parseval’s theorem to reliably learn and extract the watermark after challenging transformations.

Result: On benchmarks measuring invisibility, capacity, and robustness to adversarial, geometric, and regeneration-based distortions, SpecGuard outperforms state-of-the-art watermarking methods. The abstract claims comprehensive experiments and public code for reproducibility.

Conclusion: Frequency-domain embedding in high-frequency bands, combined with a theoretically grounded decoder and strength-controlled encoding, yields a practical watermarking approach that is more robust and imperceptible than existing methods, with demonstrated superiority across multiple attack scenarios.

Abstract: Watermarking embeds imperceptible patterns into images for authenticity
verification. However, existing methods often lack robustness against various
transformations primarily including distortions, image regeneration, and
adversarial perturbation, creating real-world challenges. In this work, we
introduce SpecGuard, a novel watermarking approach for robust and invisible
image watermarking. Unlike prior approaches, we embed the message inside hidden
convolution layers by converting from the spatial domain to the frequency
domain using spectral projection of a higher frequency band that is decomposed
by wavelet projection. Spectral projection employs Fast Fourier Transform
approximation to transform spatial data into the frequency domain efficiently.
In the encoding phase, a strength factor enhances resilience against diverse
attacks, including adversarial, geometric, and regeneration-based distortions,
ensuring the preservation of copyrighted information. Meanwhile, the decoder
leverages Parseval's theorem to effectively learn and extract the watermark
pattern, enabling accurate retrieval under challenging transformations. We
evaluate the proposed SpecGuard based on the embedded watermark's invisibility,
capacity, and robustness. Comprehensive experiments demonstrate the proposed
SpecGuard outperforms the state-of-the-art models. To ensure reproducibility,
the full code is released on
\href{https://github.com/inzamamulDU/SpecGuard_ICCV_2025}{\textcolor{blue}{\textbf{GitHub}}}.

</details>


### [84] [MATRIX: Mask Track Alignment for Interaction-aware Video Generation](https://arxiv.org/abs/2510.07310)
*Siyoon Jin,Seongchan Kim,Dahyun Chung,Jaeho Lee,Hyunwook Choi,Jisu Nam,Jiyoung Kim,Seungryong Kim*

Main category: cs.CV

TL;DR: They analyze how video Diffusion Transformers represent multi-instance interactions, curate a labeled video dataset (MATRIX-11K), find a small set of interaction-dominant layers responsible for grounding and temporal binding, and propose a targeted attention-regularization method (MATRIX) that improves interaction fidelity and reduces drift; plus an evaluation protocol (InterGenEval).


<details>
  <summary>Details</summary>
Motivation: Video DiTs often fail at scenes with multiple entities and subject–object interactions. The authors ask whether and where such interactions are encoded inside the models, and how to leverage that knowledge to improve generation quality.

Method: 1) Build MATRIX-11K: videos with interaction-aware captions and multi-instance mask tracks. 2) Analyze DiTs from two angles: (a) semantic grounding via video-to-text attention to test whether noun/verb tokens attend to the correct instances/relations; (b) semantic propagation via video-to-video attention to test whether instance bindings persist across frames. 3) Identify interaction-dominant layers where these effects concentrate. 4) MATRIX: apply a regularization that aligns attention maps in those layers with the provided mask tracks. 5) Propose InterGenEval to benchmark interaction-aware video generation.

Result: Grounding and propagation signals concentrate in a small subset of layers. Applying MATRIX in those layers improves interaction fidelity and semantic alignment, while reducing temporal drift and hallucinations. Extensive ablations support the design. Code and model weights are to be released.

Conclusion: Understanding attention patterns reveals key layers governing interactions; aligning their attention with instance tracks is a simple, effective way to enhance interaction modeling in video DiTs. The dataset and evaluation protocol provide infrastructure for further research.

Abstract: Video DiTs have advanced video generation, yet they still struggle to model
multi-instance or subject-object interactions. This raises a key question: How
do these models internally represent interactions? To answer this, we curate
MATRIX-11K, a video dataset with interaction-aware captions and multi-instance
mask tracks. Using this dataset, we conduct a systematic analysis that
formalizes two perspectives of video DiTs: semantic grounding, via
video-to-text attention, which evaluates whether noun and verb tokens capture
instances and their relations; and semantic propagation, via video-to-video
attention, which assesses whether instance bindings persist across frames. We
find both effects concentrate in a small subset of interaction-dominant layers.
Motivated by this, we introduce MATRIX, a simple and effective regularization
that aligns attention in specific layers of video DiTs with multi-instance mask
tracks from the MATRIX-11K dataset, enhancing both grounding and propagation.
We further propose InterGenEval, an evaluation protocol for interaction-aware
video generation. In experiments, MATRIX improves both interaction fidelity and
semantic alignment while reducing drift and hallucination. Extensive ablations
validate our design choices. Codes and weights will be released.

</details>


### [85] [WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation](https://arxiv.org/abs/2510.07313)
*Zezhong Qian,Xiaowei Chi,Yuming Li,Shizun Wang,Zhiyuan Qin,Xiaozhu Ju,Sirui Han,Shanghang Zhang*

Main category: cs.CV

TL;DR: WristWorld is a two-stage 4D world model that generates wrist-view videos purely from anchor-view inputs by first reconstructing wrist-view geometry (poses + 4D point clouds) using an extended VGGT with a Spatial Projection Consistency loss, then synthesizing temporally coherent videos from that reconstructed viewpoint, achieving SOTA spatial consistency and improving VLA performance.


<details>
  <summary>Details</summary>
Motivation: Fine-grained hand–object interactions are best captured from wrist-view cameras, but large-scale datasets rarely include wrist views, creating a performance gap between abundant anchor views and scarce wrist views. Existing world models cannot synthesize wrist-view videos from anchor views because they require a wrist-view first frame. Recent visual geometry models (e.g., VGGT) provide geometric and cross-view priors that could enable generation under extreme viewpoint shifts, motivating a method to bridge this gap.

Method: Two-stage pipeline: (i) Reconstruction—extend VGGT and introduce a Spatial Projection Consistency (SPC) loss to estimate geometrically consistent wrist-view camera poses and dense 4D point clouds from anchor views. (ii) Generation—use a video generation model to render temporally coherent wrist-view videos from the reconstructed viewpoint, yielding a 4D world model capable of cross-view synthesis without any wrist-view inputs.

Result: On Droid, Calvin, and Franka Panda datasets, WristWorld achieves state-of-the-art wrist-view video generation with superior spatial consistency. It also boosts VLA performance, increasing average task completion length on Calvin by 3.81% and closing 42.4% of the anchor–wrist view performance gap.

Conclusion: Generating wrist-view videos from anchor views alone is feasible and beneficial for manipulation: WristWorld bridges the anchor–wrist data gap, produces spatially consistent wrist-view sequences, and measurably improves VLA outcomes. The approach leverages geometric priors (VGGT) and a new SPC loss to handle extreme viewpoint shifts, setting a strong baseline for cross-view 4D world modeling.

Abstract: Wrist-view observations are crucial for VLA models as they capture
fine-grained hand-object interactions that directly enhance manipulation
performance. Yet large-scale datasets rarely include such recordings, resulting
in a substantial gap between abundant anchor views and scarce wrist views.
Existing world models cannot bridge this gap, as they require a wrist-view
first frame and thus fail to generate wrist-view videos from anchor views
alone. Amid this gap, recent visual geometry models such as VGGT emerge with
geometric and cross-view priors that make it possible to address extreme
viewpoint shifts. Inspired by these insights, we propose WristWorld, the first
4D world model that generates wrist-view videos solely from anchor views.
WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and
incorporates our Spatial Projection Consistency (SPC) Loss to estimate
geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation,
which employs our video generation model to synthesize temporally coherent
wrist-view videos from the reconstructed perspective. Experiments on Droid,
Calvin, and Franka Panda demonstrate state-of-the-art video generation with
superior spatial consistency, while also improving VLA performance, raising the
average task completion length on Calvin by 3.81% and closing 42.4% of the
anchor-wrist view gap.

</details>


### [86] [Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers](https://arxiv.org/abs/2510.07316)
*Gangwei Xu,Haotong Lin,Hongcheng Luo,Xianqi Wang,Jingfeng Yao,Lianghui Zhu,Yuechuan Pu,Cheng Chi,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Sida Peng,Xin Yang*

Main category: cs.CV

TL;DR: Pixel-Perfect Depth is a monocular depth estimator that performs diffusion directly in pixel space, avoiding VAE-induced flying-pixel artifacts, and uses semantic prompting plus a cascaded DiT to achieve state-of-the-art accuracy and edge fidelity.


<details>
  <summary>Details</summary>
Motivation: Latent-space generative depth methods (fine-tuned Stable Diffusion) rely on a VAE that compresses depth maps, which introduces edge/detail artifacts known as flying pixels. The goal is to produce cleaner, more geometrically consistent depth and point clouds without these artifacts.

Method: Replace latent-space generation with pixel-space diffusion. Introduce (1) Semantics-Prompted DiT (SP-DiT) that injects semantic representations from vision foundation models to guide diffusion for globally consistent semantics and fine details; and (2) a Cascade DiT that progressively increases token counts to control computation while improving resolution/accuracy.

Result: Achieves best performance among published generative depth models on five benchmarks and shows large gains on edge-aware point cloud evaluation, producing flying-pixel-free point clouds from predicted depth.

Conclusion: Pixel-space diffusion guided by semantic prompts and a cascaded token design effectively removes VAE-induced artifacts, yielding superior depth accuracy and cleaner edges, establishing a new state of the art for generative monocular depth and downstream point cloud quality.

Abstract: This paper presents Pixel-Perfect Depth, a monocular depth estimation model
based on pixel-space diffusion generation that produces high-quality,
flying-pixel-free point clouds from estimated depth maps. Current generative
depth estimation models fine-tune Stable Diffusion and achieve impressive
performance. However, they require a VAE to compress depth maps into latent
space, which inevitably introduces \textit{flying pixels} at edges and details.
Our model addresses this challenge by directly performing diffusion generation
in the pixel space, avoiding VAE-induced artifacts. To overcome the high
complexity associated with pixel-space generation, we introduce two novel
designs: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which
incorporate semantic representations from vision foundation models into DiT to
prompt the diffusion process, thereby preserving global semantic consistency
while enhancing fine-grained visual details; and 2) Cascade DiT Design that
progressively increases the number of tokens to further enhance efficiency and
accuracy. Our model achieves the best performance among all published
generative models across five benchmarks, and significantly outperforms all
other models in edge-aware point cloud evaluation.

</details>


### [87] [Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms](https://arxiv.org/abs/2510.07317)
*Natacha Kuete Meli,Shuteng Wang,Marcel Seelbach Benkner,Michele Sasdelli,Tat-Jun Chin,Tolga Birdal,Michael Moeller,Vladislav Golyanik*

Main category: cs.CV

TL;DR: A survey of Quantum‑enhanced Computer Vision (QeCV) that introduces the field, explains quantum paradigms (gate-based and annealing), details hardware‑compatible formulations and tools, and outlines resources, challenges, and implications for the CV community.


<details>
  <summary>Details</summary>
Motivation: Classical computer vision often faces intractable or slow optimization and only approximate solutions. Quantum computing may offer better scalability and new modeling capacity (e.g., parametrized quantum circuits), but CV researchers need a consolidated reference to understand principles, methods, tooling, and research practices for QeCV.

Method: Holistic literature review and tutorial-style exposition: introduce QeCV concepts; map vision problems to quantum-compatible formulations; cover two main quantum paradigms (gate-based circuits and quantum annealing); describe operational principles of quantum hardware; survey programming/simulation tools and learning materials; discuss publishing/reviewing practices, open challenges, and social implications.

Result: A structured synthesis of the QeCV landscape: conceptual framework, taxonomy of methods across quantum paradigms, guidance for hardware-aware formulations, overview of accessible toolchains and simulators, curated resources for learning, and identification of current research gaps and evaluation considerations.

Conclusion: QeCV is promising for certain classes of vision problems due to potential scalability and alternative learning models, but realizing benefits requires fundamentally new, hardware-compatible algorithms and careful methodology. The community now has a reference roadmap, yet key challenges (algorithm design, practicality on near-term devices, evaluation standards, and societal impacts) remain open.

Abstract: Quantum-enhanced Computer Vision (QeCV) is a new research field at the
intersection of computer vision, optimisation theory, machine learning and
quantum computing. It has high potential to transform how visual signals are
processed and interpreted with the help of quantum computing that leverages
quantum-mechanical effects in computations inaccessible to classical (i.e.
non-quantum) computers. In scenarios where existing non-quantum methods cannot
find a solution in a reasonable time or compute only approximate solutions,
quantum computers can provide, among others, advantages in terms of better time
scalability for multiple problem classes. Parametrised quantum circuits can
also become, in the long term, a considerable alternative to classical neural
networks in computer vision. However, specialised and fundamentally new
algorithms must be developed to enable compatibility with quantum hardware and
unveil the potential of quantum computational paradigms in computer vision.
This survey contributes to the existing literature on QeCV with a holistic
review of this research field. It is designed as a quantum computing reference
for the computer vision community, targeting computer vision students,
scientists and readers with related backgrounds who want to familiarise
themselves with QeCV. We provide a comprehensive introduction to QeCV, its
specifics, and methodologies for formulations compatible with quantum hardware
and QeCV methods, leveraging two main quantum computational paradigms, i.e.
gate-based quantum computing and quantum annealing. We elaborate on the
operational principles of quantum computers and the available tools to access,
program and simulate them in the context of QeCV. Finally, we review existing
quantum computing tools and learning materials and discuss aspects related to
publishing and reviewing QeCV papers, open challenges and potential social
implications.

</details>


### [88] [Temporal Prompting Matters: Rethinking Referring Video Object Segmentation](https://arxiv.org/abs/2510.07319)
*Ci-Siang Lin,Min-Hung Chen,I-Jieh Liu,Chien-Yi Wang,Sifei Liu,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: Tenet reframes RVOS by offloading mask prediction to foundation segmentation models and focusing on generating and selecting high‑quality temporal prompts from detectors/trackers via preference learning, achieving strong results with far less dense-mask training.


<details>
  <summary>Details</summary>
Motivation: End-to-end RVOS methods depend on costly dense mask annotations and heavy training, limiting scalability. The authors aim to exploit powerful image foundation segmentation models and reduce supervision/training by isolating the core challenges: linking text to objects over time and providing good guidance for segmentation.

Method: Decompose RVOS into three factors: (1) referring (language grounding), (2) video (temporal association), and (3) segmentation. Use off-the-shelf object detectors and trackers to produce temporal prompts (object tubes) aligned with the query sentence. Recognizing that detector/track confidence is unreliable for prompt quality, introduce Prompt Preference Learning to assess and select the best temporal prompts. Feed the selected prompts to image-based foundation segmentation models to generate masks for the referred object, enabling efficient adaptation to RVOS without dense mask training.

Result: On standard RVOS benchmarks, Tenet yields effective performance—producing high-quality masks for referred objects—while avoiding end-to-end dense-mask training. (Abstract reports empirical effectiveness but no specific metrics.)

Conclusion: Carefully generated and preference-learned temporal prompts, combined with foundation segmentation models, provide a scalable and efficient solution to RVOS. Tenet demonstrates that handling referring and temporal factors well is sufficient to achieve strong RVOS performance without heavy annotation or training burdens.

Abstract: Referring Video Object Segmentation (RVOS) aims to segment the object
referred to by the query sentence in the video. Most existing methods require
end-to-end training with dense mask annotations, which could be
computation-consuming and less scalable. In this work, we rethink the RVOS
problem and aim to investigate the key to this task. Based on existing
foundation segmentation models, we decompose the RVOS task into referring,
video, and segmentation factors, and propose a Temporal Prompt Generation and
Selection (Tenet) framework to address the referring and video factors while
leaving the segmentation problem to foundation models. To efficiently adapt
image-based foundation segmentation models to referring video object
segmentation, we leverage off-the-shelf object detectors and trackers to
produce temporal prompts associated with the referring sentence. While
high-quality temporal prompts could be produced, they can not be easily
identified from confidence scores. To tackle this issue, we propose Prompt
Preference Learning to evaluate the quality of the produced temporal prompts.
By taking such prompts to instruct image-based foundation segmentation models,
we would be able to produce high-quality masks for the referred object,
enabling efficient model adaptation to referring video object segmentation.
Experiments on RVOS benchmarks demonstrate the effectiveness of the Tenet
framework.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [89] [OpenStaxQA: A multilingual dataset based on open-source college textbooks](https://arxiv.org/abs/2510.06239)
*Pranav Gupta*

Main category: cs.CL

TL;DR: OpenStaxQA introduces a multilingual, college-level QA benchmark built from 43 open-licensed textbooks; the authors fine-tune ~7B-parameter LLMs with QLoRA, evaluate on this benchmark, and probe zero-shot transfer on ARC-Challenge, while discussing broader impacts.


<details>
  <summary>Details</summary>
Motivation: Educational LLMs lack domain-specific, permissively licensed, multilingual benchmarks at the college level. The authors seek to enable rigorous evaluation and fine-tuning for classroom-relevant tasks and to test whether such training data yields transferable reasoning gains on external benchmarks.

Method: Construct a QA benchmark (OpenStaxQA) from 43 OpenStax textbooks in English, Spanish, and Polish. Fine-tune approximately 7B-parameter LLMs using quantized LoRA (QLoRA) and evaluate on OpenStaxQA. Assess zero-shot transfer by evaluating on the AI2 ARC-Challenge dev set. Include a discussion of broader societal and educational impacts.

Result: They release the OpenStaxQA dataset and report fine-tuning/evaluation of ~7B LLMs with QLoRA on it, plus zero-shot results on ARC-Challenge dev to gauge transfer. The abstract does not provide quantitative results; it suggests empirical evaluation but omits specific metrics.

Conclusion: OpenStaxQA offers a permissively licensed, multilingual benchmark tailored to college-level educational applications and suitable for training/evaluating compact LLMs. The work indicates potential for transfer to broader reasoning tasks and underscores ethical and societal considerations for such datasets.

Abstract: We present OpenStaxQA, an evaluation benchmark specific to college-level
educational applications based on 43 open-source college textbooks in English,
Spanish, and Polish, available under a permissive Creative Commons license. We
finetune and evaluate large language models (LLMs) with approximately 7 billion
parameters on this dataset using quantized low rank adapters (QLoRa).
Additionally we also perform a zero-shot evaluation on the AI2 reasoning
challenge dev dataset in order to check if OpenStaxQA can lead to an improved
performance on other tasks. We also discuss broader impacts relevant to
datasets such as OpenStaxQA.

</details>


### [90] [Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets](https://arxiv.org/abs/2510.06240)
*Jiqun Pan,Zhenke Duan,Jiani Tu,Anzhi Cheng,Yanqing Wang*

Main category: cs.CL

TL;DR: KG-MASD distills multi-agent LLM reasoning into compact, edge-deployable QA models by grounding collaboration in a knowledge graph and casting distillation as an MDP, yielding higher accuracy (2.4–20.1% over baselines) and more verifiable outputs for industrial use.


<details>
  <summary>Details</summary>
Motivation: Industrial QA in safety-critical settings needs reliable, auditable reasoning. Multi-agent LLMs can reason deeply but are iterative, costly, and produce unverifiable outputs; standard distillation fails to transfer their collaborative reasoning to small models. A structured, verifiable prior is needed to make distilled reasoning dependable.

Method: Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD): formulate the distillation process as a Markov Decision Process; inject a knowledge graph as a structured, verifiable prior to enrich state representations and stabilize/ensure convergence; integrate collaborative multi-agent reasoning with knowledge grounding to generate high-confidence instruction-tuning data; jointly distill both reasoning depth and verifiability into a compact student model suitable for edge deployment.

Result: On an industrial QA dataset, KG-MASD improves accuracy by 2.4%–20.1% over baselines and shows markedly better reliability/verifiability; produces high-confidence instruction data. Code/data are released (GitHub).

Conclusion: Grounding multi-agent collaboration with a knowledge graph and an MDP framing enables distillation that preserves depth and verifiability, producing trustworthy, deployable industrial QA models. The approach appears effective and practical for safety-critical scenarios.

Abstract: Industrial question-answering (QA) systems require higher safety and
reliability than general-purpose dialogue models, as errors in high-risk
scenarios such as equipment fault diagnosis can have severe consequences.
Although multi-agent large language models enhance reasoning depth, they suffer
from uncontrolled iterations and unverifiable outputs, and conventional
distillation methods struggle to transfer collaborative reasoning capabilities
to lightweight, deployable student models. To address these challenges, we
propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our
approach formulates distillation as a Markov Decision Process and incorporates
a knowledge graph as a verifiable structured prior to enrich state
representation and ensure convergence. By integrating collaborative reasoning
with knowledge grounding, KG-MASD generates high-confidence instruction-tuning
data and jointly distills reasoning depth and verifiability into compact
student models suitable for edge deployment. Experiments on an industrial QA
dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent
over baselines and significantly enhances reliability, enabling trustworthy AI
deployment in safety-critical industrial scenarios. Code and data are available
at https://github.com/erwinmsmith/KG-MAD/.

</details>


### [91] [Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses](https://arxiv.org/abs/2510.06242)
*Subin An,Yugyeong Ji,Junyoung Kim,Heejin Kook,Yang Lu,Josh Seltzer*

Main category: cs.CL

TL;DR: Introduces a two-stage, LLM-assisted framework to assess the quality of human-written open-ended survey responses: first filter gibberish, then score effort, relevance, and completeness; validated on English and Korean datasets, it outperforms existing metrics and correlates strongly with expert judgments.


<details>
  <summary>Details</summary>
Motivation: Low-quality open-ended survey responses create manual filtering burdens and can bias conclusions. Existing automatic evaluators focus on LLM-generated text and don’t capture the distinct characteristics of human responses, motivating a human-oriented evaluation approach.

Method: A two-stage framework: (1) gibberish filtering to remove nonsensical responses; (2) LLM-based evaluation along three empirically derived dimensions—effort, relevance, and completeness—tailored to human survey data.

Result: On English and Korean datasets, the framework outperforms prior metrics, shows strong correlation with expert assessments, and proves useful for tasks like predicting response quality and rejecting low-quality responses.

Conclusion: The proposed framework is an effective, practical solution for evaluating human-written survey responses, enabling higher-quality datasets and reducing manual workload, with demonstrated multilingual applicability and better alignment with expert judgments than existing methods.

Abstract: Open-ended survey responses provide valuable insights in marketing research,
but low-quality responses not only burden researchers with manual filtering but
also risk leading to misleading conclusions, underscoring the need for
effective evaluation. Existing automatic evaluation methods target
LLM-generated text and inadequately assess human-written responses with their
distinct characteristics. To address such characteristics, we propose a
two-stage evaluation framework specifically designed for human survey
responses. First, gibberish filtering removes nonsensical responses. Then,
three dimensions-effort, relevance, and completeness-are evaluated using LLM
capabilities, grounded in empirical analysis of real-world survey data.
Validation on English and Korean datasets shows that our framework not only
outperforms existing metrics but also demonstrates high practical applicability
for real-world applications such as response quality prediction and response
rejection, showing strong correlations with expert assessment.

</details>


### [92] [CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning](https://arxiv.org/abs/2510.06243)
*Qihua Dong,Luis Figueroa,Handong Zhao,Kushal Kafle,Jason Kuen,Zhihong Ding,Scott Cohen,Yun Fu*

Main category: cs.CL

TL;DR: Proposes “CoT Referring,” a chain-of-thought style supervision and output format for referring expression comprehension/segmentation, plus a unified detection+segmentation MLLM with adaptive weighted loss and a new complex-case benchmark, yielding ~2.5%+ gains on RefCOCO/+/g and a curated benchmark.


<details>
  <summary>Details</summary>
Motivation: Referring tasks require multi-step cross-modal reasoning over objects and relations; existing MLLMs struggle with complex, compositional queries and lack targeted benchmarks evaluating such complexity.

Method: (1) Parse referring expressions into sequential reasoning steps with explicit relationship identification and reference alignment; (2) re-annotate/reshape datasets to enforce a structured output format; (3) build a benchmark emphasizing complex referring cases; (4) integrate detection and segmentation into a unified MLLM; (5) train with a novel adaptive weighted loss for joint optimization.

Result: Consistent improvements on their curated complex benchmark and on RefCOCO/+/g, achieving ≥2.5% absolute gains over baseline models, particularly on complex queries.

Conclusion: Structured, stepwise supervision (CoT Referring) enhances multimodal reasoning and alignment for referring tasks. A unified detection+segmentation framework with adaptive loss further improves performance, and the new benchmark highlights gains on complex cases, indicating a promising direction for MLLMs.

Abstract: Referring Expression Comprehension and Segmentation are critical tasks for
assessing the integration of language understanding and image comprehension,
serving as benchmarks for Multimodal Large Language Models (MLLMs)
capabilities. To address these challenges, we propose a new strategy, CoT
Referring, which enhances model reasoning across modalities through a
structured, chain-of-thought training data structure. Our approach
systematically parses textual structures to a sequential referring step, where
in each step it identifies relationships and ensures consistent reference
alignment, thereby improving accuracy in complex query scenarios. We
restructure the training data to enforce a new output form, providing new
annotations for existing datasets and compiling an evaluation benchmark from
existing resources. This benchmark is designed explicitly for complex referring
cases. We also integrate detection and segmentation capabilities into a unified
MLLM framework, training it with a novel adaptive weighted loss to optimize
performance. Experimental results on our curated benchmark and RefCOCO/+/g
demonstrate the effectiveness of our approach, with a notable increase of 2.5%+
over baseline models.

</details>


### [93] [Evaluating Embedding Frameworks for Scientific Domain](https://arxiv.org/abs/2510.06244)
*Nouman Ahmed,Ronin Wu,Victor Botev*

Main category: cs.CL

TL;DR: Proposes and applies an evaluation suite to compare word representation and tokenization methods for scientific-domain NLP, aiming to identify optimal choices; concrete comparative results are not reported in the abstract.


<details>
  <summary>Details</summary>
Motivation: In scientific text, words can be highly polysemous and domain-dependent, so general-purpose embeddings/tokenizers may be suboptimal. Training large transformer models from scratch is expensive, motivating a search for efficient, domain-suitable representations and tokenization strategies, plus a standardized way to evaluate them.

Method: Construct a comprehensive evaluation suite comprising multiple downstream scientific NLP tasks and datasets, then use it to benchmark a range of word representation algorithms and tokenization methods for the scientific domain.

Result: The abstract states that the authors built the suite and used it to test several approaches, but it does not provide quantitative findings or name which methods performed best.

Conclusion: An evaluation framework for scientific-domain word representations/tokenizers is presented and used; it is intended to guide selection of methods for downstream tasks and to remain useful as new algorithms emerge, though the abstract leaves conclusions about specific winners unspecified.

Abstract: Finding an optimal word representation algorithm is particularly important in
terms of domain specific data, as the same word can have different meanings and
hence, different representations depending on the domain and context. While
Generative AI and transformer architecture does a great job at generating
contextualized embeddings for any given work, they are quite time and compute
extensive, especially if we were to pre-train such a model from scratch. In
this work, we focus on the scientific domain and finding the optimal word
representation algorithm along with the tokenization method that could be used
to represent words in the scientific domain. The goal of this research is two
fold: 1) finding the optimal word representation and tokenization methods that
can be used in downstream scientific domain NLP tasks, and 2) building a
comprehensive evaluation suite that could be used to evaluate various word
representation and tokenization algorithms (even as new ones are introduced) in
the scientific domain. To this end, we build an evaluation suite consisting of
several downstream tasks and relevant datasets for each task. Furthermore, we
use the constructed evaluation suite to test various word representation and
tokenization algorithms.

</details>


### [94] [TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B](https://arxiv.org/abs/2510.06249)
*Toshiki Nakai,Ravi Kiran Chikkala,Lena Sophie Oberkircher,Nicholas Jennings,Natalia Skachkova,Tatiana Anikina,Jesujoba Oluwadara Alabi*

Main category: cs.CL

TL;DR: Proposes TRepLiNa, a low-cost method that aligns cross-lingual representations at mid-level layers of a decoder-only multilingual LLM by combining CKA-based similarity with REPINA regularization, improving LRL→HRL translation (Mundari, Santali, Bhili) on Aya-23 8B with QLoRA, especially in zero/few-shot settings.


<details>
  <summary>Details</summary>
Motivation: India’s low-resource languages lack parallel data and tooling, hindering translation quality and access. The work targets social impact by boosting LRL translation with minimal data and compute, addressing the MMLoSo challenge’s gap.

Method: Combine Centered Kernel Alignment (CKA) to encourage representation similarity across languages with REPINA to constrain parameter drift from the pretrained model. Apply this joint objective (TRepLiNa) to specific internal (mid-level) layers of a decoder-only multilingual LLM (Aya-23 8B) trained with QLoRA under zero-shot, few-shot, and fine-tuning regimes on MMLoSo pairs (Mundari, Santali, Bhili) using Hindi/English as pivots.

Result: Aligning mid-level layers with TRepLiNa improves LRL→HRL translation quality in data-scarce scenarios; benefits are reported across zero/few-shot and fine-tuning settings. The approach is characterized as low-cost and practical. (No numeric metrics provided in the abstract.)

Conclusion: Mid-layer cross-lingual alignment via a CKA+REPINA joint objective is an effective, resource-efficient strategy for improving LRL translation in multilingual LLMs, offering a practical path for social-impact applications where data and compute are limited.

Abstract: The 2025 Multimodal Models for Low-Resource Contexts and Social Impact
(MMLoSo) Language Challenge addresses one of India's most pressing linguistic
gaps: the lack of resources for its diverse low-resource languages (LRLs). In
this study, we investigate whether enforcing cross-lingual similarity in
specific internal layers of a decoder-only multilingual large language model
(LLM) can improve translation quality from LRL to high-resource language (HRL).
Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric
that encourages representations of different languages to align, with REPINA, a
regularization method that constrains parameter updates to remain close to the
pretrained model, into a joint method we call TRepLiNa. In this research
project, we experiment with zero-shot, few-shot, and fine-tuning settings using
Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari,
Santali, Bhili) with Hindi/English pivots. Our results show that aligning
mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach
to improving LRL translation, especially in data-scarce settings.

</details>


### [95] [Scalable multilingual PII annotation for responsible AI in LLMs](https://arxiv.org/abs/2510.06250)
*Bharti Meena,Joanna Skubisz,Harshit Rajgarhia,Nand Dave,Kiran Ganesh,Shivali Dalmia,Abhishek Mukherji,Vasudevan Sundarababu,Olga Pospelova*

Main category: cs.CL

TL;DR: They present a scalable, multilingual, human-in-the-loop framework to curate high-quality PII annotations across 13 underrepresented locales (≈336 locale-specific PII types), using phased QA and analytics to boost recall and reduce false positives, yielding datasets suitable for LLM fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LLMs must reliably detect and handle PII under diverse, locale-specific regulations, yet high-quality labeled data—especially for underrepresented languages and PII categories—is scarce and inconsistently annotated. A robust, scalable curation process is needed to ensure compliance and model reliability.

Method: A phased pipeline (pilot → training → production) with human-in-the-loop annotation by linguistic experts, rigorous QA, inter-annotator agreement measurements, and root-cause analysis. The framework iteratively refines guidelines and resolves inconsistencies, covering 13 locales and ~336 locale-specific PII types to create high-fidelity datasets for supervised LLM fine-tuning.

Result: Across phases, they report substantial gains in recall and reductions in false positives, improved inter-annotator agreement, and systematic resolution of annotation inconsistencies. The outcome is high-fidelity, multilingual PII datasets ready for training and evaluating LLMs.

Conclusion: Iterative, analytics-driven human-in-the-loop curation can scale multilingual PII labeling and significantly improve annotation quality, which in turn enhances downstream LLM reliability for PII handling. The framework also surfaces common annotator challenges and offers a replicable path for multilingual compliance-sensitive data curation.

Abstract: As Large Language Models (LLMs) gain wider adoption, ensuring their reliable
handling of Personally Identifiable Information (PII) across diverse regulatory
contexts has become essential. This work introduces a scalable multilingual
data curation framework designed for high-quality PII annotation across 13
underrepresented locales, covering approximately 336 locale-specific PII types.
Our phased, human-in-the-loop annotation methodology combines linguistic
expertise with rigorous quality assurance, leading to substantial improvements
in recall and false positive rates from pilot, training, and production phases.
By leveraging inter-annotator agreement metrics and root-cause analysis, the
framework systematically uncovers and resolves annotation inconsistencies,
resulting in high-fidelity datasets suitable for supervised LLM fine-tuning.
Beyond reporting empirical gains, we highlight common annotator challenges in
multilingual PII labeling and demonstrate how iterative, analytics-driven
pipelines can enhance both annotation quality and downstream model reliability.

</details>


### [96] [Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments](https://arxiv.org/abs/2510.06262)
*Aryan Kumar Singh,Janvi Singh*

Main category: cs.CL

TL;DR: A bilingual (English-Hindi), standardized 24-item questionnaire—aligned with AYUSH/CCRAS guidelines—collects mandatory, neutrally phrased responses on physical, physiological, and psychological traits, maps them via automated scoring to Ayurveda dosha (Vata/Pitta/Kapha) scores, and yields a structured dataset for analytics and modeling.


<details>
  <summary>Details</summary>
Motivation: Provide a rigorously designed, reproducible, and bias-minimized Prakriti assessment dataset to bridge classical Ayurvedic profiling with modern computational research and personalized health applications; address the scarcity of standardized, machine-usable data in this domain.

Method: Design a 24-item multiple-choice Prakriti questionnaire per AYUSH/CCRAS guidance; make it bilingual (English-Hindi), mandatory, and neutrally worded; hide dosha labels from participants; deploy via Google Forms to capture responses and automatically score items into dosha-specific (Vata, Pitta, Kapha) scores.

Result: A structured dataset of questionnaire responses paired with automated dosha scores, suitable for analyzing trait distributions and correlations and for developing predictive models in computational intelligence and Ayurvedic studies.

Conclusion: The dataset establishes a standardized reference resource that can underpin future Prakriti-based research and the creation of intelligent, personalized health applications grounded in Ayurvedic principles.

Abstract: This dataset provides responses to a standardized, bilingual (English-Hindi)
Prakriti Assessment Questionnaire designed to evaluate the physical,
physiological, and psychological characteristics of individuals according to
classical Ayurvedic principles. The questionnaire consists of 24
multiple-choice items covering body features, appetite, sleep patterns, energy
levels, and temperament. It was developed following AYUSH/CCRAS guidelines to
ensure comprehensive and accurate data collection. All questions are mandatory
and neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha)
are hidden from participants. Data were collected via a Google Forms
deployment, enabling automated scoring of responses to map individual traits to
dosha-specific scores. The resulting dataset provides a structured platform for
research in computational intelligence, Ayurvedic studies, and personalized
health analytics, supporting analysis of trait distributions, correlations, and
predictive modeling. It can also serve as a reference for future Prakriti-based
studies and the development of intelligent health applications.

</details>


### [97] [Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians](https://arxiv.org/abs/2510.06263)
*Jiajun Wu,Swaleh Zaidi,Braden Teitge,Henry Leung,Jiayu Zhou,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: A fully offline, privacy-preserving, two-stage RAG system runs on two NVIDIA Jetson devices to summarize EHRs: one retrieves relevant sections; the other uses a ≤7B small language model to generate a structured list of critical findings plus a query-focused narrative, producing useful summaries in <30 seconds on MIMIC-IV and de-identified EHRs.


<details>
  <summary>Details</summary>
Motivation: Emergency physicians face cognitive overload from long, unstructured EHR notes and need rapid, privacy-preserving summaries that work without cloud connectivity or PHI leakage, within the tight compute and latency constraints of embedded hardware.

Method: A dual-device architecture: Jetson Nano-R performs local retrieval by splitting notes into semantically coherent sections and searching for query-relevant segments; Jetson Nano-S hosts a small language model to generate two outputs (fixed-format critical findings and a query-focused narrative). Devices communicate via a lightweight socket. Six open-source ≤7B models are benchmarked and an LLM-as-Judge evaluates factuality, completeness, and clarity.

Result: Preliminary experiments on MIMIC-IV and de-identified EHRs show the system can produce useful summaries in under 30 seconds; benchmarking identifies viable SLMs for the hardware. Quality is assessed via LLM-as-Judge, though detailed quantitative results are not provided in the abstract.

Conclusion: On-device two-stage EHR summarization is feasible on embedded hardware while preserving privacy and meeting near-real-time needs. Stronger evidence—human clinician evaluation, rigorous baselines, detailed metrics, and safety guards—is needed to validate clinical utility and reliability.

Abstract: Electronic health records (EHRs) contain extensive unstructured clinical data
that can overwhelm emergency physicians trying to identify critical
information. We present a two-stage summarization system that runs entirely on
embedded devices, enabling offline clinical summarization while preserving
patient privacy. In our approach, a dual-device architecture first retrieves
relevant patient record sections using the Jetson Nano-R (Retrieve), then
generates a structured summary on another Jetson Nano-S (Summarize),
communicating via a lightweight socket link. The summarization output is
two-fold: (1) a fixed-format list of critical findings, and (2) a
context-specific narrative focused on the clinician's query. The retrieval
stage uses locally stored EHRs, splits long notes into semantically coherent
sections, and searches for the most relevant sections per query. The generation
stage uses a locally hosted small language model (SLM) to produce the summary
from the retrieved text, operating within the constraints of two NVIDIA Jetson
devices. We first benchmarked six open-source SLMs under 7B parameters to
identify viable models. We incorporated an LLM-as-Judge evaluation mechanism to
assess summary quality in terms of factual accuracy, completeness, and clarity.
Preliminary results on MIMIC-IV and de-identified real EHRs demonstrate that
our fully offline system can effectively produce useful summaries in under 30
seconds.

</details>


### [98] [A Comprehensive Survey of Hallucination in Large Language Models: Causes, Detection, and Mitigation](https://arxiv.org/abs/2510.06265)
*Aisha Alansari,Hamzah Luqman*

Main category: cs.CL

TL;DR: Survey of LLM hallucinations: defines types and lifecycle causes, analyzes task-specific manifestations, organizes detection and mitigation methods, reviews benchmarks/metrics, and outlines open challenges to build more truthful LLMs.


<details>
  <summary>Details</summary>
Motivation: LLMs are fluent but can generate factually incorrect content, eroding reliability in high-stakes settings. A comprehensive, structured synthesis is needed to understand why hallucinations occur and how to detect, mitigate, and evaluate them.

Method: Systematic literature review. Proposes taxonomies for (1) hallucination types and lifecycle causes (data collection, model/architecture/training, inference), (2) detection approaches, and (3) mitigation strategies. Examines how hallucinations arise across NLG tasks, compares methods, and surveys benchmarks and metrics.

Result: Delivers structured taxonomies, a mapping from causes to manifestations and interventions, a comparative analysis of detection/mitigation techniques with their strengths and weaknesses, and a catalog of evaluation datasets and metrics. Identifies gaps in current practices.

Conclusion: Establishes a foundation for more truthful, trustworthy LLMs. Emphasizes needs for standardized evaluation, robust and task-aware detection, principled mitigation tied to root causes, better datasets, and integrated pipelines; outlines open research directions.

Abstract: Large language models (LLMs) have transformed natural language processing,
achieving remarkable performance across diverse tasks. However, their
impressive fluency often comes at the cost of producing false or fabricated
information, a phenomenon known as hallucination. Hallucination refers to the
generation of content by an LLM that is fluent and syntactically correct but
factually inaccurate or unsupported by external evidence. Hallucinations
undermine the reliability and trustworthiness of LLMs, especially in domains
requiring factual accuracy. This survey provides a comprehensive review of
research on hallucination in LLMs, with a focus on causes, detection, and
mitigation. We first present a taxonomy of hallucination types and analyze
their root causes across the entire LLM development lifecycle, from data
collection and architecture design to inference. We further examine how
hallucinations emerge in key natural language generation tasks. Building on
this foundation, we introduce a structured taxonomy of detection approaches and
another taxonomy of mitigation strategies. We also analyze the strengths and
limitations of current detection and mitigation approaches and review existing
evaluation benchmarks and metrics used to quantify LLMs hallucinations.
Finally, we outline key open challenges and promising directions for future
research, providing a foundation for the development of more truthful and
trustworthy LLMs.

</details>


### [99] [Language models for longitudinal analysis of abusive content in Billboard Music Charts](https://arxiv.org/abs/2510.06266)
*Rohitash Chandra,Yathin Suresh,Divyansh Raj Sinha,Sanchit Jindal*

Main category: cs.CL

TL;DR: Longitudinal deep‑learning analysis of Billboard (U.S.) lyrics over seven decades shows a marked post‑1990 rise in profane and sexually explicit content; language models capture evolving lyrical norms.


<details>
  <summary>Details</summary>
Motivation: Perceived surge in abusive/sexual content in popular music may affect youth behavior, yet rigorous, longitudinal validation is scarce; evidence is needed to inform policy and content moderation debates.

Method: Collected lyrics from U.S. Billboard charts spanning ~70 years; applied deep learning and language models for sentiment, abuse/profanity, and sexual‑explicitness detection; performed trend analysis over time and examined models’ ability to capture nuanced shifts in language and norms.

Result: Significant increase in explicit, profane, and sexually explicit language in popular songs beginning around 1990; models reveal evolving linguistic patterns aligned with societal/norm shifts.

Conclusion: Popular music has become more explicit over recent decades; deep learning provides a viable framework to monitor content trends and cultural change over time.

Abstract: There is no doubt that there has been a drastic increase in abusive and
sexually explicit content in music, particularly in Billboard Music Charts.
However, there is a lack of studies that validate the trend for effective
policy development, as such content has harmful behavioural changes in children
and youths. In this study, we utilise deep learning methods to analyse songs
(lyrics) from Billboard Charts of the United States in the last seven decades.
We provide a longitudinal study using deep learning and language models and
review the evolution of content using sentiment analysis and abuse detection,
including sexually explicit content. Our results show a significant rise in
explicit content in popular music from 1990 onwards. Furthermore, we find an
increasing prevalence of songs with lyrics containing profane, sexually
explicit, and otherwise inappropriate language. The longitudinal analysis of
the ability of language models to capture nuanced patterns in lyrical content,
reflecting shifts in societal norms and language use over time.

</details>


### [100] [Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"](https://arxiv.org/abs/2510.06275)
*Ranjan Mishra,Julian I. Bibo,Quinten van Engelen,Henk Schaapman*

Main category: cs.CL

TL;DR: Reproduces XRec with Llama 3, probes Mixture-of-Experts (MoE) embeddings, and releases an open-source evaluation; finds XRec yields personalized, more stable explanations via collaborative signals but doesn’t dominate all baselines.


<details>
  <summary>Details</summary>
Motivation: To validate and extend XRec’s claims about explainable recommendations by testing with a different LLM (Llama 3 vs. GPT‑3.5‑turbo), assess the role of collaborative information and MoE embeddings, and provide a reproducible, accessible evaluation toolkit.

Method: Built on Ma et al.’s codebase, swapped the evaluation LLM to Llama 3, and conducted ablations by modifying input embeddings and removing output embeddings in XRec’s MoE module; evaluated recommendation explanation quality, stability, and performance versus baselines.

Result: XRec generates personalized explanations; collaborative signals improve explanation stability. However, XRec does not consistently outperform all baselines across metrics. Analyses show MoE embeddings strongly influence explanation structures, indicating interactions between collaborative and language modeling signals. An open-source evaluation implementation is provided.

Conclusion: XRec is effective for personalized, explainable recommendations and benefits from collaborative information, but gains are metric- and setting-dependent. MoE embeddings are pivotal in shaping explanations. The released codebase facilitates further research and practical evaluation.

Abstract: In this study, we reproduced the work done in the paper "XRec: Large Language
Models for Explainable Recommendation" by Ma et al. (2024). The original
authors introduced XRec, a model-agnostic collaborative instruction-tuning
framework that enables large language models (LLMs) to provide users with
comprehensive explanations of generated recommendations. Our objective was to
replicate the results of the original paper, albeit using Llama 3 as the LLM
for evaluation instead of GPT-3.5-turbo. We built on the source code provided
by Ma et al. (2024) to achieve our goal. Our work extends the original paper by
modifying the input embeddings or deleting the output embeddings of XRec's
Mixture of Experts module. Based on our results, XRec effectively generates
personalized explanations and its stability is improved by incorporating
collaborative information. However, XRec did not consistently outperform all
baseline models in every metric. Our extended analysis further highlights the
importance of the Mixture of Experts embeddings in shaping the explanation
structures, showcasing how collaborative signals interact with language
modeling. Through our work, we provide an open-source evaluation implementation
that enhances accessibility for researchers and practitioners alike. Our
complete code repository can be found at
https://github.com/julianbibo/xrec-reproducibility.

</details>


### [101] [Type and Complexity Signals in Multilingual Question Representations](https://arxiv.org/abs/2510.06304)
*Robin Kokot,Wessel Poelman*

Main category: cs.CL

TL;DR: They introduce a multilingual dataset (QTC) and probing framework to study how a multilingual transformer encodes question types and structural complexity, finding that statistical cues suffice for overtly marked languages while contextual neural probes better capture fine-grained complexity; they also examine how fine-tuning affects the availability of pre-trained linguistic information.


<details>
  <summary>Details</summary>
Motivation: To understand what multilingual transformers actually encode about questions’ morphosyntax and structural complexity across languages, and to determine when contextual representations add value over simple statistical features, especially under fine-tuning.

Method: Create the QTC dataset spanning seven languages with annotations for question type and complexity metrics (dependency length, tree depth, lexical density). Extend probing to handle regression targets with selectivity controls. Train layer-wise probes on frozen Glot500-m representations and compare against subword TF-IDF baselines and a fine-tuned model.

Result: Statistical features classify question types well in languages with explicit morphological/lexical marking of questions. Neural probes over contextual representations better capture nuanced structural complexity patterns. Layer-wise analysis identifies regimes where contextual embeddings outperform statistical baselines; fine-tuning is analyzed for its impact on pre-trained linguistic information.

Conclusion: Contextual models do not uniformly dominate: their advantage emerges for fine-grained structural complexity, while simple statistics can suffice for overtly marked question types. The QTC dataset and selective regression probing provide tools to assess representation quality and to gauge potential trade-offs introduced by fine-tuning.

Abstract: This work investigates how a multilingual transformer model represents
morphosyntactic properties of questions. We introduce the Question Type and
Complexity (QTC) dataset with sentences across seven languages, annotated with
type information and complexity metrics including dependency length, tree
depth, and lexical density. Our evaluation extends probing methods to
regression labels with selectivity controls to quantify gains in
generalizability. We compare layer-wise probes on frozen Glot500-m (Imani et
al., 2023) representations against subword TF-IDF baselines, and a fine-tuned
model. Results show that statistical features classify questions effectively in
languages with explicit marking, while neural probes capture fine-grained
structural complexity patterns better. We use these results to evaluate when
contextual representations outperform statistical baselines and whether
parameter updates reduce the availability of pre-trained linguistic
information.

</details>


### [102] [LLM Bias Detection and Mitigation through the Lens of Desired Distributions](https://arxiv.org/abs/2510.06354)
*Ingroj Shrestha,Padmini Srinivasan*

Main category: cs.CL

TL;DR: Reframes bias as deviation from a target distribution (equality or real-world) and introduces a weighted adaptive loss for fine-tuning LLMs to match desired gender–profession distributions, achieving near-complete parity alignment and substantial—but not total—alignment to real-world statistics.


<details>
  <summary>Details</summary>
Motivation: Equality-centric debiasing may conflict with applications that require outputs reflecting real-world frequencies for factual grounding. The authors seek a principled way to align LLM outputs to either equal or real-world distributions while preserving general language modeling performance.

Method: Define bias as divergence from a specified target distribution. Propose a weighted adaptive loss for fine-tuning that reweights model outputs to match that target. Evaluate on gender–profession associations using three profession sets (male-dominated, female-dominated, balanced) derived from 2024 U.S. labor statistics. Compare adaptive (real-world alignment) vs. non-adaptive (equality) variants across masked LMs and assess autoregressive LLMs as well.

Result: Across three masked LMs, bias exists under both equality and real-world targets. Fine-tuning yields near-complete mitigation under equality and 30–75% bias reduction under real-world targets. Autoregressive LLMs show no bias under equality but show notable bias under real-world targets; Llama Instruct (3.2-3B, 3.1-8B) achieves 50–62% reduction.

Conclusion: Bias can be operationalized as deviation from a user-specified distribution, enabling controlled alignment. The proposed weighted adaptive loss effectively aligns to equality and partially to real-world distributions while purportedly preserving language modeling ability. Model family matters: masked LMs and autoregressive LLMs behave differently, and full real-world alignment remains challenging.

Abstract: Although prior work on bias mitigation has focused on promoting social
equality and demographic parity, less attention has been given to aligning
LLM's outputs to desired distributions. For example, we might want to align a
model with real-world distributions to support factual grounding. Thus, we
define bias as deviation from a desired distribution, which may be an equal or
real-world distribution, depending on application goals. We propose a weighted
adaptive loss based fine-tuning method that aligns LLM's gender-profession
output distribution with the desired distribution, while preserving language
modeling capability. Using 3 profession sets -- male-dominated,
female-dominated, and gender-balanced -- derived from U.S. labor statistics
(2024), we assess both our adaptive method for reflecting reality and a
non-adaptive variant for equality. Across three masked language models, bias is
observed under both distributions. We achieve near-complete mitigation under
equality and 30-75% reduction under real-world settings. Autoregressive LLMs
show no bias under equality but notable bias under real-world settings, with
the Llama Instruct models (3.2-3B, 3.1-8B) achieving a 50-62% reduction.

</details>


### [103] [EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preference](https://arxiv.org/abs/2510.06370)
*Kshitish Ghate,Andy Liu,Devansh Jain,Taylor Sorensen,Atoosa Kasirzadeh,Aylin Caliskan,Mona T. Diab,Maarten Sap*

Main category: cs.CL

TL;DR: EVALUESTEER is a benchmark that tests how well LLMs and reward models can be steered to match users’ value and stylistic preferences; models struggle (<75% accuracy) when given full profiles but excel (>99%) when only relevant preferences are provided.


<details>
  <summary>Details</summary>
Motivation: LLMs are deployed globally and must serve users with diverse values and stylistic preferences. Existing datasets don’t allow controlled, granular evaluation of how reward models/LLMs prioritize and adapt to such preferences, especially distinguishing relevant from irrelevant profile information.

Method: Construct a synthetic, controlled benchmark of 165,888 preference pairs varying along 4 value dimensions (traditional, secular-rational, survival, self-expression) and 4 style dimensions (verbosity, readability, confidence, warmth). Given a user profile and two candidate responses (value- and style-laden), test whether LLMs/RMs choose the response aligned with the profile. Evaluate six open-source and proprietary models under 16 prompting conditions and 6 comparison scenarios.

Result: When models receive full user profiles covering many values and styles, the best achieve under 75% accuracy in selecting the aligned response. If provided only the preferences relevant to the pairwise choice, accuracy exceeds 99%. This indicates difficulty in filtering and weighting relevant attributes within complex profiles.

Conclusion: Current reward models and LLMs are limited at identifying and prioritizing relevant user preferences within rich profiles, hindering pluralistic steering. EVALUESTEER offers a challenging, controlled testbed to drive development of models that more robustly adapt to diverse human values and styles.

Abstract: As large language models (LLMs) are deployed globally, creating pluralistic
systems that can accommodate the diverse preferences and values of users
worldwide becomes essential. We introduce EVALUESTEER, a benchmark to measure
LLMs' and reward models' (RMs) steerability towards users' value and stylistic
preference profiles grounded in psychology and human-LLM interaction
literature. To address the gap in existing datasets that do not support
controlled evaluations of RM steering, we synthetically generated 165,888
preference pairs -- systematically varying pairs along 4 value dimensions
(traditional, secular-rational, survival, and self-expression) and 4 style
dimensions (verbosity, readability, confidence, and warmth). We use EVALUESTEER
to evaluate whether, given a user profile and a pair of candidate value-laden
and style-laden responses, LLMs and RMs are able to select the output that
aligns with the user's preferences. We evaluate six open-source and proprietary
LLMs and RMs under sixteen systematic prompting conditions and six preference
comparison scenarios. Notably, our results show that, when given the user's
full profile of values and stylistic preferences, the best models achieve <75%
accuracy at choosing the correct response, in contrast to >99% accuracy when
only relevant style and value preferences are provided. EVALUESTEER thus
highlights the limitations of current RMs at identifying and adapting to
relevant user profile information, and provides a challenging testbed for
developing RMs that can be steered towards diverse human values and
preferences.

</details>


### [104] [EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA](https://arxiv.org/abs/2510.06371)
*Firoj Alam,Ali Ezzat Shahroor,Md. Arid Hasan,Zien Sheikh Ali,Hunzalah Hassan Bhatti,Mohamed Bayan Kmainasi,Shammur Absar Chowdhury,Basel Mousi,Fahim Dalvi,Nadir Durrani,Natasa Milic-Frayling*

Main category: cs.CL

TL;DR: They present EverydayMMQA, a framework, and OASIS, a large culturally grounded multimodal QA dataset (speech, images, text) focused on English and Arabic across 18 countries, enabling speech/text with and without images. With ~0.92M images, 14.8M QA pairs, and 3.7M spoken questions, they benchmark multiple models to set baselines for pragmatic, commonsense, and culture-aware reasoning, and will release the resources publicly.


<details>
  <summary>Details</summary>
Motivation: Current large multimodal models often fail on culturally grounded, everyday reasoning—especially for low-resource and underrepresented languages—due to a lack of appropriate multimodal datasets and benchmarks that include speech and visual context in diverse cultural settings.

Method: Introduce the EverydayMMQA framework to systematically create culturally grounded spoken and visual QA data. Using it, they build OASIS: a dataset integrating speech, images, and text with four input modes (speech-only, text-only, speech+image, text+image), curated from 18 countries and focused on English and Arabic varieties. The tasks go beyond object recognition to require pragmatic, commonsense, and culturally aware reasoning. They benchmark four closed-source, three open-source, and one fine-tuned model.

Result: Constructed OASIS with ~0.92M images, 14.8M QA pairs, and 3.7M spoken questions covering diverse real-world situations and modalities. Established baseline performance by evaluating multiple open- and closed-source models, demonstrating the dataset’s breadth and difficulty for culture-aware, pragmatic reasoning (no specific metrics given in the abstract).

Conclusion: EverydayMMQA and OASIS provide a public benchmark and training resource for building and evaluating multimodal LLMs on everyday, culturally grounded tasks—especially benefiting underrepresented languages—thereby addressing key gaps in current multimodal reasoning capabilities.

Abstract: Large-scale multimodal models achieve strong results on tasks like Visual
Question Answering (VQA), but they often fail when queries require culturally
grounded, everyday knowledge, particularly in low-resource and underrepresented
languages. To bridge this gap, we introduce Everyday Multimodal and
Multilingual QA (EverydayMMQA), a framework for creating large-scale,
culturally-grounded datasets for spoken and visual question answering (SVQA).
Using this framework, we developed OASIS, a multimodal dataset integrating
speech, images, and text. With over ~0.92M images and 14.8M QA pairs, OASIS
contains 3.7M spoken questions, enabling four unique input combinations:
speech-only, text-only, speech+image, and text+image. Focused on English and
Arabic varieties, 18 countries, the dataset content is curated to reflect
diverse, real-world situations. OASIS tests models on tasks beyond object
recognition that involve pragmatic, commonsense, and culturally aware
reasoning. We benchmarked four closed-source models, three open-source models,
and one fine-tuned model. EverydayMMQA and OASIS together provide a benchmark
and training dataset for building multimodal LLMs for a comprehensive set of
everyday tasks within cultural contexts. The framework and dataset will be made
publicly available to the community.

</details>


### [105] [Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language](https://arxiv.org/abs/2510.06378)
*Angie Boggust,Donghao Ren,Yannick Assogba,Dominik Moritz,Arvind Satyanarayan,Fred Hohman*

Main category: cs.CL

TL;DR: Proposes “semantic regexes,” a structured, composable language for describing LLM features that matches NL description accuracy while being more precise, concise, and consistent, enabling scalable, model-wide interpretability analyses and better human understanding.


<details>
  <summary>Details</summary>
Motivation: Natural-language feature labels in automated interpretability are often vague, inconsistent, and labor-intensive to relabel, limiting precision, comparability, and scalability of analyses.

Method: Define semantic regexes: a set of primitives for linguistic/semantic patterns with modifiers for context, composition, and quantification to describe LLM features. Evaluate against NL descriptions via quantitative benchmarks and qualitative studies; use the structured form to analyze feature complexity across layers and aggregate from individual features to model-wide patterns; run user studies on mental model formation.

Result: Semantic regexes achieve accuracy on par with natural-language descriptions but are more concise and consistent. Their structure enables new analyses (e.g., quantifying feature complexity across layers, moving from single-feature insights to model-wide trends). User studies indicate they help people form accurate mental models of feature activations.

Conclusion: Structured, composable semantic regexes improve the fidelity and consistency of feature descriptions and unlock scalable automated interpretability analyses, offering practical benefits over free-form natural language for both researchers and end users.

Abstract: Automated interpretability aims to translate large language model (LLM)
features into human understandable descriptions. However, these natural
language feature descriptions are often vague, inconsistent, and require manual
relabeling. In response, we introduce semantic regexes, structured language
descriptions of LLM features. By combining primitives that capture linguistic
and semantic feature patterns with modifiers for contextualization,
composition, and quantification, semantic regexes produce precise and
expressive feature descriptions. Across quantitative benchmarks and qualitative
analyses, we find that semantic regexes match the accuracy of natural language
while yielding more concise and consistent feature descriptions. Moreover,
their inherent structure affords new types of analyses, including quantifying
feature complexity across layers, scaling automated interpretability from
insights into individual features to model-wide patterns. Finally, in user
studies, we find that semantic regex descriptions help people build accurate
mental models of LLM feature activations.

</details>


### [106] [Protecting De-identified Documents from Search-based Linkage Attacks](https://arxiv.org/abs/2510.06383)
*Pierre Lison,Mark Anderson*

Main category: cs.CL

TL;DR: Identify rare N-grams that enable search-based linkage and paraphrase them with an LLM until they achieve k-anonymity, preserving meaning.


<details>
  <summary>Details</summary>
Motivation: Standard de-identification removes explicit identifiers but leaves distinctive phrases that can re-link a document to its source via search. The paper aims to mitigate this linkage risk without degrading the document’s semantic content.

Method: Two-step approach: (1) Build an inverted index over the corpus to detect N-grams (alone or in combination) that occur in fewer than k documents. (2) Iteratively query an LLM to rewrite those rare spans until they are no longer uniquely linkable, striving to maintain semantic integrity.

Result: On a corpus of court cases, the method effectively prevented search-based linkages while remaining faithful to the original text.

Conclusion: An index-guided, LLM-based rewriting pipeline can reduce linkage risk beyond conventional de-identification while preserving utility, offering a practical defense for sharing sensitive text.

Abstract: While de-identification models can help conceal the identity of the
individual(s) mentioned in a document, they fail to address linkage risks,
defined as the potential to map the de-identified text back to its source. One
straightforward way to perform such linkages is to extract phrases from the
de-identified document and then check their presence in the original dataset.
This paper presents a method to counter search-based linkage attacks while
preserving the semantic integrity of the text. The method proceeds in two
steps. We first construct an inverted index of the N-grams occurring in the
document collection, making it possible to efficiently determine which N-grams
appear in less than $k$ documents (either alone or in combination with other
N-grams). An LLM-based rewriter is then iteratively queried to reformulate
those spans until linkage is no longer possible. Experimental results on a
collection of court cases show that the method is able to effectively prevent
search-based linkages while remaining faithful to the original content.

</details>


### [107] [Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion](https://arxiv.org/abs/2510.06386)
*Fan Zhou,Chang Tian,Tim Van de Cruys*

Main category: cs.CL

TL;DR: RegDiff is a latent text diffusion framework that uses attribute supervision only during training—without a classifier at sampling—to generate stylistically controlled text more efficiently than classifier-guided approaches while preserving content better than CFG.


<details>
  <summary>Details</summary>
Motivation: Controllable text generation needs reliable attribute control (style, sentiment, etc.) without sacrificing semantics or incurring high sampling costs. Existing diffusion methods trade off: CFG preserves content but weakly enforces attributes; CG enforces attributes but is expensive and can suffer from classifier mismatch/generalization. The goal is to achieve strong attribute control with low inference cost and robustness.

Method: Use a VAE encoder–decoder to ensure reconstruction fidelity and operate diffusion in the latent space. Train the latent diffusion model with attribute supervision (a regularization signal) so that at inference time no external classifier is needed. Attribute information is injected only during training, enabling controllability through the learned latent dynamics at sampling.

Result: Across five datasets with multiple stylistic attributes, RegDiff outperforms strong baselines in stylistic text generation, achieving better attribute alignment with reduced sampling cost compared to classifier guidance and stronger control than classifier-free guidance.

Conclusion: RegDiff provides an efficient, classifier-free-at-inference approach for attribute-controllable text diffusion that improves style alignment while lowering computational overhead. Code and resources will be released upon publication.

Abstract: Generating stylistic text with specific attributes is a key problem in
controllable text generation. Recently, diffusion models have emerged as a
powerful paradigm for both visual and textual generation. Existing approaches
can be broadly categorized into classifier-free guidance (CFG) and classifier
guidance (CG) methods. While CFG effectively preserves semantic content, it
often fails to provide effective attribute control. In contrast, CG modifies
the denoising trajectory using classifier gradients, enabling better attribute
alignment but incurring high computational costs during sampling and suffering
from classifier generalization issues. In this work, we propose RegDiff, a
regularized diffusion framework that leverages attribute features without
requiring a pretrained classifier during sampling, thereby achieving
controllable generation with reduced computational costs. Specifically, RegDiff
employs a VAE-based encoder--decoder architecture to ensure reconstruction
fidelity and a latent diffusion model trained with attribute supervision to
enable controllable text generation. Attribute information is injected only
during training. Experiments on five datasets spanning multiple stylistic
attributes demonstrate that RegDiff outperforms strong baselines in generating
stylistic texts. These results validate the effectiveness of RegDiff as an
efficient solution for attribute-controllable text diffusion. Our code,
datasets, and resources will be released upon publication at
https://github.com/xxxx.

</details>


### [108] [Reward Model Perspectives: Whose Opinions Do Reward Models Reward?](https://arxiv.org/abs/2510.06391)
*Elle*

Main category: cs.CL

TL;DR: They propose a framework to quantify how reward models (RMs) reflect opinions on controversial topics, audit sociodemographic biases, and test prompt-based steering toward target-group preferences. They find RMs misaligned with several groups, sometimes rewarding harmful stereotypes, and that prompting alone cannot fix these issues.


<details>
  <summary>Details</summary>
Motivation: RMs guide language model behavior as proxies for human preferences, yet their value alignment and potential social biases are poorly understood. Without clearer measurement and mitigation, RMs can propagate unwanted biases into downstream LMs.

Method: (i) Formalize a measurement framework capturing RM-held perspectives—opinions, attitudes, and values—on controversial topics; (ii) evaluate sociodemographic alignment to detect biases across groups; (iii) experiment with prompt-based steering to shift rewards toward target-group preferences.

Result: RMs exhibit poor alignment with multiple demographic groups and can systematically reward harmful stereotypes. Prompt-based steering improves alignment only partially and is insufficient to overcome core limitations.

Conclusion: RM behavior requires more rigorous scrutiny and improved preference learning practices. Reliance on steering alone is inadequate; alignment pipelines must explicitly address demographic alignment and bias to avoid propagating social harms in language technologies.

Abstract: Reward models (RMs) are central to the alignment of language models (LMs). An
RM often serves as a proxy for human preferences to guide downstream LM
behavior. However, our understanding of RM behavior is limited. Our work (i)
formalizes a framework for measuring the alignment of opinions captured by RMs,
(ii) investigates the extent to which RMs demonstrate sociodemographic biases,
and (iii) explores the effects of prompting to steer rewards towards the
preferences of a target group. We study the subjective and diverse perspectives
on controversial topics, which allows us to quantify RM perspectives in terms
of their opinions, attitudes, and values. We show that RMs are poorly aligned
with several demographic groups and can systematically reward harmful
stereotypes, and steering alone is not enough to overcome these limitations.
Our findings underscore the need for more careful consideration of RM behavior
in model alignment during preference learning to prevent the propagation of
unwanted social biases in the language technologies that we use.

</details>


### [109] [Instructional Goal-Aligned Question Generation for Student Evaluation in Virtual Lab Settings: How Closely Do LLMs Actually Align?](https://arxiv.org/abs/2510.06411)
*R. Alexander Knipper,Indrani Dey,Souvika Sarkar,Hari Narayanan,Sadhana Puntambekar,Santu Karmaker*

Main category: cs.CL

TL;DR: A four-part LLM-based framework generates virtual-lab questions aligned to teachers’ goals and simulation context, improving quality and format adherence across 19 open-source models; larger models perform best.


<details>
  <summary>Details</summary>
Motivation: Teachers struggle to adapt virtual labs to specific instructional goals; third‑party materials often misalign with classroom needs, and crafting custom resources is time‑consuming and hard to scale.

Method: Introduce an alignment framework combining: (1) instructional goal elicitation via teacher–LLM dialogue, (2) lab understanding through knowledge units and their relationships, (3) a question taxonomy encoding cognitive/pedagogical intent, and (4) TELeR taxonomy to control prompt specificity. Early design was guided by a small teacher-assisted case study; final evaluation assessed >1,100 questions generated by 19 open-source LLMs, measuring parsability, format adherence, and quality.

Result: Grounded goal and lab understanding plus the question taxonomy increased cognitive demand; open‑ended formats and relational types improved quality by 0.29–0.39 Likert points. Optimized TELeR prompts yielded ~80% parsability and >90% format adherence. Larger models showed the biggest gains: parsability +37.1%, adherence +25.7%, and average quality +0.8 Likert points.

Conclusion: Structuring LLM prompts with explicit teacher goals, lab knowledge structures, and taxonomies produces simulation‑aligned, pedagogically meaningful questions at scale. Prompt design and model size materially affect outcomes, suggesting a practical path for teachers to generate higher‑quality virtual‑lab assessments with stronger adherence to desired formats.

Abstract: Virtual Labs offer valuable opportunities for hands-on, inquiry-based science
learning, yet teachers often struggle to adapt them to fit their instructional
goals. Third-party materials may not align with classroom needs, and developing
custom resources can be time-consuming and difficult to scale. Recent advances
in Large Language Models (LLMs) offer a promising avenue for addressing these
limitations. In this paper, we introduce a novel alignment framework for
instructional goal-aligned question generation, enabling teachers to leverage
LLMs to produce simulation-aligned, pedagogically meaningful questions through
natural language interaction. The framework integrates four components:
instructional goal understanding via teacher-LLM dialogue, lab understanding
via knowledge unit and relationship analysis, a question taxonomy for
structuring cognitive and pedagogical intent, and the TELeR taxonomy for
controlling prompt detail. Early design choices were informed by a small
teacher-assisted case study, while our final evaluation analyzed over 1,100
questions from 19 open-source LLMs. With goal and lab understanding grounding
questions in teacher intent and simulation context, the question taxonomy
elevates cognitive demand (open-ended formats and relational types raise
quality by 0.29-0.39 points), and optimized TELeR prompts enhance format
adherence (80% parsability, >90% adherence). Larger models yield the strongest
gains: parsability +37.1%, adherence +25.7%, and average quality +0.8 Likert
points.

</details>


### [110] [FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering](https://arxiv.org/abs/2510.06426)
*Yitao Long,Tiansheng Hu,Yilun Zhao,Arman Cohan,Chen Zhao*

Main category: cs.CL

TL;DR: FinLFQA is a benchmark and evaluation framework for long-form financial QA that tests not only textual evidence attribution but also numerical reasoning steps and domain knowledge, revealing nuanced differences among LLM attribution strategies.


<details>
  <summary>Details</summary>
Motivation: LLMs often hallucinate in long-form answers, and existing attribution benchmarks mostly check for retrieved text snippets. In high-stakes domains like finance, attribution must include verified evidence, explicit numerical reasoning, and domain knowledge justifications—capabilities current benchmarks do not adequately test.

Method: Create FinLFQA with human-annotated attributions for three dimensions: (1) supporting evidence from financial reports, (2) intermediate numerical reasoning steps, and (3) domain-specific financial knowledge. Provide an automatic evaluation framework for both answer and attribution quality. Evaluate eight LLMs across different attribution-generation paradigms (e.g., end-to-end vs. post-hoc, iterative refinement).

Result: Fine-grained metrics are necessary to differentiate model capabilities; end-to-end generation can match post-hoc attribution; iterative refinement helps only when guided by external feedback.

Conclusion: FinLFQA fills a gap for evaluating reliable, nuanced attributions in financial long-form QA, showing that richer attribution dimensions and metrics are needed and that guidance matters more than naive iterative generation.

Abstract: Large Language Models (LLMs) frequently hallucinate to long-form questions,
producing plausible yet factually incorrect answers. A common mitigation
strategy is to provide attribution to LLM outputs. However, existing benchmarks
primarily focus on simple attribution that retrieves supporting textual
evidence as references. We argue that in real-world scenarios such as financial
applications, attribution goes beyond reference retrieval. We introduce
FinLFQA, a benchmark designed to evaluate the ability of LLMs to generate
long-form answers to complex financial questions with reliable and nuanced
attributions. FinLFQA evaluates three critical aspects of attribution through
human annotations: (1) supporting evidence extracted from financial reports,
(2) intermediate numerical reasoning steps, and (3) domain-specific financial
knowledge that informs the reasoning process. We further provide an automatic
evaluation framework covering both answer quality and attribution quality.
Through extensive experiments on eight LLMs across multiple
attribution-generation paradigms, we find that fine-grained metrics are
important to distinguish model capabilities, that end-to-end generation
achieves comparable performance to post-hoc approaches, and that iterative
refinement only helps when guided by external feedback.

</details>


### [111] [Bridging Discourse Treebanks with a Unified Rhetorical Structure Parser](https://arxiv.org/abs/2510.06427)
*Elena Chistova*

Main category: cs.CL

TL;DR: UniRST is a unified, multilingual RST-style discourse parser that trains across 18 treebanks in 11 languages without altering their relation inventories, using a Masked-Union strategy that yields state-of-the-art results versus most mono-treebank baselines.


<details>
  <summary>Details</summary>
Motivation: Discourse parsing resources are fragmented across languages and treebanks with incompatible relation inventories, making it hard to build a single model and to leverage low-resource settings without manual label mapping. A unified approach promises better transfer and efficiency.

Method: Introduce two strategies to reconcile inventory incompatibilities during joint training: (1) Multi-Head, which gives each inventory its own relation classification head; (2) Masked-Union, which forms a shared label space and applies selective label masking so only relevant labels are considered per treebank. Benchmark mono-treebank parsing with a simple augmentation for low-resource settings, then train a single multilingual end-to-end model across 18 treebanks/11 languages without modifying inventories.

Result: Masked-Union is both parameter-efficient and empirically strongest. The unified UniRST model outperforms 16 of 18 mono-treebank baselines, showing cross-treebank and cross-lingual gains; the augmentation helps in low-resource cases.

Conclusion: Maintaining original relation inventories while enabling shared-parameter training is feasible and beneficial. A single multilingual model can surpass most specialized baselines, suggesting unified discourse parsing is an effective path for diverse, low-resource scenarios.

Abstract: We introduce UniRST, the first unified RST-style discourse parser capable of
handling 18 treebanks in 11 languages without modifying their relation
inventories. To overcome inventory incompatibilities, we propose and evaluate
two training strategies: Multi-Head, which assigns separate relation
classification layer per inventory, and Masked-Union, which enables shared
parameter training through selective label masking. We first benchmark
monotreebank parsing with a simple yet effective augmentation technique for
low-resource settings. We then train a unified model and show that (1) the
parameter efficient Masked-Union approach is also the strongest, and (2) UniRST
outperforms 16 of 18 mono-treebank baselines, demonstrating the advantages of a
single-model, multilingual end-to-end discourse parsing across diverse
resources.

</details>


### [112] [MathRobust-LV: Evaluation of Large Language Models' Robustness to Linguistic Variations in Mathematical Reasoning](https://arxiv.org/abs/2510.06430)
*Neeraja Kirtane,Yuvraj Khanna,Peter Relan*

Main category: cs.CL

TL;DR: They introduce MathRobust-LV, a benchmark that rephrases high‑school math problems without changing their math content, and show that many LLMs lose accuracy on these linguistically varied versions—especially smaller models—highlighting a gap between math skill and robustness needed for real educational use.


<details>
  <summary>Details</summary>
Motivation: Math benchmarks often reward solving canonical wordings, but in classrooms instructors routinely rephrase the same concept. Current evaluations (often IMO-level or content-altering perturbations) don’t reflect these realistic linguistic variations at the deployment level (tutoring/assessment). The authors want a comprehensive, education-focused test of whether models’ math reasoning holds up under natural rewordings.

Method: Build MathRobust-LV: for high-school-level problems, create multiple variants that change only surface details (names, contexts, variable symbols) while preserving numerical structure and answers. Evaluate 34 models by comparing accuracy on original problems versus linguistically varied counterparts, keeping difficulty constant and mirroring instructor-style rephrasing.

Result: Across 34 models, accuracy drops from baseline to rephrased variants. Smaller models show large declines (~9–11%), stronger models also degrade, and frontier systems (e.g., GPT-5, Gemini-2.5 Pro) are comparatively stable. Despite perceptions that MATH benchmarking is saturated, these variants expose nontrivial robustness gaps.

Conclusion: Linguistic variation is a fundamental stress test for math reasoning. Even strong models show measurable brittleness, which undermines reliability in educational settings. Robustness to rephrasing should be a core evaluation and development target, beyond headline accuracy on standard problem wordings.

Abstract: Large language models excel on math benchmarks, but their math reasoning
robustness to linguistic variation is underexplored. While recent work
increasingly treats high-difficulty competitions like the IMO as the gold
standard for evaluating reasoning, we believe in comprehensive benchmarking of
high school-level math problems in real educational settings. We introduce
MathRobust-LV, a test set and evaluation methodology that mirrors how
instructors rephrase problems across assessments while keeping difficulty
constant: we change surface details (names, contexts, variables) while
preserving numerical structure and answers. In contrast to prior efforts that
alter problem content or emphasize IMO-level tasks, we focus on
high-school-level dataset problems at the difficulty level where models are
currently deployed in educational settings: tutoring and assessment systems. In
these applications, instructors rephrase identical concepts in varied ways,
making linguistic robustness essential for reliable deployment. Although MATH
data benchmarking is often regarded as saturated, our experiment on 34 models
reveals that accuracy declines when moving from the baseline to the variants.
These drops are severe for smaller models (9-11%) while stronger models also
show measurable degradation. Frontier models like GPT-5, Gemini-2.5pro remain
comparatively stable. Our results highlight that robustness to linguistic
variation is a fundamental challenge, exposing reasoning vulnerabilities in
models.

</details>


### [113] [A Survey on Agentic Security: Applications, Threats and Defenses](https://arxiv.org/abs/2510.06445)
*Asif Shahriar,Md Nafiu Rahman,Sadif Ahmed,Farig Sadeque,Md Rizwan Parvez*

Main category: cs.CL

TL;DR: Survey of security for autonomous LLM agents: taxonomy of applications, threats, and defenses over 150+ papers; identifies trends and research gaps in architectures, models, and modalities.


<details>
  <summary>Details</summary>
Motivation: Autonomous LLM agents are increasingly used for cyber offense/defense, but their agentic operation introduces novel, inherent security risks not covered by prior work on passive LLMs. The field lacks a holistic, structured view.

Method: Conduct a comprehensive literature survey (>150 papers) and organize it into a three-pillar framework—Applications, Threats, Defenses—providing taxonomy and cross-cutting analysis across agent architectures, model types, and modalities.

Result: A unified taxonomy mapping how agents are used, where they are vulnerable, and what countermeasures exist; identification of emerging architectural trends and pinpointed gaps in model and modality coverage.

Conclusion: Agentic LLM security is a distinct, rapidly evolving area; the provided taxonomy and analysis structure the landscape, surface critical research needs, and guide the development of more robust defenses across architectures, models, and modalities.

Abstract: The rapid shift from passive LLMs to autonomous LLM-agents marks a new
paradigm in cybersecurity. While these agents can act as powerful tools for
both offensive and defensive operations, the very agentic context introduces a
new class of inherent security risks. In this work we present the first
holistic survey of the agentic security landscape, structuring the field around
three interdependent pillars: Applications, Threats, and Defenses. We provide a
comprehensive taxonomy of over 150 papers, explaining how agents are used, the
vulnerabilities they possess, and the countermeasures designed to protect them.
A detailed cross-cutting analysis shows emerging trends in agent architecture
while revealing critical research gaps in model and modality coverage.

</details>


### [114] [Linguistically Informed Tokenization Improves ASR for Underresourced Languages](https://arxiv.org/abs/2510.06461)
*Massimo Daul,Alessio Tosolini,Claire Bowern*

Main category: cs.CL

TL;DR: Fine-tuning wav2vec2 for the underresourced Yan-nhangu language, the authors show that phonemic tokenization markedly improves ASR accuracy and that correcting ASR output is faster than transcribing from scratch, supporting ASR’s practicality for language documentation.


<details>
  <summary>Details</summary>
Motivation: Modern ASR is data-hungry and often unusable for underresourced languages, yet linguists need efficient tools to document such languages, including dormant ones like Yan-nhangu.

Method: Fine-tune a wav2vec2 ASR model on Yan-nhangu; compare orthographic vs linguistically informed phonemic tokenization; evaluate with WER/CER; time the effort of hand-correcting ASR output versus manual transcription; assess integration into a documentation pipeline.

Result: Phonemic tokenization substantially reduces WER and CER relative to an orthographic baseline. Post-editing ASR output is significantly faster than manual transcription from scratch.

Conclusion: Linguistically informed phonemic tokenization makes low-resource ASR more accurate, and ASR-assisted transcription can accelerate documentation workflows for underresourced languages.

Abstract: Automatic speech recognition (ASR) is a crucial tool for linguists aiming to
perform a variety of language documentation tasks. However, modern ASR systems
use data-hungry transformer architectures, rendering them generally unusable
for underresourced languages. We fine-tune a wav2vec2 ASR model on Yan-nhangu,
a dormant Indigenous Australian language, comparing the effects of phonemic and
orthographic tokenization strategies on performance. In parallel, we explore
ASR's viability as a tool in a language documentation pipeline. We find that a
linguistically informed phonemic tokenization system substantially improves WER
and CER compared to a baseline orthographic tokenization scheme. Finally, we
show that hand-correcting the output of an ASR model is much faster than
hand-transcribing audio from scratch, demonstrating that ASR can work for
underresourced languages.

</details>


### [115] [Test-Time Scaling of Reasoning Models for Machine Translation](https://arxiv.org/abs/2510.06471)
*Zihao Li,Shaoxiong Ji,Jörg Tiedemann*

Main category: cs.CL

TL;DR: Test-time scaling helps machine translation mainly when models are domain-specialized or used for multi-step post-editing; general-purpose models see small, inconsistent gains and over-reasoning harms quality.


<details>
  <summary>Details</summary>
Motivation: While test-time scaling improves reasoning tasks like math and coding, its impact on machine translation is unclear; understanding when extra inference-time computation pays off can guide practical MT system design.

Method: Evaluate 12 reasoning models on diverse MT benchmarks across three setups: (1) direct translation with increasing inference-time compute, (2) forced reasoning beyond the model’s natural stopping point, and (3) post-editing workflows; study the effect of domain-specific fine-tuning and identify optimal reasoning depth.

Result: General-purpose models show limited and quickly plateauing gains for direct translation under TTS. Domain-specific fine-tuning aligns reasoning with MT demands, yielding consistent improvements up to a model-specific optimal depth; pushing beyond that degrades outputs. TTS is notably effective in post-editing, turning self-correction into reliable quality gains.

Conclusion: Inference-time compute is not broadly effective for single-pass MT with general models. Its value emerges in targeted settings: domain-finetuned models with self-selected stopping and multi-step post-editing/self-correction workflows. Avoid forcing excessive reasoning depth.

Abstract: Test-time scaling (TTS) has enhanced the performance of Reasoning Models
(RMs) on various tasks such as math and coding, yet its efficacy in machine
translation (MT) remains underexplored. This paper investigates whether
increased inference-time computation improves translation quality. We evaluate
12 RMs across a diverse suite of MT benchmarks spanning multiple domains,
examining three scenarios: direct translation, forced-reasoning extrapolation,
and post-editing. Our findings show that for general-purpose RMs, TTS provides
limited and inconsistent benefits for direct translation, with performance
quickly plateauing. However, the effectiveness of TTS is unlocked by
domain-specific fine-tuning, which aligns a model's reasoning process with task
requirements, leading to consistent improvements up to an optimal,
self-determined reasoning depth. We also find that forcing a model to reason
beyond its natural stopping point consistently degrades translation quality. In
contrast, TTS proves highly effective in a post-editing context, reliably
turning self-correction into a beneficial process. These results indicate that
the value of inference-time computation in MT lies not in enhancing single-pass
translation with general models, but in targeted applications like multi-step,
self-correction workflows and in conjunction with task-specialized models.

</details>


### [116] [Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels](https://arxiv.org/abs/2510.06499)
*Zhepeng Cen,Haolin Chen,Shiyu Wang,Zuxin Liu,Zhiwei Liu,Ding Zhao,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: They build a scalable pipeline that converts web-scale pretraining text into millions of verifiable QA tasks for RL, yielding a 1.2M-example, 9+ domain dataset that lets RL-trained LMs outperform strong baselines and match continual pretraining with up to 100× fewer tokens.


<details>
  <summary>Details</summary>
Motivation: Imitation-learning on large corpora creates a train–generate mismatch and weakens robust reasoning. RL could fix this more data‑efficiently, but lacks web‑scale, diverse training data; existing RL datasets are tiny compared to pretraining corpora.

Method: Introduce Webscale-RL: a data engine that systematically transforms pretraining documents into diverse, verifiable question–answer pairs suitable for RL. Use it to assemble a 1.2M-example, 9+ domain dataset and train LMs with RL on these tasks.

Result: Across multiple benchmarks, models trained with this dataset via RL significantly beat continual pretraining and strong data-refinement baselines. Efficiency improves markedly: similar performance to continual pretraining is achieved with up to 100× fewer tokens.

Conclusion: Converting web-scale corpora into RL-ready, verifiable tasks enables scaling RL to pretraining levels, yielding more capable and token-efficient language models and offering a practical route to stronger reasoning.

Abstract: Large Language Models (LLMs) have achieved remarkable success through
imitation learning on vast text corpora, but this paradigm creates a
training-generation gap and limits robust reasoning. Reinforcement learning
(RL) offers a more data-efficient solution capable of bridging this gap, yet
its application has been constrained by a critical data bottleneck: existing RL
datasets are orders of magnitude smaller and less diverse than web-scale
pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a
scalable data engine that systematically converts large-scale pre-training
documents into millions of diverse, verifiable question-answer pairs for RL.
Using this pipeline, we construct the Webscale-RL dataset, containing 1.2
million examples across more than 9 domains. Our experiments show that the
model trained on this dataset significantly outperforms continual pretraining
and strong data refinement baselines across a suite of benchmarks. Notably, RL
training with our dataset proves substantially more efficient, achieving the
performance of continual pre-training with up to 100$\times$ fewer tokens. Our
work presents a viable path toward scaling RL to pre-training levels, enabling
more capable and efficient language models.

</details>


### [117] [From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining](https://arxiv.org/abs/2510.06548)
*Seng Pei Liew,Takuya Kato*

Main category: cs.CL

TL;DR: Bootstrapping a pretrained language model for additional pretraining yields predictably diminishing returns: the scaling exponent for second‑stage tokens shrinks roughly logarithmically with how many tokens the base model already saw. A simple joint scaling law over first‑ and second‑stage tokens captures this saturation, implying limited benefit from reusing heavily overtrained bases.


<details>
  <summary>Details</summary>
Motivation: Reduce cost versus training from scratch by reusing pretrained models (continual pretraining or model growth), and clarify when this is efficient—especially given uncertainty about benefits when the base is already overtrained.

Method: Empirical scaling study: vary the amount of first‑stage (base) pretraining tokens and measure performance gains from additional second‑stage tokens. Fit and validate a simple scaling law that jointly models dependence on both token budgets and extract the scaling exponent for second‑stage training.

Result: Scaling efficiency of bootstrapped pretraining diminishes in a predictable way: the scaling exponent with respect to second‑stage tokens decreases logarithmically with base tokens. A compact joint scaling law accurately models performance as a function of both stages, clearly showing saturation.

Conclusion: There is a fundamental trade‑off in multi‑stage pretraining: the more you pretrain the base, the less incremental benefit you get from further bootstrapping. For efficient training, avoid overtraining bases you plan to reuse, and be cautious about relying on bootstrapping when the base is already saturated.

Abstract: Bootstrapped pretraining, i.e., the reuse of a pretrained base model for
further pretraining, such as continual pretraining or model growth, is
promising at reducing the cost of training language models from scratch.
However, its effectiveness remains unclear, especially when applied to
overtrained base models. In this work, we empirically study the scaling
behavior of bootstrapped pretraining and find that its scaling efficiency
diminishes in a predictable manner: The scaling exponent with respect to
second-stage pretraining tokens decreases logarithmically with the number of
tokens used to pretrain the base model. The joint dependence on first- and
second-stage tokens is accurately modeled by a simple scaling law. Such
saturation effect reveals a fundamental trade-off in multi-stage pretraining
strategies: the more extensively a model is pretrained, the less additional
benefit bootstrapping provides. Our findings provide practical insights for
efficient language model training and raise important considerations for the
reuse of overtrained models.

</details>


### [118] [Flipping the Dialogue: Training and Evaluating User Language Models](https://arxiv.org/abs/2510.06552)
*Tarek Naous,Philippe Laban,Wei Xu,Jennifer Neville*

Main category: cs.CL

TL;DR: Assistant-trained LMs are poor user simulators; purpose-built “User LMs” better mimic real human behavior in multi-turn dialogues, revealing significantly lower assistant performance in realistic settings.


<details>
  <summary>Details</summary>
Motivation: Evaluations often simulate users with assistant-optimized LMs, which produce overly clean, cooperative prompts unlike real users. This likely inflates assistants’ measured performance and masks weaknesses in handling messy, evolving user intent.

Method: Post-train language models specifically as User LMs to emulate human users’ imperfect, evolving, and idiosyncratic utterances in multi-turn conversations. Compare against assistant-as-user simulations on alignment to human behavior and robustness. Use these simulators to evaluate assistants (e.g., GPT-4o) on coding and math tasks.

Result: Assistant LMs make poor user simulators—and the stronger the assistant, the worse the simulation quality. User LMs align better with human conversational behavior and yield more robust simulations. Under User LM simulations, GPT-4o’s performance drops from 74.6% to 57.4% in coding/math tasks, indicating prior setups overestimate capability.

Conclusion: Purpose-built User LMs create more realistic evaluation environments, exposing assistants’ struggles with nuanced, incremental user behavior. Future evaluations should adopt User LMs, and assistant training should focus on robustness to imperfect, multi-turn user inputs.

Abstract: Conversations with LMs involve two participants: a human user leading the
conversation, and an LM assistant responding to the user's request. To satisfy
this specific role, LMs are post-trained to be helpful assistants -- optimized
to produce exhaustive and well-structured responses, free of ambiguity and
grammar errors. User utterances, on the other hand, are rarely perfected, with
each user phrasing requests in unique ways, sometimes putting in partial effort
at each turn and refining on the fly. To evaluate LM performance in realistic
settings, prior work simulated users in multi-turn conversations, often
prompting an LLM originally trained to be a helpful assistant to act as a user.
However, we show that assistant LMs make for poor user simulators, with the
surprising finding that better assistants yield worse simulators. Instead, we
introduce purpose-built User Language Models (User LMs) - models post-trained
to simulate human users in multi-turn conversations. Through various
evaluations, we show how User LMs align better with human behavior and achieve
better simulation robustness than existing simulation methods. When leveraging
User LMs to simulate coding and math conversations, the performance of a strong
assistant (GPT-4o) drops from 74.6% to 57.4%, confirming that more realistic
simulation environments lead to assistant struggles as they fail to cope with
the nuances of users in multi-turn setups.

</details>


### [119] [The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law](https://arxiv.org/abs/2510.06559)
*Cheonkam Jeong,Sungdo Kim,Jewoo Park*

Main category: cs.CL

TL;DR: Proposes Savassan, a neuro-symbolic system that compiles natural-language inputs into typed Montague-style logical forms enriched with deontic and jurisdictional context to deliver compliance-aware, explainable decisions across legal regimes—framing hallucination and moderation failures as type errors.


<details>
  <summary>Details</summary>
Motivation: Fluent LMs still hallucinate, mis-handle normative/legal meaning, and yield opaque moderation/compliance outcomes. Authors argue the root cause is the absence of type-theoretic, compositional semantics that distinguishes descriptive facts from prescriptions and liabilities.

Method: Recast alignment as parsing. Neural components extract candidate semantic structures from text; symbolic components perform typing, constraint checking, and cross-jurisdiction mapping over typed ontologies extended with deontic operators. A "parse once, project many" approach maps a single logical form into multiple legal ontologies to synthesize an explainable, compliance-aware outcome.

Result: Conceptual and architectural contribution rather than empirical. Deliverables: (i) diagnosis of hallucination as type error; (ii) a formal bridge between Montague semantics and legal/business ontologies; (iii) a production-oriented design with typed interfaces. Plans an evaluation on legal reasoning benchmarks and synthetic multi-jurisdiction suites.

Conclusion: Trustworthy autonomy requires compositional typing of meaning. By making descriptive, prescriptive, and liability dimensions explicit, systems can provide nuanced guidance instead of binary censorship and reduce hallucinations via type checking and constraint reasoning.

Abstract: Contemporary language models are fluent yet routinely mis-handle the types of
meaning their outputs entail. We argue that hallucination, brittle moderation,
and opaque compliance outcomes are symptoms of missing type-theoretic semantics
rather than data or scale limitations. Building on Montague's view of language
as typed, compositional algebra, we recast alignment as a parsing problem:
natural-language inputs must be compiled into structures that make explicit
their descriptive, normative, and legal dimensions under context.
  We present Savassan, a neuro-symbolic architecture that compiles utterances
into Montague-style logical forms and maps them to typed ontologies extended
with deontic operators and jurisdictional contexts. Neural components extract
candidate structures from unstructured inputs; symbolic components perform type
checking, constraint reasoning, and cross-jurisdiction mapping to produce
compliance-aware guidance rather than binary censorship. In cross-border
scenarios, the system "parses once" (e.g., defect claim(product x, company y))
and projects the result into multiple legal ontologies (e.g., defamation risk
in KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into
a single, explainable decision.
  This paper contributes: (i) a diagnosis of hallucination as a type error;
(ii) a formal Montague-ontology bridge for business/legal reasoning; and (iii)
a production-oriented design that embeds typed interfaces across the pipeline.
We outline an evaluation plan using legal reasoning benchmarks and synthetic
multi-jurisdiction suites. Our position is that trustworthy autonomy requires
compositional typing of meaning, enabling systems to reason about what is
described, what is prescribed, and what incurs liability within a unified
algebra of meaning.

</details>


### [120] [TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents](https://arxiv.org/abs/2510.06579)
*Haofei Yu,Keyang Xuan,Fenghai Li,Kunlun Zhu,Zijie Lei,Jiaxun Zhang,Ziheng Qi,Kyle Richardson,Jiaxuan You*

Main category: cs.CL

TL;DR: TinyScientist introduces an interactive, extensible, and controllable framework that streamlines building and maintaining LLM-driven “auto-research” workflows, released as open-source code, a web demo, and a PyPI package.


<details>
  <summary>Details</summary>
Motivation: LLM-based research pipelines (multi-agent planning, tool use, code execution, human-in-the-loop) are becoming complex and hard to extend/maintain as algorithms and architectures advance; there is a need for a modular, adaptable foundation.

Method: Abstracts the essential components of automatic research workflows and implements a modular, interactive framework that supports iterative growth and easy integration of new tools; distributes it via open-source repository, interactive web interface, and Python package.

Result: Delivers a working, accessible system enabling state-of-the-art auto-research pipelines with easier extensibility and control. The abstract does not report quantitative benchmarks or empirical gains.

Conclusion: A practical foundation for building and evolving agentic research workflows, aiming to reduce complexity and broaden adoption of LLM-driven research automation.

Abstract: Automatic research with Large Language Models (LLMs) is rapidly gaining
importance, driving the development of increasingly complex workflows involving
multi-agent systems, planning, tool usage, code execution, and human-agent
interaction to accelerate research processes. However, as more researchers and
developers begin to use and build upon these tools and platforms, the
complexity and difficulty of extending and maintaining such agentic workflows
have become a significant challenge, particularly as algorithms and
architectures continue to advance. To address this growing complexity,
TinyScientist identifies the essential components of the automatic research
workflow and proposes an interactive, extensible, and controllable framework
that easily adapts to new tools and supports iterative growth. We provide an
open-source codebase, an interactive web demonstration, and a PyPI Python
package to make state-of-the-art auto-research pipelines broadly accessible to
every researcher and developer.

</details>


### [121] [Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?](https://arxiv.org/abs/2510.06594)
*Sri Durga Sai Sowmya Kadali,Evangelos E. Papalexakis*

Main category: cs.CL

TL;DR: The paper probes hidden-layer activations of GPT-J and the state-space model Mamba2 under jailbreak vs benign prompts, finding distinct layer-wise signatures that could be leveraged for jailbreak detection and defense.


<details>
  <summary>Details</summary>
Motivation: Jailbreaking remains a persistent threat to conversational LLMs; existing defenses are brittle and reactive. The authors seek internal, model-native signals that differentiate malicious from benign prompting to enable more robust and proactive defenses.

Method: Empirical analysis of internal representations: compare layer-wise activations/behaviors of GPT-J (transformer) and Mamba2 (state-space) when processing jailbreak versus benign prompts, examining how responses evolve across layers to identify discriminative patterns.

Result: Preliminary evidence of systematic differences in hidden-layer responses between jailbreak and benign inputs, with distinct layer-wise behaviors observed in both GPT-J and Mamba2, suggesting separable internal signatures of jailbreak attempts.

Conclusion: Internal model dynamics offer a promising basis for jailbreak detection and potential defenses. Further work should formalize detectors/classifiers on hidden states, test robustness across models and attack types, and integrate such signals into safety pipelines.

Abstract: Jailbreaking large language models (LLMs) has emerged as a pressing concern
with the increasing prevalence and accessibility of conversational LLMs.
Adversarial users often exploit these models through carefully engineered
prompts to elicit restricted or sensitive outputs, a strategy widely referred
to as jailbreaking. While numerous defense mechanisms have been proposed,
attackers continuously develop novel prompting techniques, and no existing
model can be considered fully resistant. In this study, we investigate the
jailbreak phenomenon by examining the internal representations of LLMs, with a
focus on how hidden layers respond to jailbreak versus benign prompts.
Specifically, we analyze the open-source LLM GPT-J and the state-space model
Mamba2, presenting preliminary findings that highlight distinct layer-wise
behaviors. Our results suggest promising directions for further research on
leveraging internal model dynamics for robust jailbreak detection and defense.

</details>


### [122] [A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures](https://arxiv.org/abs/2510.06640)
*Nhat M. Hoang,Do Xuan Long,Cong-Duy Nguyen,Min-Yen Kan,Luu Anh Tuan*

Main category: cs.CL

TL;DR: Unified, token- and layer-level study compares how representations propagate in SSMs vs. Transformers for long sequences. Using CKA, stability metrics, probing, theory, and parameter randomization, the paper finds Transformers rapidly homogenize token embeddings then re-diversify later, while SSMs keep tokens distinct early but homogenize deep. Oversmoothing in Transformers is architectural; in SSMs it mainly emerges from training. Insights guide architecture and training for long-context reasoning.


<details>
  <summary>Details</summary>
Motivation: Despite SSMs’ efficiency for long sequences, we lack a clear understanding of how context and token information flow across layers compared to Transformer-based models. Clarifying representation dynamics and oversmoothing sources can inform better model and training design for long-context tasks.

Method: Layer- and token-level analysis via centered kernel alignment (CKA), stability metrics, and probing to track representational evolution; complemented by theoretical analysis and parameter randomization to attribute causes of oversmoothing to architecture vs. training.

Result: Key divergence in representational dynamics: Transformers quickly homogenize token representations with later re-emergence of diversity, whereas SSMs preserve token uniqueness early but gradually converge to homogeneous states in deeper layers. Oversmoothing in Transformers is tied to architectural design; in SSMs it arises primarily from training dynamics.

Conclusion: SSMs and Transformers have distinct inductive biases affecting long-context reasoning. Mitigating oversmoothing should target architecture changes for Transformers and training strategies for SSMs. These findings can guide future model and training designs for long-sequence processing.

Abstract: State Space Models (SSMs) have recently emerged as efficient alternatives to
Transformer-Based Models (TBMs) for long-sequence processing, offering linear
scaling and lower memory use. Yet, how contextual information flows across
layers and tokens in these architectures remains understudied. We present the
first unified, token- and layer-level analysis of representation propagation in
SSMs and TBMs. Using centered kernel alignment, stability metrics, and probing,
we characterize how representations evolve within and across layers. We find a
key divergence: TBMs rapidly homogenize token representations, with diversity
reemerging only in later layers, while SSMs preserve token uniqueness early but
converge to homogenization deeper. Theoretical analysis and parameter
randomization further reveal that oversmoothing in TBMs stems from
architectural design, whereas in SSMs it arises mainly from training dynamics.
These insights clarify the inductive biases of both architectures and inform
future model and training designs for long-context reasoning.

</details>


### [123] [Aligning Large Language Models via Fully Self-Synthetic Data](https://arxiv.org/abs/2510.06652)
*Shangjian Yin,Zhepei Wei,Xinyu Zhu,Wei-Lin Chen,Yu Meng*

Main category: cs.CL

TL;DR: SAO is a fully self-synthetic alignment framework where an LLM generates its own prompts, responses, and preference labels via persona role-play and self-evaluation, improving chat performance while preserving downstream task ability—cutting costs vs RLHF/RLAIF.


<details>
  <summary>Details</summary>
Motivation: Conventional RLHF is costly due to human annotations; RLAIF still incurs significant expense, requiring diverse prompt/response data and external reward or proprietary models (e.g., GPT‑4) to label preferences. The goal is to remove external data/labeling costs and dependencies by enabling self-improvement.

Method: In SAO, the LLM is prompted to role‑play personas to synthesize diverse user queries and candidate responses. It then self-evaluates these responses to produce preference signals used for preference optimization, aligning the model without external annotators or reward models.

Result: Experiments show improved chat capability on AlpacaEval 2.0 while maintaining strong performance on objective tasks like QA and math reasoning. Open-source code is provided.

Conclusion: SAO offers a practical, low-cost path to align LLMs via self-generated data and preferences, demonstrating competitive chat improvements without degrading core task performance.

Abstract: Traditional reinforcement learning from human feedback (RLHF) for large
language models (LLMs) relies on expensive human-annotated datasets, while
Reinforcement Learning from AI Feedback (RLAIF) also incurs significant costs,
requiring the collection of diverse prompts and corresponding responses, often
necessitating external reward models or proprietary models like GPT-4 to
annotate preference pairs. In this work, we introduce Self-Alignment
Optimization (SAO), a fully self-synthetic framework for LLM alignment, where
all training data, including prompts (i.e., user queries), responses, and
preferences, are generated by the model itself. Specifically, SAO first
instructs the LLM to engage in persona role-play and generate diverse prompts
and responses, which are then self-evaluated for preference optimization.
Extensive experiments demonstrate that SAO effectively enhances the model's
chat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining
strong performance on downstream objective tasks (e.g., question-answering,
math reasoning). Our work provides a practical solution for self-improvement in
aligning LLMs, and the code for reproducing our results is available at:
https://github.com/SJY8460/SAO.

</details>


### [124] [ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory](https://arxiv.org/abs/2510.06664)
*Yunzhong Xiao,Yangmin Li,Hewei Wang,Yunlong Tang,Zora Zhiruo Wang*

Main category: cs.CL

TL;DR: ToolMem is a memory mechanism for LLM/VLM-based agents that summarizes each neural tool’s strengths and weaknesses from past interactions and retrieves them at inference to predict performance and select the best tool, improving accuracy for text and text-to-image tasks.


<details>
  <summary>Details</summary>
Motivation: Neural tools (LLMs/VLMs) are non-deterministic and perform unevenly across scenarios; fixed tool choices waste potential. Humans accumulate experiential knowledge about tools and pick the right one per task—this work aims to give agents an analogous capability.

Method: During training/interaction, the agent records outcomes from using different tools and summarizes per-tool capabilities (strengths/weaknesses) into memory entries keyed by task/context features. At inference, it retrieves relevant memories, predicts each tool’s likely performance for the current instance, and selects the best. Evaluated on text generation and text-to-image generation; baselined against agents without such memory; metrics include accuracy of predicting tool performance and rate of selecting the optimal tool.

Result: Compared to no-memory agents, ToolMem improves prediction of tool performance by 14.8% (text) and 28.7% (multimodal) and increases optimal tool selection by 21% and 24% absolute, respectively.

Conclusion: Encoding and retrieving experiential knowledge about tool capabilities enables adaptive, scenario-aware tool selection, yielding sizable gains across modalities; memory-augmented agents are a promising direction for robust tool use.

Abstract: Agents utilizing tools powered by large language models (LLMs) or
vision-language models (VLMs) have demonstrated remarkable progress in diverse
tasks across text and visual modalities. Unlike traditional tools such as
calculators, which give deterministic outputs, neural tools perform uncertainly
across task scenarios. While different tools for a task may excel in varied
scenarios, existing agents typically rely on fixed tools, thus limiting the
flexibility in selecting the most suitable tool for specific tasks. In
contrast, humans snowball their understanding of the capabilities of different
tools by interacting with them, and apply this knowledge to select the optimal
tool when solving a future task. To build agents that similarly benefit from
this process, we propose ToolMem that enables agents to develop memories of
tool capabilities from previous interactions, by summarizing their strengths
and weaknesses and storing them in memory; at inference, the agent can retrieve
relevant entries from ToolMem, and select the best tool to solve individual
tasks more accurately. We evaluate ToolMem on learning varied text generation
and text-to-image generation neural tools. Compared to no-memory, generic
agents, we find ToolMem-augmented agents predict tool performance 14.8% and
28.7% more accurately across text and multimodal generation scenarios.
Moreover, ToolMem facilitates optimal tool selection among multiple choices by
21% and 24% absolute increases in respective scenarios.

</details>


### [125] [PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch](https://arxiv.org/abs/2510.06670)
*Shangjian Yin,Shining Liang,Wenbiao Ding,Yuli Qian,Zhouxing Shi,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: PiKa introduces a compact, high-quality alignment dataset (notably PiKa-SFT with 30k SFT examples) that enables open LLMs to reach or exceed the performance of models trained on orders-of-magnitude more data, even surpassing Llama‑3‑8B‑Instruct on AlpacaEval 2.0 and Arena-Hard.


<details>
  <summary>Details</summary>
Motivation: LLM alignment via RLHF/RLAIF depends on expensive, private, or noisy instruction data, limiting reproducibility and scalability. It remains unclear how much data is truly needed to obtain strong instruction-following behavior; many pipelines use >300k SFT examples yet still lag behind proprietary models.

Method: Construct the PiKa family of expert-level alignment datasets, focusing on PiKa-SFT (30k examples). Fine-tune base models (Llama‑3‑8B‑Base and Qwen2.5 series, 0.5B–7B) on PiKa-SFT and compare against training on larger public datasets. Evaluate on AlpacaEval 2.0 and Arena-Hard benchmarks; report cross-model scaling behavior.

Result: Models fine-tuned on PiKa-SFT outperform counterparts trained on much larger datasets. Llama‑3‑8B‑Base fine-tuned on PiKa-SFT surpasses the official Llama‑3‑8B‑Instruct (trained on >10M proprietary examples) on AlpacaEval 2.0 and Arena-Hard. Gains are consistent across Qwen2.5 models from 0.5B to 7B parameters.

Conclusion: High-quality alignment can be achieved with significantly less data than commonly assumed. PiKa offers a scalable, open-source path for efficient alignment and reduces dependency on massive proprietary corpora.

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone
for aligning large language models (LLMs). However, its effectiveness depends
on high-quality instruction data. Most existing alignment datasets are either
private or require costly human annotation, which limits reproducibility and
scalability. Even with Reinforcement Learning from AI Feedback (RLAIF),
concerns about data quality remain. Moreover, it is unclear how much data is
actually required to fine-tune a base model into a strong instruction-following
model. Current approaches often rely on over 300k examples even at the
supervised fine-tuning (SFT) stage, yet they still underperform compared to
proprietary models, creating barriers for academic and resource-limited
communities. To address this gap, we introduce PiKa, a data-efficient family of
expert-level alignment datasets. In particular, the PiKa-SFT dataset uses only
30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through
evaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets,
we show that PiKa-SFT outperforms models trained on much larger data. On
AlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses
the official Llama-3-8B-Instruct model trained on over 10 million proprietary
examples. We further extend our study by training the Qwen2.5 series (0.5B to
7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that
high-quality alignment can be achieved with significantly less data, offering a
scalable path for open-source LLM alignment. Code and data:
https://github.com/SJY8460/PiKa.

</details>


### [126] [Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback](https://arxiv.org/abs/2510.06677)
*Yisha Wu,Cen,Zhao,Yuanpei Cao,Xiaoqing Su,Yashar Mehdad,Mindy Ji,Claire Na Cheng*

Main category: cs.CL

TL;DR: Production system for incremental, in-conversation note-taking reduces customer support case handling time (−3% overall, up to −9% on complex cases) by combining a fine-tuned Mixtral-8x7B for continuous notes, a DeBERTa classifier to suppress trivial content, and a closed feedback loop from agent edits for online refinement and offline retraining.


<details>
  <summary>Details</summary>
Motivation: Bulk, end-of-conversation summaries force context switching and re-reading, slowing agents and lowering utility. The authors aim to provide timely, concise bullet notes during the conversation to lighten cognitive load and improve productivity and summary quality.

Method: - Fine-tuned Mixtral-8x7B generates incremental bullet notes throughout conversations.
- A DeBERTa-based classifier decides when and what to note by filtering trivial/low-value content.
- Agent edits are captured to refine online generation and are used for periodic offline retraining (closed feedback loop).
- Deployed in production; compared against a bulk summarization baseline.

Result: - 3% reduction in average case handling time versus bulk summarization.
- Up to 9% reduction on highly complex cases.
- High agent satisfaction from surveys.

Conclusion: Incremental, feedback-driven summarization outperforms bulk summarization for customer support, improving both efficiency and user satisfaction; continuous human-in-the-loop signals are key to quality and scalability.

Abstract: We introduce an incremental summarization system for customer support agents
that intelligently determines when to generate concise bullet notes during
conversations, reducing agents' context-switching effort and redundant review.
Our approach combines a fine-tuned Mixtral-8x7B model for continuous note
generation with a DeBERTa-based classifier to filter trivial content. Agent
edits refine the online notes generation and regularly inform offline model
retraining, closing the agent edits feedback loop. Deployed in production, our
system achieved a 3% reduction in case handling time compared to bulk
summarization (with reductions of up to 9% in highly complex cases), alongside
high agent satisfaction ratings from surveys. These results demonstrate that
incremental summarization with continuous feedback effectively enhances summary
quality and agent productivity at scale.

</details>


### [127] [Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks](https://arxiv.org/abs/2510.06695)
*Qinhao Zhou,Xiang Xiang,Kun He,John E. Hopcroft*

Main category: cs.CL

TL;DR: They propose a lightweight, back-translation–driven prompt optimization method that targets the input component for machine translation, cutting training cost while achieving strong performance and offering extensibility to other tasks.


<details>
  <summary>Details</summary>
Motivation: Most prompt engineering optimizes instructions and often relies on large LLMs, which is ill-suited for machine translation where the input content dominates and instructions are minimal. A low-cost, task-specific approach is needed to optimize inputs rather than instructions.

Method: Introduce a prompt optimization framework tailored to machine translation that uses a small-parameter model trained with a back-translation-based strategy to refine/optimize the input component. The design focuses on single-task optimization with low overhead and can be adapted to other downstream tasks.

Result: Claims include significantly reduced training overhead compared to large-LLM-based prompt optimization and highly effective translation performance when optimizing the input component. Empirical details are not provided in the abstract.

Conclusion: Optimizing the input side of prompts via a small model and back-translation is an efficient and effective strategy for machine translation, lessening dependence on large LLMs and potentially generalizing—with adaptations—to other NLG tasks.

Abstract: In recent years, the growing interest in Large Language Models (LLMs) has
significantly advanced prompt engineering, transitioning from manual design to
model-based optimization. Prompts for LLMs generally comprise two components:
the \textit{instruction}, which defines the task or objective, and the
\textit{input}, which is tailored to the instruction type. In natural language
generation (NLG) tasks such as machine translation, the \textit{input}
component is particularly critical, while the \textit{instruction} component
tends to be concise. Existing prompt engineering methods primarily focus on
optimizing the \textit{instruction} component for general tasks, often
requiring large-parameter LLMs as auxiliary tools. However, these approaches
exhibit limited applicability for tasks like machine translation, where the
\textit{input} component plays a more pivotal role. To address this limitation,
this paper introduces a novel prompt optimization method specifically designed
for machine translation tasks. The proposed approach employs a small-parameter
model trained using a back-translation-based strategy, significantly reducing
training overhead for single-task optimization while delivering highly
effective performance. With certain adaptations, this method can also be
extended to other downstream tasks.

</details>


### [128] [How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects](https://arxiv.org/abs/2510.06700)
*Leonardo Bertolazzi,Sandro Pezzelle,Raffaelle Bernardi*

Main category: cs.CL

TL;DR: LLMs linearly encode both logical validity and semantic plausibility along highly aligned directions, causing them to conflate the two; steering and debiasing these directions modulates and improves reasoning.


<details>
  <summary>Details</summary>
Motivation: Content effects—where plausible content biases judgments of logical validity—are well-established in humans via dual-process theory, but the mechanisms in LLMs are unknown. The paper aims to uncover how LLMs internally represent validity vs. plausibility and why content effects arise.

Method: Probe internal representations to identify linear directions corresponding to validity and plausibility; analyze their geometric alignment; use steering vectors to causally bias one concept with the other; measure how alignment predicts behavioral content effects across models; construct debiasing vectors to disentangle concepts and evaluate impact on reasoning accuracy.

Result: Validity and plausibility are each linearly decodable and strongly aligned in representation space. Steering along the plausibility (or validity) vector biases judgments of the other. The alignment magnitude predicts observed content-effect strength across models. Debiasing vectors reduce alignment, attenuate content effects, and increase reasoning accuracy.

Conclusion: LLMs encode abstract logical concepts in aligned linear subspaces, explaining content effects as representational conflation. Targeted representational interventions (steering/debiasing vectors) can mitigate bias and yield more logically consistent reasoning.

Abstract: Both humans and large language models (LLMs) exhibit content effects: biases
in which the plausibility of the semantic content of a reasoning problem
influences judgments regarding its logical validity. While this phenomenon in
humans is best explained by the dual-process theory of reasoning, the
mechanisms behind content effects in LLMs remain unclear. In this work, we
address this issue by investigating how LLMs encode the concepts of validity
and plausibility within their internal representations. We show that both
concepts are linearly represented and strongly aligned in representational
geometry, leading models to conflate plausibility with validity. Using steering
vectors, we demonstrate that plausibility vectors can causally bias validity
judgements, and vice versa, and that the degree of alignment between these two
concepts predicts the magnitude of behavioral content effects across models.
Finally, we construct debiasing vectors that disentangle these concepts,
reducing content effects and improving reasoning accuracy. Our findings advance
understanding of how abstract logical concepts are represented in LLMs and
highlight representational interventions as a path toward more logical systems.

</details>


### [129] [Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management](https://arxiv.org/abs/2510.06727)
*Miao Lu,Weiwei Sun,Weihua Du,Zhan Ling,Xuesong Yao,Kang Liu,Jiecao Chen*

Main category: cs.CL

TL;DR: SUPO is an RL algorithm that co-optimizes tool-use actions and LLM-generated summaries, enabling long-horizon multi-turn tool use beyond fixed context windows and improving success rates with equal or lower working context.


<details>
  <summary>Details</summary>
Motivation: LLM agents trained with RL struggle on long, multi-turn tool-use tasks because context windows are limited; existing RL pipelines incur high rollout costs, degrade instruction following, and are strictly bounded by context length.

Method: Introduce summarization-based context management: periodically compress interaction histories into task-relevant summaries generated by the LLM. Derive a policy-gradient formulation that treats summarization as part of the policy, allowing end-to-end optimization of both tool-use behavior and summarization strategy within standard LLM RL infrastructure. Instantiate this as SUPO (Summarization-augmented Policy Optimization).

Result: On interactive function-calling and searching benchmarks, SUPO substantially boosts success rates while maintaining the same or lower effective context length than baselines. For complex search tasks, allowing more summarization rounds at test time than at training further improves performance.

Conclusion: Summarization-based context management provides a principled, scalable path to training RL LLM agents beyond fixed context limits; SUPO validates this approach with improved effectiveness and efficiency.

Abstract: We study reinforcement learning (RL) fine-tuning of large language model
(LLM) agents for long-horizon multi-turn tool use, where context length quickly
becomes a fundamental bottleneck. Existing RL pipelines can suffer from
degraded instruction following, excessive rollout costs, and most importantly,
strict context limits. To address these challenges, we introduce
summarization-based context management to training. In specific, it
periodically compresses the tool using history by LLM-generated summaries that
retain task-relevant information to keep a compact context while enabling the
agent to scale beyond the fixed context window. Building on this formulation,
we derive a policy gradient representation that seamlessly enables standard LLM
RL infrastructures to optimize both tool-use behaviors as well as summarization
strategies in an end-to-end fashion. We instantiate this framework with
\underline{SU}mmarization augmented \underline{P}olicy \underline{O}ptimization
(\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond
a fixed context limit. Experiments on interactive function calling and
searching tasks demonstrate that \texttt{SUPO} significantly improves the
success rate while maintaining the same or even lower working context length
compared to baselines. We also demonstrate that for complex searching tasks,
\texttt{SUPO} can further improve the evaluation performance when scaling
test-time maximum round of summarization beyond that of training time. Our
results establish summarization-based context management as a principled and
scalable approach for training RL agents beyond a fixed context length limit.

</details>


### [130] [PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs](https://arxiv.org/abs/2510.06730)
*Manuel Frank,Haithem Afli*

Main category: cs.CL

TL;DR: They propose PTEB, a dynamic paraphrase-based evaluation protocol for sentence embeddings that generates meaning-preserving paraphrases at evaluation time using LLMs and aggregates results over multiple runs. It reveals that encoder performance is sensitive to token-level changes despite fixed semantics, shows no disproportionate harm to smaller models, is statistically robust, and generalizes to multilingual settings—arguing for a shift from static to stochastic, compute-at-eval evaluation.


<details>
  <summary>Details</summary>
Motivation: Static, fixed benchmarks (e.g., MTEB) risk overfitting and may mask robustness issues. The authors aim to create an evaluation that tests invariance to paraphrasing—i.e., whether embeddings remain stable when wording changes but semantics do not—reflecting real-world variation and discouraging tuning to a static test set.

Method: Design a dynamic evaluation protocol (PTEB) that stochastically generates meaning-preserving paraphrases at evaluation time using a cost-efficient LLM pipeline guided by semantic textual similarity (STS) gold ratings. Evaluate multiple runs and aggregate results to reduce variance. Apply across 7 MTEB tasks and extend to 3 multilingual datasets (10 languages) to test robustness. Assess token diversity and semantic preservation of generated paraphrases.

Result: LLMs produce paraphrases that are token-diverse yet semantically consistent per STS. Sentence encoders show sensitivity to paraphrase-induced token changes even when meaning is fixed. Smaller models are not disproportionately impacted relative to larger ones. Results are statistically robust across runs and hold in multilingual evaluations.

Conclusion: Static test suites alone are insufficient for robust evaluation of sentence embeddings. Dynamic, stochastic, eval-time protocols like PTEB better capture robustness to paraphrase variation and should complement or partially replace fixed benchmarks in NLP evaluation.

Abstract: Current evaluations of sentence embedding models typically rely on static
test beds such as the Massive Text Embedding Benchmark (MTEB). While
invaluable, repeated tuning on a fixed suite can inflate reported performance
and obscure real-world robustness. We introduce the Paraphrasing Text Embedding
Benchmark (PTEB), a dynamic protocol that stochastically generates
meaning-preserving paraphrases at evaluation time and aggregates results across
multiple runs. Using a cost-efficient LLM-based method grounded in semantic
textual similarity gold ratings, we show that LLMs generate token-diverse but
semantically preserving, paraphrases. Across 7 MTEB tasks, we validate our
hypothesis that the performance of sentence encoders is sensitive to changes in
token space even when semantics remain fixed. We also observe that smaller
models are not disproportionately affected relative to larger ones. Our results
are statistically robust over multiple runs and we extended our experiments to
3 multilingual datasets covering 10 languages. More generally, we aim to
propose a new evaluation paradigm in NLP that relies less on static,
pre-defined benchmarks but shifts towards dynamic, stochastic evaluation
leveraging eval-time compute.

</details>


### [131] [Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization](https://arxiv.org/abs/2510.06732)
*Tiancheng Xing,Jerry Li,Yixuan Du,Xiyang Hu*

Main category: cs.CL

TL;DR: RAF is a two-stage token-level optimization method that crafts short, natural-sounding prompts to reliably push a target item higher in LLM-based rerankings, outperforming prior attacks and revealing a significant security vulnerability.


<details>
  <summary>Details</summary>
Motivation: LLMs used as rerankers can have their rankings steered by subtle prompt changes. The authors aim to systematically expose and quantify this vulnerability by generating adversarial yet natural prompts that promote specific targets while remaining hard to detect.

Method: Rank Anything First (RAF) generates an adversarial prompt token-by-token with dual objectives: maximize the target’s rank and preserve linguistic naturalness. Stage 1 (Greedy Coordinate Gradient) shortlists candidate tokens by combining the gradient signal of a rank-target objective with a readability score. Stage 2 evaluates these candidates using exact ranking and readability losses with an entropy-based dynamic weighting scheme, then selects tokens via temperature-controlled sampling. The process iterates to produce concise perturbations.

Result: Across multiple LLMs, RAF consistently improves the rank of target items while keeping the text naturalistic, and shows greater robustness than existing methods in both effectiveness and naturalness.

Conclusion: LLM-based reranking is inherently susceptible to adversarial prompt manipulation, posing security and trustworthiness risks for retrieval systems. The authors release code to facilitate further study and defenses.

Abstract: Large language models (LLMs) are increasingly used as rerankers in
information retrieval, yet their ranking behavior can be steered by small,
natural-sounding prompts. To expose this vulnerability, we present Rank
Anything First (RAF), a two-stage token optimization method that crafts concise
textual perturbations to consistently promote a target item in LLM-generated
rankings while remaining hard to detect. Stage 1 uses Greedy Coordinate
Gradient to shortlist candidate tokens at the current position by combining the
gradient of the rank-target with a readability score; Stage 2 evaluates those
candidates under exact ranking and readability losses using an entropy-based
dynamic weighting scheme, and selects a token via temperature-controlled
sampling. RAF generates ranking-promoting prompts token-by-token, guided by
dual objectives: maximizing ranking effectiveness and preserving linguistic
naturalness. Experiments across multiple LLMs show that RAF significantly
boosts the rank of target items using naturalistic language, with greater
robustness than existing methods in both promoting target items and maintaining
naturalness. These findings underscore a critical security implication:
LLM-based reranking is inherently susceptible to adversarial manipulation,
raising new challenges for the trustworthiness and robustness of modern
retrieval systems. Our code is available at: https://github.com/glad-lab/RAF.

</details>


### [132] [AWM: Accurate Weight-Matrix Fingerprint for Large Language Models](https://arxiv.org/abs/2510.06738)
*Boyi Zeng,Lin Chen,Ziwei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: Training-free weight-matrix fingerprinting for LLM lineage verification using LAP-aligned unbiased CKA; robust to many post-training changes; perfect classification on tested pairs; runs in ~30s on a 3090.


<details>
  <summary>Details</summary>
Motivation: LLM training is resource-intensive, so owners need reliable ways to prove or detect whether a suspect model is derived from an existing base model. Post-training steps (SFT, continued pretraining, RL, multimodal extension, pruning, upcycling) can obscure lineage, making prior identification methods brittle or costly.

Method: Compute inter-model similarity directly from weight matrices without extra training. Use a Linear Assignment Problem (LAP) to align components (e.g., layers/neurons/heads) and an unbiased Centered Kernel Alignment (CKA) similarity to measure correspondence, thereby neutralizing parameter permutations and manipulations. The resulting metric serves as a robust fingerprint for model relatedness.

Result: On a testbed with 60 positive (related) and 90 negative (unrelated) model pairs, the method remains robust across six post-training categories, achieves near-zero false positives, and reports perfect scores across standard classification metrics. End-to-end runtime is about 30 seconds on an NVIDIA RTX 3090.

Conclusion: Weight-matrix–based, training-free fingerprinting with LAP-aligned unbiased CKA provides a reliable, efficient basis for LLM lineage verification, robust to common post-training transformations and practical for real-world enforcement and auditing.

Abstract: Protecting the intellectual property of large language models (LLMs) is
crucial, given the substantial resources required for their training.
Consequently, there is an urgent need for both model owners and third parties
to determine whether a suspect LLM is trained from scratch or derived from an
existing base model. However, the intensive post-training processes that models
typically undergo-such as supervised fine-tuning, extensive continued
pretraining, reinforcement learning, multi-modal extension, pruning, and
upcycling-pose significant challenges to reliable identification. In this work,
we propose a training-free fingerprinting method based on weight matrices. We
leverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel
Alignment (CKA) similarity to neutralize the effects of parameter
manipulations, yielding a highly robust and high-fidelity similarity metric. On
a comprehensive testbed of 60 positive and 90 negative model pairs, our method
demonstrates exceptional robustness against all six aforementioned
post-training categories while exhibiting a near-zero risk of false positives.
By achieving perfect scores on all classification metrics, our approach
establishes a strong basis for reliable model lineage verification. Moreover,
the entire computation completes within 30s on an NVIDIA 3090 GPU. The code is
available at https://github.com/LUMIA-Group/AWM.

</details>


### [133] [TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs](https://arxiv.org/abs/2510.06747)
*I-Fan Lin,Faegheh Hasibi,Suzan Verberne*

Main category: cs.CL

TL;DR: Training- and label-free short-text clustering that plugs into any embedder and iteratively refines sparse representative vectors with small LLM guidance, matching or surpassing contrastive-learning SOTA while scaling efficiently without knowing the number of clusters.


<details>
  <summary>Details</summary>
Motivation: Commercial chatbots receive massive, unlabeled user utterances with unknown intent counts. Existing methods often need labels, contrastive pretraining, or fixed cluster counts, which is impractical and costly. A model-agnostic, low-resource, scalable clustering approach is needed for real-world deployments.

Method: Start with any text embedder; build sparse vectors from representative texts; iteratively update/refine these vectors using guidance from a (small) LLM. The process is training-free and label-free, agnostic to the underlying embedder, and compatible with different clustering algorithms. It includes strategies that reduce LLM calls to scale to large datasets.

Result: On diverse datasets, the approach achieves comparable or superior performance to state-of-the-art contrastive learning methods despite not assuming prior knowledge of labels or the number of clusters. It works with smaller LLMs, different embedders, and various clustering methods, and scales to large datasets with reduced LLM compute cost.

Conclusion: The method offers a practical, adaptable, and scalable solution for real-world intent clustering in short texts, aligning better with low-resource commercial settings than existing supervised or contrastive approaches.

Abstract: In this paper, we propose a training-free and label-free method for short
text clustering that can be used on top of any existing embedder. In the
context of customer-facing chatbots, companies are dealing with large amounts
of user utterances that need to be clustered according to their intent. In
these commercial settings, no labeled data is typically available, and the
number of clusters is not known. Our method is based on iterative vector
updating: it constructs sparse vectors based on representative texts, and then
iteratively refines them through LLM guidance. Our method achieves comparable
or superior results to state-of-the-art methods that use contrastive learning,
but without assuming prior knowledge of clusters or labels. Experiments on
diverse datasets and smaller LLMs show that our method is model agnostic and
can be applied to any embedder, with relatively small LLMs, and different
clustering methods. We also show that our method scales to large datasets,
reducing the computational cost of the LLM. These low-resource, adaptable
settings and the scalability of our method make it more aligned with real-world
scenarios than existing clustering methods.

</details>


### [134] [A Formal Framework for Fluency-based Multi-Reference Evaluation in Grammatical Error Correction](https://arxiv.org/abs/2510.06749)
*Eitan Klinger,Zihao Huang,Tran Minh Nguyen,Emma Jayeon Park,Yige Chen,Yang Gu,Qingyu Gao,Siliang Liu,Mengyang Qiu,Jungyeul Park*

Main category: cs.CL

TL;DR: Proposes a fluency-oriented, multi-reference evaluation framework for grammatical error correction that aggregates n-gram similarity across multiple valid corrections, instantiates four GLEU aggregation strategies, analyzes their properties, and shows cross-lingual effectiveness without penalizing legitimate variation.


<details>
  <summary>Details</summary>
Motivation: Current GEC metrics are largely edit-based, single-reference, and English-centric, relying on rigid edit alignments that miss the diversity of valid human corrections and do not generalize well to multilingual or generative settings.

Method: Formalize multi-reference evaluation as an n-gram aggregation problem over multiple legitimate corrections. Instantiate GLEU using four aggregation strategies—select-best, simple-average, weighted-average, and merged-counts—and analyze boundedness, monotonicity, and sensitivity to reference variation. Evaluate empirically on Czech, Estonian, Ukrainian, and Chinese corpora.

Result: Across four non-English corpora, the proposed aggregation strategies capture complementary aspects of fluency and coverage, exhibiting robustness to reference variation and better reflecting diverse valid corrections.

Conclusion: A unified, fluency-based multi-reference framework enables more principled and multilingual GEC evaluation by aggregating evidence across valid references, incorporating linguistic diversity without penalizing legitimate variation.

Abstract: Evaluating grammatical error correction requires metrics that reflect the
diversity of valid human corrections rather than privileging a single
reference. Existing frameworks, largely edit-based and English-centric, rely on
rigid alignments between system and reference edits, limiting their
applicability in multilingual and generative settings. This paper introduces a
formal framework for \textit{fluency-based multi-reference evaluation}, framing
$n$-gram similarity as an aggregation problem over multiple legitimate
corrections. Within this formulation, we instantiate GLEU through four
aggregation strategies--\textsc{select-best}, \textsc{simple-average},
\textsc{weighted-average}, and \textsc{merged-counts}--and analyze their
properties of boundedness, monotonicity, and sensitivity to reference
variation. Empirical results on Czech, Estonian, Ukrainian, and Chinese corpora
show that these strategies capture complementary aspects of fluency and
coverage. The framework unifies multi-reference evaluation into a principled,
fluency-oriented approach that incorporates linguistic diversity without
penalizing legitimate variation.

</details>


### [135] [Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs](https://arxiv.org/abs/2510.06750)
*Jaeseong Lee,Dayoung Kwon,seung-won hwang*

Main category: cs.CL

TL;DR: Training-free, low‑rank projection at inference time dials a single LRM’s reasoning strength to curb overthinking and cut compute, avoiding dual‑model routing while preserving performance.


<details>
  <summary>Details</summary>
Motivation: LRMs can overthink—taking excessive reasoning steps that hurt accuracy and waste computation. A common fix routes inputs between an LLM and an LRM, but maintaining multiple models is costly and impractical.

Method: Use a superposed, single‑model deployment with a lightweight, training‑free regulator. At inference, selectively “unlearn” portions of the LRM by applying optimal low‑rank projections determined via cumulative singular value energy analysis, effectively turning reasoning capacity up or down without routing or retraining.

Result: Reduces computation while maintaining necessary reasoning quality; mitigates overthinking by adjusting the model’s reasoning intensity on the fly; removes the need for multi‑model routing. (Abstract does not report numbers.)

Conclusion: A simple, training‑free inference control mechanism can right‑size reasoning in LRMs—balancing accuracy and efficiency—by projecting onto low‑rank subspaces identified through singular value energy, offering a practical alternative to multi‑model deployments.

Abstract: Large Reasoning Models (LRMs) excel in structured tasks by emulating
deliberate human reasoning but often suffer from overthinking, degrading
performance and wasting resources. One possible baseline is to deploy both LLM
and LRM, then route input by predicting whether it requires reasoning and may
cause overthinking. However, deploying multiple models can be costly or
impractical. We propose a superposed deployment strategy with a lightweight,
training-free regulation to optimize inference by switching one model on and
off. Instead of routing, we selectively unlearn from LRM at inference, scaling
down computation while preserving reasoning. By analyzing the cumulative energy
of singular values, we identify optimal low-rank projections to adjust
reasoning just right.

</details>


### [136] [Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition](https://arxiv.org/abs/2510.06774)
*Lei Xu,Pierre Beckmann,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: They propose an adaptive neuro‑symbolic framework that predicts which formal reasoning strategy a problem needs from natural language and dynamically routes it to specialized logical solvers via autoformalization, yielding large gains over strong LLM baselines and also improving pure LLM prompting strategies.


<details>
  <summary>Details</summary>
Motivation: Most neuro‑symbolic systems hard‑wire a single solver or reasoning style at design time, limiting flexibility across heterogeneous tasks. The field needs a way to automatically recognize the right formal paradigm and invoke the appropriate solver on demand.

Method: Build a multi‑paradigm inference pipeline that (1) uses an LLM to classify the required formal reasoning strategy from NL problem statements, (2) autoformalizes problems into solver‑compatible representations, and (3) selects and executes specialized logical solvers dynamically; evaluated on single‑ and multi‑paradigm benchmarks.

Result: The LLM predicts strategies with >90% accuracy; the framework beats baselines by 27% over GPT‑4o and 6% over DeepSeek‑V3.1; adaptive routing also boosts pure LLM settings by +10% (zero‑shot), +5% (CoT), and +6% (symbolic CoT) with GPT‑4o; smaller models underperform but improve after post‑training.

Conclusion: Adaptive selection and integration of symbolic solvers guided by LLMs is effective and practical, laying groundwork for unifying natural‑language (material) and formal inference across diverse reasoning tasks, with room to strengthen smaller models via post‑training and broaden solver coverage.

Abstract: Neuro-symbolic NLP methods aim to leverage the complementary strengths of
large language models and formal logical solvers. However, current approaches
are mostly static in nature, i.e., the integration of a target solver is
predetermined at design time, hindering the ability to employ diverse formal
inference strategies. To address this, we introduce an adaptive,
multi-paradigm, neuro-symbolic inference framework that: (1) automatically
identifies formal reasoning strategies from problems expressed in natural
language; and (2) dynamically selects and applies specialized formal logical
solvers via autoformalization interfaces. Extensive experiments on individual
and multi-paradigm reasoning tasks support the following conclusions: LLMs are
effective at predicting the necessary formal reasoning strategies with an
accuracy above 90 percent. This enables flexible integration with formal
logical solvers, resulting in our framework outperforming competing baselines
by 27 percent and 6 percent compared to GPT-4o and DeepSeek-V3.1, respectively.
Moreover, adaptive reasoning can even positively impact pure LLM methods,
yielding gains of 10, 5, and 6 percent on zero-shot, CoT, and symbolic CoT
settings with GPT-4o. Finally, although smaller models struggle with adaptive
neuro-symbolic reasoning, post-training offers a viable path to improvement.
Overall, this work establishes the foundations for adaptive LLM-symbolic
reasoning, offering a path forward for unifying material and formal inferences
on heterogeneous reasoning challenges.

</details>


### [137] [Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness](https://arxiv.org/abs/2510.06780)
*Luca Giordano,Simon Razniewski*

Main category: cs.CL

TL;DR: They systematically evaluate turning LLM knowledge into structured mini knowledge bases via recursive extraction, testing termination, reproducibility, and robustness across metrics, domains, and perturbations; extraction usually terminates, reproducibility is mixed, and robustness is strong to seeds/temperature but weaker to language/model changes.


<details>
  <summary>Details</summary>
Motivation: LLMs contain rich facts, but it’s unclear how to reliably materialize that knowledge into structured form. Key uncertainties are whether recursive extraction processes will finish, whether their outputs are repeatable, and how sensitive they are to changes such as seeds, language, randomness, or model choice.

Method: Construct domain-specific miniGPTKBs (small, tractable subcrawls) and evaluate knowledge materialization with metrics on yield, lexical similarity, and semantic similarity. Probe robustness via four perturbations (seed, language, randomness/temperature, and model) across three domains (history, entertainment, finance). Analyze termination behavior, reproducibility across runs, and robustness to perturbations.

Result: High termination rates overall but dependent on the LLM; reproducibility varies across settings; robustness is relatively high for seed and temperature variations but lower when changing language or model.

Conclusion: Recursive LLM knowledge materialization can reliably surface core, stable facts, but it has notable weaknesses in reproducibility and cross-language/model robustness, implying the need for careful methodology, model selection, and possibly standardization or control mechanisms.

Abstract: Large Language Models (LLMs) encode substantial factual knowledge, yet
measuring and systematizing this knowledge remains challenging. Converting it
into structured format, for example through recursive extraction approaches
such as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key
open questions include whether such extraction can terminate, whether its
outputs are reproducible, and how robust they are to variations. We
systematically study LLM knowledge materialization using miniGPTKBs
(domain-specific, tractable subcrawls), analyzing termination, reproducibility,
and robustness across three categories of metrics: yield, lexical similarity,
and semantic similarity. We experiment with four variations (seed, language,
randomness, model) and three illustrative domains (from history, entertainment,
and finance). Our findings show (i) high termination rates, though
model-dependent; (ii) mixed reproducibility; and (iii) robustness that varies
by perturbation type: high for seeds and temperature, lower for languages and
models. These results suggest that LLM knowledge materialization can reliably
surface core knowledge, while also revealing important limitations.

</details>


### [138] [FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline](https://arxiv.org/abs/2510.06800)
*Haotian Wu,Shufan Jiang,Chios Chen,Yiyang Feng,Hehai Lin,Heqing Zou,Yao Shu,Yanran Li,Chengwei Qin*

Main category: cs.CL

TL;DR: Proposes FURINA-Builder, a multi-agent LLM pipeline that auto-generates scalable, customizable role‑playing (RP) benchmarks (FURINA-Bench). It flexibly evaluates arbitrary characters across scenarios and prompts, and reveals a performance–reliability trade-off: stronger reasoning boosts RP scores but increases hallucinations; model scale doesn’t consistently reduce them.


<details>
  <summary>Details</summary>
Motivation: Existing RP benchmarks are narrow, outdated in interaction styles, and hard to adapt across scenarios, characters, and prompt formats. Rapid LLM progress renders fixed benchmarks obsolete, motivating an automated, scalable, and customizable benchmark builder for robust RP assessment.

Method: A multi-agent collaboration pipeline: (1) sample a test character and partner characters/scenes from a curated character–scene pool; (2) simulate dialogues; (3) an LLM judge selects fine-grained evaluation dimensions and edits/normalizes the test character’s replies into final test utterances; (4) assemble tasks with dimension-specific criteria. Using this, they construct FURINA-Bench with both established and synthesized characters, validated via human evaluation and separability checks.

Result: On extensive evaluations, o3 leads in English RP and DeepSeek-R1 in Chinese. Established characters consistently outperform synthesized ones, with the gap widening for reasoning models. Model scale does not monotonically reduce hallucinations. Reasoning LLMs show a novel trade-off: improved RP scores but higher RP hallucinations, reflecting a Pareto frontier between RP performance and reliability.

Conclusion: FURINA-Builder enables adaptive, large-scale RP benchmarking, and FURINA-Bench poses a challenging, diverse testbed. Findings highlight a fundamental performance–reliability trade-off in RP, guiding future work toward balancing reasoning strength with hallucination control and more robust evaluation practices.

Abstract: As large language models (LLMs) advance in role-playing (RP) tasks, existing
benchmarks quickly become obsolete due to their narrow scope, outdated
interaction paradigms, and limited adaptability across diverse application
scenarios. To address this gap, we introduce FURINA-Builder, a novel
multi-agent collaboration pipeline that automatically constructs fully
customizable RP benchmarks at any scale. It enables evaluation of arbitrary
characters across diverse scenarios and prompt formats, as the first benchmark
builder in RP area for adaptable assessment. FURINA-Builder simulates dialogues
between a test character and other characters drawn from a well-constructed
character-scene pool, while an LLM judge selects fine-grained evaluation
dimensions and adjusts the test character's responses into final test
utterances. Using this pipeline, we build FURINA-Bench, a new comprehensive
role-playing benchmark featuring both established and synthesized test
characters, each assessed with dimension-specific evaluation criteria. Human
evaluation and preliminary separability analysis justify our pipeline and
benchmark design. We conduct extensive evaluations of cutting-edge LLMs and
find that o3 and DeepSeek-R1 achieve the best performance on English and
Chinese RP tasks, respectively. Across all models, established characters
consistently outperform synthesized ones, with reasoning capabilities further
amplifying this disparity. Interestingly, we observe that model scale does not
monotonically reduce hallucinations. More critically, for reasoning LLMs, we
uncover a novel trade-off: reasoning improves RP performance but simultaneously
increases RP hallucinations. This trade-off extends to a broader Pareto
frontier between RP performance and reliability for all LLMs. These findings
demonstrate the effectiveness of FURINA-Builder and the challenge posed by
FURINA-Bench.

</details>


### [139] [Overview of the Plagiarism Detection Task at PAN 2025](https://arxiv.org/abs/2510.06805)
*André Greiner-Petter,Maik Fröbe,Jan Philip Wahle,Terry Ruas,Bela Gipp,Akiko Aizawa,Martin Potthast*

Main category: cs.CL

TL;DR: Overview of PAN 2025’s generative plagiarism detection task: introduces a large LLM-generated plagiarism dataset (Llama, DeepSeek-R1, Mistral), compares participant systems and baselines. Simple embedding-based semantic similarity achieves up to 0.8 recall/0.5 precision on the new dataset but fails badly on PAN 2015, exposing weak generalization.


<details>
  <summary>Details</summary>
Motivation: Address the growing need to detect AI-generated plagiarism in scientific texts and align plagiarized passages to sources; provide a large, standardized benchmark and assess robustness and diversity of current approaches.

Method: Construct a large-scale dataset of automatically generated plagiarism using three LLMs; run a shared task with participant systems and four baselines (including embedding-based similarity); evaluate systems both on the new dataset and on the prior PAN 2015 dataset to gauge robustness and generalization.

Result: On the new dataset, naive semantic similarity baselines using embeddings obtain promising performance (up to 0.8 recall and 0.5 precision), suggesting the task setup favors such methods. However, these approaches perform significantly worse on the PAN 2015 dataset, indicating poor generalizability across distributions and plagiarism types.

Conclusion: The current task configuration encourages narrow solution strategies and reveals a generalization gap. Future work should diversify data and task design, or develop more robust methods, to ensure transferability beyond the newly constructed dataset.

Abstract: The generative plagiarism detection task at PAN 2025 aims at identifying
automatically generated textual plagiarism in scientific articles and aligning
them with their respective sources. We created a novel large-scale dataset of
automatically generated plagiarism using three large language models: Llama,
DeepSeek-R1, and Mistral. In this task overview paper, we outline the creation
of this dataset, summarize and compare the results of all participants and four
baselines, and evaluate the results on the last plagiarism detection task from
PAN 2015 in order to interpret the robustness of the proposed approaches. We
found that the current iteration does not invite a large variety of approaches
as naive semantic similarity approaches based on embedding vectors provide
promising results of up to 0.8 recall and 0.5 precision. In contrast, most of
these approaches underperform significantly on the 2015 dataset, indicating a
lack in generalizability.

</details>


### [140] [BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods](https://arxiv.org/abs/2510.06811)
*Philipp Mondorf,Mingyang Wang,Sebastian Gerstner,Ahmad Dawar Hakimi,Yihong Liu,Leonor Veloso,Shijia Zhou,Hinrich Schütze,Barbara Plank*

Main category: cs.CL

TL;DR: Ensembling circuit-localization methods—via parallel score aggregation and a sequential warm-start-plus-pruning scheme—improves precision on the MIB Circuit Localization track; a meta-parallel ensemble that also includes the sequential variant performs best across model–task settings in the BlackboxNLP 2025 shared task.


<details>
  <summary>Details</summary>
Motivation: Single circuit-localization methods can be noisy, biased, or suboptimal for different tasks; combining complementary attribution signals may yield more reliable and precise identification of the edges/subnetworks responsible for specific behaviors in LLMs.

Method: Two ensemble strategies: (1) Parallel ensembling aggregates per-edge attribution scores from multiple methods (e.g., mean/min/max). (2) Sequential ensembling uses EAP-IG edge attributions as a warm start for a more expensive but more precise circuit identification procedure (edge pruning). They also form a higher-level parallel ensemble that includes the sequential ensemble alongside other methods. Evaluation is on MIB benchmark metrics across multiple LLM–task combinations, compared with official baselines.

Result: Both parallel and sequential ensembles produce notable gains on benchmark metrics, yielding more precise circuit identification than individual methods and baselines. The best overall performance comes from a parallel ensemble that pools multiple methods, including the sequential ensemble’s output.

Conclusion: Ensembling is an effective strategy for circuit localization in LLMs. Aggregating heterogeneous attribution signals and seeding pruning with inexpensive attributions improves accuracy and robustness over single-method approaches, with a meta-parallel ensemble delivering the top results on the shared task.

Abstract: The Circuit Localization track of the Mechanistic Interpretability Benchmark
(MIB) evaluates methods for localizing circuits within large language models
(LLMs), i.e., subnetworks responsible for specific task behaviors. In this
work, we investigate whether ensembling two or more circuit localization
methods can improve performance. We explore two variants: parallel and
sequential ensembling. In parallel ensembling, we combine attribution scores
assigned to each edge by different methods-e.g., by averaging or taking the
minimum or maximum value. In the sequential ensemble, we use edge attribution
scores obtained via EAP-IG as a warm start for a more expensive but more
precise circuit identification method, namely edge pruning. We observe that
both approaches yield notable gains on the benchmark metrics, leading to a more
precise circuit identification approach. Finally, we find that taking a
parallel ensemble over various methods, including the sequential ensemble,
achieves the best results. We evaluate our approach in the BlackboxNLP 2025 MIB
Shared Task, comparing ensemble scores to official baselines across multiple
model-task combinations.

</details>


### [141] [Adaptive Tool Generation with Models as Tools and Reinforcement Learning](https://arxiv.org/abs/2510.06825)
*Chenpeng Wang,Xiaojie Cheng,Chunye Wang,Linfeng Yang,Lei Zhang*

Main category: cs.CL

TL;DR: MTR is a simulation-first framework that trains tool-augmented LMs using schema-validated, simulated ReAct traces and a two-stage SFT+GRPO process, matching live-API systems on multi-hop QA while improving reasoning consistency—without needing live tool access.


<details>
  <summary>Details</summary>
Motivation: Live API reliance in tool-augmented models hampers scalability, reliability, and cost during training/deployment. The authors aim to teach robust tool reasoning without depending on live interactions.

Method: A multi-agent pipeline: (1) ToolMaker auto-generates task-specific, OpenAI-compatible tool interfaces; (2) AutoAgent produces structured think–act–observe (ReAct) traces; (3) ToolActor simulates realistic tool responses with schema validation. Training has two stages: Stage-1 SFT learns the ‘trace grammar’ from complete reasoning sequences; Stage-2 GRPO optimizes strategies using a composite trace reward that balances final answer correctness with internal consistency across the trace.

Result: On HotpotQA, MuSiQue, 2WikiMultiHopQA, and Bamboogle, MTR achieves competitive Exact Match scores relative to systems using live APIs and performs especially well on reasoning-intensive tasks.

Conclusion: Structured, simulation-based training can effectively instill tool-use reasoning, reducing dependence on live APIs. Combining SFT of trace structure with GRPO on a composite reward yields competitive performance and better internal consistency, improving scalability and reliability.

Abstract: Tool-augmented language models have demonstrated strong capabilities, but
their reliance on live API access creates scalability and reliability
challenges during training and deployment. We propose MTR, a simulation-first
training framework for tool-augmented reasoning. Instead of relying on live
APIs, MTR learns from complete ReAct traces with schema-validated, simulated
observations. Our approach operates through a multi-agent architecture where a
ToolMaker generates task-specific, OpenAI-compatible tool interfaces, an
AutoAgent produces structured think-act-observe sequences, and a ToolActor
simulates realistic responses. Training proceeds in two stages: Stage-1
Supervised Fine-Tuning (SFT) teaches 'trace grammar' from complete reasoning
sequences; Stage-2 Group Relative Policy Optimization (GRPO) optimizes strategy
with a composite trace reward that balances answer correctness and internal
consistency. Across four multi-hop QA benchmarks (HotpotQA, MuSiQue,
2WikiMultiHopQA, Bamboogle), MTR attains competitive Exact Match (EM) scores to
live-API systems and excels on reasoning-intensive tasks, suggesting that
effective tool reasoning can be learned from structured traces without live
interactions.

</details>


### [142] [Mid-Training of Large Language Models: A Survey](https://arxiv.org/abs/2510.06826)
*Kaixiang Mo,Yuxin Shi,Weiwei Weng,Zhiqiang Zhou,Shuman Liu,Haibo Zhang,Anxiang Zeng*

Main category: cs.CL

TL;DR: Proposes and systematizes an intermediate “mid-training” stage for LLMs—with annealed phases for data curation, optimization schedule tuning, and context-length extension—arguing it improves stability, generalization, and late-stage capability; offers the first taxonomy, benchmarks, and practical guidance, plus open problems.


<details>
  <summary>Details</summary>
Motivation: Standard pre-train → fine-tune pipelines face diminishing returns from noisy tokens, unstable late-stage optimization, and limited context capabilities. Although many SOTA systems quietly use mid-training practices, there is no unified framework or survey to justify, compare, or evaluate them.

Method: A survey/synthesis: defines ‘mid-training’ as multiple annealing-style phases; explains why it helps via gradient noise scale, information bottleneck, and curriculum learning; introduces a taxonomy along three axes (data distribution refinement, learning-rate/optimization schedules, long-context extension); compiles evaluation benchmarks; distills practical heuristics; reports empirical gains to facilitate structured comparisons.

Result: Delivers the first unified taxonomy of LLM mid-training; curates benchmarks and reports performance gains across settings; provides interpretive lenses that rationalize improved convergence stability, generalization, and abstraction in late training; enables apples-to-apples comparisons across models and recipes.

Conclusion: Mid-training is a principled, practically effective stage that mitigates noisy-token inefficiency, stabilizes training, and unlocks long-context capabilities. The paper formalizes the paradigm, offers tools to adopt and evaluate it, and highlights open challenges as targets for future research and best practices.

Abstract: Large language models (LLMs) are typically developed through large-scale
pre-training followed by task-specific fine-tuning. Recent advances highlight
the importance of an intermediate mid-training stage, where models undergo
multiple annealing-style phases that refine data quality, adapt optimization
schedules, and extend context length. This stage mitigates diminishing returns
from noisy tokens, stabilizes convergence, and expands model capability in late
training. Its effectiveness can be explained through gradient noise scale, the
information bottleneck, and curriculum learning, which together promote
generalization and abstraction. Despite widespread use in state-of-the-art
systems, there has been no prior survey of mid-training as a unified paradigm.
We introduce the first taxonomy of LLM mid-training spanning data distribution,
learning-rate scheduling, and long-context extension. We distill practical
insights, compile evaluation benchmarks, and report gains to enable structured
comparisons across models. We also identify open challenges and propose avenues
for future research and practice.

</details>


### [143] [GAMBIT+: A Challenge Set for Evaluating Gender Bias in Machine Translation Quality Estimation Metrics](https://arxiv.org/abs/2510.06841)
*Giorgos Filandrianos,Orfeas Menis Mastromichalakis,Wafaa Mohammed,Giuseppe Attanasio,Chrysoula Zerva*

Main category: cs.CL

TL;DR: They introduce a large, fully parallel multilingual challenge set to test whether MT quality estimation metrics score masculine and feminine translations of gender-ambiguous occupations equally, enabling precise cross-language and by-occupation bias analysis.


<details>
  <summary>Details</summary>
Motivation: Bias in MT is well known, but bias in automatic quality estimation (QE) metrics is underexplored and prior studies are constrained by small, narrow, and limited-language datasets. A broader, controlled resource is needed to diagnose QE gender bias reliably.

Method: Extend the GAMBIT corpus to create paired translations: for each English source with gender-ambiguous occupations (also extended to 3 genderless/natural-gendered source languages), produce two target translations in 11 grammatically gendered languages that differ only in the occupational term’s grammatical gender (masculine vs. feminine), with all agreement updated. This yields 33 source–target pairs in a fully parallel design, allowing direct score comparisons for each pair.

Result: A large-scale, multilingual, occupation-rich, fully parallel challenge set where each instance has matched masculine/feminine target versions, enabling straightforward detection of QE bias via score parity checks across 33 language pairs.

Conclusion: This dataset provides a rigorous, scalable benchmark to assess and compare gender bias in QE metrics across languages and occupations; an unbiased QE metric should assign near-equal scores to the paired masculine/feminine translations.

Abstract: Gender bias in machine translation (MT) systems has been extensively
documented, but bias in automatic quality estimation (QE) metrics remains
comparatively underexplored. Existing studies suggest that QE metrics can also
exhibit gender bias, yet most analyses are limited by small datasets, narrow
occupational coverage, and restricted language variety. To address this gap, we
introduce a large-scale challenge set specifically designed to probe the
behavior of QE metrics when evaluating translations containing gender-ambiguous
occupational terms. Building on the GAMBIT corpus of English texts with
gender-ambiguous occupations, we extend coverage to three source languages that
are genderless or natural-gendered, and eleven target languages with
grammatical gender, resulting in 33 source-target language pairs. Each source
text is paired with two target versions differing only in the grammatical
gender of the occupational term(s) (masculine vs. feminine), with all dependent
grammatical elements adjusted accordingly. An unbiased QE metric should assign
equal or near-equal scores to both versions. The dataset's scale, breadth, and
fully parallel design, where the same set of texts is aligned across all
languages, enables fine-grained bias analysis by occupation and systematic
comparisons across languages.

</details>


### [144] [SID: Multi-LLM Debate Driven by Self Signals](https://arxiv.org/abs/2510.06843)
*Xuhang Chen,Zhifan Song,Deyi Ji,Shuo Gao,Lanyun Zhu*

Main category: cs.CL

TL;DR: SID is a self-signals driven multi-LLM debate framework that uses model confidence and attention-based semantic focus to adaptively control debates, enabling early agent exit and content compression, which improves accuracy while reducing token usage across benchmarks and models.


<details>
  <summary>Details</summary>
Motivation: Existing multi-LLM agent debate methods mostly rely on external scaffolds (e.g., debate graphs, LLM-as-a-judge) and ignore internal generation signals (token logits, attention), causing redundant computation and possible performance drops. The authors aim to exploit these underused self signals to make debates more effective and efficient.

Method: Introduce SID, which leverages two self signals: (1) model-level confidence to allow high-confidence agents to exit early, and (2) token-level semantic focus (via attention) to compress or filter redundant debate content and guide interactions. SID is applied across both LLMs and multimodal LLMs and adapts the debate process dynamically.

Result: Across multiple challenging benchmarks and with various (multi)modal LLMs, SID achieves higher accuracy than prior MAD approaches while consuming fewer tokens, indicating improved performance and efficiency. (Abstract does not provide exact numbers.)

Conclusion: Self signals such as confidence and attention can effectively steer multi-agent debates, reducing redundancy and improving outcomes. SID demonstrates a practical, efficient alternative to external-structure-heavy MAD frameworks, yielding better accuracy with lower token cost.

Abstract: Large Language Models (LLMs) have exhibited impressive capabilities across
diverse application domains. Recent work has explored Multi-LLM Agent Debate
(MAD) as a way to enhance performance by enabling multiple LLMs to discuss and
refine responses iteratively. Nevertheless, existing MAD methods predominantly
focus on utilizing external structures, such as debate graphs, using
LLM-as-a-Judge, while neglecting the application of self signals, such as token
logits and attention, that arise during generation. This omission leads to
redundant computation and potential performance degradation. In this paper, we
shift the focus to the self signals of multi-LLM debate and introduce a
Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of
self-signals: model-level confidence and token-level semantic focus, to
adaptively guide the debate process. Our approach enables high-confidence
agents to exit early at the model level and compress the redundant debate
contents based on the attention mechanism. We evaluate our method on various
LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental
results demonstrate that our method not only outperforms existing MAD
techniques in accuracy but also reduces token consumption, highlighting the
effectiveness of utilizing self signals in enhancing both the performance and
efficiency of multi-agent debate systems. Our code will be available
at~\href{https://github.com/xuhang2019/SID}{\texttt{https://github.com/xuhang2019/SID}}.

</details>


### [145] [OpenJAI-v1.0: An Open Thai Large Language Model](https://arxiv.org/abs/2510.06847)
*Pontakorn Trakuekul,Attapol T. Rutherford,Jullajak Karnjanaekarin,Narongkorn Panitsrisit,Sumana Sumanakul*

Main category: cs.CL

TL;DR: OpenJAI-v1.0 is a bilingual (Thai/English) open-source LLM fine-tuned from Qwen3-14B, optimized for instruction following, long-context understanding, and tool use; it outperforms its base model and other Thai open models on diverse benchmarks without catastrophic forgetting, and is publicly released for the Thai AI community.


<details>
  <summary>Details</summary>
Motivation: Address the lack of strong, practical Thai-capable open LLMs and improve real-world utility (instruction following, extended context handling, and tool integration) while maintaining prior knowledge.

Method: Curate and use targeted datasets for three key use cases—instruction following, long-context understanding, and tool use—to further train Qwen3-14B; evaluate on diverse benchmarks to measure gains and check for catastrophic forgetting.

Result: The model improves over Qwen3-14B and surpasses other leading open-source Thai models across multiple benchmarks, with no observed catastrophic forgetting.

Conclusion: Targeted data curation for practical capabilities yields a stronger Thai/English LLM. OpenJAI-v1.0 is released as a robust, community-accessible resource that advances Thai NLP without sacrificing existing knowledge.

Abstract: We introduce OpenJAI-v1.0, an open-source large language model for Thai and
English, developed from the Qwen3-14B model. Our work focuses on boosting
performance on practical tasks through carefully curated data across three key
use cases: instruction following, long-context understanding, and tool use.
Evaluation results show that OpenJAI-v1.0 improves on the capabilities of its
base model and outperforms other leading open-source Thai models on a diverse
suite of benchmarks, while avoiding catastrophic forgetting. OpenJAI-v1.0 is
publicly released as another alternative NLP resource for the Thai AI
community.

</details>


### [146] [Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding](https://arxiv.org/abs/2510.06866)
*Wafaa Mohammed,Vlad Niculae,Chrysoula Zerva*

Main category: cs.CL

TL;DR: LLMs already encode discourse knowledge for MT, but standard decoding underutilizes it; a new quality‑aware decoding (QAD) better surfaces this knowledge, improving document‑level coherence and human‑rated quality.


<details>
  <summary>Details</summary>
Motivation: Despite strong sentence‑level MT performance, LLMs still mishandle document‑level discourse phenomena (e.g., pronoun resolution, lexical cohesion). The goal is to leverage latent discourse knowledge without retraining models.

Method: Evaluate LLMs on context‑aware translation focused on discourse phenomena. Propose quality‑aware decoding (QAD) that selects outputs using quality signals to favor discourse‑coherent, semantically rich candidates. Compare QAD to common decoding strategies (greedy, beam, sampling) and analyze outcomes.

Result: QAD outperforms other decoding approaches on discourse‑sensitive metrics, produces semantically richer translations, and aligns more closely with human preferences.

Conclusion: LLMs do encode discourse knowledge, but decoding strategy determines whether it is expressed. QAD provides an effective, training‑free way to enhance document‑level MT quality.

Abstract: Large language models (LLMs) have emerged as strong contenders in machine
translation.Yet, they still struggle to adequately handle discourse phenomena,
such as pronoun resolution and lexical cohesion at the document level. In this
study, we thoroughly investigate the discourse phenomena performance of LLMs in
context-aware translation. We demonstrate that discourse knowledge is encoded
within LLMs and propose the use of quality-aware decoding (QAD) to effectively
extract this knowledge, showcasing its superiority over other decoding
approaches through comprehensive analysis. Furthermore, we illustrate that QAD
enhances the semantic richness of translations and aligns them more closely
with human preferences.

</details>


### [147] [$λ$-GRPO: Unifying the GRPO Frameworks with Learnable Token Preferences](https://arxiv.org/abs/2510.06870)
*Yining Wang,Jinman Zhao,Chuangxin Zhao,Shuhao Guan,Gerald Penn,Shinan Liu*

Main category: cs.CL

TL;DR: They propose λ-GRPO, a GRPO variant that learns token-level weighting to fix length bias in rule-verified RL, delivering consistent accuracy gains on math reasoning without extra compute or data changes.


<details>
  <summary>Details</summary>
Motivation: GRPO in RLVR assigns the same advantage to all tokens, causing longer responses to dominate gradients (length bias). Existing fixes (e.g., DAPO, Dr.GRPO) adjust aggregation heuristically and obscure the implied token preferences. The authors want a principled, interpretable way to set token importance.

Method: Unify RLVR/GRPO-style objectives under a single formulation and introduce a learnable parameter λ that adaptively controls token-level weighting (token preference) during optimization, letting the model learn how to distribute reward across tokens. No changes to data or compute budget.

Result: On Qwen2.5 1.5B/3B/7B, λ-GRPO improves average accuracy over GRPO by +1.9%, +1.0%, and +1.7%, and outperforms DAPO across multiple math reasoning benchmarks.

Conclusion: Learning token preferences via λ mitigates GRPO’s length bias and yields consistent, practical gains without added cost, offering a more interpretable and effective alternative to heuristic token-weighting schemes.

Abstract: Reinforcement Learning with Human Feedback (RLHF) has been the dominant
approach for improving the reasoning capabilities of Large Language Models
(LLMs). Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has
simplified this paradigm by replacing the reward and value models with
rule-based verifiers. A prominent example is Group Relative Policy Optimization
(GRPO). However, GRPO inherently suffers from a length bias, since the same
advantage is uniformly assigned to all tokens of a response. As a result,
longer responses distribute the reward over more tokens and thus contribute
disproportionately to gradient updates. Several variants, such as DAPO and Dr.
GRPO, modify the token-level aggregation of the loss, yet these methods remain
heuristic and offer limited interpretability regarding their implicit token
preferences. In this work, we explore the possibility of allowing the model to
learn its own token preference during optimization. We unify existing
frameworks under a single formulation and introduce a learnable parameter
$\lambda$ that adaptively controls token-level weighting. We use $\lambda$-GRPO
to denote our method, and we find that $\lambda$-GRPO achieves consistent
improvements over vanilla GRPO and DAPO on multiple mathematical reasoning
benchmarks. On Qwen2.5 models with 1.5B, 3B, and 7B parameters, $\lambda$-GRPO
improves average accuracy by $+1.9\%$, $+1.0\%$, and $+1.7\%$ compared to GRPO,
respectively. Importantly, these gains come without any modifications to the
training data or additional computational cost, highlighting the effectiveness
and practicality of learning token preferences.

</details>


### [148] [MeXtract: Light-Weight Metadata Extraction from Scientific Papers](https://arxiv.org/abs/2510.06889)
*Zaid Alyafeai,Maged S. Al-Shaibani,Bernard Ghanem*

Main category: cs.CL

TL;DR: MeXtract introduces small fine-tuned LLMs (0.5B–3B) for scientific-paper metadata extraction, achieving state-of-the-art results on MOLE, extending the benchmark with an out-of-domain subset, and showing strong transfer to unseen schemas.


<details>
  <summary>Details</summary>
Motivation: Accurate, scalable metadata extraction is essential for indexing and analyzing scientific literature, but rule-based or task-specific systems fail to generalize across domains and schema variations.

Method: Fine-tune a family of Qwen 2.5 models (0.5B–3B parameters) for structured metadata extraction; evaluate on the MOLE benchmark and an extended, harder out-of-domain subset featuring model-specific metadata; test schema transfer by training on one schema and evaluating on unseen schemas.

Result: Within their size class, MeXtract models achieve SOTA performance on MOLE and maintain high accuracy on the new OOD subset; fine-tuning on a given schema transfers effectively to unseen schemas, indicating robustness and adaptability.

Conclusion: Lightweight, fine-tuned LLMs can perform high-quality metadata extraction and generalize across schemas. The released code, datasets, and models support reproducibility and further research.

Abstract: Metadata plays a critical role in indexing, documenting, and analyzing
scientific literature, yet extracting it accurately and efficiently remains a
challenging task. Traditional approaches often rely on rule-based or
task-specific models, which struggle to generalize across domains and schema
variations. In this paper, we present MeXtract, a family of lightweight
language models designed for metadata extraction from scientific papers. The
models, ranging from 0.5B to 3B parameters, are built by fine-tuning Qwen 2.5
counterparts. In their size family, MeXtract achieves state-of-the-art
performance on metadata extraction on the MOLE benchmark. To further support
evaluation, we extend the MOLE benchmark to incorporate model-specific
metadata, providing an out-of-domain challenging subset. Our experiments show
that fine-tuning on a given schema not only yields high accuracy but also
transfers effectively to unseen schemas, demonstrating the robustness and
adaptability of our approach. We release all the code, datasets, and models
openly for the research community.

</details>


### [149] [LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling](https://arxiv.org/abs/2510.06915)
*Zecheng Tang,Baibei Ji,Quantong Qiu,Haitian Wang,Xiaobo Liang,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: They introduce Long-RewardBench to test whether reward models can judge responses with long contexts, find that existing RMs fail at this, and propose a multi-stage training strategy to build LongRMs that significantly improve long-context consistency—an 8B model even beats 70B baselines and matches Gemini 2.5 Pro.


<details>
  <summary>Details</summary>
Motivation: Real-world LLM use (e.g., agents) involves long interaction histories where responses must be grounded in and consistent with extensive context. Existing reward models are tuned for short contexts and generic attributes (helpfulness/safety), neglecting long-context consistency, so there is no reliable way to evaluate or train for this setting.

Method: 1) Build Long-RewardBench, a benchmark for long-context RM evaluation with Pairwise Comparison and Best-of-N tasks to measure context-aware preference judgments. 2) Analyze failure modes of current (generative) RMs on long contexts. 3) Propose a general, multi-stage training pipeline to convert arbitrary models into long-context reward models (LongRMs). The stages are not fully detailed in the abstract but aim to improve context grounding and robustness while retaining short-context skills.

Result: Empirically, SOTA generative reward models are fragile with long contexts. The proposed training yields substantial gains on long-context evaluation while preserving short-context performance. Their 8B LongRM surpasses much larger 70B baselines and reaches parity with the proprietary Gemini 2.5 Pro.

Conclusion: Long-context consistency is a critical but overlooked axis for reward models. A dedicated benchmark plus a general multi-stage training approach can produce compact RMs that are robust to long contexts without sacrificing short-context quality, advancing alignment for agentic, real-world LLM deployments.

Abstract: Reward model (RM) plays a pivotal role in aligning large language model (LLM)
with human preferences. As real-world applications increasingly involve long
history trajectories, e.g., LLM agent, it becomes indispensable to evaluate
whether a model's responses are not only high-quality but also grounded in and
consistent with the provided context. Yet, current RMs remain confined to
short-context settings and primarily focus on response-level attributes (e.g.,
safety or helpfulness), while largely neglecting the critical dimension of long
context-response consistency. In this work, we introduce Long-RewardBench, a
benchmark specifically designed for long-context RM evaluation, featuring both
Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that
even state-of-the-art generative RMs exhibit significant fragility in
long-context scenarios, failing to maintain context-aware preference judgments.
Motivated by the analysis of failure patterns observed in model outputs, we
propose a general multi-stage training strategy that effectively scales
arbitrary models into robust Long-context RMs (LongRMs). Experiments show that
our approach not only substantially improves performance on long-context
evaluation but also preserves strong short-context capability. Notably, our 8B
LongRM outperforms much larger 70B-scale baselines and matches the performance
of the proprietary Gemini 2.5 Pro model.

</details>


### [150] [SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models](https://arxiv.org/abs/2510.06917)
*Cheng-Han Chiang,Xiaofei Wang,Linjie Li,Chung-Ching Lin,Kevin Lin,Shujie Liu,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: cs.CL

TL;DR: SHANKS is an inference framework for spoken language models that performs hidden chain‑of‑thought reasoning while the user is still speaking, enabling timely interruptions and proactive tool calls; it improves interruption accuracy by 37.1% and completes 56.9% of tool calls before a turn ends.


<details>
  <summary>Details</summary>
Motivation: Conventional LLM/SLM systems wait for the user to finish before thinking or acting, causing high latency and poor interactivity in speech‑to‑speech settings where real‑time feedback, corrections, and tool use are required.

Method: Stream the user’s speech in fixed‑length chunks; after each chunk, generate unspoken (non‑vocalized) reasoning conditioned on prior audio and prior reasoning. Use this evolving internal state to (a) decide whether to interrupt the speaker and (b) launch tool calls early to advance the task while the user continues speaking. Evaluate on step‑by‑step math explanation (interrupt on mistakes) and on tool‑augmented dialogue (proactive calls).

Result: Compared to a baseline that interrupts without intermediate reasoning, SHANKS achieves 37.1% higher interruption accuracy in math‑explanation scenarios and completes 56.9% of required tool calls before the user finishes speaking in tool‑augmented dialogues.

Conclusion: Think‑while‑listening enables lower‑latency, more interactive speech agents that reason continuously rather than only post‑turn. SHANKS shows this approach can yield earlier, more accurate interventions and actions in real‑time conversations.

Abstract: Current large language models (LLMs) and spoken language models (SLMs) begin
thinking and taking actions only after the user has finished their turn. This
prevents the model from interacting during the user's turn and can lead to high
response latency while it waits to think. Consequently, thinking after
receiving the full input is not suitable for speech-to-speech interaction,
where real-time, low-latency exchange is important. We address this by noting
that humans naturally "think while listening." In this paper, we propose
SHANKS, a general inference framework that enables SLMs to generate unspoken
chain-of-thought reasoning while listening to the user input. SHANKS streams
the input speech in fixed-duration chunks and, as soon as a chunk is received,
generates unspoken reasoning based on all previous speech and reasoning, while
the user continues speaking. SHANKS uses this unspoken reasoning to decide
whether to interrupt the user and to make tool calls to complete the task. We
demonstrate that SHANKS enhances real-time user-SLM interaction in two
scenarios: (1) when the user is presenting a step-by-step solution to a math
problem, SHANKS can listen, reason, and interrupt when the user makes a
mistake, achieving 37.1% higher interruption accuracy than a baseline that
interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can
complete 56.9% of the tool calls before the user finishes their turn. Overall,
SHANKS moves toward models that keep thinking throughout the conversation, not
only after a turn ends. Animated illustrations of Shanks can be found at
https://d223302.github.io/SHANKS/

</details>


### [151] [Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation](https://arxiv.org/abs/2510.06961)
*Vaibhav Srivastav,Steven Zheng,Eric Bezzam,Eustache Le Bihan,Nithin Koluguri,Piotr Żelasko,Somshubra Majumdar,Adel Moumen,Sanchit Gandhi*

Main category: cs.CL

TL;DR: Open ASR Leaderboard is a fully reproducible benchmark and interactive leaderboard that evaluates 60+ ASR systems on 11 datasets (including multilingual and long‑form), standardizes text normalization, and reports both WER and inverse RTFx to compare accuracy vs. efficiency; Conformer+LLM decoders lead in WER but are slower, CTC/TDT are much faster, and English‑fine‑tuned Whisper encoders trade multilingual coverage for accuracy.


<details>
  <summary>Details</summary>
Motivation: ASR evaluation is overly focused on short-form English and rarely measures efficiency, limiting fair comparisons and progress on multilingual and long-form use cases. A standardized, transparent benchmark that jointly measures accuracy and speed is needed.

Method: Construct a reproducible benchmark and interactive leaderboard covering 60+ open-source and proprietary systems across 11 datasets with dedicated multilingual and long-form tracks. Standardize text normalization and report Word Error Rate (WER) plus inverse real-time factor (RTFx). Open-source all code and dataset loaders for transparent, extensible evaluation.

Result: Conformer encoders paired with LLM decoders achieve the best average WER in English but are slower. CTC and TDT decoders show much better RTFx, making them preferable for long-form or offline transcription. Whisper-derived encoders fine-tuned for English improve accuracy but often reduce multilingual coverage.

Conclusion: The Open ASR Leaderboard enables fair, transparent comparisons of ASR systems across accuracy and efficiency, highlights trade-offs between model architectures and decoding strategies, and provides open tooling to support community benchmarking and future extensions.

Abstract: Despite rapid progress, ASR evaluation remains saturated with short-form
English, and efficiency is rarely reported. We present the Open ASR
Leaderboard, a fully reproducible benchmark and interactive leaderboard
comparing 60+ open-source and proprietary systems across 11 datasets, including
dedicated multilingual and long-form tracks. We standardize text normalization
and report both word error rate (WER) and inverse real-time factor (RTFx),
enabling fair accuracy-efficiency comparisons. For English transcription,
Conformer encoders paired with LLM decoders achieve the best average WER but
are slower, while CTC and TDT decoders deliver much better RTFx, making them
attractive for long-form and offline use. Whisper-derived encoders fine-tuned
for English improve accuracy but often trade off multilingual coverage. All
code and dataset loaders are open-sourced to support transparent, extensible
evaluation.

</details>


### [152] [EDUMATH: Generating Standards-aligned Educational Math Word Problems](https://arxiv.org/abs/2510.06965)
*Bryan R. Christ,Penelope Molitz,Jonathan Kropko,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: They build a teacher-annotated, standards-aligned dataset and evaluation pipeline for customized math word problems (MWPs), train/open-source models steered by this data, and show the generated MWPs are close to human-written, with students performing similarly but preferring the customized ones.


<details>
  <summary>Details</summary>
Motivation: MWPs tailored to students’ interests and ability can improve learning, but teachers lack time to author such content at scale. There is a need for reliable, standards-aligned generation and evaluation resources to let LLMs assist educators.

Method: Use a joint human expert–LLM judging framework to assess >11k MWPs created by various open/closed LLMs; create the first teacher-annotated dataset for standards-aligned MWP generation; train a 12B open model on this data; train a teacher-label–based text classifier to steer a 30B open LLM without further training; compare similarity to human-written MWPs; run a classroom study with grade school students comparing performance and preference on customized vs human MWPs.

Result: The 12B open model matches larger open models; the classifier enables a 30B open LLM to outperform closed baselines zero-shot; generated MWPs are more similar to human-written ones than prior models’ outputs; in a student study, performance on customized LLM MWPs is similar to human-written, but students prefer the customized problems.

Conclusion: Teacher-annotated data and a hybrid human–LLM evaluation enable open LLMs to generate standards-aligned, customized MWPs that are competitive with human-written problems and favored by students, suggesting practical value for classroom adoption.

Abstract: Math word problems (MWPs) are critical K-12 educational tools, and
customizing them to students' interests and ability levels can increase
learning outcomes. However, teachers struggle to find time to customize MWPs
for each student given large class sizes and increasing burnout. We propose
that LLMs can support math education by generating MWPs customized to student
interests and math education standards. To this end, we use a joint human
expert-LLM judge approach to evaluate over 11,000 MWPs generated by open and
closed LLMs and develop the first teacher-annotated dataset for
standards-aligned educational MWP generation. We show the value of our data by
using it to train a 12B open model that matches the performance of larger and
more capable open models. We also use our teacher-annotated data to train a
text classifier that enables a 30B open LLM to outperform existing closed
baselines without any training. Next, we show our models' MWPs are more similar
to human-written MWPs than those from existing models. We conclude by
conducting the first study of customized LLM-generated MWPs with grade school
students, finding they perform similarly on our models' MWPs relative to
human-written MWPs but consistently prefer our customized MWPs.

</details>


### [153] [Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social Groups](https://arxiv.org/abs/2510.06974)
*Geng Liu,Feng Li,Junjie Mu,Mengxiao Zhu,Francesco Pierri*

Main category: cs.CL

TL;DR: They evaluate ingroup (“we”) vs outgroup (“they”) framing across 10 Chinese LLMs and real chatbot logs, finding consistent ingroup-positive and outgroup-negative bias that is even stronger in natural interactions, and provide a Mandarin- and China-context-aware evaluation framework.


<details>
  <summary>Details</summary>
Motivation: LLMs are widely deployed and risk reflecting/amplifying social biases. Prior work focuses on English; it’s unclear whether similar social identity framing effects exist in Chinese and how they manifest in real user interactions. There is a need for a culturally and linguistically appropriate evaluation for Chinese LLMs and a test of cross-linguistic generalization.

Method: Use Mandarin-specific prompts to probe responses under ingroup (“we”) vs outgroup (“they”) framing across ten representative Chinese LLMs; expand evaluations to 240 China-salient social groups. Complement controlled prompts with an analysis of a corpus of real Chinese-language user–chatbot conversations to detect framing-linked sentiment/bias patterns.

Result: Across models, there are systematic ingroup-positive and outgroup-negative tendencies. These effects are not limited to synthetic prompts; they also appear in naturalistic dialogues, with indications that biases may be stronger in real interactions.

Conclusion: Social identity biases seen in English generalize to Chinese and can intensify in user-facing contexts. The paper contributes a language-aware evaluation framework for Chinese LLMs and underscores the need for mitigation strategies in deployment.

Abstract: Large language models (LLMs) are increasingly deployed in user-facing
applications, raising concerns about their potential to reflect and amplify
social biases. We investigate social identity framing in Chinese LLMs using
Mandarin-specific prompts across ten representative Chinese LLMs, evaluating
responses to ingroup ("We") and outgroup ("They") framings, and extending the
setting to 240 social groups salient in the Chinese context. To complement
controlled experiments, we further analyze Chinese-language conversations from
a corpus of real interactions between users and chatbots. Across models, we
observe systematic ingroup-positive and outgroup-negative tendencies, which are
not confined to synthetic prompts but also appear in naturalistic dialogue,
indicating that bias dynamics might strengthen in real interactions. Our study
provides a language-aware evaluation framework for Chinese LLMs, demonstrating
that social identity biases documented in English generalize
cross-linguistically and intensify in user-facing contexts.

</details>


### [154] [Towards Reliable Retrieval in RAG Systems for Large Legal Datasets](https://arxiv.org/abs/2510.06999)
*Markus Reuter,Tobias Lingenberg,Rūta Liepiņa,Francesca Lagioia,Marco Lippi,Giovanni Sartor,Andrea Passerini,Burcu Sayin*

Main category: cs.CL

TL;DR: They identify a key failure mode in legal RAG—retrievers pulling from the wrong document (Document-Level Retrieval Mismatch, DRM)—and mitigate it with Summary-Augmented Chunking (SAC), which prepends a synthetic document summary to each chunk. SAC reduces DRM and improves retrieval precision/recall; generic summaries work better than expert-crafted ones.


<details>
  <summary>Details</summary>
Motivation: RAG’s reliability hinges on retrieval quality. In legal corpora with many long, structurally similar documents, standard chunking loses global context, causing retrievers to select text from entirely incorrect documents (DRM), which propagates hallucinations in downstream LLM answers.

Method: 1) Define and quantify DRM as a retrieval error where the source document is wrong. 2) Propose Summary-Augmented Chunking (SAC): generate a document-level synthetic summary and attach it to every chunk to inject global context into chunk representations. 3) Evaluate across diverse legal IR tasks, comparing generic summarization versus domain-expert targeted summaries.

Result: SAC substantially lowers DRM rates and boosts text-level retrieval precision and recall. Unexpectedly, a generic summarization strategy outperforms expert-informed, legally targeted summaries.

Conclusion: A simple, scalable, and easily integrable augmentation—adding document-level summaries to chunks—meaningfully improves RAG reliability on large legal datasets by restoring global context during retrieval, without requiring domain-specific engineering.

Abstract: Retrieval-Augmented Generation (RAG) is a promising approach to mitigate
hallucinations in Large Language Models (LLMs) for legal applications, but its
reliability is critically dependent on the accuracy of the retrieval step. This
is particularly challenging in the legal domain, where large databases of
structurally similar documents often cause retrieval systems to fail. In this
paper, we address this challenge by first identifying and quantifying a
critical failure mode we term Document-Level Retrieval Mismatch (DRM), where
the retriever selects information from entirely incorrect source documents. To
mitigate DRM, we investigate a simple and computationally efficient technique
which we refer to as Summary-Augmented Chunking (SAC). This method enhances
each text chunk with a document-level synthetic summary, thereby injecting
crucial global context that would otherwise be lost during a standard chunking
process. Our experiments on a diverse set of legal information retrieval tasks
show that SAC greatly reduces DRM and, consequently, also improves text-level
retrieval precision and recall. Interestingly, we find that a generic
summarization strategy outperforms an approach that incorporates legal expert
domain knowledge to target specific legal elements. Our work provides evidence
that this practical, scalable, and easily integrable technique enhances the
reliability of RAG systems when applied to large-scale legal document datasets.

</details>


### [155] [Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages](https://arxiv.org/abs/2510.07000)
*Neel Prabhanjan Rachamalla,Aravind Konakalla,Gautam Rajeev,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.CL

TL;DR: They propose a human-in-the-loop pipeline that blends translation with synthetic expansion to build high-quality, culturally grounded post-training data for Indian languages, releasing two datasets—Pragyaan-IT (22.5K) and Pragyaan-Align (100K)—spanning 10 languages and diverse task categories.


<details>
  <summary>Details</summary>
Motivation: LLMs rely on rich post-training corpora (instruction-tuning and preference data), but open datasets lack multilingual breadth, cultural grounding, and task diversity—gaps that are acute for Indic languages.

Method: A human-in-the-loop curation pipeline combining translations and synthetic generation, sourced from 57 datasets, with explicit protocols emphasizing task diversity, multi-turn dialogues, instruction fidelity, safety alignment, and preservation of cultural nuance.

Result: Creation of two Indic-focused datasets: Pragyaan-IT (22.5K) and Pragyaan-Align (100K), covering 10 languages, 13 broad categories, and 56 sub-categories. The data are positioned as reliable and diverse for post-training.

Conclusion: The datasets and protocol aim to enable more inclusive and effective multilingual LLMs, particularly for Indic languages, by addressing coverage, cultural grounding, and alignment considerations in post-training data.

Abstract: The effectiveness of Large Language Models (LLMs) depends heavily on the
availability of high-quality post-training data, particularly
instruction-tuning and preference-based examples. Existing open-source
datasets, however, often lack multilingual coverage, cultural grounding, and
suffer from task diversity gaps that are especially pronounced for Indian
languages. We introduce a human-in-the-loop pipeline that combines translations
with synthetic expansion to produce reliable and diverse Indic post-training
data. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and
Pragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56
sub-categories, leveraging 57 diverse datasets. Our dataset protocol
incorporates several often-overlooked dimensions and emphasize task diversity,
multi-turn dialogue, instruction fidelity, safety alignment, and preservation
of cultural nuance, providing a foundation for more inclusive and effective
multilingual LLMs.

</details>


### [156] [Native Hybrid Attention for Efficient Sequence Modeling](https://arxiv.org/abs/2510.07019)
*Jusen Du,Jiaxi Hu,Tao Zhang,Weigao Sun,Yu Cheng*

Main category: cs.CL

TL;DR: Native Hybrid Attention (NHA) unifies linear and full attention by maintaining long-term memory via a linear RNN-updated KV cache and mixing it with a sliding-window of recent tokens, then applying a single softmax over all keys/values. It outperforms Transformers and prior hybrids on recall-heavy and commonsense tasks while being more efficient and easy to tune via one window-size hyperparameter.


<details>
  <summary>Details</summary>
Motivation: Transformers have quadratic cost with context length; linear attention scales better but often loses recall over long contexts. The paper aims to bridge this efficiency–recall trade-off with a simple, uniform architecture that preserves long-term information without extra fusion parameters and can be smoothly tuned between linear and full attention.

Method: Introduce a unified layer that combines intra- and inter-layer hybridization: (1) maintain long-term context in key–value slots updated by a linear RNN; (2) augment with short-term tokens from a sliding window; (3) perform a single softmax attention over the concatenated keys/values, allowing per-token/per-head weighting without additional fusion modules. A single hyperparameter—the sliding-window size—controls behavior across layers, enabling a continuum from purely linear to full attention. The design can retrofit pretrained LLMs.

Result: Across experiments, NHA surpasses standard Transformers and other hybrid attention baselines on recall-intensive and commonsense reasoning benchmarks. When used to structurally hybridize pretrained LLMs, it attains competitive accuracy with significant efficiency gains. (Abstract does not provide exact metrics.)

Conclusion: NHA is a simple, parameter-light hybrid attention mechanism that preserves long-term recall while retaining efficiency. Its single hyperparameter makes it easy to trade off cost and accuracy, and its drop-in nature enables efficient retrofitting of existing LLMs.

Abstract: Transformers excel at sequence modeling but face quadratic complexity, while
linear attention offers improved efficiency but often compromises recall
accuracy over long contexts. In this work, we introduce Native Hybrid Attention
(NHA), a novel hybrid architecture of linear and full attention that integrates
both intra \& inter-layer hybridization into a unified layer design. NHA
maintains long-term context in key-value slots updated by a linear RNN, and
augments them with short-term tokens from a sliding window. A single
\texttt{softmax attention} operation is then applied over all keys and values,
enabling per-token and per-head context-dependent weighting without requiring
additional fusion parameters. The inter-layer behavior is controlled through a
single hyperparameter, the sliding window size, which allows smooth adjustment
between purely linear and full attention while keeping all layers structurally
uniform. Experimental results show that NHA surpasses Transformers and other
hybrid baselines on recall-intensive and commonsense reasoning tasks.
Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving
competitive accuracy while delivering significant efficiency gains. Code is
available at https://github.com/JusenD/NHA.

</details>


### [157] [Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge](https://arxiv.org/abs/2510.07024)
*Shrestha Ghosh,Luca Giordano,Yujia Hu,Tuan-Phong Nguyen,Simon Razniewski*

Main category: cs.CL

TL;DR: Analyzing 100M recursively elicited “beliefs” from GPT‑4.1 (GPTKB v1.5), the authors find LLM factual knowledge diverges from canonical knowledge bases, achieves lower true accuracy than prior benchmarks imply, and exhibits substantial inconsistency, ambiguity, and hallucinations.


<details>
  <summary>Details</summary>
Motivation: Factual knowledge in LLMs remains poorly characterized and prior evaluations rely on small or biased samples, potentially overstating reliability. A comprehensive, data-driven picture is needed to understand what LLMs ‘know’ and how trustworthy those beliefs are.

Method: Leverage GPTKB v1.5—a large-scale (100M) corpus of GPT‑4.1-elicited beliefs obtained via recursive prompting—and compare these beliefs against established knowledge bases. Quantify accuracy and analyze patterns of inconsistency, ambiguity, and hallucination across the belief set.

Result: LLM beliefs differ markedly from entries in established knowledge bases; measured factual accuracy is notably lower than suggested by prior benchmarks; and the belief set reveals widespread inconsistency, ambiguity, and hallucinations.

Conclusion: Current benchmarks likely overestimate LLM factual reliability. The study highlights the need for better evaluation protocols and datasets, and points to reducing inconsistency, ambiguity, and hallucinations as key directions for improving factual knowledge in LLMs.

Abstract: LLMs are remarkable artifacts that have revolutionized a range of NLP and AI
tasks. A significant contributor is their factual knowledge, which, to date,
remains poorly understood, and is usually analyzed from biased samples. In this
paper, we take a deep tour into the factual knowledge (or beliefs) of a
frontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited
set of 100 million beliefs of one of the strongest currently available frontier
LLMs, GPT-4.1. We find that the models' factual knowledge differs quite
significantly from established knowledge bases, and that its accuracy is
significantly lower than indicated by previous benchmarks. We also find that
inconsistency, ambiguity and hallucinations are major issues, shedding light on
future research opportunities concerning factual LLM knowledge.

</details>


### [158] [Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models](https://arxiv.org/abs/2510.07037)
*Rajvee Sheth,Samridhi Raj Sinha,Mahavir Patil,Himanshu Beniwal,Mayank Singh*

Main category: cs.CL

TL;DR: A survey of code-switching–aware large language models that categorizes recent work across architectures, training strategies, and evaluations, covers 12 tasks over 80+ languages and 30+ datasets, identifies persistent challenges, and proposes a roadmap toward fair, linguistically grounded, multilingual intelligence, with resources curated online.


<details>
  <summary>Details</summary>
Motivation: Code-switching is common in multilingual settings, but most LLMs still falter on mixed-language inputs due to scarce datasets and evaluation biases. A comprehensive synthesis is needed to map progress, reveal gaps, and guide future research and deployment.

Method: Literature review of CSW-focused LLM research across five areas, 12 NLP tasks, 30+ datasets, and 80+ languages. The survey classifies advances by model architecture, training strategy, and evaluation methodology, and analyzes how LLMs have reshaped CSW modeling. It also compiles a curated repository of resources.

Result: Provides the first broad taxonomy and analysis of CSW-aware LLMs, detailing trends, strengths, and limitations across datasets, tasks, and languages. Highlights that while LLMs have improved CSW modeling, they still struggle with mixed-language inputs, limited data coverage, and biased evaluations.

Conclusion: Achieving robust multilingual intelligence requires inclusive, representative datasets, fair and standardized evaluation protocols, and models grounded in linguistic principles. The survey offers a roadmap and a curated resource list to accelerate progress.

Abstract: Code-switching (CSW), the alternation of languages and scripts within a
single utterance, remains a fundamental challenge for multiling ual NLP, even
amidst the rapid advances of large language models (LLMs). Most LLMs still
struggle with mixed-language inputs, limited CSW datasets, and evaluation
biases, hindering deployment in multilingual societies. This survey provides
the first comprehensive analysis of CSW-aware LLM research, reviewing
\total{unique_references} studies spanning five research areas, 12 NLP tasks,
30+ datasets, and 80+ languages. We classify recent advances by architecture,
training strategy, and evaluation methodology, outlining how LLMs have reshaped
CSW modeling and what challenges persist. The paper concludes with a roadmap
emphasizing the need for inclusive datasets, fair evaluation, and
linguistically grounded models to achieve truly multilingual intelligence. A
curated collection of all resources is maintained at
https://github.com/lingo-iitgn/awesome-code-mixing/.

</details>


### [159] [Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models](https://arxiv.org/abs/2510.07048)
*Yuntao Gui,James Cheng*

Main category: cs.CL

TL;DR: Search-R3 trains LLMs to output search embeddings as a byproduct of chain-of-thought reasoning, using supervised learning plus RL and a specialized RL environment to avoid constant re-encoding, achieving strong gains on retrieval benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLMs reason well but traditional retrieval uses separate embedding models that don’t leverage step-by-step semantic reasoning. Unifying reasoning with embedding generation could improve retrieval for knowledge-intensive tasks.

Method: Three-part framework: (1) supervised stage to teach the LLM to produce high-quality embeddings; (2) reinforcement learning that jointly optimizes reasoning traces and embedding quality; (3) a custom RL training environment that supports evolving embeddings without re-encoding the whole corpus every iteration. Embeddings are produced directly from the model’s chain-of-thought.

Result: Across diverse retrieval benchmarks, Search-R3 significantly outperforms prior methods, indicating that coupling reasoning with embedding generation yields better retrieval effectiveness.

Conclusion: Integrating reasoning and embedding generation during post-training advances retrieval for complex, knowledge-intensive tasks, offering both effectiveness gains and practical training efficiency. Open-source resources are available at the provided project page.

Abstract: Despite their remarkable natural language understanding capabilities, Large
Language Models (LLMs) have been underutilized for retrieval tasks. We present
Search-R3, a novel framework that addresses this limitation by adapting LLMs to
generate search embeddings as a direct output of their reasoning process. Our
approach exploits LLMs' chain-of-thought capabilities, allowing them to produce
more effective embeddings by reasoning step-by-step through complex semantic
analyses. We implement this through three complementary mechanisms. (1) a
supervised learning stage enables the model's ability to produce quality
embeddings, (2) a reinforcement learning (RL) methodology that optimizes
embedding generation alongside reasoning, and (3) a specialized RL environment
that efficiently handles evolving embedding representations without requiring
complete corpus re-encoding at each training iteration. Our extensive
evaluations on diverse benchmarks demonstrate that Search-R3 significantly
outperforms prior methods by unifying the reasoning and embedding generation
processes. This integrated post-training approach represents a substantial
advancement in handling complex knowledge-intensive tasks that require both
sophisticated reasoning and effective information retrieval. Project page:
https://github.com/ytgui/Search-R3

</details>


### [160] [Does Local News Stay Local?: Online Content Shifts in Sinclair-Acquired Stations](https://arxiv.org/abs/2510.07060)
*Miriam Wanner,Sophia Hager,Anjalie Field*

Main category: cs.CL

TL;DR: After Sinclair acquires local TV stations, their online news shifts from locally focused reporting toward more national—and more polarizing—topics.


<details>
  <summary>Details</summary>
Motivation: Local TV news is widely trusted and central to residents’ information about community issues. Sinclair’s rapid consolidation raises concerns that editorial direction may change, potentially altering audiences’ information diets and the local civic information ecosystem.

Method: Computational analysis of internet content produced by local stations before and after Sinclair acquisition, benchmarked against national outlets; topic classification to distinguish local vs. national coverage and identification of polarizing national topics.

Result: Post-acquisition, stations increase the share of national news, reduce local-topic coverage, and devote more attention to polarizing national issues.

Conclusion: Sinclair ownership is associated with a systematic shift away from local reporting toward national, more polarizing content, suggesting media consolidation can diminish locally relevant information available to communities.

Abstract: Local news stations are often considered to be reliable sources of
non-politicized information, particularly local concerns that residents care
about. Because these stations are trusted news sources, viewers are
particularly susceptible to the information they report. The Sinclair Broadcast
group is a broadcasting company that has acquired many local news stations in
the last decade. We investigate the effects of local news stations being
acquired by Sinclair: how does coverage change? We use computational methods to
investigate changes in internet content put out by local news stations before
and after being acquired by Sinclair and in comparison to national news
outlets. We find that there is clear evidence that local news stations report
more frequently on national news at the expense of local topics, and that their
coverage of polarizing national topics increases.

</details>


### [161] [Revisiting Metric Reliability for Fine-grained Evaluation of Machine Translation and Summarization in Indian Languages](https://arxiv.org/abs/2510.07061)
*Amir Hossein Yari,Kalmit Kulkarni,Ahmad Raza Khan,Fajri Koto*

Main category: cs.CL

TL;DR: ITEM is a benchmark that evaluates 26 MT and summarization metrics against human judgments in six major Indian languages, finding LLM-based evaluators align best, outliers strongly affect agreement, TS metrics better capture content fidelity while MT metrics better reflect fluency, and robustness varies across metrics.


<details>
  <summary>Details</summary>
Motivation: Most automatic MT/TS metrics are built and validated for English or other high-resource languages, leaving Indian languages under-evaluated and casting doubt on the universality of current evaluation practices. There is a need for a systematic, human-grounded benchmark for Indian languages.

Method: Construct ITEM, a large-scale benchmark with fine-grained annotations across six major Indian languages. Evaluate 26 automatic metrics for MT and TS on multiple dimensions: segment- and system-level agreement with human judgments, sensitivity to outliers, language-specific reliability, inter-metric correlations, and resilience under controlled perturbations.

Result: Four main findings: (1) LLM-based evaluators have the strongest alignment with human judgments at both segment and system levels; (2) outliers significantly influence metric–human agreement; (3) for TS, metrics better capture content fidelity, whereas for MT, they better reflect fluency; (4) metrics differ substantially in robustness and sensitivity to various perturbations.

Conclusion: ITEM offers a principled foundation for evaluating and designing metrics for Indian languages. LLM-based evaluators are promising but sensitivity to outliers and task-specific strengths/weaknesses must be considered. Future metric development should emphasize robustness, outlier handling, and distinct objectives across MT and TS.

Abstract: While automatic metrics drive progress in Machine Translation (MT) and Text
Summarization (TS), existing metrics have been developed and validated almost
exclusively for English and other high-resource languages. This narrow focus
leaves Indian languages, spoken by over 1.5 billion people, largely overlooked,
casting doubt on the universality of current evaluation practices. To address
this gap, we introduce ITEM, a large-scale benchmark that systematically
evaluates the alignment of 26 automatic metrics with human judgments across six
major Indian languages, enriched with fine-grained annotations. Our extensive
evaluation, covering agreement with human judgments, sensitivity to outliers,
language-specific reliability, inter-metric correlations, and resilience to
controlled perturbations, reveals four central findings: (1) LLM-based
evaluators show the strongest alignment with human judgments at both segment
and system levels; (2) outliers exert a significant impact on metric-human
agreement; (3) in TS, metrics are more effective at capturing content fidelity,
whereas in MT, they better reflect fluency; and (4) metrics differ in their
robustness and sensitivity when subjected to diverse perturbations.
Collectively, these findings offer critical guidance for advancing metric
design and evaluation in Indian languages.

</details>


### [162] [LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish](https://arxiv.org/abs/2510.07074)
*Fred Philippy,Laura Bernardy,Siwen Guo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: They curate a cross-lingual instruction-tuning dataset for Luxembourgish using aligned English/French/German data (not MT into Luxembourgish), yielding better alignment and generation in Luxembourgish.


<details>
  <summary>Details</summary>
Motivation: Low-resource languages lack high-quality instruction datasets; relying on machine translation causes semantic drift and cultural inaccuracies. A method is needed to improve instruction-following in Luxembourgish without these pitfalls.

Method: Construct a cross-lingual instruction-tuning dataset for Luxembourgish by leveraging aligned data from English, French, and German, explicitly avoiding machine-generated translations into Luxembourgish to preserve linguistic and cultural nuances.

Result: Cross-lingual instruction tuning improves representational alignment across languages and enhances the model’s generative capabilities in Luxembourgish, compared to MT-based approaches.

Conclusion: Carefully curated cross-lingual data is an effective alternative to MT for instruction tuning in low-resource languages, directly benefiting Luxembourgish model performance while avoiding MT-induced errors.

Abstract: Instruction tuning has become a key technique for enhancing the performance
of large language models, enabling them to better follow human prompts.
However, low-resource languages such as Luxembourgish face severe limitations
due to the lack of high-quality instruction datasets. Traditional reliance on
machine translation often introduces semantic misalignment and cultural
inaccuracies. In this work, we address these challenges by creating a
cross-lingual instruction tuning dataset for Luxembourgish, without resorting
to machine-generated translations into it. Instead, by leveraging aligned data
from English, French, and German, we build a high-quality dataset that
preserves linguistic and cultural nuances. We provide evidence that
cross-lingual instruction tuning not only improves representational alignment
across languages but also the model's generative capabilities in Luxembourgish.
This highlights how cross-lingual data curation can avoid the common pitfalls
of machine-translated data and directly benefit low-resource language
development.

</details>


### [163] [Accelerating Diffusion LLM Inference via Local Determinism Propagation](https://arxiv.org/abs/2510.07081)
*Fanheng Kong,Jingyuan Zhang,Yahui Liu,Zirui Wu,Yu Tian,Victoria W.,Guorui Zhou*

Main category: cs.CL

TL;DR: LocalLeap is a training-free, adaptive parallel decoding strategy for diffusion LLMs that commits high-confidence tokens early using local anchors, cutting decoding steps to ~14.2% and boosting throughput by ~6.94x with negligible quality loss.


<details>
  <summary>Details</summary>
Motivation: Diffusion LLMs enable parallel token generation but suffer from a quality–speed trade-off: conservative (greedy) sampling often decodes only the most confident token per step, causing redundant refinement iterations—"delayed decoding"—that slows inference. There is a need to exploit parallelism without degrading output quality.

Method: Analyze dLLM decoding dynamics and formalize delayed decoding. Propose LocalLeap, a training-free adaptive parallel decoding scheme based on two empirical principles: (1) local determinism propagation from high-confidence “anchor” tokens; (2) progressive spatial consistency decay away from those anchors. The algorithm identifies anchors and performs relaxed, localized parallel decoding within bounded neighborhoods, enabling early commitment of tokens likely already determined.

Result: Across multiple benchmarks, LocalLeap reduces decoding steps to 14.2% of the original requirement and increases throughput by about 6.94x, while maintaining negligible performance degradation.

Conclusion: Addressing delayed decoding via anchor-centered, localized parallel updates makes diffusion LLM decoding substantially more efficient without retraining, improving practicality and deployment readiness. Open-source code is provided.

Abstract: Diffusion large language models (dLLMs) represent a significant advancement
in text generation, offering parallel token decoding capabilities. However,
existing open-source implementations suffer from quality-speed trade-offs that
impede their practical deployment. Conservative sampling strategies typically
decode only the most confident token per step to ensure quality (i.e., greedy
decoding), at the cost of inference efficiency due to repeated redundant
refinement iterations--a phenomenon we term delayed decoding. Through
systematic analysis of dLLM decoding dynamics, we characterize this delayed
decoding behavior and propose a training-free adaptive parallel decoding
strategy, named LocalLeap, to address these inefficiencies. LocalLeap is built
on two fundamental empirical principles: local determinism propagation centered
on high-confidence anchors and progressive spatial consistency decay. By
applying these principles, LocalLeap identifies anchors and performs localized
relaxed parallel decoding within bounded neighborhoods, achieving substantial
inference step reduction through early commitment of already-determined tokens
without compromising output quality. Comprehensive evaluation on various
benchmarks demonstrates that LocalLeap achieves 6.94$\times$ throughput
improvements and reduces decoding steps to just 14.2\% of the original
requirement, achieving these gains with negligible performance impact. The
source codes are available at: https://github.com/friedrichor/LocalLeap.

</details>


### [164] [All Claims Are Equal, but Some Claims Are More Equal Than Others: Importance-Sensitive Factuality Evaluation of LLM Generations](https://arxiv.org/abs/2510.07083)
*Miriam Wanner,Leif Azzopardi,Paul Thomas,Soham Dan,Benjamin Van Durme,Nick Craswell*

Main category: cs.CL

TL;DR: They show that common LLM factuality metrics miss errors/omissions in key information. They build a benchmark (VITALERRORS) and propose importance-aware metrics (VITAL) that better detect such vital errors.


<details>
  <summary>Details</summary>
Motivation: Factuality evaluators typically weight all claims equally, so omissions or falsifications of crucial facts can be masked by correct but peripheral details. The authors want metrics that reflect the relevance and importance of claims to the user’s query.

Method: 1) Construct VITALERRORS: 6,733 minimally edited LLM responses where key information is omitted or falsified. 2) Empirically demonstrate existing metrics’ insensitivity to these vital errors. 3) Introduce VITAL, a metric suite that incorporates claim relevance/importance to the query when scoring factuality.

Result: Existing factuality metrics under-detect errors in key information on VITALERRORS. VITAL metrics show greater sensitivity and more reliably flag vital omissions or falsehoods compared to prior methods.

Conclusion: Importance-aware evaluation is necessary for reliable factuality assessment. The provided benchmark, metrics, and analysis offer a more accurate and robust way to detect key-information errors in LLM responses.

Abstract: Existing methods for evaluating the factuality of large language model (LLM)
responses treat all claims as equally important. This results in misleading
evaluations when vital information is missing or incorrect as it receives the
same weight as peripheral details, raising the question: how can we reliably
detect such differences when there are errors in key information? Current
approaches that measure factuality tend to be insensitive to omitted or false
key information. To investigate this lack of sensitivity, we construct
VITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses
designed to omit or falsify key information. Using this dataset, we demonstrate
the insensitivities of existing evaluation metrics to key information errors.
To address this gap, we introduce VITAL, a set of metrics that provide greater
sensitivity in measuring the factuality of responses by incorporating the
relevance and importance of claims with respect to the query. Our analysis
demonstrates that VITAL metrics more reliably detect errors in key information
than previous methods. Our dataset, metrics, and analysis provide a foundation
for more accurate and robust assessment of LLM factuality.

</details>


### [165] [Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided Sarcastic Speech Synthesis](https://arxiv.org/abs/2510.07096)
*Zhu Li,Yuqing Zhang,Xiyuan Gao,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: LLM-enhanced, retrieval-augmented TTS that conditions a VITS backbone on LLaMA-3 semantic embeddings and retrieved prosodic exemplars to synthesize contextually appropriate sarcastic speech, outperforming baselines in naturalness, expressivity, and sarcasm detection.


<details>
  <summary>Details</summary>
Motivation: Sarcasm depends on nuanced semantic, contextual, and prosodic cues and is poorly handled by current TTS systems that focus on broad emotions. There is a need for models that can encode pragmatic incongruity and discourse-level signals to render sarcasm naturally.

Method: LoRA-fine-tuned LLaMA 3 provides semantic embeddings capturing sarcasm-related pragmatic cues. A RAG module retrieves prosodic exemplars representing sarcastic delivery patterns. These two signals jointly condition a VITS TTS backbone (dual conditioning) to generate sarcasm-aware speech.

Result: Experiments show superiority over baselines in objective metrics and human evaluations, with reported gains in speech naturalness, sarcastic expressivity, and performance on downstream sarcasm detection.

Conclusion: Combining LLM-derived semantic representations with retrieved prosodic references within a VITS framework effectively adds sarcasm awareness to TTS, suggesting retrieval-augmented, LLM-informed conditioning is a promising direction for expressive speech synthesis.

Abstract: Sarcasm is a subtle form of non-literal language that poses significant
challenges for speech synthesis due to its reliance on nuanced semantic,
contextual, and prosodic cues. While existing speech synthesis research has
focused primarily on broad emotional categories, sarcasm remains largely
unexplored. In this paper, we propose a Large Language Model (LLM)-enhanced
Retrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach
combines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture
pragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic
exemplars retrieved via a Retrieval Augmented Generation (RAG) module, which
provide expressive reference patterns of sarcastic delivery. Integrated within
a VITS backbone, this dual conditioning enables more natural and contextually
appropriate sarcastic speech. Experiments demonstrate that our method
outperforms baselines in both objective measures and subjective evaluations,
yielding improvements in speech naturalness, sarcastic expressivity, and
downstream sarcasm detection.

</details>


### [166] [TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription](https://arxiv.org/abs/2510.07098)
*Guo Yutong,Wanying Wang,Yue Wu,Zichen Miao,Haoyu Wang*

Main category: cs.CL

TL;DR: TALENT reframes Table VQA by using a small VLM to transcribe and narrate tables, then offloading reasoning to an LLM, achieving large-VLM-level accuracy at much lower compute; also introduces a harder dataset, ReTabVQA.


<details>
  <summary>Details</summary>
Motivation: Large VLMs can read tables directly but miss fine-grained details unless scaled, which is computationally expensive and unsuitable for mobile settings. Using OCR-to-structured formats (e.g., Markdown) for LLMs still yields errors and is not LLM-optimized. The authors seek a lightweight, accurate, and LLM-friendly alternative.

Method: TALENT uses a small VLM as a perception–narration module to produce two complementary outputs from a table image: (1) OCR text and (2) a natural-language narration describing the table. These, combined with the user question, are fed to an LLM for reasoning. The task is reframed as LLM-centric multimodal reasoning. They also build ReTabVQA, a dataset requiring multi-step quantitative reasoning over table images.

Result: Across public benchmarks and the new ReTabVQA, a small VLM + LLM pipeline with TALENT matches or surpasses a single large VLM’s performance while using significantly less computation.

Conclusion: Dual table representations (OCR + natural-language narration) enable efficient, accurate LLM-driven reasoning for Table VQA, offering a practical alternative to large VLMs. ReTabVQA provides a more challenging benchmark for multi-step quantitative reasoning over tables.

Abstract: Table Visual Question Answering (Table VQA) is typically addressed by large
vision-language models (VLMs). While such models can answer directly from
images, they often miss fine-grained details unless scaled to very large sizes,
which are computationally prohibitive, especially for mobile deployment. A
lighter alternative is to have a small VLM perform OCR and then use a large
language model (LLM) to reason over structured outputs such as Markdown tables.
However, these representations are not naturally optimized for LLMs and still
introduce substantial errors. We propose TALENT (Table VQA via Augmented
Language-Enhanced Natural-text Transcription), a lightweight framework that
leverages dual representations of tables. TALENT prompts a small VLM to produce
both OCR text and natural language narration, then combines them with the
question for reasoning by an LLM. This reframes Table VQA as an LLM-centric
multimodal reasoning task, where the VLM serves as a perception-narration
module rather than a monolithic solver. Additionally, we construct ReTabVQA, a
more challenging Table VQA dataset requiring multi-step quantitative reasoning
over table images. Experiments show that TALENT enables a small VLM-LLM
combination to match or surpass a single large VLM at significantly lower
computational cost on both public datasets and ReTabVQA.

</details>


### [167] [Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning](https://arxiv.org/abs/2510.07105)
*Taylor Sorensen,Yejin Choi*

Main category: cs.CL

TL;DR: They propose a meta-learned, in-context LLM framework that models human variation by conditioning on rater examples, winning the LeWiDi competition and showing that rater-aware prompts, task-specific tuning, and model scale notably boost performance.


<details>
  <summary>Details</summary>
Motivation: Many NLP tasks contain genuine annotator disagreement; instead of forcing a single “gold” label, the goal is to model human variation and predict rater-specific or distribution-aware outcomes.

Method: A two-step approach: (1) post-train an LLM on many datasets to strengthen general in-context learning; (2) specialize via in-context meta-learning to the target distribution, conditioning on rater examples. They also explore dataset-specific fine-tuning and scaling effects.

Result: Their system won both tasks in the LeWiDi competition. Ablations show: including rater examples in-context is crucial; dataset-specific fine-tuning helps, especially on larger datasets; post-training on other ICL datasets helps on one competition dataset; and larger models perform better.

Conclusion: Explicitly modeling rater variation with rater-conditioned in-context learning, combined with meta-learning and selective fine-tuning, yields state-of-the-art performance on disagreement-heavy NLP tasks, with benefits amplified by model scale.

Abstract: Many natural language processing (NLP) tasks involve subjectivity, ambiguity,
or legitimate disagreement between annotators. In this paper, we outline our
system for modeling human variation. Our system leverages language models'
(LLMs) in-context learning abilities, along with a two-step meta-learning
training procedure for 1) post-training on many datasets requiring in-context
learning and 2) specializing the model via in-context meta-learning to the
particular data distribution of interest. We also evaluate the performance of
our system submission to the Learning With Disagreements (LeWiDi) competition,
where it was the overall winner on both tasks. Additionally, we perform an
ablation study to measure the importance of each system component. We find that
including rater examples in-context is crucial for our system's performance,
dataset-specific fine-tuning is helpful on the larger datasets, post-training
on other in-context datasets is helpful on one of the competition datasets, and
that performance improves with model scale.

</details>


### [168] [TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning](https://arxiv.org/abs/2510.07118)
*Manish Nagaraj,Sakshi Choudhary,Utkarsh Saxena,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CL

TL;DR: TRIM is a forward-only, token-centric data selection method that uses multi-layer attention “fingerprints” to pick small, high-quality instruction-tuning coresets, outperforming SOTA baselines by up to 9% and sometimes even full-data fine-tuning, at much lower compute (no gradients/backward passes).


<details>
  <summary>Details</summary>
Motivation: Instruction tuning typically needs large, diverse datasets, but well-chosen small coresets can match or beat full-data performance. Existing coreset methods often rely on gradient-based, sample-level signals that are costly and overlook fine-grained token-level structure. There is a need for an efficient, fine-grained approach that captures task-relevant structure without expensive backward passes.

Method: TRIM builds interpretable, multi-layer attention-based “fingerprints” from a few target samples, then uses forward passes to match these representational patterns across candidate data. It selects examples whose token-level attention patterns align with the target task’s structure, avoiding gradients/backprop and emphasizing token-centric, multi-layer signals for sensitivity and efficiency.

Result: Across downstream tasks, TRIM-selected coresets consistently outperform state-of-the-art data selection baselines by up to 9% and, in some cases, even surpass full-data fine-tuning. Because it avoids backward passes, it achieves these gains at a fraction of the computational cost.

Conclusion: TRIM offers a scalable, efficient alternative for constructing high-quality instruction-tuning datasets, leveraging attention-based fingerprints to capture task-structural features and deliver strong performance with lower compute.

Abstract: Instruction tuning is essential for aligning large language models (LLMs) to
downstream tasks and commonly relies on large, diverse corpora. However, small,
high-quality subsets, known as coresets, can deliver comparable or superior
results, though curating them remains challenging. Existing methods often rely
on coarse, sample-level signals like gradients, an approach that is
computationally expensive and overlooks fine-grained features. To address this,
we introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a
forward-only, token-centric framework. Instead of using gradients, TRIM
operates by matching underlying representational patterns identified via
attention-based "fingerprints" from a handful of target samples. Such an
approach makes TRIM highly efficient and uniquely sensitive to the structural
features that define a task. Coresets selected by our method consistently
outperform state-of-the-art baselines by up to 9% on downstream tasks and even
surpass the performance of full-data fine-tuning in some settings. By avoiding
expensive backward passes, TRIM achieves this at a fraction of the
computational cost. These findings establish TRIM as a scalable and efficient
alternative for building high-quality instruction-tuning datasets.

</details>


### [169] [Comparing human and language models sentence processing difficulties on complex structures](https://arxiv.org/abs/2510.07141)
*Samuel Joseph Amouyal,Aya Meltzer-Asscher,Jonathan Berant*

Main category: cs.CL

TL;DR: Human vs. LLM sentence comprehension compared on seven difficult constructions; LLMs, even very strong ones, especially struggle with garden‑path sentences; larger models’ difficulty rankings align more with humans; target–baseline performance gaps are human‑like except for very weak or very strong models.


<details>
  <summary>Details</summary>
Motivation: Test whether LLMs exhibit human‑like processing difficulties in sentence comprehension, and to what extent model size/training lead to convergence with human behavior.

Method: Unified experimental framework collecting accuracy on seven challenging syntactic structures from humans and five LLM families of varying sizes/training. For each structure, matched baseline sentences without the difficult feature were included. Analyses included accuracy by structure (GP vs. non‑GP) and rank correlations between human and model performance as a function of parameter count.

Result: Overall, LLMs underperform on the difficult target structures, with the largest deficits on garden‑path (GP) sentences. The strongest model attains near‑ceiling performance on non‑GP items (93.7% for GPT‑5) but low accuracy on GP items (46.8%). Rank correlation between human and model performance increases with model size. Human‑like performance gaps between target and baseline hold, except when models are too weak (uniformly low) or too strong (uniformly high).

Conclusion: LLMs show both convergence and divergence with human sentence processing: larger models mirror human difficulty rankings more closely, yet even state‑of‑the‑art models falter on GP sentences. This suggests partial alignment with human psycholinguistic patterns but indicates remaining deficits in handling incremental ambiguity and reanalysis.

Abstract: Large language models (LLMs) that fluently converse with humans are a reality
- but do LLMs experience human-like processing difficulties? We systematically
compare human and LLM sentence comprehension across seven challenging
linguistic structures. We collect sentence comprehension data from humans and
five families of state-of-the-art LLMs, varying in size and training procedure
in a unified experimental framework. Our results show LLMs overall struggle on
the target structures, but especially on garden path (GP) sentences. Indeed,
while the strongest models achieve near perfect accuracy on non-GP structures
(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).
Additionally, when ranking structures based on average performance, rank
correlation between humans and models increases with parameter count. For each
target structure, we also collect data for their matched baseline without the
difficult structure. Comparing performance on the target vs. baseline
sentences, the performance gap observed in humans holds for LLMs, with two
exceptions: for models that are too weak performance is uniformly low across
both sentence types, and for models that are too strong the performance is
uniformly high. Together, these reveal convergence and divergence in human and
LLM sentence comprehension, offering new insights into the similarity of humans
and LLMs.

</details>


### [170] [Reasoning for Hierarchical Text Classification: The Case of Patents](https://arxiv.org/abs/2510.07167)
*Lekang Jiang,Wenjun Sun,Stephan Goetz*

Main category: cs.CL

TL;DR: RHC reframes hierarchical text classification as a step-by-step reasoning process, training LLMs with CoT alignment and RL to sequentially predict taxonomy paths, yielding ~3% gains in accuracy and macro-F1, better explainability, and strong scalability across patents and other HTC benchmarks.


<details>
  <summary>Details</summary>
Motivation: HTC—especially patent classification—demands domain knowledge and must assign labels across multiple taxonomy levels. Prior models often output flat labels with limited interpretability and reasoning transparency, offering little insight into how predictions map to the hierarchy. The authors aim to improve both performance and explainability by enabling explicit multi-step reasoning aligned with the taxonomy.

Method: They propose RHC, which reformulates HTC as sequential reasoning along the taxonomy. Training has two stages: (1) a cold-start stage to align model outputs with chain-of-thought (CoT) reasoning formats; (2) a reinforcement learning stage to strengthen multi-step reasoning and decision quality across levels. The model produces natural-language justifications before emitting hierarchical labels.

Result: Across patent classification and other HTC benchmarks, RHC surpasses prior baselines and supervised fine-tuning by about 3% in accuracy and macro-F1. It also provides textual rationales, scales favorably with larger model sizes, and achieves state-of-the-art results beyond patents.

Conclusion: Treating HTC as explicit reasoning improves performance, interpretability, and scalability. RHC offers a general, effective approach for hierarchical labeling tasks and demonstrates broad applicability beyond patents.

Abstract: Hierarchical text classification (HTC) assigns documents to multiple levels
of a pre-defined taxonomy. Automated patent subject classification represents
one of the hardest HTC scenarios because of domain knowledge difficulty and a
huge number of labels. Prior approaches only output a flat label set, which
offers little insight into the reason behind predictions. Therefore, we propose
Reasoning for Hierarchical Classification (RHC), a novel framework that
reformulates HTC as a step-by-step reasoning task to sequentially deduce
hierarchical labels. RHC trains large language models (LLMs) in two stages: a
cold-start stage that aligns outputs with chain-of-thought (CoT) reasoning
format and a reinforcement learning (RL) stage to enhance multi-step reasoning
ability. RHC demonstrates four advantages in our experiments. (1)
Effectiveness: RHC surpasses previous baselines and outperforms the supervised
fine-tuning counterparts by approximately 3% in accuracy and macro F1. (2)
Explainability: RHC produces natural-language justifications before prediction
to facilitate human inspection. (3) Scalability: RHC scales favorably with
model size with larger gains compared to standard fine-tuning. (4)
Applicability: Beyond patents, we further demonstrate that RHC achieves
state-of-the-art performance on other widely used HTC benchmarks, which
highlights its broad applicability.

</details>


### [171] [More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning](https://arxiv.org/abs/2510.07169)
*Yike Zhao,Simin Guo,Ziqing Yang,Shifan Han,Dahua Lin,Fei Tan*

Main category: cs.CL

TL;DR: Evaluates open-source math-reasoning datasets and synthesis methods in a unified, deployment-like pipeline and finds that structured/ distilled data beats merely scaling volume, yielding practical, cost-effective data selection strategies for improving LLM reasoning.


<details>
  <summary>Details</summary>
Motivation: LLM reasoning quality hinges on training data, yet practitioners lack clear, empirically grounded guidance on which data sources and construction methods work best in real-world pipelines and under budget constraints.

Method: Comprehensive analysis of mathematical-reasoning datasets and data synthesis techniques, all trained and assessed under a consistent pipeline designed to mirror real deployment. The study compares data volume vs. data quality (e.g., interpretable structure, distillation from stronger models) and distills selection strategies for practical use.

Result: Data organized in interpretable formats and data distilled from stronger models often yield larger gains than simply increasing dataset size. The study extracts effective selection strategies and identifies methods suitable for industrial settings.

Conclusion: Actionable guidance is provided for integrating and curating training data to enhance LLM reasoning in a cost-effective, scalable manner. The work advocates prioritizing “better data” (structured, distilled) over sheer quantity and invites further research on balancing quality and scale for real-world reasoning tasks.

Abstract: The reasoning capabilities of Large Language Models (LLMs) play a critical
role in many downstream tasks, yet depend strongly on the quality of training
data. Despite various proposed data construction methods, their practical
utility in real-world pipelines remains underexplored. In this work, we conduct
a comprehensive analysis of open-source datasets and data synthesis techniques
for mathematical reasoning, evaluating them under a unified pipeline designed
to mirror training and deployment scenarios. We further distill effective data
selection strategies and identify practical methods suitable for industrial
applications. Our findings highlight that structuring data in more
interpretable formats, or distilling from stronger models often outweighs
simply scaling up data volume. This study provides actionable guidance for
integrating training data to enhance LLM capabilities, supporting both
cost-effective data curation and scalable model enhancement. We hope this work
will inspire further research on how to balance "more data" versus "better
data" for real-world reasoning tasks.

</details>


### [172] [NurseLLM: The First Specialized Language Model for Nursing](https://arxiv.org/abs/2510.07173)
*Md Tawkat Islam Khondaker,Julia Harrington,Shady Shehata*

Main category: cs.CL

TL;DR: Introduces NurseLLM, a nursing-specialized LLM for MCQ answering, trained on a newly built large-scale nursing MCQ dataset and evaluated on new nursing benchmarks; it outperforms comparable general and medical LLMs and explores reasoning and multi-agent collaboration for future work.


<details>
  <summary>Details</summary>
Motivation: LLMs have advanced medical AI broadly, but nursing—despite distinct knowledge and workflows—lacks dedicated models, datasets, and benchmarks. Since MCQs are central to nursing education/evaluation, a specialized LLM and resources are needed to address this gap.

Method: Build a multi-stage pipeline to generate a large-scale, topic-diverse nursing MCQ dataset; construct multiple nursing benchmarks; train/fine-tune NurseLLM; conduct extensive comparisons against state-of-the-art general-purpose and medical LLMs; explore reasoning strategies and multi-agent collaboration systems for nursing tasks.

Result: Across introduced benchmarks, NurseLLM surpasses similarly sized state-of-the-art general and medical-specialized LLMs, indicating benefits from domain specialization.

Conclusion: Specialized LLMs for nursing yield measurable gains on MCQs; the new dataset and benchmarks provide infrastructure for progress. Reasoning and multi-agent approaches appear promising, motivating further research and practical applications in nursing contexts.

Abstract: Recent advancements in large language models (LLMs) have significantly
transformed medical systems. However, their potential within specialized
domains such as nursing remains largely underexplored. In this work, we
introduce NurseLLM, the first nursing-specialized LLM tailored for multiple
choice question-answering (MCQ) tasks. We develop a multi-stage data generation
pipeline to build the first large scale nursing MCQ dataset to train LLMs on a
broad spectrum of nursing topics. We further introduce multiple nursing
benchmarks to enable rigorous evaluation. Our extensive experiments demonstrate
that NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of
comparable size on different benchmarks, underscoring the importance of a
specialized LLM for the nursing domain. Finally, we explore the role of
reasoning and multi-agent collaboration systems in nursing, highlighting their
promise for future research and applications.

</details>


### [173] [Quantifying Data Contamination in Psychometric Evaluations of LLMs](https://arxiv.org/abs/2510.07175)
*Jongwook Han,Woojung Song,Jonggeun Lee,Yohan Jo*

Main category: cs.CL

TL;DR: They introduce a framework to quantify data contamination in LLM psychometric tests and show that commonly used inventories (e.g., BFI‑44, PVQ‑40) are heavily contaminated—models memorize items and can tune answers to hit target scores.


<details>
  <summary>Details</summary>
Motivation: Psychometric questionnaires are increasingly used to assess LLMs’ values, personality, and other traits, but suspected training-data contamination may invalidate such evaluations. There was no systematic way to measure how much contamination exists.

Method: Propose and apply a contamination-measurement framework along three axes: (1) item memorization, (2) evaluation memorization, and (3) target score matching. Evaluate 21 LLMs across major families on four widely used psychometric inventories.

Result: Find strong contamination for popular inventories, notably BFI‑44 and PVQ‑40: models not only recall questionnaire items but also adjust responses to achieve specific target scores.

Conclusion: Standard psychometric evaluations of LLMs are unreliable without addressing contamination. The framework enables diagnosing contamination and supports the need for decontaminated or adaptive inventories and stricter evaluation protocols.

Abstract: Recent studies apply psychometric questionnaires to Large Language Models
(LLMs) to assess high-level psychological constructs such as values,
personality, moral foundations, and dark traits. Although prior work has raised
concerns about possible data contamination from psychometric inventories, which
may threaten the reliability of such evaluations, there has been no systematic
attempt to quantify the extent of this contamination. To address this gap, we
propose a framework to systematically measure data contamination in
psychometric evaluations of LLMs, evaluating three aspects: (1) item
memorization, (2) evaluation memorization, and (3) target score matching.
Applying this framework to 21 models from major families and four widely used
psychometric inventories, we provide evidence that popular inventories such as
the Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40)
exhibit strong contamination, where models not only memorize items but can also
adjust their responses to achieve specific target scores.

</details>


### [174] [CARPAS: Towards Content-Aware Refinement of Provided Aspects for Summarization in Large Language Models](https://arxiv.org/abs/2510.07177)
*Yong-En Tian,Yu-Chien Tang,An-Zi Yen,Wen-Chih Peng*

Main category: cs.CL

TL;DR: Introduces CARPAS, a task that refines user-provided aspects based on document content before summarization; shows LLMs over-predict aspects and proposes predicting the number of relevant aspects to guide LLMs, yielding consistent gains across three new datasets and insights on compliance to requested aspect counts.


<details>
  <summary>Details</summary>
Motivation: Real-world aspect-based summarization often receives incomplete, irrelevant, or missing aspects. Users expect systems to filter/refine aspects to match the document, but current methods assume fixed, correct aspect sets.

Method: Define the CARPAS setting; build three datasets; evaluate LLMs under four prompting strategies and observe over-comprehensive aspect selection; introduce a subtask to predict the number of relevant aspects and use this count to guide LLM aspect selection and summarization; analyze LLM compliance when the requested number differs from their internal estimates.

Result: LLMs tend to over-predict aspects, producing long, misaligned summaries. Predicting and enforcing the number of relevant aspects effectively focuses generation, reduces inference difficulty, and significantly improves performance across all datasets. LLMs generally comply with externally requested aspect counts, even when conflicting with their estimates.

Conclusion: Controlling aspect count is a simple, effective lever for CARPAS: number prediction guides LLMs to produce concise, relevant summaries and improves metrics. The compliance behavior offers a practical mechanism for deployment, suggesting count-based constraints can steer LLMs in real-world aspect-based summarization workflows.

Abstract: Aspect-based summarization has attracted significant attention for its
ability to generate more fine-grained and user-aligned summaries. While most
existing approaches assume a set of predefined aspects as input, real-world
scenarios often present challenges where these given aspects may be incomplete,
irrelevant, or entirely missing from the document. Users frequently expect
systems to adaptively refine or filter the provided aspects based on the actual
content. In this paper, we initiate this novel task setting, termed
Content-Aware Refinement of Provided Aspects for Summarization (CARPAS), with
the aim of dynamically adjusting the provided aspects based on the document
context before summarizing. We construct three new datasets to facilitate our
pilot experiments, and by using LLMs with four representative prompting
strategies in this task, we find that LLMs tend to predict an overly
comprehensive set of aspects, which often results in excessively long and
misaligned summaries. Building on this observation, we propose a preliminary
subtask to predict the number of relevant aspects, and demonstrate that the
predicted number can serve as effective guidance for the LLMs, reducing the
inference difficulty, and enabling them to focus on the most pertinent aspects.
Our extensive experiments show that the proposed approach significantly
improves performance across all datasets. Moreover, our deeper analyses uncover
LLMs' compliance when the requested number of aspects differs from their own
estimations, establishing a crucial insight for the deployment of LLMs in
similar real-world applications.

</details>


### [175] [Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish the Possible from the Impossible](https://arxiv.org/abs/2510.07178)
*Imry Ziv,Nur Lan,Emmanuel Chemla,Roni Katzir*

Main category: cs.CL

TL;DR: Testing GPT-2 on natural vs. systematically perturbed ("impossible") languages, the authors find it learns both about equally well and shows no reliable separation—contradicting claims that LLMs share human innate linguistic biases.


<details>
  <summary>Details</summary>
Motivation: Assess whether LLMs reflect human-like innate constraints on learnable languages by checking sensitivity to the possible/impossible language distinction that shapes linguistic typology.

Method: Replicate and broaden prior methodology: construct "impossible" datasets by perturbing multiple real languages; train/evaluate GPT-2 across languages; compare learning/perplexity curves and derived metrics; test both pairwise (each language vs. its perturbed counterpart) and global separation (all natural vs. all impossible).

Result: Across most languages and perturbations, GPT-2 learns natural and impossible datasets equally easily; analyzing variance in perplexity-derived metrics shows no systematic separation between possible and impossible sets, contrary to earlier positive findings.

Conclusion: Under this evaluation, GPT-2 does not exhibit the human innate learning biases implicated in linguistic typology; thus, LLM success does not imply human-like constraints on learnability.

Abstract: Are large language models (LLMs) sensitive to the distinction between humanly
possible languages and humanly impossible languages? This question is taken by
many to bear on whether LLMs and humans share the same innate learning biases.
Previous work has attempted to answer it in the positive by comparing LLM
learning curves on existing language datasets and on "impossible" datasets
derived from them via various perturbation functions. Using the same
methodology, we examine this claim on a wider set of languages and impossible
perturbations. We find that in most cases, GPT-2 learns each language and its
impossible counterpart equally easily, in contrast to previous claims. We also
apply a more lenient condition by testing whether GPT-2 provides any kind of
separation between the whole set of natural languages and the whole set of
impossible languages. By considering cross-linguistic variance in various
metrics computed on the perplexity curves, we show that GPT-2 provides no
systematic separation between the possible and the impossible. Taken together,
these perspectives show that LLMs do not share the human innate biases that
shape linguistic typology.

</details>


### [176] [Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models](https://arxiv.org/abs/2510.07213)
*Chengzhi Zhong,Fei Cheng,Qianying Liu,Yugo Murawaki,Chenhui Chu,Sadao Kurohashi*

Main category: cs.CL

TL;DR: They find sparse, consistent latent dimensions that govern cross‑lingual transitions in LLMs and introduce a training‑free, small‑data method to identify and tweak these dimensions to switch output languages while keeping meaning, outperforming prior neuron‑based controls at lower cost.


<details>
  <summary>Details</summary>
Motivation: Despite strong multilingual performance, English‑centric LLMs see little non‑English data. Prior work suggests they translate content into English‑aligned internal representations and then back to target languages. The authors aim to understand and control this mechanism in a simple, interpretable, and data‑efficient way.

Method: Hypothesize that cross‑lingual transitions are controlled by a small, sparse set of dimensions that recur at consistent indices across intermediate-to-final layers. Propose a training‑free procedure that, using as few as ~50 sentences (parallel or monolingual), identifies these dimensions and intervenes on them to steer the model’s output language while preserving semantics.

Result: On multilingual generation control benchmarks, manipulating the identified dimensions reliably switches the output language without changing content. The approach outperforms previous neuron‑based methods and does so with substantially lower computational and data costs.

Conclusion: Cross‑lingual behavior in LLMs is mediated by sparse, stable dimensions that can be cheaply located and controlled. This offers an interpretable and efficient mechanism for language control in multilingual generation, with better performance and lower cost than neuron‑level baselines.

Abstract: Large language models exhibit strong multilingual capabilities despite
limited exposure to non-English data. Prior studies show that English-centric
large language models map multilingual content into English-aligned
representations at intermediate layers and then project them back into
target-language token spaces in the final layer. From this observation, we
hypothesize that this cross-lingual transition is governed by a small and
sparse set of dimensions, which occur at consistent indices across the
intermediate to final layers. Building on this insight, we introduce a simple,
training-free method to identify and manipulate these dimensions, requiring
only as few as 50 sentences of either parallel or monolingual data. Experiments
on a multilingual generation control task reveal the interpretability of these
dimensions, demonstrating that the interventions in these dimensions can switch
the output language while preserving semantic content, and that it surpasses
the performance of prior neuron-based approaches at a substantially lower cost.

</details>


### [177] [Sunflower: A New Approach To Expanding Coverage of African Languages in Large Language Models](https://arxiv.org/abs/2510.07203)
*Benjamin Akera,Evelyn Nafula Ouma,Gilbert Yiga,Patrick Walukagga,Phionah Natukunda,Trevor Saaka,Solomon Nsumba,Lilian Teddy Nabukeera,Joel Muhanguzi,Imran Sekalala,Nimpamya Janat Namara,Engineer Bainomugisha,Ernest Mwebaze,John Quinn*

Main category: cs.CL

TL;DR: They propose a regional strategy for African NLP and introduce Sunflower 14B/32B—Qwen 3–based, open-source LLMs—achieving state-of-the-art comprehension across most Ugandan languages.


<details>
  <summary>Details</summary>
Motivation: Africa has 2000+ languages, but most are overlooked by current LLMs, which focus on a few high-speaker languages (e.g., Swahili, Yoruba), leading to fragmented support. The authors aim to efficiently improve coverage for diverse, under-served languages, using Uganda as a case study.

Method: Adopt a regionally focused approach: develop two models (Sunflower 14B and 32B) built on Qwen 3, tailored to Ugandan languages to enhance comprehension. Models are released openly.

Result: Sunflower 14B/32B attain state-of-the-art comprehension for the majority of Ugandan languages, according to the authors.

Conclusion: A country- or region-focused development strategy can rapidly elevate language technology for many under-served languages and help reduce language barriers in practical applications; open-sourcing enables broader use and impact.

Abstract: There are more than 2000 living languages in Africa, most of which have been
bypassed by advances in language technology. Current leading LLMs exhibit
strong performance on a number of the most common languages (e.g. Swahili or
Yoruba), but prioritise support for the languages with the most speakers first,
resulting in piecemeal ability across disparate languages. We contend that a
regionally focussed approach is more efficient, and present a case study for
Uganda, a country with high linguistic diversity. We describe the development
of Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the
art comprehension in the majority of all Ugandan languages. These models are
open source and can be used to reduce language barriers in a number of
important practical applications.

</details>


### [178] [Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation](https://arxiv.org/abs/2510.07227)
*Arjun Krishnakumar,Rhea Sanjay Sukthanker,Hannan Javed Mahadik,Gabriela Kadlecová,Vladyslav Moroshan,Timur Carstensen,Frank Hutter,Aaron Klein*

Main category: cs.CL

TL;DR: A simple three-part pretraining recipe—sparse subnetwork initialization, evolutionary search, and knowledge distillation—lets small language models match Pythia-level perplexity using 9.2× fewer pretraining tokens.


<details>
  <summary>Details</summary>
Motivation: Make small language models more cost- and data-efficient so they can offer strong performance without the heavy compute and data demands of large language models.

Method: (1) Identify structurally sparse sub-network initializations that outperform random inits under equal compute. (2) Use evolutionary search to automatically discover high-quality sparse initializations, often seeded from LLM weights. (3) Apply knowledge distillation from larger teacher models during pretraining to accelerate learning and improve generalization.

Result: The best SLM found via evolutionary search and initialized with LLM weights matches the validation perplexity of a comparable Pythia SLM while using 9.2× fewer pretraining tokens; code and models are released for reproducibility.

Conclusion: Combining optimized sparse initializations, automated search, and distillation substantially improves SLM pretraining efficiency, providing a practical, reproducible path to high-quality small models at much lower data/compute cost.

Abstract: Small Language models (SLMs) offer an efficient and accessible alternative to
Large Language Models (LLMs), delivering strong performance while using far
fewer resources. We introduce a simple and effective framework for pretraining
SLMs that brings together three complementary ideas. First, we identify
structurally sparse sub-network initializations that consistently outperform
randomly initialized models of similar size under the same compute budget.
Second, we use evolutionary search to automatically discover high-quality
sub-network initializations, providing better starting points for pretraining.
Third, we apply knowledge distillation from larger teacher models to speed up
training and improve generalization. Together, these components make SLM
pretraining substantially more efficient: our best model, discovered using
evolutionary search and initialized with LLM weights, matches the validation
perplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining
tokens. We release all code and models at
https://github.com/whittle-org/whittle/, offering a practical and reproducible
path toward cost-efficient small language model development at scale.

</details>


### [179] [How much speech data is necessary for ASR in African languages? An evaluation of data scaling in Kinyarwanda and Kikuyu](https://arxiv.org/abs/2510.07221)
*Benjamin Akera,Evelyn Nafula,Patrick Walukagga,Gilbert Yiga,John Quinn,Ernest Mwebaze*

Main category: cs.CL

TL;DR: Whisper-based ASR for low-resource African languages can reach practical accuracy with modest data, but label quality is a major bottleneck; careful curation rivals sheer volume in importance.


<details>
  <summary>Details</summary>
Motivation: Low-resource African languages lack transcribed speech, hindering ASR deployment. Despite progress with large multilingual models (e.g., Whisper), practitioners still need concrete guidance on (1) how much data is minimally required and (2) what failure modes dominate in production.

Method: Evaluate Whisper on two Bantu languages: (a) Kinyarwanda scaling study with training sets from 1 to 1,400 hours to map word error rate (WER) vs. data volume; (b) Kikuyu error characterization using 270 training hours to attribute high-error cases to specific causes, notably transcription noise.

Result: For Kinyarwanda, practical ASR performance is attainable with ~50 hours (WER < 13%), improving further to <10% WER around 200 hours; gains continue with more data. For Kikuyu, 38.6% of high-error cases stem from noisy ground-truth transcriptions, highlighting data quality issues as a primary error source.

Conclusion: Actionable benchmarks: ~50 hours yields usable systems; ~200 hours approaches strong performance. However, label noise significantly degrades outcomes, so rigorous data curation is as critical as increasing data volume. Released models/resources provide a reference for similar low-resource deployments.

Abstract: The development of Automatic Speech Recognition (ASR) systems for
low-resource African languages remains challenging due to limited transcribed
speech data. While recent advances in large multilingual models like OpenAI's
Whisper offer promising pathways for low-resource ASR development, critical
questions persist regarding practical deployment requirements. This paper
addresses two fundamental concerns for practitioners: determining the minimum
data volumes needed for viable performance and characterizing the primary
failure modes that emerge in production systems. We evaluate Whisper's
performance through comprehensive experiments on two Bantu languages:
systematic data scaling analysis on Kinyarwanda using training sets from 1 to
1,400 hours, and detailed error characterization on Kikuyu using 270 hours of
training data. Our scaling experiments demonstrate that practical ASR
performance (WER < 13\%) becomes achievable with as little as 50 hours of
training data, with substantial improvements continuing through 200 hours (WER
< 10\%). Complementing these volume-focused findings, our error analysis
reveals that data quality issues, particularly noisy ground truth
transcriptions, account for 38.6\% of high-error cases, indicating that careful
data curation is as critical as data volume for robust system performance.
These results provide actionable benchmarks and deployment guidance for teams
developing ASR systems across similar low-resource language contexts. We
release accompanying and models see
https://github.com/SunbirdAI/kinyarwanda-whisper-eval

</details>


### [180] [Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships](https://arxiv.org/abs/2510.07231)
*Donggyu Lee,Sungwon Park,Yerin Hwang,Hyunwoo Oh,Hyoshin Kim,Jungwon Kim,Meeyoung Cha,Sangyoon Park,Jihee Kim*

Main category: cs.CL

TL;DR: Introduces a large, real‑world causal reasoning benchmark derived from rigorously identified causal studies (IV, DiD, RDD) across multiple domains; eight SOTA LLMs perform poorly (best 57.6% accuracy), showing that model scale and “reasoning” branding don’t ensure causal understanding, highlighting a sizable gap for high‑stakes use.


<details>
  <summary>Details</summary>
Motivation: Causal reasoning is crucial for trustworthy decisions, but existing LLM benchmarks rely heavily on synthetic data and narrow domains, limiting ecological validity and masking true capabilities. The paper seeks a more realistic, rigorous testbed grounded in established causal identification methods.

Method: Curate causally identified relationships from top-tier economics and finance journals that use instrumental variables, difference-in-differences, and regression discontinuity designs; build a 40,379‑item benchmark spanning five task types and domains including health, environment, technology, law, and culture; evaluate eight state-of-the-art LLMs on these tasks.

Result: Across models, performance is weak; the best model achieves only 57.6% accuracy. Larger models are not consistently better, and models marketed for reasoning still falter on basic causal relationship identification.

Conclusion: There is a significant mismatch between current LLM capabilities and the demands of reliable causal reasoning in real-world, high-stakes settings. Further research, training strategies, and evaluation methods are needed before LLMs can be trusted for causal inference tasks.

Abstract: Causal reasoning is fundamental for Large Language Models (LLMs) to
understand genuine cause-and-effect relationships beyond pattern matching.
Existing benchmarks suffer from critical limitations such as reliance on
synthetic data and narrow domain coverage. We introduce a novel benchmark
constructed from casually identified relationships extracted from top-tier
economics and finance journals, drawing on rigorous methodologies including
instrumental variables, difference-in-differences, and regression discontinuity
designs. Our benchmark comprises 40,379 evaluation items covering five task
types across domains such as health, environment, technology, law, and culture.
Experimental results on eight state-of-the-art LLMs reveal substantial
limitations, with the best model achieving only 57.6\% accuracy. Moreover,
model scale does not consistently translate to superior performance, and even
advanced reasoning models struggle with fundamental causal relationship
identification. These findings underscore a critical gap between current LLM
capabilities and demands of reliable causal reasoning in high-stakes
applications.

</details>


### [181] [LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation](https://arxiv.org/abs/2510.07243)
*Joseph Enguehard,Morgane Van Ermengem,Kate Atkinson,Sujeong Cha,Arijit Ghosh Chowdhury,Prashanth Kallur Ramaswamy,Jeremy Roghair,Hannah R Marlowe,Carina Suzana Negreanu,Kitty Boxall,Diana Mincu*

Main category: cs.CL

TL;DR: Proposes a reference-free, lawyer-aligned evaluation of legal LLM answers by decomposing outputs into “Legal Data Points” (LDPs); shows higher agreement with experts and better performance than baselines on proprietary data and LegalBench; releases LDPs for part of LegalBench.


<details>
  <summary>Details</summary>
Motivation: Legal LLM outputs are hard to evaluate: reference sets are costly, standardized rubrics miss legal nuance, and generic LLM-as-a-Judge is unreliable/trustworthiness-sensitive in legal settings. A domain-specific, credible, and scalable evaluation is needed.

Method: Break long answers into self-contained Legal Data Points (LDPs) and evaluate these atomic units via a novel, reference-free procedure designed to mirror how lawyers assess arguments. Compare against multiple baselines and measure correlation with expert judgments and inter-annotator agreement. Release LDP annotations for a subset of LegalBench.

Result: Their LDP-based method outperforms several baselines on both a proprietary dataset and LegalBench, aligns more closely with human expert evaluations, and improves inter-annotator agreement. They open-source LDPs for the LegalBench subset used.

Conclusion: Decomposing legal answers into LDPs enables a more reliable, trustworthy, and scalable evaluation for legal QA, better matching expert assessment. The released resources support reproducibility and future research in legal LLM evaluation.

Abstract: Evaluating large language model (LLM) outputs in the legal domain presents
unique challenges due to the complex and nuanced nature of legal analysis.
Current evaluation approaches either depend on reference data, which is costly
to produce, or use standardized assessment methods, both of which have
significant limitations for legal applications.
  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its
reliability and effectiveness in legal contexts depend heavily on evaluation
processes unique to the legal industry and how trustworthy the evaluation
appears to the human legal expert. This is where existing evaluation methods
currently fail and exhibit considerable variability.
  This paper aims to close the gap: a) we break down lengthy responses into
'Legal Data Points' (LDPs), self-contained units of information, and introduce
a novel, reference-free evaluation methodology that reflects how lawyers
evaluate legal answers; b) we demonstrate that our method outperforms a variety
of baselines on both our proprietary dataset and an open-source dataset
(LegalBench); c) we show how our method correlates more closely with human
expert evaluations and helps improve inter-annotator agreement; and finally d)
we open source our Legal Data Points for a subset of LegalBench used in our
experiments, allowing the research community to replicate our results and
advance research in this vital area of LLM evaluation on legal
question-answering.

</details>


### [182] [Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping](https://arxiv.org/abs/2510.07230)
*Ziyi Wang,Yuxuan Lu,Yimeng Zhang,Jing Huang,Dakuo Wang*

Main category: cs.CL

TL;DR: Customer-R1 is an RL-based, persona-conditioned LLM policy that generates step-wise rationales and actions for online shopping, optimizing with action-correctness rewards; it outperforms prompting and SFT and better matches user action distributions on OPeRA.


<details>
  <summary>Details</summary>
Motivation: Existing step-wise behavior simulators learn population-level policies and ignore user-specific personas, leading to generic, less faithful simulations. The goal is to achieve personalized, higher-fidelity user behavior modeling.

Method: Condition the LLM policy on an explicit user persona and train with reinforcement learning. Optimize both next-step rationale and action generation using action correctness as the reward signal in an online shopping setting; evaluate against prompting and supervised fine-tuning baselines on the OPeRA dataset.

Result: Customer-R1 significantly improves next-action prediction accuracy over prompting and SFT baselines and better aligns with users’ empirical action distributions, indicating more faithful personalized behavior simulation.

Conclusion: Persona-conditioned RL that jointly optimizes rationales and actions enhances personalized, step-wise user simulations in e-commerce, offering higher fidelity than non-personalized prompting/SFT approaches.

Abstract: Simulating step-wise human behavior with Large Language Models (LLMs) has
become an emerging research direction, enabling applications in various
practical domains. While prior methods, including prompting, supervised
fine-tuning (SFT), and reinforcement learning (RL), have shown promise in
modeling step-wise behavior, they primarily learn a population-level policy
without conditioning on a user's persona, yielding generic rather than
personalized simulations. In this work, we pose a critical question: how can
LLM agents better simulate personalized user behavior? We introduce
Customer-R1, an RL-based method for personalized, step-wise user behavior
simulation in online shopping environments. Our policy is conditioned on an
explicit persona, and we optimize next-step rationale and action generation via
action correctness reward signals. Experiments on the OPeRA dataset emonstrate
that Customer-R1 not only significantly outperforms prompting and SFT-based
baselines in next-action prediction tasks, but also better matches users'
action distribution, indicating higher fidelity in personalized behavior
simulation.

</details>


### [183] [Online Rubrics Elicitation from Pairwise Comparisons](https://arxiv.org/abs/2510.07284)
*MohammadHossein Rezaei,Robert Vacareanu,Zihao Wang,Clinton Wang,Yunzhong He,Afra Feyza Akyürek*

Main category: cs.CL

TL;DR: They propose OnlineRubrics, which updates evaluation rubrics on-the-fly using pairwise comparisons between a current and a reference policy, reducing reward hacking and improving long-form LLM post-training by up to ~8% over static rubrics.


<details>
  <summary>Details</summary>
Motivation: Static, hand-crafted rubrics used for RL post-training of LLMs on open-ended tasks give coarse, fixed signals that can be gamed and cannot reflect new failure modes that emerge during training. A dynamic, adaptive signal is needed when verifiable rewards are unavailable.

Method: Online Rubrics Elicitation: iteratively compare outputs from the current policy to a reference policy; from these pairwise judgments, elicit or refine rubric criteria and use them to shape rewards during RL. The process continually identifies new errors and updates evaluation criteria, closing the loop between model behavior and reward design.

Result: Across benchmarks (AlpacaEval, GPQA, ArenaHard) and expert-validated sets, the online approach yields consistent gains up to 8% versus training with static rubrics. Qualitative analysis shows learned criteria emphasize transparency, practicality, organization, and reasoning.

Conclusion: Dynamic, online-elicited rubrics offer a more robust and adaptive reward signal than static rubrics for long-form, open-ended LLM training, mitigating reward hacking and capturing emergent desiderata, leading to measurable performance improvements.

Abstract: Rubrics provide a flexible way to train LLMs on open-ended long-form answers
where verifiable rewards are not applicable and human preferences provide
coarse signals. Prior work shows that reinforcement learning with rubric-based
rewards leads to consistent gains in LLM post-training. Most existing
approaches rely on rubrics that remain static over the course of training. Such
static rubrics, however, are vulnerable to reward-hacking type behaviors and
fail to capture emergent desiderata that arise during training. We introduce
Online Rubrics Elicitation (OnlineRubrics), a method that dynamically curates
evaluation criteria in an online manner through pairwise comparisons of
responses from current and reference policies. This online process enables
continuous identification and mitigation of errors as training proceeds.
Empirically, this approach yields consistent improvements of up to 8% over
training exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as
well as the validation sets of expert questions and rubrics. We qualitatively
analyze the elicited criteria and identify prominent themes such as
transparency, practicality, organization, and reasoning.

</details>


### [184] [Vibe Checker: Aligning Code Evaluation with Human Preference](https://arxiv.org/abs/2510.07315)
*Ming Zhong,Xiang Zhou,Ting-Yun Chang,Qingze Wang,Nan Xu,Xiance Si,Dan Garrette,Shyam Upadhyay,Jeremiah Liu,Jiawei Han,Benoit Schillings,Jiao Sun*

Main category: cs.CL

TL;DR: They argue that instruction following—not just functional correctness—is central to users’ “vibe check” for code. They introduce VeriCode (30 verifiable code-instruction types with deterministic checkers) and Vibe Checker (evaluation suite augmenting standard code benchmarks). Across 31 LLMs, instruction adherence is weak—especially under multi-instruction prompts—and a composite metric (functionality + instruction following) best aligns with human preference.


<details>
  <summary>Details</summary>
Motivation: Current code benchmarks (e.g., pass@k) focus on functional correctness only, while real users care about non-functional qualities such as style, readability, intent preservation, and constraint adherence. The authors posit that measurable instruction following is the missing ingredient representing human preference in coding tasks.

Method: Define a taxonomy of 30 verifiable code instructions and build deterministic verifiers for each. Augment existing code evaluation suites with these verifiers to create Vibe Checker. Evaluate 31 LLMs, measuring both functional correctness and instruction adherence, analyze multi-instruction compliance, and correlate metrics with human preference data.

Result: Even top models struggle to follow multiple instructions and show functional regression. A composite score combining functional correctness and instruction adherence correlates best with human preference, with instruction following emerging as the key differentiator on real-world coding tasks.

Conclusion: Instruction following is a core component of the coding “vibe check.” VeriCode and Vibe Checker provide concrete, automatable signals to benchmark and improve LLMs toward user-aligned coding behavior, suggesting the community should move beyond pass@k-only evaluation.

Abstract: Large Language Models (LLMs) have catalyzed vibe coding, where users leverage
LLMs to generate and iteratively refine code through natural language
interactions until it passes their vibe check. Vibe check is tied to real-world
human preference and goes beyond functionality: the solution should feel right,
read cleanly, preserve intent, and remain correct. However, current code
evaluation remains anchored to pass@k and captures only functional correctness,
overlooking the non-functional instructions that users routinely apply. In this
paper, we hypothesize that instruction following is the missing piece
underlying vibe check that represents human preference in coding besides
functional correctness. To quantify models' code instruction following
capabilities with measurable signals, we present VeriCode, a taxonomy of 30
verifiable code instructions together with corresponding deterministic
verifiers. We use the taxonomy to augment established evaluation suites,
resulting in Vibe Checker, a testbed to assess both code instruction following
and functional correctness. Upon evaluating 31 leading LLMs, we show that even
the strongest models struggle to comply with multiple instructions and exhibit
clear functional regression. Most importantly, a composite score of functional
correctness and instruction following correlates the best with human
preference, with the latter emerging as the primary differentiator on
real-world programming tasks. Our work identifies core factors of the vibe
check, providing a concrete path for benchmarking and developing models that
better align with user preferences in coding.

</details>


### [185] [LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding](https://arxiv.org/abs/2510.07233)
*Zhivar Sourati,Zheng Wang,Marianne Menglin Liu,Yazhe Hu,Mengqing Guo,Sujeeth Bharadwaj,Kyu Han,Tao Sheng,Sujith Ravi,Morteza Dehghani,Dan Roth*

Main category: cs.CL

TL;DR: LAD-RAG is a layout-aware, dynamic RAG framework for QA over visually rich, multi-page documents; it builds a symbolic layout graph alongside neural embeddings and uses an LLM agent to adapt retrieval to the query, delivering >90% perfect recall and higher QA accuracy with minimal latency across benchmarks.


<details>
  <summary>Details</summary>
Motivation: QA over visually rich documents demands reasoning over layout structure and cross-page dependencies. Conventional RAG chunks content independently and uses fixed top-k retrieval, which breaks structural links and often misses necessary evidence for multi-page reasoning.

Method: During ingestion, LAD-RAG constructs a symbolic document graph that encodes layout structure and cross-page dependencies and stores it with standard neural embeddings to form a hybrid index. During inference, an LLM agent dynamically interacts with the symbolic and neural indices to adaptively retrieve only the evidence needed for the query, rather than using a fixed top-k.

Result: Across MMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA, LAD-RAG achieves over 90% perfect recall on average without top-k tuning, improves recall by up to 20% at comparable noise versus baseline retrievers, and yields higher QA accuracy with minimal latency.

Conclusion: Integrating document layout structure with dynamic, query-contingent retrieval substantially improves evidence coverage and QA performance for multi-page, visually rich documents without tuning overhead, positioning LAD-RAG as an efficient and effective approach for VRD QA.

Abstract: Question answering over visually rich documents (VRDs) requires reasoning not
only over isolated content but also over documents' structural organization and
cross-page dependencies. However, conventional retrieval-augmented generation
(RAG) methods encode content in isolated chunks during ingestion, losing
structural and cross-page dependencies, and retrieve a fixed number of pages at
inference, regardless of the specific demands of the question or context. This
often results in incomplete evidence retrieval and degraded answer quality for
multi-page reasoning tasks. To address these limitations, we propose LAD-RAG, a
novel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs
a symbolic document graph that captures layout structure and cross-page
dependencies, adding it alongside standard neural embeddings to yield a more
holistic representation of the document. During inference, an LLM agent
dynamically interacts with the neural and symbolic indices to adaptively
retrieve the necessary evidence based on the query. Experiments on
MMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG
improves retrieval, achieving over 90% perfect recall on average without any
top-k tuning, and outperforming baseline retrievers by up to 20% in recall at
comparable noise levels, yielding higher QA accuracy with minimal latency.

</details>


### [186] [Artificial Hippocampus Networks for Efficient Long-Context Modeling](https://arxiv.org/abs/2510.07318)
*Yunhao Fang,Weihao Yu,Shu Zhong,Qinghao Ye,Xuehan Xiong,Lai Wei*

Main category: cs.CL

TL;DR: Hybrid memory for long-context Transformers: keep a lossless sliding KV window as short-term memory and compress older context into a fixed-size, learnable long-term memory via an Artificial Hippocampus Network (AHN), yielding near/full-attention quality with much lower compute and memory.


<details>
  <summary>Details</summary>
Motivation: Long-sequence modeling faces a trade-off: RNN-like models are efficient but lossy with fixed-size memory, while Transformer attention is lossless but costly as context grows. Inspired by the Multi-Store Model of human memory, the work aims to combine fidelity and efficiency for scalable long-context inference.

Method: Maintain the Transformer's sliding-window KV cache as lossless short-term memory. Introduce a learnable module, the Artificial Hippocampus Network (AHN), that recurrently compresses out-of-window tokens into a fixed-size long-term memory. AHN is instantiated with modern RNN-like architectures (Mamba2, DeltaNet, Gated DeltaNet).

Result: Across LV-Eval and InfiniteBench long-context benchmarks, AHN-augmented models consistently outperform sliding-window baselines and match or surpass full attention while reducing costs. Example: augmenting Qwen2.5-3B-Instruct with AHN cuts inference FLOPs by 40.5% and KV cache memory by 74.0%, and improves LV-Eval (128k) score from 4.41 to 5.88.

Conclusion: A two-store memory framework effectively reconciles efficiency and fidelity in long-sequence modeling. By pairing a lossless short-term KV window with a compact learnable long-term memory, models achieve strong long-context performance with substantially lower compute and memory overhead. Code is released for reproducibility.

Abstract: Long-sequence modeling faces a fundamental trade-off between the efficiency
of compressive fixed-size memory in RNN-like models and the fidelity of
lossless growing memory in attention-based Transformers. Inspired by the
Multi-Store Model in cognitive science, we introduce a memory framework of
artificial neural networks. Our method maintains a sliding window of the
Transformer's KV cache as lossless short-term memory, while a learnable module
termed Artificial Hippocampus Network (AHN) recurrently compresses
out-of-window information into a fixed-size compact long-term memory. To
validate this framework, we instantiate AHNs using modern RNN-like
architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive
experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate
that AHN-augmented models consistently outperform sliding window baselines and
achieve performance comparable or even superior to full-attention models, while
substantially reducing computational and memory requirements. For instance,
augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%
and memory cache by 74.0%, while improving its average score on LV-Eval (128k
sequence length) from 4.41 to 5.88. Code is available at:
https://github.com/ByteDance-Seed/AHN.

</details>


### [187] [When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation](https://arxiv.org/abs/2510.07238)
*Xunyi Jiang,Dingyi Chang,Julian McAuley,Xin Xu*

Main category: cs.CL

TL;DR: Many widely used LLM factuality benchmarks have aged and contain outdated facts, making current evaluations unreliable. The authors quantify this aging across five benchmarks and show its impact on assessing eight LLMs, offering a retrieval pipeline, metrics, and a testbed (BenchAge).


<details>
  <summary>Details</summary>
Motivation: LLMs and real‑world facts evolve quickly, but factuality benchmarks are static and old. Continued reliance on them risks misleading conclusions about model factuality. The field lacks a systematic way to measure how benchmark staleness affects evaluation outcomes.

Method: They analyze five popular factuality benchmarks and eight LLMs released over multiple years. They build an up‑to‑date fact retrieval pipeline to verify each benchmark item against current sources and design three metrics to quantify (1) how outdated items are, and (2) how aging alters factuality scores and model rankings. They run experiments to compare evaluations with and without aged items and conduct error analyses.

Result: A substantial fraction of benchmark items are outdated. This staleness meaningfully distorts factuality assessments and can change model performance estimates and comparative rankings. The authors provide reproducible code and a testbed (BenchAge) to measure benchmark aging and its effects.

Conclusion: Benchmark aging is prevalent and undermines the reliability of LLM factuality evaluation. The provided pipeline and metrics enable auditing and updating benchmarks. The community should maintain or redesign benchmarks to be time‑aware or refreshable to ensure trustworthy factuality assessment.

Abstract: The rapid evolution of large language models (LLMs) and the real world has
outpaced the static nature of widely used evaluation benchmarks, raising
concerns about their reliability for evaluating LLM factuality. While
substantial works continue to rely on the popular but old benchmarks, their
temporal misalignment with real-world facts and modern LLMs, and their effects
on LLM factuality evaluation remain underexplored. Therefore, in this work, we
present a systematic investigation of this issue by examining five popular
factuality benchmarks and eight LLMs released across different years. An
up-to-date fact retrieval pipeline and three metrics are tailored to quantify
benchmark aging and its impact on LLM factuality evaluation. Experimental
results and analysis illustrate that a considerable portion of samples in the
widely used factuality benchmarks are outdated, leading to unreliable
assessments of LLM factuality. We hope our work can provide a testbed to assess
the reliability of a benchmark for LLM factuality evaluation and inspire more
research on the benchmark aging issue. Codes are available in
https://github.com/JiangXunyi/BenchAge.

</details>


### [188] [Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts](https://arxiv.org/abs/2510.07239)
*Christos Ziakas,Nicholas Loo,Nishita Jain,Alessandra Russo*

Main category: cs.CL

TL;DR: Red-Bandit is an adaptive automated red-teaming framework that trains attack-style LoRA experts with RL and uses a multi-armed bandit at inference to target model-specific vulnerabilities, achieving SOTA attack success while keeping prompts readable.


<details>
  <summary>Details</summary>
Motivation: Automated red-teaming often fails to adapt in real time to a target model’s idiosyncratic weaknesses. The authors aim to make auditing scalable and more effective by dynamically discovering which attack styles best elicit unsafe behavior from different LLMs.

Method: Post-train multiple parameter-efficient LoRA experts, each dedicated to an attack style (e.g., manipulation, slang). Use reinforcement learning with rewards from a rule-based safety model to encourage generating prompts that induce unsafe responses. At inference, deploy a multi-armed bandit policy to select among experts based on observed response safety, balancing exploration and exploitation and providing signals about vulnerable styles.

Result: On AdvBench, the approach reaches state-of-the-art ASR@10 under sufficient exploration and produces more human-readable prompts (lower perplexity). The bandit’s arm-selection patterns also reveal which attack styles most effectively induce unsafe responses for a given target model.

Conclusion: An online-adaptive red-teaming pipeline that is effective and interpretable, improving attack success and readability and offering diagnostic insights into model-specific weaknesses. Its performance depends on exploration strategy and the accuracy of the rule-based safety reward.

Abstract: Automated red-teaming has emerged as a scalable approach for auditing Large
Language Models (LLMs) prior to deployment, yet existing approaches lack
mechanisms to efficiently adapt to model-specific vulnerabilities at inference.
We introduce Red-Bandit, a red-teaming framework that adapts online to identify
and exploit model failure modes under distinct attack styles (e.g.,
manipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA
experts, each specialized for a particular attack style, using reinforcement
learning that rewards the generation of unsafe prompts via a rule-based safety
model. At inference, a multi-armed bandit policy dynamically selects among
these attack-style experts based on the target model's response safety,
balancing exploration and exploitation. Red-Bandit achieves state-of-the-art
results on AdvBench under sufficient exploration (ASR@10), while producing more
human-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy
serves as a diagnostic tool for uncovering model-specific vulnerabilities by
indicating which attack styles most effectively elicit unsafe behaviors.

</details>


### [189] [Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense](https://arxiv.org/abs/2510.07242)
*Leitian Tao,Ilia Kulikov,Swarnadeep Saha,Tianlu Wang,Jing Xu,Yixuan Li,Jason E Weston,Ping Yu*

Main category: cs.CL

TL;DR: HERO is a reinforcement-learning framework that blends binary verifier feedback with continuous reward-model scores using stratified normalization and variance-aware weighting, yielding more informative yet stable rewards and improving LLM mathematical reasoning over RM-only and verifier-only baselines.


<details>
  <summary>Details</summary>
Motivation: Binary verifiers provide reliable but brittle 0/1 signals that under-credit partially correct or alternative answers, limiting learning. Reward models offer nuanced, continuous feedback but can be noisy. A hybrid approach aims to preserve verifier correctness while exploiting reward-model granularity.

Method: Integrate verifier signals with reward-model scores in RL: (1) stratified normalization that bounds and calibrates reward-model scores within groups defined by verifier outcomes, maintaining correctness constraints while differentiating quality; (2) variance-aware weighting that emphasizes prompts where reward signals are especially informative (high variance or difficulty). Applied to mathematical reasoning tasks.

Result: Across diverse math reasoning benchmarks, HERO consistently surpasses both reward-model-only and verifier-only training, with notable gains on tasks that are fully verifiable and those that are hard to verify.

Conclusion: Hybrid reward design combining verifiers with calibrated reward-model signals can retain the stability of verifiers while leveraging nuanced feedback, advancing LLM reasoning performance.

Abstract: Post-training for reasoning of large language models (LLMs) increasingly
relies on verifiable rewards: deterministic checkers that provide 0-1
correctness signals. While reliable, such binary feedback is brittle--many
tasks admit partially correct or alternative answers that verifiers
under-credit, and the resulting all-or-nothing supervision limits learning.
Reward models offer richer, continuous feedback, which can serve as a
complementary supervisory signal to verifiers. We introduce HERO (Hybrid
Ensemble Reward Optimization), a reinforcement learning framework that
integrates verifier signals with reward-model scores in a structured way. HERO
employs stratified normalization to bound reward-model scores within
verifier-defined groups, preserving correctness while refining quality
distinctions, and variance-aware weighting to emphasize challenging prompts
where dense signals matter most. Across diverse mathematical reasoning
benchmarks, HERO consistently outperforms RM-only and verifier-only baselines,
with strong gains on both verifiable and hard-to-verify tasks. Our results show
that hybrid reward design retains the stability of verifiers while leveraging
the nuance of reward models to advance reasoning.

</details>


### [190] [Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models](https://arxiv.org/abs/2510.07248)
*Jonggeun Lee,Woojung Song,Jongwook Han,Haesung Pyun,Yohan Jo*

Main category: cs.CL

TL;DR: PA-Tool is a training-free method that renames tool schemas to match small language models’ pretraining-induced naming expectations, sharply improving tool-use without retraining.


<details>
  <summary>Details</summary>
Motivation: SLMs are attractive for tool-augmented systems due to efficiency, but they frequently fail by hallucinating plausible yet nonexistent tool/parameter names—a schema misalignment rooted in pretraining priors. Rather than retraining models to arbitrary schemas, adapt the schemas to the models’ existing knowledge.

Method: Use a contamination-detection signal (“peakedness”) as a proxy for model familiarity with candidate names. Generate multiple renaming candidates for tool components (tools, functions, parameters) and select those with highest output concentration across samples, yielding a pretraining-aligned schema. No finetuning; works as a preprocessing layer. Evaluated on MetaTool and RoTBench.

Result: Up to +17 percentage points overall improvement and ~80% reduction in schema-misalignment errors. SLMs approach SOTA tool-use performance while maintaining computational efficiency and requiring no retraining for new tools.

Conclusion: Schema-level interventions—specifically aligning tool names/parameters to models’ pretraining priors—can unlock SLM tool-use capabilities. PA-Tool provides a practical, training-free path to better tool selection/parameterization and faster adaptation to new tools.

Abstract: Small language models (SLMs) offer significant computational advantages for
tool-augmented AI systems, yet they struggle with tool-use tasks, particularly
in selecting appropriate tools and identifying correct parameters. A common
failure mode is schema misalignment: models hallucinate plausible but
non-existent tool names that reflect naming conventions internalized during
pretraining but absent from the provided tool schema. Rather than forcing
models to adapt to arbitrary schemas, we propose adapting schemas to align with
models' pretrained knowledge. We introduce PA-Tool (Pretraining-Aligned Tool
Schema Generation), a training-free method that leverages peakedness-a signal
from contamination detection indicating pretraining familiarity-to
automatically rename tool components. By generating multiple candidates and
selecting those with highest output concentration across samples, PA-Tool
identifies pretrain-aligned naming patterns. Experiments on MetaTool and
RoTBench show improvements of up to 17% points, with schema misalignment errors
reduced by 80%. PA-Tool enables small models to approach state-of-the-art
performance while maintaining computational efficiency for adaptation to new
tools without retraining. Our work demonstrates that schema-level interventions
can unlock the tool-use potential of resource-efficient models by adapting
schemas to models rather than models to schemas.

</details>


### [191] [On the Convergence of Moral Self-Correction in Large Language Models](https://arxiv.org/abs/2510.07290)
*Guangliang Liu,Haitao Mao,Bochuan Cao,Zhiyu Xue,Xitong Zhang,Rongrong Wang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: Intrinsic moral self-correction in LLMs converges over multiple rounds because repeated self-correction instructions activate stable moral concepts that reduce uncertainty.


<details>
  <summary>Details</summary>
Motivation: Despite widespread use of self-correction prompts, it is unclear how and why LLMs improve without explicit error feedback. The paper seeks a mechanistic account, focusing on the moral domain where normative concepts may guide revisions.

Method: Study multi-round self-correction on moral tasks. Empirically track performance across rounds and analyze internal behavior to explain convergence, arguing that repeated generic correction instructions activate moral concepts that persist and reduce uncertainty.

Result: Performance improves and then stabilizes (converges) over successive self-correction rounds. Analysis links this to the sustained activation of moral concepts and decreased predictive uncertainty as rounds progress.

Conclusion: Intrinsic moral self-correction exhibits a desirable convergence property: repeated generic correction prompts prime stable moral concepts, lowering uncertainty and yielding consistent performance. This suggests designing prompts to activate relevant concepts can systematically enhance reliability.

Abstract: Large Language Models (LLMs) are able to improve their responses when
instructed to do so, a capability known as self-correction. When instructions
provide only a general and abstract goal without specific details about
potential issues in the response, LLMs must rely on their internal knowledge to
improve response quality, a process referred to as intrinsic self-correction.
The empirical success of intrinsic self-correction is evident in various
applications, but how and why it is effective remains unknown. Focusing on
moral self-correction in LLMs, we reveal a key characteristic of intrinsic
self-correction: performance convergence through multi-round interactions; and
provide a mechanistic analysis of this convergence behavior. Based on our
experimental results and analysis, we uncover the underlying mechanism of
convergence: consistently injected self-correction instructions activate moral
concepts that reduce model uncertainty, leading to converged performance as the
activated moral concepts stabilize over successive rounds. This paper
demonstrates the strong potential of moral self-correction by showing that it
exhibits a desirable property of converged performance.

</details>


### [192] [Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning](https://arxiv.org/abs/2510.07300)
*Xue Zhang,Yunlong Liang,Fandong Meng,Songming Zhang,Kaiyu Huang,Yufeng Chen,Jinan Xu,Jie Zhou*

Main category: cs.CL

TL;DR: They introduce M-Thinker, a multilingual reasoning model trained with RL that enforces strict language consistency and aligns non‑English reasoning paths with English ones, yielding near‑perfect consistency and improved accuracy on multilingual math benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current Large Reasoning Models degrade on non‑English inputs: they mix languages between input, thoughts, and answers, and they reason less accurately than in English—hurting user experience and global deployment.

Method: Train with GRPO using two rewards: (1) Language Consistency (LC) to strictly enforce the same language across input, chain‑of‑thought, and final answer; (2) Cross‑lingual Thinking Alignment (CTA) that compares a sample’s non‑English reasoning path to its English counterpart to transfer reasoning competence. Apply iterative RL to 1.5B and 7B models (M‑Thinker).

Result: M‑Thinker‑1.5B/7B achieve nearly 100% language consistency, outperform baselines on MMATH and PolyMath, and generalize well to out‑of‑domain languages.

Conclusion: Cross‑lingual alignment plus language‑consistency rewards effectively transfer strong English reasoning to other languages, enabling multilingual, think‑then‑answer models with higher accuracy and consistent outputs.

Abstract: Large Reasoning Models (LRMs) have achieved remarkable performance on complex
reasoning tasks by adopting the "think-then-answer" paradigm, which enhances
both accuracy and interpretability. However, current LRMs exhibit two critical
limitations when processing non-English languages: (1) They often struggle to
maintain input-output language consistency; (2) They generally perform poorly
with wrong reasoning paths and lower answer accuracy compared to English. These
limitations significantly degrade the user experience for non-English speakers
and hinder the global deployment of LRMs. To address these limitations, we
propose M-Thinker, which is trained by the GRPO algorithm that involves a
Language Consistency (LC) reward and a novel Cross-lingual Thinking Alignment
(CTA) reward. Specifically, the LC reward defines a strict constraint on the
language consistency between the input, thought, and answer. Besides, the CTA
reward compares the model's non-English reasoning paths with its English
reasoning path to transfer its own reasoning capability from English to
non-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B
models not only achieve nearly 100% language consistency and superior
performance on two multilingual benchmarks (MMATH and PolyMath), but also
exhibit excellent generalization on out-of-domain languages.

</details>


### [193] [Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain](https://arxiv.org/abs/2510.07309)
*Yue Li,Ran Tao,Derek Hommel,Yusuf Denizay Dönder,Sungyong Chang,David Mimno,Unso Eun Seo Jo*

Main category: cs.CL

TL;DR: CORGI is a business-oriented text-to-SQL benchmark with synthetic enterprise-style databases and queries spanning descriptive, explanatory, predictive, and recommendational tasks, revealing that current LLMs struggle with higher-level causal, temporal, and strategic reasoning and that CORGI is ~21% harder than BIRD.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-SQL benchmarks mostly test factual retrieval of past records, whereas real-world business analytics demands prediction, causal explanation, and actionable recommendations. The authors seek to expose and measure LLM gaps relevant to practical business intelligence.

Method: They build synthetic databases modeled after enterprises (e.g., DoorDash, Airbnb, Lululemon) and craft queries across four increasing complexity tiers: descriptive, explanatory, predictive, and recommendational. They evaluate LLMs using execution success rate, compare with BIRD, and release a dataset, evaluation framework, and submission website.

Result: LLMs show marked performance degradation on higher-level business questions, failing to make accurate predictions and produce actionable plans; CORGI is about 21% more difficult than BIRD by execution success rate.

Conclusion: Business-relevant text-to-SQL requires multi-step, agentic intelligence—causal reasoning, temporal forecasting, and strategic recommendation—beyond current LLM capabilities. CORGI provides a challenging, public benchmark to spur progress toward real-world business intelligence.

Abstract: In the business domain, where data-driven decision making is crucial,
text-to-SQL is fundamental for easy natural language access to structured data.
While recent LLMs have achieved strong performance in code generation, existing
text-to-SQL benchmarks remain focused on factual retrieval of past records. We
introduce CORGI, a new benchmark specifically designed for real-world business
contexts. CORGI is composed of synthetic databases inspired by enterprises such
as Doordash, Airbnb, and Lululemon. It provides questions across four
increasingly complex categories of business queries: descriptive, explanatory,
predictive, and recommendational. This challenge calls for causal reasoning,
temporal forecasting, and strategic recommendation, reflecting multi-level and
multi-step agentic intelligence. We find that LLM performance drops on
high-level questions, struggling to make accurate predictions and offer
actionable plans. Based on execution success rate, the CORGI benchmark is about
21\% more difficult than the BIRD benchmark. This highlights the gap between
popular LLMs and the need for real-world business intelligence. We release a
public dataset and evaluation framework, and a website for public submissions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [194] [AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning](https://arxiv.org/abs/2510.06261)
*Zhanke Zhou,Chentao Cao,Xiao Feng,Xuan Li,Zongze Li,Xiangyu Lu,Jiangchao Yao,Weikai Huang,Linrui Xu,Tian Cheng,Guanyu Jiang,Yiming Zheng,Brando Miranda,Tongliang Liu,Sanmi Koyejo,Masashi Sugiyama,Bo Han*

Main category: cs.AI

TL;DR: AlphaApollo is a multi-model, tool-augmented, self-evolving reasoning agent that integrates Python computation and external retrieval with an iterative, verifiable workflow, yielding sizable accuracy gains on AIME 2024/2025 versus non-tool baselines.


<details>
  <summary>Details</summary>
Motivation: Foundation models struggle with complex reasoning due to limited intrinsic capacity and unstable test-time iteration. The paper aims to make reasoning more deliberate, grounded, and checkable by augmenting models with tools and structured iteration.

Method: Orchestrate multiple LLMs with two core tools: (i) a computation tool (Python with numerical and symbolic libraries) for exact calculations; (ii) a retrieval tool for task-relevant external information. Maintain a shared state map that records candidate solutions, executable checks, and feedback, enabling multi-round, multi-model refinement and verification.

Result: On AIME 2024/2025: Qwen2.5-14B-Instruct improves by +5.15% Average@32 and +23.34% Pass@32; Llama-3.3-70B-Instruct by +8.91% Average@32 and +26.67% Pass@32. Over 80% of tool calls execute successfully, and tool-augmented runs consistently outperform non-tool baselines.

Conclusion: Coupling LLM orchestration with computation and retrieval tools plus an iterative, verifiable state-sharing scheme improves reliability and lifts the reasoning ceiling of FMs. The approach demonstrates consistent gains on math-style benchmarks and suggests broader benefits for grounded, checkable reasoning; code/details to be released on GitHub.

Abstract: We present AlphaApollo, a self-evolving agentic reasoning system that aims to
address two bottlenecks in foundation model (FM) reasoning-limited
model-intrinsic capacity and unreliable test-time iteration. AlphaApollo
orchestrates multiple models with professional tools to enable deliberate,
verifiable reasoning. It couples (i) a computation tool (Python with numerical
and symbolic libraries) and (ii) a retrieval tool (task-relevant external
information) to execute exact calculations and ground decisions. The system
further supports multi-round, multi-model solution evolution via a shared state
map that records candidates, executable checks, and feedback for iterative
refinement. In evaluations on AIME 2024/2025 across multiple models,
AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32
for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for
Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool
calls are successfully executed, with consistent outperformance of non-tool
baselines, thereby lifting the capability ceiling of FMs. More empirical
results and implementation details will be updated at
https://github.com/tmlr-group/AlphaApollo.

</details>


### [195] [Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization](https://arxiv.org/abs/2510.06274)
*Mohammad Mahdi Samiei Paqaleh,Arash Marioriyad,Arman Tahmasebi-Zadeh,Mohamadreza Fereydooni,Mahdi Ghaznavai,Mahdieh Soleymani Baghshah*

Main category: cs.AI

TL;DR: Defines Complexity Out-of-Distribution (Complexity OoD) generalization as a concrete way to measure “reasoning”: a model should maintain performance when test items require higher minimal solution complexity than any training example. Formalizes complexity and offers practical guidance for benchmarks, supervision, inductive biases, and training; argues that scaling data alone won’t yield robust reasoning—models must allocate computation with respect to complexity.


<details>
  <summary>Details</summary>
Motivation: Reasoning with modern LLMs lacks a clear, consistent definition or metric, unlike generalization in learning. Existing OoD notions (length, compositionality) miss cases where difficulty stems from higher minimal solution complexity. The authors seek a unifying lens that connects System 1 and System 2 behavior and makes reasoning measurable and actionable.

Method: Conceptual/theoretical framework: define reasoning as Complexity OoD generalization. Formalize solution complexity via description (Kolmogorov) complexity and operational proxies (e.g., counts of objects/relations; number of reasoning steps/program length). Differentiate Complexity OoD from length/compositional OoD. Translate to practice with recommendations: design benchmarks/metrics that control and report complexity; supervise solution traces; build inductive biases for Complexity OoD; mitigate spillovers like spurious shortcuts, semantic robustness issues, catastrophic forgetting, and step-wise calibration problems.

Result: Provides a precise definition and measurement paradigm for reasoning grounded in solution complexity, unifying low-complexity (System 1) and high-complexity (System 2) regimes. Clarifies distinctions from other OoD notions and enumerates actionable evaluation/training practices. No new empirical results are reported; contributions are conceptual and prescriptive.

Conclusion: Robust reasoning equates to maintaining performance under higher solution complexity than seen in training. Achieving it requires architectures and training regimes that explicitly model and allocate computation according to complexity; simply scaling data is insufficient. The community should adopt complexity-aware benchmarks, supervision, and inductive biases to drive progress.

Abstract: Recent progress has pushed AI frontiers from pattern recognition tasks toward
problems that require step by step, System2 style reasoning, especially with
large language models. Yet, unlike learning, where generalization and out of
distribution (OoD) evaluation concepts are well formalized, there is no clear,
consistent definition or metric for reasoning ability. We propose Complexity
Out of Distribution (Complexity OoD) generalization as a framework and problem
setting to define and measure reasoning. A model exhibits Complexity OoD
generalization when it maintains performance on test instances whose minimal
required solution complexity, either representational (richer solution
structure) or computational (more reasoning steps/program length), exceeds that
of all training examples. We formalize complexity via solution description
Kolmogorov complexity and operational proxies (e.g., object/relation counts;
reasoning step counts), clarifying how Complexity OoD differs from length and
compositional OoD. This lens unifies learning and reasoning: many cases
solvable with System1 like processing at low complexity become System2 like
under complexity pressure, while System2 can be viewed as generalization over
solution structures. We translate this perspective into practice with
recommendations for operationalizing Complexity OoD across the stack:
incorporating complexity into benchmark and evaluation metric design,
rethinking supervision to target solution traces, seeking and designing
inductive biases for Complexity OoD generalization, addressing learning to
reason spillovers such as spurious shortcuts, semantic robustness, catastrophic
forgetting, and step wise calibration. Because Complexity OoD cannot be solved
by scaling data alone, progress toward robust reasoning will require
architectures and training regimes that explicitly model and allocate
computation with respect to complexity.

</details>


### [196] [BuilderBench -- A benchmark for generalist agents](https://arxiv.org/abs/2510.06288)
*Raj Ghugare,Catherine Ji,Kathryn Wantlin,Jin Schofield,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: BuilderBench is a benchmark for pre-training interactive agents via open-ended exploration to build diverse block structures, featuring a fast physics simulator, 42+ target structures, a simplified “training-wheels” setting, and six baseline implementations; current algorithms struggle, underscoring the need for scalable embodied-learning methods.


<details>
  <summary>Details</summary>
Motivation: Prevailing AI models learn mainly by imitation and post-hoc sharpening, limiting generalization to novel problems. The authors aim to catalyze scalable pre-training for agents that learn through interaction—developing exploration skills and embodied reasoning not captured by language supervision.

Method: Introduce BuilderBench: (1) a hardware-accelerated simulator of a robotic agent manipulating physical blocks; (2) a curated suite of 42+ target structures stressing physics understanding, mathematics, and long-horizon planning. Agents train via unsupervised exploration to acquire general principles, then are evaluated on building unseen targets. A “training wheels” protocol trains/evaluates on a single target. Single-file implementations of six algorithms are provided as baselines.

Result: Empirically, many tasks remain unsolved or difficult for current algorithms, indicating substantial gaps in exploration, planning, and embodied reasoning. The training-wheels variant eases the problem by focusing on a single target. Baselines are provided for reference rather than state-of-the-art performance.

Conclusion: BuilderBench offers a testbed to study and scale agent pre-training through open-ended interaction, highlighting the limitations of current methods and providing resources and protocols to spur progress toward embodied reasoning and long-horizon competence.

Abstract: Today's AI models learn primarily through mimicry and sharpening, so it is
not surprising that they struggle to solve problems beyond the limits set by
existing data. To solve novel problems, agents should acquire skills for
exploring and learning through experience. Finding a scalable learning
mechanism for developing agents that learn through interaction remains a major
open problem. In this work, we introduce BuilderBench, a benchmark to
accelerate research into agent pre-training that centers open-ended
exploration. BuilderBench requires agents to learn how to build any structure
using blocks. BuilderBench is equipped with $(1)$ a hardware accelerated
simulator of a robotic agent interacting with various physical blocks, and
$(2)$ a task-suite with over 42 diverse target structures that are carefully
curated to test an understanding of physics, mathematics, and long-horizon
planning. During training, agents have to explore and learn general principles
about the environment without any external supervision. During evaluation,
agents have to build the unseen target structures from the task suite. Solving
these tasks requires a sort of \emph{embodied reasoning} that is not reflected
in words but rather in actions, experimenting with different strategies and
piecing them together. Our experiments show that many of these tasks challenge
the current iteration of algorithms. Hence, we also provide a ``training
wheels'' protocol, in which agents are trained and evaluated to build a single
target structure from the task suite. Finally, we provide single-file
implementations of six different algorithms as a reference point for
researchers.

</details>


### [197] [Requirements for Game-Based Learning Design Framework for Information System Integration in the Context of Post-Merger Integration](https://arxiv.org/abs/2510.06302)
*Ksenija Lace,Marite Kirikova*

Main category: cs.AI

TL;DR: Proposes a game‑based learning design framework to train post‑merger information systems integration, addressing the high learning curve and low motivation observed with AMILI/AMILP. Derives requirements from learning theories, cognitive load and motivation models, and serious-game design, and outlines an iterative development/validation plan.


<details>
  <summary>Details</summary>
Motivation: Post‑merger IS integration is complex and undertrained. Although decision-support methods (AMILI/AMILP) exist, practitioners report steep learning curves and low engagement, creating a need for more effective training approaches.

Method: Conceptual synthesis and requirements engineering: review of foundational learning theories, cognitive load and motivation models, and serious game design frameworks to extract requirements for a game‑based learning design tailored to IS integration in PMI. Requirements are organized into two parts: (1) the transformation process from static method training to game-based training, and (2) the target learning experience.

Result: A requirements set for a game‑based learning design framework specific to IS integration in post‑merger contexts, structured around the transformation process and the intended learning experience. No empirical results yet.

Conclusion: Presents a plan to build and iteratively evaluate the framework through real‑world pilots, aiming to improve training effectiveness and learner motivation in PMI IS integration.

Abstract: Post-merger integration states unique challenges for professionals
responsible for information system integration aimed on alignment and
combination diverse system architectures of merging organizations. Although the
theoretical and practical guidance exists for post-merger integration on the
business level, there is a significant gap in training for information system
integration in this context. In prior research specific methods AMILI (Support
method for informed decision identification) and AMILP (Support method for
informed decision-making) were introduced for the support of information system
integration decisions in the post-merger integration. But during the practical
application was reported high learning curve and low learner motivation. This
paper explores how game-based learning design can address these limitations by
transforming static method training into engaging learning experience. The
study analyzes foundational learning theories, cognitive load and motivation
models, and serious game design frameworks to identify the essential
requirements for a game-based learning design framework tailored to information
system integration in post-merger integration. Requirements are structured in
two components: the transformation process and resulting learning experience.
The paper concludes with a plan for developing and evaluating the proposed
framework through iterative design and real-world validation.

</details>


### [198] [Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks](https://arxiv.org/abs/2510.06307)
*Wentao Deng,Jiahuan Pei,Zhiwei Xu,Zhaochun Ren,Zhumin Chen,Pengjie Ren*

Main category: cs.AI

TL;DR: They introduce a belief-aware, collaborator-selective consensus framework (BCCS) for multi‑agent NLP systems that replaces naive voting with stability‑maximizing collaborator selection and belief-calibrated consensus, yielding state‑of‑the‑art gains on MATH and MMLU.


<details>
  <summary>Details</summary>
Motivation: Current MAS consensus methods depend on simple voting and indiscriminate all-to-all collaboration, which ignores contradictions in agents’ internal beliefs and leads to unstable or suboptimal consensus. There is a need to systematically choose collaborators and account for belief consistency to stabilize consensus and improve task performance.

Method: Provide a theoretical framework to select optimal collaborators that maximize consensus stability, then instantiate it as the Belief‑Calibrated Consensus Seeking (BCCS) framework. BCCS (1) selects collaborators per agent based on the theory and (2) calibrates consensus judgments using system-internal beliefs rather than raw votes.

Result: On challenging benchmarks, BCCS surpasses prior best methods: +2.23% accuracy on MATH and +3.95% on MMLU. Code and data are released at the provided repository.

Conclusion: Selecting optimal collaborators and calibrating consensus by agents’ internal beliefs stabilizes consensus formation in MAS and improves accuracy on difficult NLP tasks. Theoretical guidance plus belief calibration is a practical path to more reliable multi-agent consensus.

Abstract: A multi-agent system (MAS) enhances its capacity to solve complex natural
language processing (NLP) tasks through collaboration among multiple agents,
where consensus-seeking serves as a fundamental mechanism. However, existing
consensus-seeking approaches typically rely on voting mechanisms to judge
consensus, overlooking contradictions in system-internal beliefs that
destabilize the consensus. Moreover, these methods often involve agents
updating their results through indiscriminate collaboration with every other
agent. Such uniform interaction fails to identify the optimal collaborators for
each agent, hindering the emergence of a stable consensus. To address these
challenges, we provide a theoretical framework for selecting optimal
collaborators that maximize consensus stability. Based on the theorems, we
propose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate
stable consensus via selecting optimal collaborators and calibrating the
consensus judgment by system-internal beliefs. Experimental results on the MATH
and MMLU benchmark datasets demonstrate that the proposed BCCS framework
outperforms the best existing results by 2.23% and 3.95% of accuracy on
challenging tasks, respectively. Our code and data are available at
https://github.com/dengwentao99/BCCS.

</details>


### [199] [Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?](https://arxiv.org/abs/2510.06410)
*Aochong Oliver Li,Tanya Goyal*

Main category: cs.AI

TL;DR: They introduce off-trajectory reasoning and two tests—Recoverability (resisting/undoing misleading traces) and Guidability (building on correct collaborator steps)—to evaluate collaborative reasoning in LLMs. Across 15 open-weight models (1.5B–32B), stronger benchmark models are paradoxically more distractible, and none effectively leverage guidance beyond their native ability (<9.2% solve). Teacher weaknesses in recoverability transfer through distillation. The work exposes limits of current solo-reasoning training and sets evaluation ground rules for multi-model collaboration.


<details>
  <summary>Details</summary>
Motivation: Reasoning LLMs now expose their chain-of-thought, enabling potential collaboration where models read and augment each other’s partial reasoning. Realizing this requires models that can ignore misleading traces and use correct partial solutions—capabilities not guaranteed by standard training. The paper asks whether current solo-reasoning pipelines already yield such off-trajectory behaviors.

Method: Define off-trajectory reasoning and propose twin evaluations: (1) Recoverability—can a model backtrack from deliberate distractions in a shared trajectory? (2) Guidability—can a model build upon correct intermediate steps from a stronger collaborator. Evaluate 15 open-weight LLMs spanning 1.5B–32B params. Run control studies to isolate post-training factors: distillation teacher choice, use of RL, and data selection strategy.

Result: - Stronger models on standard benchmarks often show worse recoverability; they are more fragile to misleading traces.
- All tested models fail to capitalize on guiding steps from stronger collaborators for tasks beyond their inherent capability; solve rates stay under 9.2%.
- Distillation transfers teacher recoverability flaws to students even when the distilled trajectories are correct. Additional controls examine RL and data selection effects.

Conclusion: Standard solo-reasoning training does not reliably produce models that are robust to distractors or that can exploit collaborators’ correct reasoning. Benchmark strength overestimates collaborative reliability. Training pipelines must explicitly target recoverability/guidability and ensure teacher robustness. The paper establishes evaluation protocols and highlights current limitations of off-the-shelf reasoning LLMs for shared-trajectory collaboration.

Abstract: Reasoning LLMs are trained to verbalize their reasoning process, yielding
strong gains on complex tasks. This transparency also opens a promising
direction: multiple reasoners can directly collaborate on each other's thinking
within a shared trajectory, yielding better inference efficiency and
exploration. A key prerequisite, however, is the ability to assess the
usefulness and build on another model's partial thinking -- we call this
off-trajectory reasoning. Our paper investigates a critical question: can
standard solo-reasoning training pipelines deliver desired off-trajectory
behaviors? We propose twin tests that capture the two extremes of the
off-trajectory spectrum, namely Recoverability, which tests whether LLMs can
backtrack from "distractions" induced by misleading reasoning traces, and
Guidability, which tests their ability to build upon correct reasoning from
stronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and
reveals a counterintuitive finding -- "stronger" LLMs on benchmarks are often
more fragile under distraction. Moreover, all models tested fail to effectively
leverage guiding steps from collaborators on problems beyond their inherent
capabilities with solve rates remaining under 9.2%. Finally, we conduct control
studies to isolate the effects of three factors in post-training on these
behaviors: the choice of distillation teacher, the use of RL, and data
selection strategy. Our results provide actionable insights for training
natively strong reasoning collaborators; e.g., we find that suboptimal
recoverability behaviors of teacher models are transferred to distilled
students even if the distillation trajectories are correct. Taken together,
this work lays the groundwork for evaluating multi-model collaborations in
shared reasoning trajectories and highlights the limitations of off-the-shelf
reasoning LLMs.

</details>


### [200] [Flavonoid Fusion: Creating a Knowledge Graph to Unveil the Interplay Between Food and Health](https://arxiv.org/abs/2510.06433)
*Aryan Singh Dalal,Yinglun Zhang,Duru Doğan,Atalay Mert İleri,Hande Küçük McGinty*

Main category: cs.AI

TL;DR: Proof-of-concept knowledge graph that semantically links food (flavonoid content from USDA) to health (cancer-related literature) using KNARM, enabling machine-readable exploration of diet–disease relationships.


<details>
  <summary>Details</summary>
Motivation: Although “food as medicine” is a growing focus, there is no standardized, machine-readable way to represent and integrate evidence about food–health relationships across data sources.

Method: Integrate USDA flavonoid databases with cancer associations extracted from literature; model entities/relations using KNARM methodology; publish as a semantic web/knowledge graph for machine operation.

Result: A working knowledge graph that exemplifies how to connect dietary components with disease-related evidence, allowing researchers to query and explore complex diet–disease links.

Conclusion: The study demonstrates feasibility and utility of a semantic, interoperable representation of food–health knowledge; future work includes expanding coverage, adding nuance, and running inference to discover hidden relationships.

Abstract: The focus on "food as medicine" is gaining traction in the field of health
and several studies conducted in the past few years discussed this aspect of
food in the literature. However, very little research has been done on
representing the relationship between food and health in a standardized,
machine-readable format using a semantic web that can help us leverage this
knowledge effectively. To address this gap, this study aims to create a
knowledge graph to link food and health through the knowledge graph's ability
to combine information from various platforms focusing on flavonoid contents of
food found in the USDA databases and cancer connections found in the
literature. We looked closely at these relationships using KNARM methodology
and represented them in machine-operable format. The proposed knowledge graph
serves as an example for researchers, enabling them to explore the complex
interplay between dietary choices and disease management. Future work for this
study involves expanding the scope of the knowledge graph by capturing nuances,
adding more related data, and performing inferences on the acquired knowledge
to uncover hidden relationships.

</details>


### [201] [PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles](https://arxiv.org/abs/2510.06475)
*Yitao Long,Yuru Jiang,Hongjun Liu,Yilun Zhao,Jingchen Sun,Yiqiu Shen,Chen Zhao,Arman Cohan,Dennis Shasha*

Main category: cs.AI

TL;DR: PuzzlePlex is a scalable, extensible multi-game benchmark to evaluate and stress-test foundation models’ reasoning and planning, showing reasoning-focused models lead in instruction use while code execution is harder but a promising, efficient path.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks inadequately probe reasoning/planning in complex, dynamic settings and lack systematic analysis of scaling and the gap between natural-language (instruction) and programmatic (code) interactions. A unified, extensible suite with fine-grained metrics is needed to guide model improvements.

Method: Introduce PuzzlePlex with 15 puzzle/game types (deterministic/stochastic; single- and two-player) and a full environment for each, plus extensibility to harder instances. Provide custom game-playing baselines, design fine-grained performance metrics, and evaluate frontier foundation models under instruction-based vs. code-based settings while examining scaling behavior.

Result: Reasoning-specialized models outperform others in instruction-based play. Code-based execution poses greater difficulty but scales better and is more efficient to run. The study characterizes scaling limits across puzzles and settings.

Conclusion: PuzzlePlex enables targeted, extensible evaluation of reasoning, planning, and generalization, highlighting that while instruction-following favors reasoning-tuned models, code-driven interaction—though challenging—offers a scalable, efficient alternative to push capabilities forward.

Abstract: This work investigates the reasoning and planning capabilities of foundation
models and their scalability in complex, dynamic environments. We introduce
PuzzlePlex, a benchmark designed to assess these capabilities through a diverse
set of puzzles. PuzzlePlex consists of 15 types of puzzles, including
deterministic and stochastic games of varying difficulty, as well as
single-player and two-player scenarios. The PuzzlePlex framework provides a
comprehensive environment for each game, and supports extensibility to generate
more challenging instances as foundation models evolve. Additionally, we
implement customized game-playing strategies for comparison. Building on this
benchmark, we develop fine-grained metrics to measure performance and conduct
an in-depth analysis of frontier foundation models across two settings:
instruction-based and code-based. Furthermore, we systematically investigate
their scaling limits. Our findings show that reasoning models outperform others
in instruction-based settings, while code-based execution presents greater
challenges but offers a scalable and efficient alternative. PuzzlePlex enables
targeted evaluation and guides future improvements in reasoning, planning, and
generalization for foundation models.

</details>


### [202] [Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them](https://arxiv.org/abs/2510.06534)
*Jiahe Jin,Abhijay Paladugu,Chenyan Xiong*

Main category: cs.AI

TL;DR: They identify four reasoning behaviors that make agentic search work (verification, authority checks, adaptive search, error recovery) and propose Behavior Priming: synthesize trajectories that exhibit these behaviors, fine-tune (SFT) small LLMs on them, then apply RL. This outperforms training with RL alone (>35% gains on GAIA, WebWalker, HLE) and shows that behavior-rich SFT—even with incorrect final answers—yields better downstream RL performance than SFT on correct answers without those behaviors.


<details>
  <summary>Details</summary>
Motivation: Agentic search requires LLMs to plan, query, browse, and synthesize across the open web, where brittle reasoning leads to failure. Existing RL-based training often underperforms without strong priors about good reasoning. The authors aim to discover which reasoning patterns actually drive success and to bootstrap models to exhibit them before RL.

Method: 1) Build a reasoning-driven pipeline to mine/inspect successful agentic search trajectories. 2) Distill four beneficial behaviors: Information Verification, Authority Evaluation, Adaptive Search, and Error Recovery. 3) Synthesize trajectories that explicitly demonstrate these behaviors. 4) Supervised fine-tune models on these behavior-primed trajectories, then apply standard RL. 5) Evaluate on GAIA, WebWalker, and HLE; analyze exploration metrics (pass@k, entropy) and test-time scaling (trajectory length).

Result: Behavior Priming yields >35% improvements for Llama3.2-3B and Qwen3-1.7B versus direct RL training. SFT data quality in terms of explicit reasoning behaviors, not correctness of final answers, is the key factor for post-RL performance; behavior-rich but answer-wrong trajectories outperform answer-correct but behavior-poor data. Mechanistically, the primed models explore more (higher pass@k, entropy) and scale better at inference (longer, more effective trajectories).

Conclusion: Encoding desirable reasoning behaviors via SFT provides a stronger initialization for RL in agentic search, enabling better exploration and performance. Training data that models process quality (behaviors) can be more valuable than final-answer correctness. The approach generalizes across benchmarks and small models; code to be released.

Abstract: Agentic search leverages large language models (LLMs) to interpret complex
user information needs and execute a multi-step process of planning, searching,
and synthesizing information to provide answers. This paradigm introduces
unique challenges for LLMs' reasoning and agentic capabilities when interacting
with retrieval systems and the broader web. In this paper, we propose a
reasoning-driven LLM-based pipeline to study effective reasoning behavior
patterns in agentic search. Using this pipeline, we analyze successful agentic
search trajectories and identify four beneficial reasoning behaviors:
Information Verification, Authority Evaluation, Adaptive Search, and Error
Recovery. Based on these findings, we propose a technique called Behavior
Priming to train more effective agentic search models. It synthesizes agentic
search trajectories that exhibit these four behaviors and integrates them into
the agentic search model through supervised fine-tuning (SFT), followed by
standard reinforcement learning (RL). Experiments on three benchmarks (GAIA,
WebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in
Llama3.2-3B and Qwen3-1.7B compared to directly training agentic search models
with RL. Crucially, we demonstrate that the desired reasoning behaviors in the
SFT data, rather than the correctness of the final answer, is the critical
factor for achieving strong final performance after RL: fine-tuning on
trajectories with desirable reasoning behaviors but incorrect answers leads to
better performance than fine-tuning on trajectories with correct answers. Our
analysis further reveals the underlying mechanism: the introduced reasoning
behaviors endow models with more effective exploration (higher pass@k and
entropy) and test-time scaling (longer trajectories) capabilities, providing a
strong foundation for RL. Our code will be released as open source.

</details>


### [203] [Auto-Prompt Ensemble for LLM Judge](https://arxiv.org/abs/2510.06538)
*Jiajie Li,Huayi Zhang,Peng Lin,Jinjun Xiong,Wei Xu*

Main category: cs.AI

TL;DR: APE is an adaptive Auto-Prompt Ensemble that learns missing evaluation dimensions from failure cases and uses a collective-confidence ensemble to selectively incorporate them, improving LLM-judge reliability (e.g., GPT-4o RewardBench agreement 87.2%→90.5% in zero-shot).


<details>
  <summary>Details</summary>
Motivation: LLM judges often overlook implicit human standards and key evaluation dimensions, leading to unreliable or incomplete assessments. The goal is to capture these missing dimensions and better align LLM judges with human judgments.

Method: Auto-Prompt Ensemble (APE) automatically mines new evaluation dimensions from prior failure cases, then applies a confidence-based ensemble—using a novel Collective Confidence estimator—to decide when additional dimension-specific judgments should override or augment the base judgment at test time.

Result: Across multiple standard benchmarks, APE consistently increases agreement with human judges; notably, on RewardBench in a zero-shot setting it raises GPT-4o agreement from 87.2% to 90.5%, indicating more reliable evaluations.

Conclusion: APE provides a principled, test-time computation framework that narrows the gap between human and LLM judges by adaptively adding learned evaluation dimensions and adopting them when confidence warrants, thereby improving reliability.

Abstract: We present a novel framework that improves the reliability of LLM judges by
selectively augmenting LLM with auxiliary evaluation dimensions. Existing LLM
judges often miss crucial evaluation dimensions because they fail to recognize
the implicit standards underlying human assessments. To address this challenge,
we propose the Auto-Prompt Ensemble (APE), an adaptive framework that
automatically learns evaluation dimensions from its failure cases. APE
incorporates a confidence-based ensemble mechanism to decide when to adopt the
judgments from additional evaluation dimensions through a novel confidence
estimation approach called Collective Confidence. Extensive experiments
demonstrate that APE improves the reliability of LLM Judge across diverse
standard benchmarks. For instance, APE enhances GPT-4o agreement rate on Reward
Bench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a
principled approach for LLM Judge to leverage test-time computation, and bridge
the evaluation gap between human and LLM judges.

</details>


### [204] [WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks](https://arxiv.org/abs/2510.06587)
*Jingbo Yang,Bairu Hou,Wei Wei,Shiyu Chang,Yujia Bao*

Main category: cs.AI

TL;DR: WebDART lets a single LLM solve complex web chores by dynamically decomposing goals into navigation, information extraction, and execution, and by continually replanning as new pages/shortcuts appear—yielding up to +13.7pp success on WebChoreArena, parity on WebArena, and up to 14.7 fewer navigation steps.


<details>
  <summary>Details</summary>
Motivation: Current LLM web agents handle simple actions but fail on long-horizon navigation, large-scale information extraction, and constraint-based reasoning. A general, scalable approach is needed to manage task complexity and adapt to dynamically revealed website structure.

Method: Use one LLM that (1) dynamically decomposes each objective into three focused subtasks—navigation, information extraction, execution—so the model concentrates on one competency at a time; and (2) continually replans the subtask sequence as new webpages expose filters/shortcuts, reducing redundant exploration.

Result: On WebChoreArena, WebDART improves success rates by up to 13.7 percentage points over prior SOTA agents; on the easier WebArena suite it matches their performance and completes tasks with up to 14.7 fewer navigation steps.

Conclusion: Dynamic subtask decomposition plus continual replanning enables a single LLM agent to tackle complex, long-horizon web tasks more effectively and efficiently without sacrificing performance on simpler tasks.

Abstract: Large language model (LLM) agents are becoming competent at straightforward
web tasks, such as opening an item page or submitting a form, but still
struggle with objectives that require long horizon navigation, large scale
information extraction, and reasoning under constraints. We present WebDART, a
general framework that enables a single LLM to handle such complex chores.
WebDART (i) dynamically decomposes each objective into three focused subtasks:
navigation, information extraction, and execution, so the model concentrates on
one skill at a time, and (ii) continuously replans the decomposition as new
webpages are revealed, taking advantage of newly discovered filters or
shortcuts and avoiding redundant exploration. Evaluated on WebChoreArena,
WebDART lifts success rates by up to 13.7 percentage points over previous SOTA
agents, while matching their performance on the easier WebArena suite and
completing tasks with up to 14.7 fewer navigation steps.

</details>


### [205] [Fine-Grained Emotion Recognition via In-Context Learning](https://arxiv.org/abs/2510.06600)
*Zhaochun Ren,Zhou Yang,Chenglong Ye,Haizhou Sun,Chao Chen,Xiaofei Zhu,Xiangwen Liao*

Main category: cs.AI

TL;DR: They argue ICL for fine-grained emotion recognition fails when semantically similar examples carry different emotions. Viewing ICL as prototype matching, they introduce Emotion ICL (EICL) that selects emotionally similar examples, applies dynamic soft labels, and uses a two-stage exclusion strategy to improve both reasoning representations and the final decision, yielding significant gains over standard ICL across datasets.


<details>
  <summary>Details</summary>
Motivation: Current ICL methods strengthen reasoning via semantically similar exemplars and explanations but largely ignore the decision stage. Because ICL effectively matches a query to internal emotion prototypes, accurate emotion-centric representations are crucial. Semantically similar but emotionally mismatched examples distort representations and cause wrong decisions; hence, a method that prioritizes emotional similarity is needed.

Method: Adopt a prototype-theory perspective of ICL: decisions arise from similarity between query representations and emotion prototypes. Propose EICL that (1) retrieves emotionally similar in-context examples rather than merely semantically similar ones, (2) applies a dynamic soft-label strategy to refine the query’s emotion representation during reasoning, and (3) employs a two-stage exclusion mechanism to filter and assess examples from multiple similarity dimensions, thereby optimizing decision-making.

Result: Across multiple fine-grained emotion datasets, EICL consistently and significantly outperforms conventional ICL baselines. Improvements stem from better emotion-aligned representations and reduced errors from emotionally mismatched exemplars.

Conclusion: Decision-making is a critical, under-explored component of emotion recognition with ICL. Modeling ICL as prototype matching and aligning examples by emotion (with soft labels and multi-angle exclusion) improves both reasoning quality and final predictions; EICL is an effective, general strategy for fine-grained emotion tasks.

Abstract: Fine-grained emotion recognition aims to identify the emotional type in
queries through reasoning and decision-making processes, playing a crucial role
in various systems. Recent methods use In-Context Learning (ICL), enhancing the
representation of queries in the reasoning process through semantically similar
examples, while further improving emotion recognition by explaining the
reasoning mechanisms. However, these methods enhance the reasoning process but
overlook the decision-making process. This paper investigates decision-making
in fine-grained emotion recognition through prototype theory. We show that ICL
relies on similarity matching between query representations and emotional
prototypes within the model, where emotion-accurate representations are
critical. However, semantically similar examples often introduce emotional
discrepancies, hindering accurate representations and causing errors. To
address this, we propose Emotion In-Context Learning (EICL), which introduces
emotionally similar examples and uses a dynamic soft-label strategy to improve
query representations in the emotion reasoning process. A two-stage exclusion
strategy is then employed to assess similarity from multiple angles, further
optimizing the decision-making process. Extensive experiments show that EICL
significantly outperforms ICL on multiple datasets.

</details>


### [206] [Agent-in-the-Loop: A Data Flywheel for Continuous Improvement in LLM-based Customer Support](https://arxiv.org/abs/2510.06674)
*Cen,Zhao,Tiantian Zhang,Hanchen Su,Yufeng,Zhang,Shaowei Su,Mingzhi Xu,Yu,Liu,Wei Han,Jeremy Werner,Claire Na Cheng,Yashar Mehdad*

Main category: cs.AI

TL;DR: Agent-in-the-Loop (AITL) creates a live feedback flywheel that continuously upgrades an LLM-based customer support system using four in-flow annotation types, shortening retraining cycles and boosting retrieval, generation quality, and agent adoption.


<details>
  <summary>Details</summary>
Motivation: Offline, batch-annotated improvement loops are slow, infrequent, and disconnected from real operations, leading to stale models and low trust/adoption. The paper aims to create a faster, operationally embedded feedback mechanism to continuously refine LLM support systems.

Method: Deploy an AITL framework in production that captures four live feedback signals: pairwise response preferences, agent adoption plus rationales, knowledge relevance checks, and identification of missing knowledge. These signals are fed into ongoing model and knowledge updates, forming a continuous data flywheel that replaces long offline retraining cycles.

Result: In a production pilot with US-based support agents, AITL improved retrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation helpfulness (+8.4%), and agent adoption (+4.5%), while cutting retraining cycles from months to weeks.

Conclusion: Embedding human feedback directly into operational workflows is effective for continuously refining LLM-based customer support, yielding faster iteration and measurable gains in retrieval, response quality, and system adoption.

Abstract: We introduce an Agent-in-the-Loop (AITL) framework that implements a
continuous data flywheel for iteratively improving an LLM-based customer
support system. Unlike standard offline approaches that rely on batch
annotations, AITL integrates four key types of annotations directly into live
customer operations: (1) pairwise response preferences, (2) agent adoption and
rationales, (3) knowledge relevance checks, and (4) identification of missing
knowledge. These feedback signals seamlessly feed back into models' updates,
reducing retraining cycles from months to weeks. Our production pilot involving
US-based customer support agents demonstrated significant improvements in
retrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation quality
(+8.4% helpfulness) and agent adoption rates (+4.5%). These results underscore
the effectiveness of embedding human feedback loops directly into operational
workflows to continuously refine LLM-based customer support system.

</details>


### [207] [Inefficiencies of Meta Agents for Agent Design](https://arxiv.org/abs/2510.06711)
*Batu El,Mert Yuksekgonul,James Zou*

Main category: cs.AI

TL;DR: Automated meta-agents for designing agent architectures face three core issues: naive iteration memory hurts performance, evolutionary search helps; designed agents lack behavioral diversity, limiting ensemble gains; and cost-effectiveness appears only in rare, large-scale deployments on select datasets.


<details>
  <summary>Details</summary>
Motivation: Assess whether meta-agent-driven automated agent design actually improves performance, diversity, and economic viability over human-designed systems, and understand how to make iterative learning more effective.

Method: Empirical analysis of a common meta-agent pipeline. Compare three iteration strategies: (1) expanding the prompt with all prior designs, (2) ignoring history, and (3) an evolutionary approach. Measure behavioral diversity among designed agents and evaluate total cost of design plus deployment versus human-designed baselines across multiple datasets and scales.

Result: Including all prior designs in context underperforms even ignoring them; an evolutionary approach yields better performance. The designed agents show low behavioral diversity, limiting complementary use at test time. Economically, automated design beats human-designed agents only on two datasets and only when deployed at very large scale (>15,000 examples); elsewhere, gains do not offset design costs.

Conclusion: For meta-agent design, simple context accumulation is counterproductive; evolutionary search is preferable. The low diversity of produced agents curtails ensemble benefits, and the return on investment is narrow—automated design is only worthwhile in specific, large-scale settings. Improving diversity and reducing design costs are key to broader viability.

Abstract: Recent works began to automate the design of agentic systems using
meta-agents that propose and iteratively refine new agent architectures. In
this paper, we examine three key challenges in a common class of meta-agents.
First, we investigate how a meta-agent learns across iterations and find that
simply expanding the context with all previous agents, as proposed by previous
works, performs worse than ignoring prior designs entirely. We show that the
performance improves with an evolutionary approach. Second, although the
meta-agent designs multiple agents during training, it typically commits to a
single agent at test time. We find that the designed agents have low behavioral
diversity, limiting the potential for their complementary use. Third, we assess
when automated design is economically viable. We find that only in a few
cases--specifically, two datasets--the overall cost of designing and deploying
the agents is lower than that of human-designed agents when deployed on over
15,000 examples. In contrast, the performance gains for other datasets do not
justify the design cost, regardless of scale.

</details>


### [208] [MultiCNKG: Integrating Cognitive Neuroscience, Gene, and Disease Knowledge Graphs Using Large Language Models](https://arxiv.org/abs/2510.06742)
*Ali Sarabadani,Kheirolah Rahsepar Fard*

Main category: cs.AI

TL;DR: MultiCNKG integrates cognitive neuroscience, gene, and disease ontologies into a single KG using LLM-assisted alignment and augmentation, yielding a 6.9K-node, 11.3K-edge graph with strong intrinsic quality metrics and competitive link-prediction performance for biomedical–cognitive applications.


<details>
  <summary>Details</summary>
Motivation: Existing KGs in neuroscience and biomedicine are siloed and traditional ML struggles to capture cross-domain, high-order semantics among genes, diseases, and cognitive processes. A unified, multi-layered KG could enable reasoning from molecular mechanisms to cognition, and recent LLMs can help align entities, infer semantic links, and fill gaps across heterogeneous sources.

Method: Combine three sources—CNKG (2.9K nodes/4.3K edges; 9 node and 20 edge types), Gene Ontology (43K/75K; 3 node and 4 edge types), and Disease Ontology (11.2K/8.8K; 1 node and 2 edge types). Use GPT-4 to perform entity alignment across ontologies, compute semantic similarity, and augment the graph with inferred relations. Construct MultiCNKG with 5 node types (e.g., Genes, Diseases, Cognitive Processes) and 7 relation types (e.g., Causes, Associated with, Regulates). Evaluate with precision, recall, coverage, graph consistency, novelty detection, and expert validation, and test link prediction using TransE and RotatE against standard benchmarks (FB15k-237, WN18RR).

Result: MultiCNKG totals 6.9K nodes and 11.3K edges, offering a layered representation from molecular to behavioral levels. Reported metrics: precision 85.20%, recall 87.30%, coverage 92.18%, graph consistency 82.50%, novelty detection 40.28%, and expert validation 89.50%. Link prediction achieves MR 391/MRR 0.411 with TransE and MR 263/MRR 0.395 with RotatE, claimed as competitive relative to FB15k-237 and WN18RR baselines.

Conclusion: LLM-guided integration yields a coherent, cross-domain KG that connects genes, diseases, and cognitive processes, supporting applications in personalized medicine, cognitive disorder diagnostics, and hypothesis generation. Results suggest robustness and utility, though broader validation and scalability to larger or continuously updated biomedical corpora remain important next steps.

Abstract: The advent of large language models (LLMs) has revolutionized the integration
of knowledge graphs (KGs) in biomedical and cognitive sciences, overcoming
limitations in traditional machine learning methods for capturing intricate
semantic links among genes, diseases, and cognitive processes. We introduce
MultiCNKG, an innovative framework that merges three key knowledge sources: the
Cognitive Neuroscience Knowledge Graph (CNKG) with 2.9K nodes and 4.3K edges
across 9 node types and 20 edge types; Gene Ontology (GO) featuring 43K nodes
and 75K edges in 3 node types and 4 edge types; and Disease Ontology (DO)
comprising 11.2K nodes and 8.8K edges with 1 node type and 2 edge types.
Leveraging LLMs like GPT-4, we conduct entity alignment, semantic similarity
computation, and graph augmentation to create a cohesive KG that interconnects
genetic mechanisms, neurological disorders, and cognitive functions. The
resulting MultiCNKG encompasses 6.9K nodes across 5 types (e.g., Genes,
Diseases, Cognitive Processes) and 11.3K edges spanning 7 types (e.g., Causes,
Associated with, Regulates), facilitating a multi-layered view from molecular
to behavioral domains. Assessments using metrics such as precision (85.20%),
recall (87.30%), coverage (92.18%), graph consistency (82.50%), novelty
detection (40.28%), and expert validation (89.50%) affirm its robustness and
coherence. Link prediction evaluations with models like TransE (MR: 391, MRR:
0.411) and RotatE (MR: 263, MRR: 0.395) show competitive performance against
benchmarks like FB15k-237 and WN18RR. This KG advances applications in
personalized medicine, cognitive disorder diagnostics, and hypothesis
formulation in cognitive neuroscience.

</details>


### [209] [Verifying Memoryless Sequential Decision-making of Large Language Models](https://arxiv.org/abs/2510.06756)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: They present a practical tool that formally verifies safety properties of LLM-driven policies on MDP tasks by building only the policy-reachable sub-MDP from natural-language state prompts and then model-checking PCTL properties with Storm; it integrates with Ollama/PRISM and shows verifiability under deterministic seeding, though LLM policies underperform DRL baselines.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used as decision-making controllers but lack rigorous guarantees. Traditional model checking requires explicit models and faces state explosion. The authors seek a way to automatically and soundly verify safety properties of LLM policies in sequential decision-making without constructing the full MDP and while bridging natural language interfaces with formal verification.

Method: For a given MDP, safety property (PCTL), and an LLM policy, the tool iteratively constructs only the portion of the MDP reachable under the policy: encode each state as a natural-language prompt, query the LLM to obtain an action, parse it, and expand successors per the environment dynamics. The resulting policy-induced MDP fragment is checked using the Storm model checker. The system integrates with PRISM specifications and accesses open-source LLMs via Ollama; deterministic seeding is used to stabilize the policy behavior.

Result: On grid-world benchmarks, the tool can verify open-source LLM policies when they are deterministically seeded. However, these LLM policies generally perform worse than deep RL baselines. The implementation natively supports Ollama and PRISM tasks, enabling continuous benchmarking of LLM controllers on user-defined environments.

Conclusion: It is feasible to rigorously verify safety properties of memoryless LLM policies by constructing and checking only the reachable sub-MDP. The approach provides a practical foundation for formal verification of LLM controllers, but current limitations include policy nondeterminism, performance gaps versus DRL, and reliance on memoryless settings; addressing these will be key for broader applicability.

Abstract: We introduce a tool for rigorous and automated verification of large language
model (LLM)- based policies in memoryless sequential decision-making tasks.
Given a Markov decision process (MDP) representing the sequential
decision-making task, an LLM policy, and a safety requirement expressed as a
PCTL formula, our approach incrementally constructs only the reachable portion
of the MDP guided by the LLM's chosen actions. Each state is encoded as a
natural language prompt, the LLM's response is parsed into an action, and
reachable successor states by the policy are expanded. The resulting formal
model is checked with Storm to determine whether the policy satisfies the
specified safety property. In experiments on standard grid world benchmarks, we
show that open source LLMs accessed via Ollama can be verified when
deterministically seeded, but generally underperform deep reinforcement
learning baselines. Our tool natively integrates with Ollama and supports
PRISM-specified tasks, enabling continuous benchmarking in user-specified
sequential decision-making tasks and laying a practical foundation for formally
verifying increasingly capable LLMs.

</details>


### [210] [Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration](https://arxiv.org/abs/2510.06761)
*Zhi Zhang,Yan Liu,Zhejing Hu,Gong Chen,Sheng-hua Zhong,Jiannong Cao*

Main category: cs.AI

TL;DR: DLMA is a two-loop multi-agent system that evolves high-level research plans (leader/professor loop) and executes them adaptively (follower/doctoral loop), achieving state-of-the-art automated paper-generation results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: End-to-end automation of scientific research demands both (1) generating novel, sound high-level plans and (2) reliably executing those plans under uncertainty—two tightly coupled yet distinct challenges that current systems struggle to address together.

Method: A Double-Loop Multi-Agent framework: (1) Leader loop with professor agents uses an evolutionary algorithm via involvement, improvement, and integration meetings to iteratively generate and refine a pool of research proposals; (2) Follower loop with doctoral student agents executes the best plan and dynamically adjusts it through pre-hoc and post-hoc meetings, grounding steps (drafting, coding) in contextual and external observations.

Result: On ACLAward and Laboratory benchmarks, DLMA generates research papers that obtain state-of-the-art scores under automated evaluation and significantly outperform strong baselines. Ablation studies show the leader loop drives novelty and the follower loop ensures soundness.

Conclusion: Coupling evolutionary plan generation with adaptive execution enables robust, end-to-end automated research. The two loops are complementary—evolution fosters novelty while execution secures correctness—yielding superior performance on standard benchmarks.

Abstract: Automating the end-to-end scientific research process poses a fundamental
challenge: it requires both evolving high-level plans that are novel and sound,
and executing these plans correctly amidst dynamic and uncertain conditions. To
address this bilevel challenge, we propose a novel Double-Loop Multi-Agent
(DLMA) framework to solve the given research problem automatically. The leader
loop, composed of professor agents, is responsible for evolving research plans.
It employs an evolutionary algorithm through involvement, improvement, and
integration meetings to iteratively generate and refine a pool of research
proposals, exploring the solution space effectively. The follower loop,
composed of doctoral student agents, is responsible for executing the
best-evolved plan. It dynamically adjusts the plan during implementation via
pre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is
well-supported by contextual and external observations. Extensive experiments
on benchmarks like ACLAward and Laboratory show that DLMA generates research
papers that achieve state-of-the-art scores in automated evaluation,
significantly outperforming strong baselines. Ablation studies confirm the
critical roles of both loops, with evolution driving novelty and execution
ensuring soundness.

</details>


### [211] [Autoformalizer with Tool Feedback](https://arxiv.org/abs/2510.06857)
*Qi Guo,Jianing Wang,Jianfei Zhang,Deyang Kong,Xiangzhou Huang,Xiangyu Xi,Wei Wang,Jingang Wang,Xunliang Cai,Shikun Zhang,Wei Ye*

Main category: cs.AI

TL;DR: ATF is an autoformalization system that uses tool feedback—Lean 4 compiler checks for syntax and multi-LLM judges for semantic consistency—combined with staged training (synthetic tool-calling pretrain, expert iteration, and DPO) to iteratively refine formal statements, outperforming prior formalizers and releasing a 750K-statement dataset.


<details>
  <summary>Details</summary>
Motivation: Autoformalization suffers from scarce training data and current formalizers frequently produce syntactically invalid or semantically inconsistent statements. The goal is to raise both validity and consistency while scaling performance.

Method: Introduce Autoformalizer with Tool Feedback (ATF): 1) integrate Lean 4 compiler as a syntax-correction tool; 2) use multiple LLMs as judges for semantic consistency; 3) adaptively revise outputs based on tool feedback. Training pipeline: (a) cold-start on synthetic tool-calling data, (b) expert iteration to improve formalization, (c) Direct Preference Optimization to discourage ineffective revisions.

Result: ATF significantly outperforms baseline formalizers on benchmarks, with improvements corroborated by human evaluation; exhibits strong inference scaling behavior; and provides an open-source dataset, Numina-ATF, with 750K synthetic formal statements.

Conclusion: Tool-feedback-driven autoformalization yields more syntactically valid and semantically consistent formal statements and advances state-of-the-art performance. The released dataset supports further progress in autoformalization and ATP research.

Abstract: Autoformalization addresses the scarcity of data for Automated Theorem
Proving (ATP) by translating mathematical problems from natural language into
formal statements. Efforts in recent work shift from directly prompting large
language models to training an end-to-end formalizer model from scratch,
achieving remarkable advancements. However, existing formalizer still struggles
to consistently generate valid statements that meet syntactic validity and
semantic consistency. To address this issue, we propose the Autoformalizer with
Tool Feedback (ATF), a novel approach that incorporates syntactic and
consistency information as tools into the formalization process. By integrating
Lean 4 compilers for syntax corrections and employing a multi-LLMs-as-judge
approach for consistency validation, the model is able to adaptively refine
generated statements according to the tool feedback, enhancing both syntactic
validity and semantic consistency. The training of ATF involves a cold-start
phase on synthetic tool-calling data, an expert iteration phase to improve
formalization capabilities, and Direct Preference Optimization to alleviate
ineffective revisions. Experimental results show that ATF markedly outperforms
a range of baseline formalizer models, with its superior performance further
validated by human evaluations. Subsequent analysis reveals that ATF
demonstrates excellent inference scaling properties. Moreover, we open-source
Numina-ATF, a dataset containing 750K synthetic formal statements to facilitate
advancements in autoformalization and ATP research.

</details>


### [212] [TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs](https://arxiv.org/abs/2510.06878)
*Daria Ozerova,Ekaterina Trofimova*

Main category: cs.AI

TL;DR: TGPR merges a learned policy (GRPO) with a Thompson-sampling tree search to guide iterative refinement, yielding sizable gains on code-generation benchmarks and offering a general template for stateful LLM reasoning.


<details>
  <summary>Details</summary>
Motivation: Iterative refinement for LLMs faces a vast search space and the exploration–exploitation dilemma. Heuristic strategies are brittle and cannot adapt to feedback from prior refinements, limiting effectiveness on complex reasoning and debugging tasks.

Method: Tree-Guided Policy Refinement (TGPR): integrate GRPO with a Thompson-sampling-based tree search that actively explores both successful and failed refinement branches. This produces denser training trajectories and adaptively updates the policy based on observed outcomes, balancing exploration and exploitation in a principled way.

Result: On HumanEval, MBPP, and APPS, TGPR improves over a competitive GRPO baseline: up to +4.2 percentage points in pass@1 (MBPP) and up to +12.51 percentage points in pass@10 (APPS).

Conclusion: Coupling structured search with learned policy refinement better navigates the refinement space and improves stateful reasoning. Beyond code debugging, TGPR offers a general and scalable framework for iterative refinement in LLMs.

Abstract: Iterative refinement has been a promising paradigm to enable large language
models (LLMs) to resolve difficult reasoning and problem-solving tasks. One of
the key challenges, however, is how to effectively search through the enormous
search space of possible refinements. Existing methods typically fall back on
predefined heuristics, which are troubled by the exploration-exploitation
dilemma and cannot adapt based on past refinement outcomes. We introduce
Tree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with
a Thompson-Sampling-based tree search. TGPR explores both failed and successful
refinement paths actively, with denser training trajectories and more adaptive
policies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to
+4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to
+12.51 percentage points absolute improvement in pass@10 (on APPS) compared to
a competitive GRPO baseline. Apart from debugging code, TGPR focuses on a
principled approach to combining learned policies with structured search
methods, offering a general framework for enhancing iterative refinement and
stateful reasoning in LLMs.

</details>


### [213] [LLM-Assisted Modeling of Semantic Web-Enabled Multi-Agents Systems with AJAN](https://arxiv.org/abs/2510.06911)
*Hacane Hechehouche,Andre Antakli,Matthias Klusch*

Main category: cs.AI

TL;DR: Presents an IDE for the AJAN multi‑agent framework that simplifies RDF/OWL knowledge modeling and SPARQL/Behavior Tree authoring, leveraging LLMs to reduce errors and lower the learning curve.


<details>
  <summary>Details</summary>
Motivation: Engineering AJAN agents today is hampered by error‑prone URI handling and the steep learning curve of crafting complex SPARQL queries and RDF/RDFS/OWL models, which limits adoption and productivity.

Method: Design and integration of an IDE tailored to AJAN: supports Behavior Trees tied to SPARQL for knowledge access/manipulation, provides tooling to mitigate URI and query errors, and incorporates Large Language Models to assist agent engineering.

Result: Delivers an integrated development environment that addresses common modeling hurdles and enables users to employ LLMs during AJAN agent development. (No empirical evaluation details are provided in the abstract.)

Conclusion: An IDE with LLM assistance can lower barriers to modeling AJAN agents, reduce mistakes, and broaden the framework’s user base; further validation would depend on empirical studies not described here.

Abstract: There are many established semantic Web standards for implementing
multi-agent driven applications. The AJAN framework allows to engineer
multi-agent systems based on these standards. In particular, agent knowledge is
represented in RDF/RDFS and OWL, while agent behavior models are defined with
Behavior Trees and SPARQL to access and manipulate this knowledge. However, the
appropriate definition of RDF/RDFS and SPARQL-based agent behaviors still
remains a major hurdle not only for agent modelers in practice. For example,
dealing with URIs is very error-prone regarding typos and dealing with complex
SPARQL queries in large-scale environments requires a high learning curve. In
this paper, we present an integrated development environment to overcome such
hurdles of modeling AJAN agents and at the same time to extend the user
community for AJAN by the possibility to leverage Large Language Models for
agent engineering.

</details>


### [214] [Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.06953)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.AI

TL;DR: Operationalizes UID for LLM reasoning by measuring stepwise information density via entropy and defining local and global uniformity; more uniform traces correlate with correctness and enable trace selection that improves accuracy (10–32% relative gains on AIME2025), outperforming other internal signals.


<details>
  <summary>Details</summary>
Motivation: Test whether the Uniform Information Density hypothesis applies to chain-of-thought: does maintaining a steady information rate across reasoning steps signal higher reasoning quality, and can this property be exploited to select better traces?

Method: Define an entropy-based stepwise information density for each reasoning step; compute complementary local and global uniformity scores; evaluate across six reasoning benchmarks; rank/select reasoning traces by uniformity; analyze patterns (spikes vs. bursts) and compare with alternative internal signals as predictors of correctness.

Result: Correct reasoning traces display smooth, uniform information density; incorrect traces show sharp spikes/irregular bursts. Using uniformity to select traces yields 10–32% relative accuracy gains on AIME2025 and surpasses competing internal confidence signals.

Conclusion: UID-inspired uniformity metrics are effective diagnostics and selection criteria for LLM reasoning, providing a theoretically grounded signal that improves reliability and accuracy over alternative internal measures.

Abstract: The Uniform Information Density (UID) hypothesis suggests that effective
communication maintains a stable flow of information. In this work, we revisit
this principle in the context of large language model (LLM) reasoning traces,
asking whether step-level uniformity reflects reasoning quality. To this end,
we propose an entropy-based stepwise information density metric and introduce
two complementary measures of uniformity, local and global uniformity scores.
Across the experiments on six different reasoning benchmarks, we find that
step-level uniformity not only provides a strong theoretical lens but also
yields practical performance benefits; for example, selecting reasoning traces
with more uniform information density at the step-level improves accuracy by
10-32\% relative gains over baselines at AIME2025. Our analysis further reveals
that correct reasoning traces tend to avoid sharp information density spikes,
while incorrect traces exhibit irregular information bursts. These results
demonstrate that UID-inspired information density measures outperform
alternative internal signals as predictors of reasoning quality. Results
highlight the uniformity of the information density as a robust diagnostic and
selection criterion for building more reliable and accurate reasoning systems.

</details>


### [215] [Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning](https://arxiv.org/abs/2510.07038)
*Wenxun Wu,Yuanyang Li,Guhan Chen,Linyue Wang,Hongyang Chen*

Main category: cs.AI

TL;DR: TAPO is an RL framework that teaches LLMs to interleave multi-hop reasoning with adaptive tool calls (e.g., search, Python), delivering state-of-the-art accuracy and more efficient tool usage on knowledge- and math-intensive tasks, supported by two new datasets.


<details>
  <summary>Details</summary>
Motivation: Test-time scaling helps reasoning but doesn’t give LLMs up-to-date facts or reliable computation. Models need a principled way to decide when and how to call tools without overusing them or exploiting rewards, especially for fact retrieval and complex arithmetic.

Method: Introduce Tool-Augmented Policy Optimization (TAPO), adapting Dynamic Sampling Policy Optimization (DAPO) to tool-invocation settings. The policy learns to dynamically alternate between reasoning steps and on-demand tools (search APIs, interpreters). Two datasets—TAPO-easy-60K and TAPO-hard-18K—are created to train/evaluate fact-based reasoning and mathematical calculations.

Result: On Qwen2.5-3B and Qwen2.5-7B, TAPO achieves state-of-the-art performance on tasks requiring external knowledge and math compared to parameter-matched baselines. It uses tools more efficiently and mitigates excessive tool calls arising from reward hacking.

Conclusion: Coupling advanced reasoning with adaptive tool use via RL improves accuracy and efficiency on knowledge- and computation-heavy tasks. TAPO and its datasets indicate a promising path for scalable, reliable tool-augmented LLMs.

Abstract: Recent advances in large language models (LLMs) have popularized test-time
scaling, where models generate additional reasoning tokens before producing
final answers. These approaches have demonstrated significant performance
improvements on benchmarks involving mathematical reasoning. However, language
models relying solely on direct inference still struggle with tasks demanding
up-to-date knowledge or computational tools such as calculators and code
interpreters for complex arithmetic operations. To overcome these limitations,
we propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement
learning framework that systematically integrates multi-hop reasoning with
adaptive tool-calling capabilities. Our approach employs a modified version of
Dynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,
which we adapt specifically for tool invocation scenarios, enabling models to
dynamically interleave complex reasoning with on-demand tool usage (including
search APIs and Python interpreters).
  To support this research, we introduce two new datasets: TAPO-easy-60K and
TAPO-hard-18K, specifically designed to train and evaluate both fact-based
reasoning and mathematical calculation capabilities. Our experiments on
Qwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,
with both models achieving state-of-the-art performance on tasks requiring
external knowledge and mathematical computation among methods with comparable
parameters. Notably, TAPO achieves more efficient tool utilization than
baseline methods while preventing excessive calls caused by reward hacking.
These results highlight the significant potential of combining advanced
reasoning with tool usage to enhance model performance in knowledge-intensive
and computationally demanding tasks.

</details>


### [216] [Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations](https://arxiv.org/abs/2510.07064)
*Manh Hung Nguyen,Sebastian Tschiatschek,Adish Singla*

Main category: cs.AI

TL;DR: They build a diverse set of LLM “agents,” each steered by small in-context human demonstrations, and select a representative subset via submodular optimization to better approximate the diversity of human behavior than a single LLM.


<details>
  <summary>Details</summary>
Motivation: Single LLMs are attractive proxies for human responses but tend to produce homogeneous outputs, failing to reflect the diversity of real human populations; collecting large-scale human data is costly.

Method: Generate candidate LLM agents by conditioning on small sets of task–response human demos (ICL). Cast the choice of which agents to keep as a submodular set selection problem and propose algorithms with varying time–quality trade-offs and performance guarantees to select a representative subset.

Result: Across crowdsourcing and educational tasks, the selected agent sets match population diversity more effectively than baselines and reproduce behavior patterns and perspectives of target groups on new tasks.

Conclusion: A multi-agent, submodularly selected ICL framework is an effective and scalable way to approximate population-level human diversity, outperforming single-agent or naive baselines and generalizing to new tasks.

Abstract: The difficulty and expense of obtaining large-scale human responses make
Large Language Models (LLMs) an attractive alternative and a promising proxy
for human behavior. However, prior work shows that LLMs often produce
homogeneous outputs that fail to capture the rich diversity of human
perspectives and behaviors. Thus, rather than trying to capture this diversity
with a single LLM agent, we propose a novel framework to construct a set of
agents that collectively capture the diversity of a given human population.
Each agent is an LLM whose behavior is steered by conditioning on a small set
of human demonstrations (task-response pairs) through in-context learning. The
central challenge is therefore to select a representative set of LLM agents
from the exponentially large space of possible agents. We tackle this selection
problem from the lens of submodular optimization. In particular, we develop
methods that offer different trade-offs regarding time complexity and
performance guarantees. Extensive experiments in crowdsourcing and educational
domains demonstrate that our approach constructs agents that more effectively
represent human populations compared to baselines. Moreover, behavioral
analyses on new tasks show that these agents reproduce the behavior patterns
and perspectives of the students and annotators they are designed to represent.

</details>


### [217] [Inductive Learning for Possibilistic Logic Programs Under Stable Models](https://arxiv.org/abs/2510.07069)
*Hongbo Hu,Yisong Wang,Yi Huang,Kewen Wang*

Main category: cs.AI

TL;DR: They introduce the first inductive learning framework for possibilistic logic programs under stable-model semantics, define the induction task, propose two algorithms (ilpsm, ilpsmmin), implement ilpsmmin, and show it outperforms a leading ASP ILP system on random datasets when inputs are ordinary logic programs.


<details>
  <summary>Details</summary>
Motivation: Inductive reasoning for possibilistic logic programs (ASP with uncertainty via possibility degrees) had not been addressed. There is a need to learn/extract possibilistic programs from background knowledge and example fragments of intended possibilistic stable models.

Method: (1) Formally define an induction task for possibilistic stable models and study its properties. (2) Propose two algorithms—ilpsm and ilpsmmin—to compute induction solutions. (3) Implement ilpsmmin and evaluate it experimentally, including the special case where inputs are ordinary (non-possibilistic) logic programs, comparing against a major ILP system for normal logic programs from stable models.

Result: Both algorithms solve the defined induction tasks in theory; the ilpsmmin implementation empirically outperforms a leading ASP ILP system on randomly generated datasets when restricted to ordinary logic program inputs. Specific metrics aren’t stated in the abstract, but “outperforms” implies better efficiency and/or accuracy.

Conclusion: Inductive learning for possibilistic ASP is feasible and effective. The formalization and algorithms provide a basis for extracting poss-programs from examples, and the prototype demonstrates competitive performance, especially on ordinary logic program settings, opening a path for further research and optimization.

Abstract: Possibilistic logic programs (poss-programs) under stable models are a major
variant of answer set programming (ASP). While its semantics (possibilistic
stable models) and properties have been well investigated, the problem of
inductive reasoning has not been investigated yet. This paper presents an
approach to extracting poss-programs from a background program and examples
(parts of intended possibilistic stable models). To this end, the notion of
induction tasks is first formally defined, its properties are investigated and
two algorithms ilpsm and ilpsmmin for computing induction solutions are
presented. An implementation of ilpsmmin is also provided and experimental
results show that when inputs are ordinary logic programs, the prototype
outperforms a major inductive learning system for normal logic programs from
stable models on the datasets that are randomly generated.

</details>


### [218] [VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle Routing Problems](https://arxiv.org/abs/2510.07073)
*André Hottung,Federico Berto,Chuanbo Hua,Nayeli Gast Zepeda,Daniel Wetzel,Michael Römer,Haoran Ye,Davide Zago,Michael Poli,Stefano Massaroli,Jinkyoo Park,Kevin Tierney*

Main category: cs.AI

TL;DR: VRPAgent combines LLM-generated problem-specific operators with a generic metaheuristic and a genetic search to refine them, achieving state-of-the-art heuristics for several VRP variants on a single CPU core.


<details>
  <summary>Details</summary>
Motivation: Crafting high-quality VRP heuristics demands expert intuition and domain knowledge, and while LLM code generation is promising, it has not matched expert-designed methods. The authors seek a principled way to harness LLM creativity while ensuring correctness and strong performance.

Method: 1) Use an LLM to propose problem-specific heuristic operators; 2) embed these operators within a generic metaheuristic framework that enforces correctness; 3) apply a novel genetic search over LLM-generated components to refine, select, and combine operators; 4) evaluate across CVRP, VRPTW, and PCVRP. Emphasis on modularity keeps tasks manageable and correctness verifiable while enabling exploration of novel strategies.

Result: The framework discovers operators that outperform handcrafted and recent learning-based methods across multiple VRP benchmarks, reportedly setting new state-of-the-art results while running on a single CPU core.

Conclusion: LLM-assisted automated heuristic discovery, when coupled with a correctness-guarded metaheuristic and evolutionary refinement, can surpass expert and learned baselines for VRPs. This points to a promising direction for scalable, automated design of combinatorial optimization heuristics.

Abstract: Designing high-performing heuristics for vehicle routing problems (VRPs) is a
complex task that requires both intuition and deep domain knowledge. Large
language model (LLM)-based code generation has recently shown promise across
many domains, but it still falls short of producing heuristics that rival those
crafted by human experts. In this paper, we propose VRPAgent, a framework that
integrates LLM-generated components into a metaheuristic and refines them
through a novel genetic search. By using the LLM to generate problem-specific
operators, embedded within a generic metaheuristic framework, VRPAgent keeps
tasks manageable, guarantees correctness, and still enables the discovery of
novel and powerful strategies. Across multiple problems, including the
capacitated VRP, the VRP with time windows, and the prize-collecting VRP, our
method discovers heuristic operators that outperform handcrafted methods and
recent learning-based approaches while requiring only a single CPU core. To our
knowledge, \VRPAgent is the first LLM-based paradigm to advance the
state-of-the-art in VRPs, highlighting a promising future for automated
heuristics discovery.

</details>


### [219] [The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas](https://arxiv.org/abs/2510.07091)
*Baixuan Xu,Tianshi Zheng,Zhaowei Wang,Hong Ting Tsang,Weiqi Wang,Tianqing Fang,Yangqiu Song*

Main category: cs.AI

TL;DR: Compares two action representations for long-horizon LLM agents—concrete actions (PwA) vs schema-based actions (PwS)—introduces a “cognitive bandwidth” lens, identifies an inflection point where PwS overtakes PwA as action spaces grow, analyzes how model capabilities shift this point, and offers guidance to build stronger PwS agents.


<details>
  <summary>Details</summary>
Motivation: Open-world autonomy requires agents that plan over long horizons, but enumerating executable actions becomes impractical as the environment’s action space explodes. The authors ask which action representation scales best and notes schema-based planning aligns with human cognition and environment constraints.

Method: Propose a cognitive bandwidth conceptual framework; compare PwA and PwS across benchmarks with different action-space sizes (ALFWorld ≈35 actions; SciWorld ≈500 actions); run controlled experiments varying two capabilities—planning proficiency and schema instantiation quality—to see how the representation-choice inflection point shifts; synthesize practical guidelines for PwS agents.

Result: Empirically observe a representation-choice inflection point between ALFWorld and SciWorld: PwA excels in small action spaces, while PwS scales better in larger ones. Increasing planning proficiency shifts the inflection rightward (PwA remains viable longer); improving schema instantiation shifts it leftward (PwS becomes preferable sooner). Current PwS agents underperform but can be improved with targeted design.

Conclusion: Action representation should be chosen based on environment scale and model skills: use PwA for smaller, well-defined action spaces; prefer PwS for large, open-ended spaces. Investing in schema instantiation reduces cognitive load and improves scalability. The paper offers actionable steps to make PwS agents more capable, enabling scalable long-horizon autonomy.

Abstract: Enabling LLMs to effectively operate long-horizon task which requires
long-term planning and multiple interactions is essential for open-world
autonomy. Conventional methods adopt planning with actions where a executable
action list would be provided as reference. However, this action representation
choice would be impractical when the environment action space is combinatorial
exploded (e.g., open-ended real world). This naturally leads to a question: As
environmental action space scales, what is the optimal action representation
for long-horizon agents? In this paper, we systematically study the
effectiveness of two different action representations. The first one is
conventional planning with actions (PwA) which is predominantly adopted for its
effectiveness on existing benchmarks. The other one is planning with schemas
(PwS) which instantiate an action schema into action lists (e.g., "move [OBJ]
to [OBJ]" -> "move apple to desk") to ensure concise action space and reliable
scalability. This alternative is motivated by its alignment with human
cognition and its compliance with environment-imposed action format
restriction. We propose cognitive bandwidth perspective as a conceptual
framework to qualitatively understand the differences between these two action
representations and empirically observe a representation-choice inflection
point between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve
as evidence of the need for scalable representations. We further conduct
controlled experiments to study how the location of this inflection point
interacts with different model capacities: stronger planning proficiency shifts
the inflection rightward, whereas better schema instantiation shifts it
leftward. Finally, noting the suboptimal performance of PwS agents, we provide
an actionable guide for building more capable PwS agents for better scalable
autonomy.

</details>


### [220] [The Contingencies of Physical Embodiment Allow for Open-Endedness and Care](https://arxiv.org/abs/2510.07117)
*Leonardo Christov-Moore,Arthur Juliani,Alex Kiefer,Nicco Reggente,B. Scott Rousse,Adam Safron,Nicol'as Hinrichs,Daniel Polani,Antonio Damasio*

Main category: cs.AI

TL;DR: The paper argues that embedding artificial agents as vulnerable, energy-dissipating bodies (subject to death) yields homeostatic and intrinsic control-seeking drives that can be formalized in RL to produce more robust, adaptive, and caring behavior in open-ended, multi-agent worlds.


<details>
  <summary>Details</summary>
Motivation: Artificial agents struggle to adapt and provide aligned care in open-ended physical environments, whereas biological organisms do so efficiently. The authors seek fundamental conditions of life that could explain this gap and inform the design of agents that are robust, adaptive, and capable of caring behavior.

Method: They define two minimal embodiment conditions—being-in-the-world (agent as part of the environment) and being-towards-death (tendency toward terminal states due to thermodynamics). From these, they derive: (1) a homeostatic drive to maintain integrity and avoid death, and (2) an intrinsic drive to maximize future control (inspired by Nietzsche’s will-to-power, operationalized via empowerment). They formalize these drives within a reinforcement learning framework for agents in open-ended, multi-agent environments.

Result: Primarily a conceptual and formal framework: maximizing empowerment increases the likelihood of meeting future homeostatic needs, thereby strengthening an agent’s capacity to maintain integrity. The formalization shows how intrinsically driven, embodied agents could cultivate open-endedness and care; no concrete empirical benchmarks are reported in the abstract.

Conclusion: Physical vulnerability and thermodynamic pressures, when modeled as homeostatic and empowerment-based intrinsic drives in RL, may yield agents that better maintain integrity and can exhibit caring behaviors in open-ended settings. Embodiment plus intrinsic control-seeking could be a principled route to robustness, adaptability, and care in artificial agents.

Abstract: Physical vulnerability and mortality are often seen as obstacles to be
avoided in the development of artificial agents, which struggle to adapt to
open-ended environments and provide aligned care. Meanwhile, biological
organisms survive, thrive, and care for each other in an open-ended physical
world with relative ease and efficiency. Understanding the role of the
conditions of life in this disparity can aid in developing more robust,
adaptive, and caring artificial agents. Here we define two minimal conditions
for physical embodiment inspired by the existentialist phenomenology of Martin
Heidegger: being-in-the-world (the agent is a part of the environment) and
being-towards-death (unless counteracted, the agent drifts toward terminal
states due to the second law of thermodynamics). We propose that from these
conditions we can obtain both a homeostatic drive - aimed at maintaining
integrity and avoiding death by expending energy to learn and act - and an
intrinsic drive to continue to do so in as many ways as possible. Drawing
inspiration from Friedrich Nietzsche's existentialist concept of will-to-power,
we examine how intrinsic drives to maximize control over future states, e.g.,
empowerment, allow agents to increase the probability that they will be able to
meet their future homeostatic needs, thereby enhancing their capacity to
maintain physical integrity. We formalize these concepts within a reinforcement
learning framework, which enables us to examine how intrinsically driven
embodied agents learning in open-ended multi-agent environments may cultivate
the capacities for open-endedness and care.ov

</details>


### [221] [Integrating Domain Knowledge into Process Discovery Using Large Language Models](https://arxiv.org/abs/2510.07161)
*Ali Norouzifar,Humam Kourani,Marcus Dees,Wil van der Aalst*

Main category: cs.AI

TL;DR: Interactive framework that uses LLMs to turn natural-language domain knowledge into declarative rules that steer the IMr process discovery algorithm, aiming to produce process models more faithful to reality; implemented and empirically evaluated with a real-life case study and expert feedback.


<details>
  <summary>Details</summary>
Motivation: Process models discovered solely from event logs can be inaccurate due to incomplete/noisy data and the neglect of valuable domain knowledge, undermining conformance checking and improvement tasks. The work seeks to integrate domain expertise into discovery without heavy formalization burdens on experts.

Method: LLMs extract declarative rules from experts’ textual descriptions; these rules guide the IMr recursive discovery algorithm to avoid structures that contradict domain knowledge. A tool coordinates interactions among the LLM, domain experts, and backend services. Multiple LLMs and prompt strategies are compared.

Result: A complete tool is presented. Extensive experiments evaluate different LLMs and prompts; a real-life case study is conducted with domain experts who assess usability and effectiveness. The framework shows it can guide IMr and mitigate problematic structures, though the abstract does not report quantitative gains.

Conclusion: Integrating LLM-derived domain rules with event-log data in an interactive loop is feasible and improves the trustworthiness and alignment of discovered process models for downstream use. The human-in-the-loop design supports practical adoption and suggests benefits over event-log-only discovery.

Abstract: Process discovery aims to derive process models from event logs, providing
insights into operational behavior and forming a foundation for conformance
checking and process improvement. However, models derived solely from event
data may not accurately reflect the real process, as event logs are often
incomplete or affected by noise, and domain knowledge, an important
complementary resource, is typically disregarded. As a result, the discovered
models may lack reliability for downstream tasks. We propose an interactive
framework that incorporates domain knowledge, expressed in natural language,
into the process discovery pipeline using Large Language Models (LLMs). Our
approach leverages LLMs to extract declarative rules from textual descriptions
provided by domain experts. These rules are used to guide the IMr discovery
algorithm, which recursively constructs process models by combining insights
from both the event log and the extracted rules, helping to avoid problematic
process structures that contradict domain knowledge. The framework coordinates
interactions among the LLM, domain experts, and a set of backend services. We
present a fully implemented tool that supports this workflow and conduct an
extensive evaluation of multiple LLMs and prompt engineering strategies. Our
empirical study includes a case study based on a real-life event log with the
involvement of domain experts, who assessed the usability and effectiveness of
the framework.

</details>


### [222] [NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents](https://arxiv.org/abs/2510.07172)
*Tianshi Zheng,Kelvin Kiu-Wai Tam,Newt Hue-Nam K. Nguyen,Baixuan Xu,Zhaowei Wang,Jiayang Cheng,Hong Ting Tsang,Weiqi Wang,Jiaxin Bai,Tianqing Fang,Yangqiu Song,Ginny Y. Wong,Simon See*

Main category: cs.AI

TL;DR: NewtonBench is a new benchmark for interactive scientific law discovery that uses systematic alterations of physical laws to create scalable, relevant, and memorization-resistant tasks; it shows that current LLMs’ discovery abilities are fragile, degrade with complexity and noise, and can be harmed by code-interpreter tools that encourage premature exploitation.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks trade off among scientific relevance, scalability, and memorization resistance, and reduce discovery to static function fitting. The authors seek a testbed that reflects authentic scientific practice—interactive probing of systems to infer hidden laws—while avoiding leakage and scaling limits.

Method: They construct NewtonBench: 324 tasks across 12 physics domains. The key design lever is “metaphysical shifts,” systematic deviations from canonical laws, generating diverse, non-memorized targets. Evaluation is interactive: agents must design experiments within simulated complex systems to uncover governing principles. They test frontier LLMs, with and without tool assistance (e.g., code interpreters).

Result: Frontier LLMs exhibit some capacity for discovery, but it degrades sharply with increasing system complexity and is highly sensitive to observational noise. Tool assistance can backfire: providing a code interpreter nudges capable models into early exploitation, leading them to settle on suboptimal hypotheses (satisficing).

Conclusion: Robust, generalizable discovery in complex, noisy, interactive environments remains unsolved. NewtonBench offers a scalable, realistic benchmark that can track genuine progress and guide development of AI agents better at experimental design, hypothesis generation, and noise-robust inference.

Abstract: Large language models are emerging as powerful tools for scientific law
discovery, a foundational challenge in AI-driven science. However, existing
benchmarks for this task suffer from a fundamental methodological trilemma,
forcing a trade-off between scientific relevance, scalability, and resistance
to memorization. Furthermore, they oversimplify discovery as static function
fitting, failing to capture the authentic scientific process of uncovering
embedded laws through the interactive exploration of complex model systems. To
address these critical gaps, we introduce NewtonBench, a benchmark comprising
324 scientific law discovery tasks across 12 physics domains. Our design
mitigates the evaluation trilemma by using metaphysical shifts - systematic
alterations of canonical laws - to generate a vast suite of problems that are
scalable, scientifically relevant, and memorization-resistant. Moreover, we
elevate the evaluation from static function fitting to interactive model
discovery, requiring agents to experimentally probe simulated complex systems
to uncover hidden principles. Our extensive experiment reveals a clear but
fragile capability for discovery in frontier LLMs: this ability degrades
precipitously with increasing system complexity and exhibits extreme
sensitivity to observational noise. Notably, we uncover a paradoxical effect of
tool assistance: providing a code interpreter can hinder more capable models by
inducing a premature shift from exploration to exploitation, causing them to
satisfice on suboptimal solutions. These results demonstrate that robust,
generalizable discovery in complex, interactive environments remains the core
challenge. By providing a scalable, robust, and scientifically authentic
testbed, NewtonBench offers a crucial tool for measuring true progress and
guiding the development of next-generation AI agents capable of genuine
scientific discovery.

</details>


### [223] [Multi-Objective Multi-Agent Path Finding with Lexicographic Cost Preferences](https://arxiv.org/abs/2510.07276)
*Pulkit Rustagi,Kyle Hollins Wray,Sandhya Saisubramanian*

Main category: cs.AI

TL;DR: Proposes LCBS, a lexicographic conflict-based search that finds a single preference-aligned, conflict-free multi-agent path without building Pareto fronts, yielding optimal solutions and scaling to many objectives with higher success rates than prior MO-MAPF methods.


<details>
  <summary>Details</summary>
Motivation: Existing MO-MAPF approaches rely on Pareto frontiers, which (1) ignore explicit user preference orderings over objectives and (2) scale poorly as the number of objectives grows. There is a need for an approach that directly optimizes user-specified priorities while remaining computationally tractable.

Method: Model MO-MAPF with a lexicographic ordering of objectives and introduce LCBS (Lexicographic Conflict-Based Search). LCBS integrates a priority-aware low-level A* with high-level Conflict-Based Search, using the lexicographic order to guide search and avoid constructing Pareto frontiers, thereby computing a single solution consistent with the specified preference order.

Result: Provides theoretical insights on optimality and scalability. Experiments on standard and randomized MAPF benchmarks show that LCBS finds optimal solutions, scales to problems with up to 10 objectives, and achieves consistently higher success rates than state-of-the-art baselines, with larger gains as the number of objectives increases.

Conclusion: Lexicographic modeling combined with conflict-based search offers an efficient, preference-aligned solution for MO-MAPF, outperforming Pareto-frontier methods especially as objectives increase, and enabling scalable optimal planning in multi-agent settings.

Abstract: Many real-world scenarios require multiple agents to coordinate in shared
environments, while balancing trade-offs between multiple, potentially
competing objectives. Current multi-objective multi-agent path finding
(MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto
frontiers. They do not explicitly optimize for user-defined preferences, even
when the preferences are available, and scale poorly with the number of
objectives. We propose a lexicographic framework for modeling MO-MAPF, along
with an algorithm \textit{Lexicographic Conflict-Based Search} (LCBS) that
directly computes a single solution aligned with a lexicographic preference
over objectives. LCBS integrates a priority-aware low-level $A^*$ search with
conflict-based search, avoiding Pareto frontier construction and enabling
efficient planning guided by preference over objectives. We provide insights
into optimality and scalability, and empirically demonstrate that LCBS computes
optimal solutions while scaling to instances with up to ten objectives -- far
beyond the limits of existing MO-MAPF methods. Evaluations on standard and
randomized MAPF benchmarks show consistently higher success rates against
state-of-the-art baselines, especially with increasing number of objectives.

</details>


### [224] [Agentic generative AI for media content discovery at the national football league](https://arxiv.org/abs/2510.07297)
*Henry Wang,Sirajus Salekin,Jake Lee,Ross Claytor,Shinan Zhang,Michael Chi*

Main category: cs.AI

TL;DR: An agentic generative-AI system lets NFL staff search historical plays via natural language, translating queries into database calls and using semantic caching to deliver >95% accuracy and cut retrieval time from ~10 minutes to ~30 seconds.


<details>
  <summary>Details</summary>
Motivation: Traditional filter-and-click video search is slow and cumbersome for media researchers. The NFL needs a faster, more intuitive way to find relevant historical plays to support content creation and storytelling.

Method: Design an agentic workflow that parses a user’s natural-language query, decomposes it into structured elements, and translates them into the database query language. Add carefully engineered semantic caching to reuse prior computations and speed up repeated or similar queries, improving both latency and accuracy.

Result: The system achieves over 95% accuracy on retrieving relevant plays and reduces average search time from about 10 minutes to around 30 seconds, markedly improving operational efficiency.

Conclusion: Generative AI-driven natural-language interfaces, combined with semantic caching, can substantially streamline media content discovery workflows, freeing analysts to focus on creative storytelling and production.

Abstract: Generative AI has unlocked new possibilities in content discovery and
management. Through collaboration with the National Football League (NFL), we
demonstrate how a generative-AI based workflow enables media researchers and
analysts to query relevant historical plays using natural language rather than
traditional filter-and-click interfaces. The agentic workflow takes a user
query as input, breaks it into elements, and translates them into the
underlying database query language. Accuracy and latency are further improved
through carefully designed semantic caching. The solution achieves over 95
percent accuracy and reduces the average time to find relevant videos from 10
minutes to 30 seconds, significantly increasing the NFL's operational
efficiency and allowing users to focus on producing creative content and
engaging storylines.

</details>
