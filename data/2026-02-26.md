<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 64]
- [cs.CL](#cs.CL) [Total: 34]
- [cs.AI](#cs.AI) [Total: 42]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VISION-ICE: Video-based Interpretation and Spatial Identification of Arrhythmia Origins via Neural Networks in Intracardiac Echocardiography](https://arxiv.org/abs/2602.20165)
*Dorsa EPMoghaddam,Feng Gao,Drew Bernard,Kavya Sinha,Mehdi Razavi,Behnaam Aazhang*

Main category: cs.CV

TL;DR: AI uses intracardiac echocardiography (ICE) videos with a 3D CNN to classify rhythm as normal, left-sided, or right-sided arrhythmia, achieving 66.2% accuracy on unseen patients versus a 33.3% chance baseline, indicating feasibility to speed and target ablation; larger datasets are needed for robustness.


<details>
  <summary>Details</summary>
Motivation: Conventional arrhythmia localization (high-density mapping, pre-op CT/MRI) is slow and resource heavy. ICE is routinely obtained during EP procedures and AI has proven effective in echocardiography, motivating a real-time, intra-procedural tool that can direct clinicians toward arrhythmogenic regions and reduce procedure time.

Method: Define localization as a 3-class video classification task (normal sinus, left-sided arrhythmia, right-sided arrhythmia). Train a 3D convolutional neural network on ICE videos. Evaluate via 10-fold cross-validation with testing on four previously unseen patients.

Result: The model achieved a mean accuracy of 66.2% on unseen patients, substantially outperforming the 33.3% random baseline, indicating meaningful discrimination of arrhythmia laterality and normal rhythm from ICE sequences.

Conclusion: ICE plus deep learning appears feasible for automated arrhythmia localization and could enable faster, more targeted EP interventions and reduce ablation burden. However, small sample size and limited external validation constrain generalizability; expanding and diversifying the dataset is the key next step.

Abstract: Contemporary high-density mapping techniques and preoperative CT/MRI remain time and resource intensive in localizing arrhythmias. AI has been validated as a clinical decision aid in providing accurate, rapid real-time analysis of echocardiographic images. Building on this, we propose an AI-enabled framework that leverages intracardiac echocardiography (ICE), a routine part of electrophysiology procedures, to guide clinicians toward areas of arrhythmogenesis and potentially reduce procedural time. Arrhythmia source localization is formulated as a three-class classification task, distinguishing normal sinus rhythm, left-sided, and right-sided arrhythmias, based on ICE video data. We developed a 3D Convolutional Neural Network trained to discriminate among the three aforementioned classes. In ten-fold cross-validation, the model achieved a mean accuracy of 66.2% when evaluated on four previously unseen patients (substantially outperforming the 33.3% random baseline). These results demonstrate the feasibility and clinical promise of using ICE videos combined with deep learning for automated arrhythmia localization. Leveraging ICE imaging could enable faster, more targeted electrophysiological interventions and reduce the procedural burden of cardiac ablation. Future work will focus on expanding the dataset to improve model robustness and generalizability across diverse patient populations.

</details>


### [2] [OTPrune: Distribution-Aligned Visual Token Pruning via Optimal Transport](https://arxiv.org/abs/2602.20205)
*Xiwen Chen,Wenhui Zhu,Gen Li,Xuanzhao Dong,Yujian Xiong,Hao Wang,Peijie Qiu,Qingquan Song,Zhipeng Wang,Shao Tang,Yalin Wang,Abolfazl Razi*

Main category: cs.CV

TL;DR: OTPrune is a training‑free visual token pruning framework for MLLMs that selects tokens by aligning the distribution of pruned tokens to the full set via optimal transport, yielding faster inference with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: MLLMs process many redundant visual tokens, causing high latency and compute costs. Prior pruning methods ignore the distributional structure of visual representations, risking loss of semantic diversity and representativeness.

Method: Formulate pruning as distribution alignment: minimize the 2‑Wasserstein distance between the full visual token distribution and the subset kept after pruning. Derive a tractable submodular surrogate objective enabling efficient selection with guarantees. The approach preserves local diversity and global representativeness, is training‑free, and comes with proofs of monotonicity and submodularity.

Result: Across broad benchmarks, OTPrune attains better performance‑efficiency trade‑offs than state‑of‑the‑art pruning methods, reducing inference cost while maintaining task performance. Authors provide analyses explaining stability and semantic faithfulness, and release code.

Conclusion: Distribution‑aware, OT‑based pruning provides a principled and efficient way to reduce visual tokens in MLLMs, delivering stable acceleration without retraining and with strong empirical results versus prior art.

Abstract: Multi-modal large language models (MLLMs) achieve strong visual-language reasoning but suffer from high inference cost due to redundant visual tokens. Recent work explores visual token pruning to accelerate inference, while existing pruning methods overlook the underlying distributional structure of visual representations. We propose OTPrune, a training-free framework that formulates pruning as distribution alignment via optimal transport (OT). By minimizing the 2-Wasserstein distance between the full and pruned token distributions, OTPrune preserves both local diversity and global representativeness while reducing inference cost. Moreover, we derive a tractable submodular objective that enables efficient optimization, and theoretically prove its monotonicity and submodularity, providing a principled foundation for stable and efficient pruning. We further provide a comprehensive analysis that explains how distributional alignment contributes to stable and semantically faithful pruning. Comprehensive experiments on wider benchmarks demonstrate that OTPrune achieves superior performance-efficiency tradeoffs compared to state-of-the-art methods. The code is available at https://github.com/xiwenc1/OTPrune.

</details>


### [3] [De-rendering, Reasoning, and Repairing Charts with Vision-Language Models](https://arxiv.org/abs/2602.20291)
*Valentin Bonas,Martin Sinnona,Viviana Siless,Emmanuel Iarussi*

Main category: cs.CV

TL;DR: They present an LLM-powered pipeline that de-renders chart images, diagnoses design flaws with vision-language reasoning grounded in vis principles, and proposes concrete, iterative fixes; on 1,000 charts it produced 10,452 recommendations clustered into 10 categories, suggesting practical, principle-based feedback for better visualizations.


<details>
  <summary>Details</summary>
Motivation: Visualization errors are common and can mislead. Existing rule-based linters lack contextual judgment and actionable redesigns, while general LLMs give inconsistent, unprincipled feedback. There is a need for interpretable, principle-grounded, and actionable guidance on chart design.

Method: A three-stage framework: (1) chart de-rendering to reconstruct chart structure from images; (2) automated analysis using vision-language reasoning aligned with established visualization principles to detect issues; (3) proposal of concrete modifications that users can selectively apply, re-render, and iterate, forming a feedback loop. Evaluated on the Chart2Code benchmark with clustering of generated recommendations into thematic categories (e.g., axes, color, legends).

Result: On 1,000 charts, the system generated 10,452 design recommendations that grouped into 10 coherent categories such as axis formatting, color accessibility, and legend consistency, demonstrating breadth and structure of feedback.

Conclusion: Combining de-rendering with LLM-based, principle-aware analysis yields actionable, interpretable design feedback, pointing toward smarter, more accessible visualization authoring tools and improved visualization literacy.

Abstract: Data visualizations are central to scientific communication, journalism, and everyday decision-making, yet they are frequently prone to errors that can distort interpretation or mislead audiences. Rule-based visualization linters can flag violations, but they miss context and do not suggest meaningful design changes. Directly querying general-purpose LLMs about visualization quality is unreliable: lacking training to follow visualization design principles, they often produce inconsistent or incorrect feedback. In this work, we introduce a framework that combines chart de-rendering, automated analysis, and iterative improvement to deliver actionable, interpretable feedback on visualization design. Our system reconstructs the structure of a chart from an image, identifies design flaws using vision-language reasoning, and proposes concrete modifications supported by established principles in visualization research. Users can selectively apply these improvements and re-render updated figures, creating a feedback loop that promotes both higher-quality visualizations and the development of visualization literacy. In our evaluation on 1,000 charts from the Chart2Code benchmark, the system generated 10,452 design recommendations, which clustered into 10 coherent categories (e.g., axis formatting, color accessibility, legend consistency). These results highlight the promise of LLM-driven recommendation systems for delivering structured, principle-based feedback on visualization design, opening the door to more intelligent and accessible authoring tools.

</details>


### [4] [N4MC: Neural 4D Mesh Compression](https://arxiv.org/abs/2602.20312)
*Guodong Chen,Huanshuo Dong,Mallesham Dasari*

Main category: cs.CV

TL;DR: N4MC is a 4D neural compression framework for animated (time-varying) meshes that leverages motion-compensated, video-like inter-frame coding: it voxelizes consecutive meshes into 4D tensors, compresses them with an auto-decoder capturing spatiotemporal redundancy, and uses a transformer-based interpolation module to predict in-betweens, achieving better rate–distortion and real-time decoding.


<details>
  <summary>Details</summary>
Motivation: Existing neural mesh compressors treat each frame independently and miss large temporal redundancy in mesh sequences, leading to suboptimal rate–distortion and temporal coherence. There is a need for a video-style, motion-aware approach that scales to long sequences and supports real-time decoding.

Method: 1) Convert consecutive irregular mesh frames to a regular 4D tensor representation to unify geometry over space and time. 2) Learn an auto-decoder that jointly models spatial and temporal correlations to remove redundancy (motion-compensated coding). 3) Introduce a transformer-based interpolation network that predicts intermediate frames from latent embeddings derived from tracked volume centers, improving temporal coherence and reducing motion ambiguity.

Result: Across extensive evaluations, N4MC surpasses state-of-the-art methods in rate–distortion efficiency and supports real-time decoding of 4D mesh sequences. Open-source implementation is provided.

Conclusion: Treating animated meshes with video-inspired inter-frame compression and learned interpolation yields superior compression with coherent reconstructions and practical decoding speed, establishing N4MC as a strong baseline for 4D neural mesh compression.

Abstract: We present N4MC, the first 4D neural compression framework to efficiently compress time-varying mesh sequences by exploiting their temporal redundancy. Unlike prior neural mesh compression methods that treat each mesh frame independently, N4MC takes inspiration from inter-frame compression in 2D video codecs, and learns motion compensation in long mesh sequences. Specifically, N4MC converts consecutive irregular mesh frames into regular 4D tensors to provide a uniform and compact representation. These tensors are then condensed using an auto-decoder, which captures both spatial and temporal correlations for redundancy removal. To enhance temporal coherence, we introduce a transformer-based interpolation model that predicts intermediate mesh frames conditioned on latent embeddings derived from tracked volume centers, eliminating motion ambiguities. Extensive evaluations show that N4MC outperforms state-of-the-art in rate-distortion performance, while enabling real-time decoding of 4D mesh sequences. The implementation of our method is available at: https://github.com/frozzzen3/N4MC.

</details>


### [5] [GSNR: Graph Smooth Null-Space Representation for Inverse Problems](https://arxiv.org/abs/2602.20328)
*Romario Gualdrón-Hurtado,Roman Jacome,Rafael S. Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: Proposes GSNR, a graph-smooth null-space representation that injects structure only into the unobservable component of inverse problems, yielding faster convergence and higher-quality reconstructions across classic imaging tasks.


<details>
  <summary>Details</summary>
Motivation: Inverse imaging problems have infinitely many solutions because measurements leave a non-trivial null-space. Standard priors (sparsity, smoothness, scores) act on the full image manifold and leave the null-space unconstrained, inducing bias. The goal is to add meaningful constraints specifically in the null-space component.

Method: Construct a Graph-Smooth Null-Space Representation (GSNR): using a graph Laplacian, build a null-restricted Laplacian that models similarity among neighboring pixels of the null-space signal; then project the null-space onto the p smoothest graph spectral modes (lowest frequencies), yielding a low-dimensional, structured parameterization. Integrate a null-only graph regularizer into standard solvers (PnP, DIP, diffusion) to guide optimization and inference.

Result: Across deblurring, compressed sensing, demosaicing, and super-resolution, GSNR consistently improves PSNR: up to +4.3 dB over baseline formulations and up to +1 dB over end-to-end learned models, with observed benefits in convergence, coverage of null-space variance by p modes, and predictability of the modes from measurements.

Conclusion: Encoding structure exclusively in the null-space via graph-smooth spectral modes mitigates reconstruction bias, improves convergence, and generalizes across solvers and tasks, outperforming both classical and learned baselines.

Abstract: Inverse problems in imaging are ill-posed, leading to infinitely many solutions consistent with the measurements due to the non-trivial null-space of the sensing matrix. Common image priors promote solutions on the general image manifold, such as sparsity, smoothness, or score function. However, as these priors do not constrain the null-space component, they can bias the reconstruction. Thus, we aim to incorporate meaningful null-space information in the reconstruction framework. Inspired by smooth image representation on graphs, we propose Graph-Smooth Null-Space Representation (GSNR), a mechanism that imposes structure only into the invisible component. Particularly, given a graph Laplacian, we construct a null-restricted Laplacian that encodes similarity between neighboring pixels in the null-space signal, and we design a low-dimensional projection matrix from the $p$-smoothest spectral graph modes (lowest graph frequencies). This approach has strong theoretical and practical implications: i) improved convergence via a null-only graph regularizer, ii) better coverage, how much null-space variance is captured by $p$ modes, and iii) high predictability, how well these modes can be inferred from the measurements. GSNR is incorporated into well-known inverse problem solvers, e.g., PnP, DIP, and diffusion solvers, in four scenarios: image deblurring, compressed sensing, demosaicing, and image super-resolution, providing consistent improvement of up to 4.3 dB over baseline formulations and up to 1 dB compared with end-to-end learned models in terms of PSNR.

</details>


### [6] [Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking](https://arxiv.org/abs/2602.20330)
*Jingcheng Yang,Tianhu Xiong,Shengyi Qian,Klara Nahrstedt,Mingyuan Wu*

Main category: cs.CV

TL;DR: Proposes the first transparent circuit-tracing framework for vision–language models that maps how visual and semantic features combine, identifies distinct causal circuits (including for math reasoning and cross-modal links), and validates control via feature steering and circuit patching.


<details>
  <summary>Details</summary>
Motivation: VLMs achieve strong performance but act as opaque black boxes; to build trustworthy, debuggable, and controllable multimodal systems, we need mechanistic explanations of how visual and textual features interact during reasoning.

Method: Combine transcoders to align/translate internal representations, attribution graphs to trace influence paths across modalities, and attention-based analyses to map hierarchical integration of concepts. Validate causality through interventions: feature steering (activating/suppressing features) and circuit patching (modifying components) to test behavioral effects.

Result: Mapped hierarchical circuits integrating visual and semantic concepts; discovered distinct visual-feature circuits that support mathematical reasoning and cross-modal associations; demonstrated that intervening on these circuits predictably changes model behavior, indicating causality and controllability.

Conclusion: The framework advances explainability for VLMs by revealing controllable causal circuits, offering a basis for more reliable, steerable multimodal models and opening avenues for diagnostics and safety controls.

Abstract: Vision-language models (VLMs) are powerful but remain opaque black boxes. We introduce the first framework for transparent circuit tracing in VLMs to systematically analyze multimodal reasoning. By utilizing transcoders, attribution graphs, and attention-based methods, we uncover how VLMs hierarchically integrate visual and semantic concepts. We reveal that distinct visual feature circuits can handle mathematical reasoning and support cross-modal associations. Validated through feature steering and circuit patching, our framework proves these circuits are causal and controllable, laying the groundwork for more explainable and reliable VLMs.

</details>


### [7] [Large-scale Photorealistic Outdoor 3D Scene Reconstruction from UAV Imagery Using Gaussian Splatting Techniques](https://arxiv.org/abs/2602.20342)
*Christos Maikos,Georgios Angelidis,Georgios Th. Papadopoulos*

Main category: cs.CV

TL;DR: An end-to-end UAV pipeline streams drone video and fuses sensors to build and live-update 3D reconstructions using 3D Gaussian Splatting, achieving AR/VR-ready rendering with much lower latency and higher throughput than NeRF while keeping quality within ~4–7% of offline baselines.


<details>
  <summary>Details</summary>
Motivation: Real-time aerial perception needs low-latency, high-fidelity 3D reconstructions for interactive visualization (e.g., AR/VR), but existing 3DGS advances haven’t been integrated into complete UAV systems that handle streaming, pose estimation, optimization, and deployment.

Method: Design an efficient architecture that ingests live RTMP video, performs synchronized sensor fusion and camera pose estimation, and optimizes a 3D Gaussian Splatting model online, enabling continuous model updates and low-latency delivery to interactive AR/VR visualization environments.

Result: Experiments show competitive visual fidelity while significantly improving rendering performance and substantially reducing end-to-end latency compared with NeRF-based methods; reconstruction quality is within 4–7% of high-fidelity offline references.

Conclusion: Integrating 3DGS into a full UAV pipeline enables real-time, scalable augmented perception from aerial platforms, delivering near-offline quality with superior latency and performance for immersive AR/VR applications.

Abstract: In this study, we present an end-to-end pipeline capable of converting drone-captured video streams into high-fidelity 3D reconstructions with minimal latency. Unmanned aerial vehicles (UAVs) are extensively used in aerial real-time perception applications. Moreover, recent advances in 3D Gaussian Splatting (3DGS) have demonstrated significant potential for real-time neural rendering. However, their integration into end-to-end UAV-based reconstruction and visualization systems remains underexplored. Our goal is to propose an efficient architecture that combines live video acquisition via RTMP streaming, synchronized sensor fusion, camera pose estimation, and 3DGS optimization, achieving continuous model updates and low-latency deployment within interactive visualization environments that supports immersive augmented and virtual reality (AR/VR) applications. Experimental results demonstrate that the proposed method achieves competitive visual fidelity, while delivering significantly higher rendering performance and substantially reduced end-to-end latency, compared to NeRF-based approaches. Reconstruction quality remains within 4-7\% of high-fidelity offline references, confirming the suitability of the proposed system for real-time, scalable augmented perception from aerial platforms.

</details>


### [8] [BiRQA: Bidirectional Robust Quality Assessment for Images](https://arxiv.org/abs/2602.20351)
*Aleksandr Gushchin,Dmitriy S. Vatolin,Anastasia Antsiferova*

Main category: cs.CV

TL;DR: BiRQA is a compact, bidirectional multiscale full‑reference IQA model that fuses fast features with bottom‑up and top‑down gating and trains with Anchored Adversarial Training, yielding SOTA accuracy, ~3× speed, and markedly improved robustness to white‑box attacks.


<details>
  <summary>Details</summary>
Motivation: Current FR IQA metrics used in compression, restoration, and generative modeling are accurate but computationally heavy and fragile to adversarial perturbations; a fast yet robust alternative is needed.

Method: A bidirectional multiscale pyramid processes four complementary, efficient features. A bottom‑up attention module injects fine‑scale cues into coarse levels via an uncertainty‑aware gate, and a top‑down cross‑gating block routes semantic context back to high resolution. Robustness is improved via Anchored Adversarial Training that uses clean “anchor” samples and a ranking loss to theoretically bound pointwise prediction error under attacks.

Result: On five public FR IQA benchmarks, BiRQA matches or surpasses prior SOTA while running about three times faster. Under unseen white‑box attacks on KADID‑10k, it boosts SROCC from 0.30–0.57 to 0.60–0.84, indicating strong robustness gains.

Conclusion: BiRQA is presented as the only FR IQA model to concurrently offer competitive accuracy, real‑time throughput, and strong adversarial resilience.

Abstract: Full-Reference image quality assessment (FR IQA) is important for image compression, restoration and generative modeling, yet current neural metrics remain slow and vulnerable to adversarial perturbations. We present BiRQA, a compact FR IQA metric model that processes four fast complementary features within a bidirectional multiscale pyramid. A bottom-up attention module injects fine-scale cues into coarse levels through an uncertainty-aware gate, while a top-down cross-gating block routes semantic context back to high resolution. To enhance robustness, we introduce Anchored Adversarial Training, a theoretically grounded strategy that uses clean "anchor" samples and a ranking loss to bound pointwise prediction error under attacks. On five public FR IQA benchmarks BiRQA outperforms or matches the previous state of the art (SOTA) while running ~3x faster than previous SOTA models. Under unseen white-box attacks it lifts SROCC from 0.30-0.57 to 0.60-0.84 on KADID-10k, demonstrating substantial robustness gains. To our knowledge, BiRQA is the only FR IQA model combining competitive accuracy with real-time throughput and strong adversarial resilience.

</details>


### [9] [3DSPA: A 3D Semantic Point Autoencoder for Evaluating Video Realism](https://arxiv.org/abs/2602.20354)
*Bhavik Chandna,Kelsey R. Allen*

Main category: cs.CV

TL;DR: Introduces 3DSPA, a reference-free 3D spatiotemporal point autoencoder that fuses 3D trajectories, depth, and DINO semantics to automatically assess video realism, temporal consistency, and physical plausibility, aligning well with human judgments and detecting motion/physics violations.


<details>
  <summary>Details</summary>
Motivation: Video generators need reliable, scalable realism evaluation, but current practice depends on human raters or narrow bespoke datasets, limiting scope and reproducibility. A general, automated, reference-free metric that captures both semantics and coherent 3D structure is missing.

Method: 3DSPA: a 3D spatiotemporal point autoencoder that builds a unified representation by integrating 3D point trajectories, depth cues, and DINO-derived semantic features. By modeling object motions and scene semantics jointly, it scores realism, temporal coherence, and physical plausibility without requiring a ground-truth reference video.

Result: Across multiple datasets, 3DSPA more closely matches human judgments of video quality/realism, is more sensitive to motion artifacts, and reliably flags violations of physical laws compared with baselines.

Conclusion: Augmenting trajectory-based representations with 3D semantic cues provides a stronger, reference-free foundation for benchmarking generative video models and implicitly captures physical rule violations; code and pretrained weights will be released.

Abstract: AI video generation is evolving rapidly. For video generators to be useful for applications ranging from robotics to film-making, they must consistently produce realistic videos. However, evaluating the realism of generated videos remains a largely manual process -- requiring human annotation or bespoke evaluation datasets which have restricted scope. Here we develop an automated evaluation framework for video realism which captures both semantics and coherent 3D structure and which does not require access to a reference video. Our method, 3DSPA, is a 3D spatiotemporal point autoencoder which integrates 3D point trajectories, depth cues, and DINO semantic features into a unified representation for video evaluation. 3DSPA models how objects move and what is happening in the scene, enabling robust assessments of realism, temporal consistency, and physical plausibility. Experiments show that 3DSPA reliably identifies videos which violate physical laws, is more sensitive to motion artifacts, and aligns more closely with human judgments of video quality and realism across multiple datasets. Our results demonstrate that enriching trajectory-based representations with 3D semantics offers a stronger foundation for benchmarking generative video models, and implicitly captures physical rule violations. The code and pretrained model weights will be available at https://github.com/TheProParadox/3dspa_code.

</details>


### [10] [Aesthetic Camera Viewpoint Suggestion with 3D Aesthetic Field](https://arxiv.org/abs/2602.20363)
*Sheyang Tang,Armin Shafiee Sarvestani,Jialu Xu,Xiaoyu Xu,Zhou Wang*

Main category: cs.CV

TL;DR: They propose a 3D aesthetic field learned with 3D Gaussian Splatting that distills a 2D aesthetic model into 3D, enabling fast, geometry-aware viewpoint suggestion from sparse views via coarse sampling plus gradient refinement, outperforming single-view and RL-heavy 3D methods.


<details>
  <summary>Details</summary>
Motivation: A scene’s aesthetic appeal is highly viewpoint-dependent, but current methods either (1) adjust a single view without 3D understanding or (2) require dense 3D captures and costly RL search. There is a need for efficient, geometry-grounded aesthetic reasoning that works with sparse inputs.

Method: Learn a feedforward 3D Gaussian Splatting network as a 3D aesthetic field by distilling high-level aesthetic signals from a pretrained 2D aesthetic model into 3D. Use a two-stage search: coarse viewpoint sampling to find promising regions, then gradient-based refinement over the learned field to converge to high-aesthetic viewpoints, all from sparse captures (no dense recon or RL).

Result: Across extensive experiments, the approach suggests viewpoints with better framing and composition than existing single-image predictors and prior 3D/RL exploration methods, while being more efficient and not requiring dense captures.

Conclusion: A 3D-aware, geometry-grounded aesthetic modeling paradigm is feasible from sparse views. The learned aesthetic field plus coarse-to-fine optimization efficiently yields appealing viewpoints, pointing to a new direction for aesthetic reasoning and camera planning without heavy RL or dense 3D data.

Abstract: The aesthetic quality of a scene depends strongly on camera viewpoint. Existing approaches for aesthetic viewpoint suggestion are either single-view adjustments, predicting limited camera adjustments from a single image without understanding scene geometry, or 3D exploration approaches, which rely on dense captures or prebuilt 3D environments coupled with costly reinforcement learning (RL) searches. In this work, we introduce the notion of 3D aesthetic field that enables geometry-grounded aesthetic reasoning in 3D with sparse captures, allowing efficient viewpoint suggestions in contrast to costly RL searches. We opt to learn this 3D aesthetic field using a feedforward 3D Gaussian Splatting network that distills high-level aesthetic knowledge from a pretrained 2D aesthetic model into 3D space, enabling aesthetic prediction for novel viewpoints from only sparse input views. Building on this field, we propose a two-stage search pipeline that combines coarse viewpoint sampling with gradient-based refinement, efficiently identifying aesthetically appealing viewpoints without dense captures or RL exploration. Extensive experiments show that our method consistently suggests viewpoints with superior framing and composition compared to existing approaches, establishing a new direction toward 3D-aware aesthetic modeling.

</details>


### [11] [CLIPoint3D: Language-Grounded Few-Shot Unsupervised 3D Point Cloud Domain Adaptation](https://arxiv.org/abs/2602.20409)
*Mainak Singha,Sarthak Mehrotra,Paolo Casari,Subhasis Chaudhuri,Elisa Ricci,Biplab Banerjee*

Main category: cs.CV

TL;DR: CLIPoint3D adapts CLIP to 3D point-cloud domain adaptation via multi-view depth projections, knowledge-driven prompt tuning with lightweight geometric cues, parameter-efficient fine-tuning, confident view selection, and dual alignment losses, yielding 3–16% accuracy gains on PointDA-10 and GraspNetPC-10.


<details>
  <summary>Details</summary>
Motivation: Vision-language models like CLIP generalize across modalities but are brittle under domain shifts (e.g., synthetic→real 3D). Conventional 3D DA methods are accurate but rely on heavy, fully trainable encoders, hurting efficiency. A lighter, CLIP-based approach is needed to retain generalization while adapting to 3D domains efficiently.

Method: Project each 3D point cloud into multiple depth-map views and process them with a largely frozen CLIP. Use knowledge-driven prompt tuning that fuses language priors with geometric cues from a lightweight 3D encoder. Apply parameter-efficient fine-tuning to CLIP’s encoders and entropy-guided view sampling to keep only confident projections. Bridge source–target gaps with an optimal-transport alignment loss plus an uncertainty-aware prototype alignment loss to preserve class separability.

Result: On PointDA-10 and GraspNetPC-10, CLIPoint3D consistently improves accuracy by 3–16% over both CLIP-based and conventional heavy-encoder baselines. Code is publicly available.

Conclusion: Leveraging CLIP with multi-view projections, geometry-aware prompt tuning, PEFT, and principled alignment losses enables efficient few-shot unsupervised 3D domain adaptation that maintains class separation and substantially improves accuracy over prior baselines.

Abstract: Recent vision-language models (VLMs) such as CLIP demonstrate impressive cross-modal reasoning, extending beyond images to 3D perception. Yet, these models remain fragile under domain shifts, especially when adapting from synthetic to real-world point clouds. Conventional 3D domain adaptation approaches rely on heavy trainable encoders, yielding strong accuracy but at the cost of efficiency. We introduce CLIPoint3D, the first framework for few-shot unsupervised 3D point cloud domain adaptation built upon CLIP. Our approach projects 3D samples into multiple depth maps and exploits the frozen CLIP backbone, refined through a knowledge-driven prompt tuning scheme that integrates high-level language priors with geometric cues from a lightweight 3D encoder. To adapt task-specific features effectively, we apply parameter-efficient fine-tuning to CLIP's encoders and design an entropy-guided view sampling strategy for selecting confident projections. Furthermore, an optimal transport-based alignment loss and an uncertainty-aware prototype alignment loss collaboratively bridge source-target distribution gaps while maintaining class separability. Extensive experiments on PointDA-10 and GraspNetPC-10 benchmarks show that CLIPoint3D achieves consistent 3-16% accuracy gains over both CLIP-based and conventional encoder-based baselines. Codes are available at https://github.com/SarthakM320/CLIPoint3D.

</details>


### [12] [SimLBR: Learning to Detect Fake Images by Learning to Detect Real Images](https://arxiv.org/abs/2602.20412)
*Aayush Dhakal,Subash Khanal,Srikumar Sastry,Jacob Arndt,Philipe Ambrozio Dias,Dalton Lunga,Nathan Jacobs*

Main category: cs.CV

TL;DR: SimLBR is a simple, fast fake-image detector that learns a tight boundary around real images and treats fakes as a sink class. Using Latent Blending Regularization, it markedly improves cross-generator robustness (up to +24.85% accuracy, +69.62% recall on Chameleon) and trains orders of magnitude faster, while advocating reliability-focused evaluation with risk-adjusted and worst-case metrics.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art detectors overfit to specific generators and collapse under distribution shift, especially on curated hard test sets. A principled alternative is to tightly model the real image distribution and avoid overfitting to the ever-changing fake distributions.

Method: Propose SimLBR, a framework using Latent Blending Regularization to learn a compact decision boundary around real images, treating all fakes as a sink class. Complement this with reliability-oriented evaluation via risk-adjusted metrics and worst-case performance estimates.

Result: On the challenging Chameleon benchmark, SimLBR achieves substantial cross-generator gains (up to +24.85% accuracy and +69.62% recall) and trains orders of magnitude faster than prior approaches.

Conclusion: Modeling real images tightly and treating fakes as a sink improves robustness and efficiency in fake-image detection. SimLBR offers strong cross-generator generalization and speed, and the proposed reliability-centric evaluation better reflects robustness; code/models will be released.

Abstract: The rapid advancement of generative models has made the detection of AI-generated images a critical challenge for both research and society. Recent works have shown that most state-of-the-art fake image detection methods overfit to their training data and catastrophically fail when evaluated on curated hard test sets with strong distribution shifts. In this work, we argue that it is more principled to learn a tight decision boundary around the real image distribution and treat the fake category as a sink class. To this end, we propose SimLBR, a simple and efficient framework for fake image detection using Latent Blending Regularization (LBR). Our method significantly improves cross-generator generalization, achieving up to +24.85\% accuracy and +69.62\% recall on the challenging Chameleon benchmark. SimLBR is also highly efficient, training orders of magnitude faster than existing approaches. Furthermore, we emphasize the need for reliability-oriented evaluation in fake image detection, introducing risk-adjusted metrics and worst-case estimates to better assess model robustness. All code and models will be released on HuggingFace and GitHub.

</details>


### [13] [gQIR: Generative Quanta Image Reconstruction](https://arxiv.org/abs/2602.20417)
*Aryan Garg,Sizhuo Ma,Mohit Gupta*

Main category: cs.CV

TL;DR: They adapt large text-to-image latent diffusion models to reconstruct images from SPAD quanta bursts, explicitly modeling Bernoulli photon statistics and performing latent-space restoration with burst-level spatiotemporal reasoning. The method yields photometrically accurate and perceptually pleasing images, outperforming classical and learned baselines on synthetic data, a new color SPAD burst dataset, and a deforming video benchmark.


<details>
  <summary>Details</summary>
Motivation: Recovering coherent images from SPAD quanta frames is hard because detections are sparse, binary, and noisy, with alignment, denoising, and (for color) demosaicing challenges under Bernoulli statistics that standard pipelines and generative models don’t handle well. There is a need to leverage strong semantic priors while respecting photon-limited noise models.

Method: Adapt internet-scale text-to-image latent diffusion models to photon-limited SPAD imaging by incorporating Bernoulli photon likelihoods, performing latent-space restoration, and integrating burst-level spatiotemporal reasoning (alignment, denoising, and demosaicing) to handle motion and color in quanta bursts.

Result: On synthetic benchmarks and new real datasets—including the first color SPAD burst dataset and a challenging Deforming (XD) video benchmark—the approach substantially improves perceptual quality and maintains photometric fidelity compared to classical and modern learning-based baselines. Code is released (gQIR).

Conclusion: Large generative priors can be successfully adapted to extreme photon-limited sensing, enabling high-quality reconstructions from few photons and setting strong baselines for SPAD burst imaging and fast deforming scenes.

Abstract: Capturing high-quality images from only a few detected photons is a fundamental challenge in computational imaging. Single-photon avalanche diode (SPAD) sensors promise high-quality imaging in regimes where conventional cameras fail, but raw \emph{quanta frames} contain only sparse, noisy, binary photon detections. Recovering a coherent image from a burst of such frames requires handling alignment, denoising, and demosaicing (for color) under noise statistics far outside those assumed by standard restoration pipelines or modern generative models. We present an approach that adapts large text-to-image latent diffusion models to the photon-limited domain of quanta burst imaging. Our method leverages the structural and semantic priors of internet-scale diffusion models while introducing mechanisms to handle Bernoulli photon statistics. By integrating latent-space restoration with burst-level spatio-temporal reasoning, our approach produces reconstructions that are both photometrically faithful and perceptually pleasing, even under high-speed motion. We evaluate the method on synthetic benchmarks and new real-world datasets, including the first color SPAD burst dataset and a challenging \textit{Deforming (XD)} video benchmark. Across all settings, the approach substantially improves perceptual quality over classical and modern learning-based baselines, demonstrating the promise of adapting large generative priors to extreme photon-limited sensing. Code at \href{https://github.com/Aryan-Garg/gQIR}{https://github.com/Aryan-Garg/gQIR}.

</details>


### [14] [MedCLIPSeg: Probabilistic Vision-Language Adaptation for Data-Efficient and Generalizable Medical Image Segmentation](https://arxiv.org/abs/2602.20423)
*Taha Koleilat,Hojat Asgariandehkordi,Omid Nejati Manzari,Berardino Barile,Yiming Xiao,Hassan Rivaz*

Main category: cs.CV

TL;DR: MedCLIPSeg adapts CLIP with probabilistic cross‑modal attention and a soft patch‑level contrastive loss to enable text‑guided, data‑efficient, and uncertainty‑aware medical image segmentation, outperforming prior methods across diverse modalities while providing interpretable uncertainty maps.


<details>
  <summary>Details</summary>
Motivation: Medical segmentation suffers from scarce labels, ambiguous anatomy, and domain shifts. Although CLIP offers strong cross‑modal features, its use for dense, text‑driven segmentation—especially with uncertainty modeling and data efficiency—remains underexplored in medical imaging.

Method: Leverage patch‑level CLIP embeddings and introduce probabilistic cross‑modal attention to enable bidirectional interaction between image and text tokens and to explicitly model predictive uncertainty. Add a soft patch‑level contrastive loss across diverse textual prompts to encourage nuanced semantic alignment. The framework adapts CLIP for dense prediction with uncertainty estimation.

Result: On 16 datasets covering five imaging modalities and six organs, MedCLIPSeg outperforms prior methods in accuracy, efficiency, and robustness, improves data efficiency and domain generalization, and produces interpretable uncertainty maps that indicate local reliability of segmentations.

Conclusion: Probabilistic vision‑language modeling with patch‑level alignment and uncertainty estimation is effective for text‑driven medical image segmentation, yielding robust performance and actionable confidence estimates across domains.

Abstract: Medical image segmentation remains challenging due to limited annotations for training, ambiguous anatomical features, and domain shifts. While vision-language models such as CLIP offer strong cross-modal representations, their potential for dense, text-guided medical image segmentation remains underexplored. We present MedCLIPSeg, a novel framework that adapts CLIP for robust, data-efficient, and uncertainty-aware medical image segmentation. Our approach leverages patch-level CLIP embeddings through probabilistic cross-modal attention, enabling bidirectional interaction between image and text tokens and explicit modeling of predictive uncertainty. Together with a soft patch-level contrastive loss that encourages more nuanced semantic learning across diverse textual prompts, MedCLIPSeg effectively improves data efficiency and domain generalizability. Extensive experiments across 16 datasets spanning five imaging modalities and six organs demonstrate that MedCLIPSeg outperforms prior methods in accuracy, efficiency, and robustness, while providing interpretable uncertainty maps that highlight local reliability of segmentation results. This work demonstrates the potential of probabilistic vision-language modeling for text-driven medical image segmentation.

</details>


### [15] [SceMoS: Scene-Aware 3D Human Motion Synthesis by Planning with Geometry-Grounded Tokens](https://arxiv.org/abs/2602.20476)
*Anindita Ghosh,Vladislav Golyanik,Taku Komura,Philipp Slusallek,Christian Theobalt,Rishabh Dabral*

Main category: cs.CV

TL;DR: SceMoS generates text-driven 3D human motions in realistic scenes by replacing heavy 3D supervision with structured 2D cues, disentangling global planning from local, contact-aware execution to achieve state-of-the-art realism and contact accuracy with far fewer scene-encoding parameters.


<details>
  <summary>Details</summary>
Motivation: Existing approaches jointly learn high-level intent and low-level physical feasibility using expensive 3D scene representations (point clouds/voxels), leading to high computational cost and entanglement that hinders efficiency and generalization. The goal is to retain physical plausibility and semantic alignment while reducing supervision and model complexity.

Method: A two-part 2D-driven framework: (1) a text-conditioned autoregressive global motion planner that operates on a DINOv2-encoded bird’s-eye-view (BEV) of the scene for semantic layout and affordance reasoning; (2) a geometry-grounded motion tokenizer via conditional VQ-VAE that conditions on local 2D heightmaps to encode surface physics and contacts into a discrete motion vocabulary. This factorization separates planning from execution and uses lightweight 2D scene cues instead of full 3D volumes.

Result: On the TRUMANS benchmark, SceMoS attains state-of-the-art motion realism and contact accuracy while cutting trainable scene-encoding parameters by over 50%, demonstrating an improved efficiency–fidelity trade-off.

Conclusion: Structured 2D scene representations (BEV semantics + local heightmaps) are sufficient to effectively ground 3D human–scene interaction, enabling efficient, physically adherent, and semantically aligned text-to-motion synthesis without full 3D volumetric supervision.

Abstract: Synthesizing text-driven 3D human motion within realistic scenes requires learning both semantic intent ("walk to the couch") and physical feasibility (e.g., avoiding collisions). Current methods use generative frameworks that simultaneously learn high-level planning and low-level contact reasoning, and rely on computationally expensive 3D scene data such as point clouds or voxel occupancy grids. We propose SceMoS, a scene-aware motion synthesis framework that shows that structured 2D scene representations can serve as a powerful alternative to full 3D supervision in physically grounded motion synthesis. SceMoS disentangles global planning from local execution using lightweight 2D cues and relying on (1) a text-conditioned autoregressive global motion planner that operates on a bird's-eye-view (BEV) image rendered from an elevated corner of the scene, encoded with DINOv2 features, as the scene representation, and (2) a geometry-grounded motion tokenizer trained via a conditional VQ-VAE, that uses 2D local scene heightmap, thus embedding surface physics directly into a discrete vocabulary. This 2D factorization reaches an efficiency-fidelity trade-off: BEV semantics capture spatial layout and affordance for global reasoning, while local heightmaps enforce fine-grained physical adherence without full 3D volumetric reasoning. SceMoS achieves state-of-the-art motion realism and contact accuracy on the TRUMANS benchmark, reducing the number of trainable parameters for scene encoding by over 50%, showing that 2D scene cues can effectively ground 3D human-scene interaction.

</details>


### [16] [Path-Decoupled Hyperbolic Flow Matching for Few-Shot Adaptation](https://arxiv.org/abs/2602.20479)
*Lin Li,Ziqi Jiang,Gefan Ye,Zhenqi He,Jiahui Li,Jun Xiao,Kwang-Ting Cheng,Long Chen*

Main category: cs.CV

TL;DR: They replace Euclidean Flow Matching with a hyperbolic (Lorentz-manifold) formulation that decouples transport paths using a centripetal hierarchy and class-specific geodesic constraints, plus an adaptive stopping rule, achieving state-of-the-art results on 11 cross-modal few-shot benchmarks.


<details>
  <summary>Details</summary>
Motivation: Euclidean (flat) Flow Matching suffers from polynomial volume growth, which poorly fits diverse, hierarchical feature distributions and causes severe path entanglement during visual–semantic transport in few-shot adaptation.

Method: Hyperbolic Flow Matching (HFM) on a Lorentz manifold. Key designs: (1) Centripetal hyperbolic alignment that anchors textual roots and pushes visual leaves toward the boundary to form an orderly, hierarchical initialization; (2) A path-decoupled objective that enforces class-specific geodesic corridors with step-wise supervision acting as a semantic guardrail; plus an adaptive diameter-based stopping criterion to avoid over-transport toward the origin based on intrinsic semantic scale.

Result: Extensive ablations across 11 benchmarks show consistent gains over Euclidean FM baselines, establishing new state-of-the-art performance.

Conclusion: Employing hyperbolic geometry for cross-modal flow matching alleviates path entanglement, better accommodates hierarchical/heterogeneous distributions, and improves few-shot adaptation. The approach is robust across many benchmarks; code and models will be released.

Abstract: Recent advances in cross-modal few-shot adaptation treat visual-semantic alignment as a continuous feature transport problem via Flow Matching (FM). However, we argue that Euclidean-based FM overlooks fundamental limitations of flat geometry, where polynomial volume growth fails to accommodate diverse feature distributions, leading to severe path entanglement. To this end, we propose path-decoupled Hyperbolic Flow Matching (HFM), leveraging the Lorentz manifold's exponential expansion for trajectory decoupling. HFM structures the transport via two key designs: 1) Centripetal hyperbolic alignment: It constructs a centripetal hierarchy by anchoring textual roots, which pushes visual leaves to the boundary to initialize orderly flows. 2) Path-decoupled objective: It acts as a ``semantic guardrail'' rigidly confining trajectories within isolated class-specific geodesic corridors via step-wise supervision. Furthermore, we devise an adaptive diameter-based stopping to prevent over-transportation into the crowded origin based on the intrinsic semantic scale. Extensive ablations on 11 benchmarks have shown that HFM establishes a new state-of-the-art, consistently outperforming its Euclidean counterparts. Our codes and models will be released.

</details>


### [17] [Pip-Stereo: Progressive Iterations Pruner for Iterative Optimization based Stereo Matching](https://arxiv.org/abs/2602.20496)
*Jintu Zheng,Qizhe Liu,HuangXin Xu,Zhuojie Chen*

Main category: cs.CV

TL;DR: PipStereo makes iterative stereo matching practical on edge devices by pruning redundant refinement, transferring monocular depth priors without extra encoders, and introducing a hardware-aware RNN (FlashGRU), achieving near-single-pass inference with large speed and memory gains while retaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Iterative stereo methods rely on RNN-based refinement that is compute- and memory-heavy, blocking real-time edge deployment. Existing work underexplores how to keep iterative accuracy while removing the RNN overhead.

Method: (1) Progressive iteration pruning: analyze refinement behavior to suppress temporally redundant and spatially sparse disparity updates, collapsing many RNN steps into near-single-pass inference. (2) Collaborative monocular prior transfer: inject monocular depth priors implicitly, avoiding a separate monocular encoder and its cost. (3) FlashGRU: a structured-sparse, I/O-conscious ConvGRU replacement optimized for hardware, reducing memory traffic and exploiting sparsity for speedups.

Result: FlashGRU vs native ConvGRU at 2K: 7.28× speedup, 76.6% peak memory reduction, 80.9% fewer global memory requests. End-to-end: 320×640 processed in 75 ms on Jetson Orin NX (FP16) and 19 ms on RTX 4090. Accuracy matches large iterative models; generalization and accuracy exceed existing real-time methods.

Conclusion: Through iteration pruning, lightweight prior transfer, and a hardware-aware RNN operator, PipStereo delivers real-time, high-fidelity stereo on edge hardware without sacrificing accuracy, offering a practical deployment path for iterative stereo models; code/resources to be released on the linked GitHub.

Abstract: While iterative stereo matching achieves high accuracy, its dependence on Recurrent Neural Networks (RNN) hinders edge deployment, a challenge underexplored in existing researches. We analyze iterative refinement and reveal that disparity updates are spatially sparse and temporally redundant. First, we introduce a progressive iteration pruning strategy that suppresses redundant update steps, effectively collapsing the recursive computation into a near-single-pass inference. Second, we propose a collaborative monocular prior transfer framework that implicitly embeds depth priors without requiring a dedicated monocular encoder, thereby eliminating its associated computational burden. Third, we develop FlashGRU, a hardware-aware RNN operator leveraging structured sparsity and I/O-conscious design, achieving a 7.28$\times$ speedup, 76.6\% memory peak reduction and 80.9\% global memory requests reduction over natvie ConvGRUs under 2K resolution. Our PipStereo enables real-time, high-fidelity stereo matching on edge hardware: it processes 320$\times$640 frames in just 75ms on an NVIDIA Jetson Orin NX (FP16) and 19ms on RTX 4090, matching the accuracy of large iterative based models, and our generalization ability and accuracy far exceeds that of existing real-time methods. Our embedded AI projects will be updated at: https://github.com/XPENG-Aridge-AI.

</details>


### [18] [LESA: Learnable Stage-Aware Predictors for Diffusion Model Acceleration](https://arxiv.org/abs/2602.20497)
*Peiliang Cai,Jiacheng Liu,Haowen Xu,Xinyu Wang,Chang Zou,Linfeng Zhang*

Main category: cs.CV

TL;DR: LESA is a learnable, stage-aware feature predictor for Diffusion Transformers that uses KAN-based temporal mappings and multi-expert modules per noise stage to forecast features, achieving 5–6.25× speedups with minimal or improved quality on both text-to-image and text-to-video tasks.


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers are expensive at inference; simple feature reuse or training-free forecasting cannot handle the stage-dependent dynamics of the diffusion process, causing quality drops and inconsistency with standard denoising. A learnable, stage-aware predictor is needed to accelerate sampling while preserving fidelity.

Method: A two-stage training framework (LESA) learns to forecast intermediate features in the denoising trajectory. It leverages Kolmogorov–Arnold Networks to model nonlinear temporal feature mappings and uses a multi-stage, multi-expert architecture that assigns specialized predictors to different noise-level stages. The predictors are integrated into the denoising loop to replace some expensive forward passes while staying consistent with the standard process.

Result: LESA delivers 5.00× acceleration on FLUX.1-dev with ~1.0% quality drop, 6.25× speedup on Qwen-Image with a 20.2% quality improvement over TaylorSeer, and 5.00× acceleration on HunyuanVideo with a 24.7% PSNR improvement over TaylorSeer. It attains state-of-the-art performance on text-to-image and text-to-video generation and generalizes across different base models.

Conclusion: Learnable, stage-aware feature forecasting can robustly accelerate Diffusion Transformers with negligible or negative (improved) quality impact, outperforming reuse/training-free baselines like TaylorSeer. The approach generalizes across models and tasks and is practical for deployment; code will be released.

Abstract: Diffusion models have achieved remarkable success in image and video generation tasks. However, the high computational demands of Diffusion Transformers (DiTs) pose a significant challenge to their practical deployment. While feature caching is a promising acceleration strategy, existing methods based on simple reusing or training-free forecasting struggle to adapt to the complex, stage-dependent dynamics of the diffusion process, often resulting in quality degradation and failing to maintain consistency with the standard denoising process. To address this, we propose a LEarnable Stage-Aware (LESA) predictor framework based on two-stage training. Our approach leverages a Kolmogorov-Arnold Network (KAN) to accurately learn temporal feature mappings from data. We further introduce a multi-stage, multi-expert architecture that assigns specialized predictors to different noise-level stages, enabling more precise and robust feature forecasting. Extensive experiments show our method achieves significant acceleration while maintaining high-fidelity generation. Experiments demonstrate 5.00x acceleration on FLUX.1-dev with minimal quality degradation (1.0% drop), 6.25x speedup on Qwen-Image with a 20.2% quality improvement over the previous SOTA (TaylorSeer), and 5.00x acceleration on HunyuanVideo with a 24.7% PSNR improvement over TaylorSeer. State-of-the-art performance on both text-to-image and text-to-video synthesis validates the effectiveness and generalization capability of our training-based framework across different models. Our code is included in the supplementary materials and will be released on GitHub.

</details>


### [19] [Probing and Bridging Geometry-Interaction Cues for Affordance Reasoning in Vision Foundation Models](https://arxiv.org/abs/2602.20501)
*Qing Zhang,Xuesong Li,Jing Zhang*

Main category: cs.CV

TL;DR: Affordance understanding in VFMs decomposes into two composable capacities—geometric part perception and action-conditioned interaction perception—and a simple, training-free fusion of DINO’s geometric prototypes with Flux’s verb-conditioned attention yields zero-shot affordance maps competitive with weakly supervised methods.


<details>
  <summary>Details</summary>
Motivation: Clarify what it means for a visual system to ‘understand’ affordances, test whether existing VFMs already encode the primitives needed (parts and interactions), and provide a mechanistic account of how perception can ground action without extra training or labels.

Method: Probe VFMs along two axes: (1) extract part-level geometric structure from DINO (self-supervised features/prototypes), and (2) extract verb-conditioned spatial attention as interaction priors from a generative model (Flux). Then fuse these signals in a training-free, zero-shot manner to estimate affordances and compare against weakly supervised baselines.

Result: DINO encodes object parts as geometric prototypes; Flux produces rich, verb-conditioned interaction attention maps; fusing them yields zero-shot affordance estimates that are competitive with weakly supervised approaches, indicating the two capacities are distinct yet composable.

Conclusion: Geometric perception and interaction perception are fundamental, composable building blocks of affordance in VFMs. Their training-free fusion provides a mechanistic pathway from perception to action and shows VFMs already harbor latent affordance knowledge.

Abstract: What does it mean for a visual system to truly understand affordance? We argue that this understanding hinges on two complementary capacities: geometric perception, which identifies the structural parts of objects that enable interaction, and interaction perception, which models how an agent's actions engage with those parts. To test this hypothesis, we conduct a systematic probing of Visual Foundation Models (VFMs). We find that models like DINO inherently encode part-level geometric structures, while generative models like Flux contain rich, verb-conditioned spatial attention maps that serve as implicit interaction priors. Crucially, we demonstrate that these two dimensions are not merely correlated but are composable elements of affordance. By simply fusing DINO's geometric prototypes with Flux's interaction maps in a training-free and zero-shot manner, we achieve affordance estimation competitive with weakly-supervised methods. This final fusion experiment confirms that geometric and interaction perception are the fundamental building blocks of affordance understanding in VFMs, providing a mechanistic account of how perception grounds action.

</details>


### [20] [Leveraging Causal Reasoning Method for Explaining Medical Image Segmentation Models](https://arxiv.org/abs/2602.20511)
*Limai Jiang,Ruitao Xie,Bokai Yang,Huazhen Huang,Juan He,Yufu Huo,Zikai Wang,Yang Wei,Yunpeng Cai*

Main category: cs.CV

TL;DR: They propose a causal-inference-based explainability method for medical image segmentation that backpropagates Average Treatment Effect (ATE) to quantify how input regions and network components influence target segmentation, outperforming prior explainers on two datasets and revealing diverse perceptual strategies across models.


<details>
  <summary>Details</summary>
Motivation: Deep segmentation models are accurate but opaque, which undermines trust in clinical settings. Existing explainability work mainly targets classification, leaving segmentation explanations insufficiently developed.

Method: Introduce an explanation framework that uses causal inference. It estimates Average Treatment Effect (ATE) for perturbations and backpropagates this signal to produce a quantitative influence map over input regions and internal network components for specified segmentation targets. The framework is then applied to several foundational segmentation models.

Result: Across two representative medical imaging datasets, the method yields more faithful (i.e., better aligned with model behavior) explanations than recent segmentation explainability techniques. Using the tool to analyze multiple segmentation architectures uncovers substantial heterogeneity in how models—and even different inputs for the same model—form their perceptual strategies.

Conclusion: Causal, ATE-based attribution improves faithfulness of segmentation explanations and exposes model- and input-dependent perceptual differences, offering actionable insights for optimizing medical segmentation systems; code is available at the provided repository.

Abstract: Medical image segmentation plays a vital role in clinical decision-making, enabling precise localization of lesions and guiding interventions. Despite significant advances in segmentation accuracy, the black-box nature of most deep models has raised growing concerns about their trustworthiness in high-stakes medical scenarios. Current explanation techniques have primarily focused on classification tasks, leaving the segmentation domain relatively underexplored. We introduced an explanation model for segmentation task which employs the causal inference framework and backpropagates the average treatment effect (ATE) into a quantification metric to determine the influence of input regions, as well as network components, on target segmentation areas. Through comparison with recent segmentation explainability techniques on two representative medical imaging datasets, we demonstrated that our approach provides more faithful explanations than existing approaches. Furthermore, we carried out a systematic causal analysis of multiple foundational segmentation models using our method, which reveals significant heterogeneity in perceptual strategies across different models, and even between different inputs for the same model. Suggesting the potential of our method to provide notable insights for optimizing segmentation models. Our code can be found at https://github.com/lcmmai/PdCR.

</details>


### [21] [How Do Inpainting Artifacts Propagate to Language?](https://arxiv.org/abs/2602.20520)
*Pratham Yashwante,Davit Abrahamyan,Shresth Grover,Sukruth Rao*

Main category: cs.CV

TL;DR: Diffusion-based inpainting artifacts measurably alter vision–language captioning: lower-fidelity reconstructions correlate with worse lexical/semantic outputs and induce layer-dependent shifts in internal representations; the paper offers a simple two-stage diagnostic to quantify these effects.


<details>
  <summary>Details</summary>
Motivation: Multimodal systems often rely on reconstructed or edited images (e.g., inpainting), but it is unclear how visual artifacts introduced during reconstruction propagate into language generation. A diagnostic understanding would help evaluate robustness and guide model design and data curation.

Method: A two-stage pipeline: (1) mask images and reconstruct with diffusion-based inpainting; (2) feed original vs reconstructed images to captioning models for controlled comparisons. Evaluate across multiple datasets using pixel-level and perceptual reconstruction metrics, and assess caption quality with lexical and semantic measures. Probe intermediate visual representations and attention maps to analyze where and how artifacts affect model behavior.

Result: Reconstruction fidelity (both pixel-level and perceptual) consistently predicts caption quality (lexical and semantic). Inpainting artifacts cause systematic, layer-dependent changes in representations and attention within vision-language models. Findings replicate across datasets.

Conclusion: Visual reconstruction quality meaningfully shapes language generation in multimodal systems. The proposed setup serves as a practical diagnostic framework, linking inpainting fidelity to caption outcomes and revealing where in the model pipeline artifacts exert influence.

Abstract: We study how visual artifacts introduced by diffusion-based inpainting affect language generation in vision-language models. We use a two-stage diagnostic setup in which masked image regions are reconstructed and then provided to captioning models, enabling controlled comparisons between captions generated from original and reconstructed inputs. Across multiple datasets, we analyze the relationship between reconstruction fidelity and downstream caption quality. We observe consistent associations between pixel-level and perceptual reconstruction metrics and both lexical and semantic captioning performance. Additional analysis of intermediate visual representations and attention patterns shows that inpainting artifacts lead to systematic, layer-dependent changes in model behavior. Together, these results provide a practical diagnostic framework for examining how visual reconstruction quality influences language generation in multimodal systems.

</details>


### [22] [A Lightweight Vision-Language Fusion Framework for Predicting App Ratings from User Interfaces and Metadata](https://arxiv.org/abs/2602.20531)
*Azrin Sultana,Firoz Ahmed*

Main category: cs.CV

TL;DR: Lightweight multimodal model fusing UI visuals (MobileNetV3) and text semantics (DistilBERT) via gated fusion achieves strong app-rating prediction with high correlation and low error, suitable for edge deployment.


<details>
  <summary>Details</summary>
Motivation: Prior rating predictors often use only text or only UI cues, missing complementary signals. There is a need for accurate yet efficient models that jointly exploit visual layout and semantic information and can run on resource-constrained devices.

Method: Extract visual features from mobile UI layouts using MobileNetV3 and textual features using DistilBERT; fuse them with a gated fusion module using Swish activations; predict ratings with an MLP regression head. Evaluate with MAE, RMSE/MSE, R^2, and Pearson correlation; conduct ablations on different encoder combinations.

Result: After 20 training epochs, the model reports MAE=0.1060, RMSE=0.1433, MSE=0.0205, R^2=0.8529, Pearson=0.9251. Ablation studies show multimodal combinations outperform unimodal variants, supporting the effectiveness of the fusion approach.

Conclusion: A compact vision–language architecture effectively predicts app ratings, offering practical value for developers and users and enabling sustainable, on-device deployment. Multimodal fusion is key to performance while keeping the model lightweight.

Abstract: App ratings are among the most significant indicators of the quality, usability, and overall user satisfaction of mobile applications. However, existing app rating prediction models are largely limited to textual data or user interface (UI) features, overlooking the importance of jointly leveraging UI and semantic information. To address these limitations, this study proposes a lightweight vision--language framework that integrates both mobile UI and semantic information for app rating prediction. The framework combines MobileNetV3 to extract visual features from UI layouts and DistilBERT to extract textual features. These multimodal features are fused through a gated fusion module with Swish activations, followed by a multilayer perceptron (MLP) regression head. The proposed model is evaluated using mean absolute error (MAE), root mean square error (RMSE), mean squared error (MSE), coefficient of determination (R2), and Pearson correlation. After training for 20 epochs, the model achieves an MAE of 0.1060, an RMSE of 0.1433, an MSE of 0.0205, an R2 of 0.8529, and a Pearson correlation of 0.9251. Extensive ablation studies further demonstrate the effectiveness of different combinations of visual and textual encoders. Overall, the proposed lightweight framework provides valuable insights for developers and end users, supports sustainable app development, and enables efficient deployment on edge devices.

</details>


### [23] [PFGNet: A Fully Convolutional Frequency-Guided Peripheral Gating Network for Efficient Spatiotemporal Predictive Learning](https://arxiv.org/abs/2602.20537)
*Xinyong Cai,Changbin Sun,Yong Wang,Hongyu Yang,Yuankai Wu*

Main category: cs.CV

TL;DR: PFGNet is a fully convolutional spatiotemporal predictor that uses pixel-wise frequency-guided gating to adaptively modulate large-kernel peripheral responses (with learnable center suppression), yielding efficient, structure-aware forecasts that achieve SOTA or near-SOTA on multiple benchmarks with fewer parameters and FLOPs.


<details>
  <summary>Details</summary>
Motivation: Pure convolutional STPL models are efficient and parallel but suffer from fixed receptive fields that cannot adapt to spatially varying motion; the authors seek an adaptive yet efficient alternative to recurrent or attention-based models.

Method: Introduce PFGNet with a Peripheral Frequency Gating (PFG) block that: (1) extracts localized spectral cues at each pixel; (2) fuses multi-scale large-kernel peripheral responses; (3) applies learnable center suppression to form spatially adaptive band-pass filters. Large kernels are implemented via separable 1D convolutions (1×k then k×1), cutting per-channel compute from O(k^2) to O(2k). The framework is fully convolutional, with no recurrence or attention.

Result: On Moving MNIST, TaxiBJ, Human3.6M, and KTH, PFGNet achieves state-of-the-art or near-SOTA forecasting accuracy while using substantially fewer parameters and FLOPs than competing methods.

Conclusion: Pixel-wise frequency-guided gating can endow fully convolutional models with spatially adaptive receptive fields, enabling efficient, parallel, and accurate spatiotemporal prediction without recurrence or attention.

Abstract: Spatiotemporal predictive learning (STPL) aims to forecast future frames from past observations and is essential across a wide range of applications. Compared with recurrent or hybrid architectures, pure convolutional models offer superior efficiency and full parallelism, yet their fixed receptive fields limit their ability to adaptively capture spatially varying motion patterns. Inspired by biological center-surround organization and frequency-selective signal processing, we propose PFGNet, a fully convolutional framework that dynamically modulates receptive fields through pixel-wise frequency-guided gating. The core Peripheral Frequency Gating (PFG) block extracts localized spectral cues and adaptively fuses multi-scale large-kernel peripheral responses with learnable center suppression, effectively forming spatially adaptive band-pass filters. To maintain efficiency, all large kernels are decomposed into separable 1D convolutions ($1 \times k$ followed by $k \times 1$), reducing per-channel computational cost from $O(k^2)$ to $O(2k)$. PFGNet enables structure-aware spatiotemporal modeling without recurrence or attention. Experiments on Moving MNIST, TaxiBJ, Human3.6M, and KTH show that PFGNet delivers SOTA or near-SOTA forecasting performance with substantially fewer parameters and FLOPs. Our code is available at https://github.com/fhjdqaq/PFGNet.

</details>


### [24] [Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion](https://arxiv.org/abs/2602.20577)
*Jiaru Zhang,Manav Gagvani,Can Cui,Juntong Peng,Ruqi Zhang,Ziran Wang*

Main category: cs.CV

TL;DR: MVLAD-AD is a masked vision–language–action diffusion planner that tokenizes actions as discrete, geometry-aware waypoint codes and decodes them with action priority, delivering faster, more precise, and explainable driving plans than autoregressive and prior diffusion baselines on nuScenes.


<details>
  <summary>Details</summary>
Motivation: LLM/VLM-based end-to-end driving suffers from slow token-by-token inference, low action precision when forcing trajectories into generic language tokens, and weak geometric structure/explainability. The goal is to couple semantic reasoning with efficient, geometry-faithful planning.

Method: 1) Build a compact, discrete action codebook from real driving to represent kinematically feasible waypoints. 2) Learn geometry-aware embeddings so latent distances reflect physical metrics. 3) Use a masked V-L-A diffusion model and an action-priority decoding strategy to generate trajectories first while supporting semantic reasoning tokens.

Result: On nuScenes and related benchmarks, MVLAD-AD reduces inference latency and improves planning precision, outperforming state-of-the-art autoregressive and diffusion planners, while providing high-fidelity, explainable reasoning.

Conclusion: Discretizing actions with a geometry-aligned latent space and prioritizing trajectory decoding enables an efficient, accurate, and interpretable end-to-end autonomous driving planner that bridges the gap between planning performance and semantic explainability.

Abstract: Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged as promising candidates for end-to-end autonomous driving. However, these models typically face challenges in inference latency, action precision, and explainability. Existing autoregressive approaches struggle with slow token-by-token generation, while prior diffusion-based planners often rely on verbose, general-purpose language tokens that lack explicit geometric structure. In this work, we propose Masked Vision-Language-Action Diffusion for Autonomous Driving (MVLAD-AD), a novel framework designed to bridge the gap between efficient planning and semantic explainability via a masked vision-language-action diffusion model. Unlike methods that force actions into the language space, we introduce a discrete action tokenization strategy that constructs a compact codebook of kinematically feasible waypoints from real-world driving distributions. Moreover, we propose geometry-aware embedding learning to ensure that embeddings in the latent space approximate physical geometric metrics. Finally, an action-priority decoding strategy is introduced to prioritize trajectory generation. Extensive experiments on nuScenes and derived benchmarks demonstrate that MVLAD-AD achieves superior efficiency and outperforms state-of-the-art autoregressive and diffusion baselines in planning precision, while providing high-fidelity and explainable reasoning.

</details>


### [25] [PropFly: Learning to Propagate via On-the-Fly Supervision from Pre-trained Video Diffusion Models](https://arxiv.org/abs/2602.20583)
*Wonyong Seo,Jaeho Moon,Jaehyup Lee,Soo Ye Kim,Munchurl Kim*

Main category: cs.CV

TL;DR: PropFly is a training pipeline that turns a pre-trained video diffusion model into a propagation-based editor without needing paired source–edited videos by generating supervision on-the-fly via CFG scaling and optimizing an attached adapter with a new guidance-modulated flow matching loss.


<details>
  <summary>Details</summary>
Motivation: Propagation-based video editing needs large paired (source, edited) video datasets to learn how an edit in one frame should persist across time; such datasets are expensive and hard to curate. The authors seek a data-free (or data-light) way to obtain temporally consistent supervisory signals directly from strong pre-trained video diffusion models.

Method: Generate on-the-fly paired supervision by sampling intermediate noised latents from a pre-trained VDM and producing one-step clean latent estimates under different Classifier-Free Guidance (CFG) scales: low-CFG latents act as structure-preserving “source,” high-CFG latents as “edited” targets. Attach an adapter to the frozen (or pre-trained) VDM and train it with a Guidance-Modulated Flow Matching (GMFM) loss to learn to propagate the target transformation across frames, preserving motion/structure while matching the high-CFG edit.

Result: Across diverse video editing tasks, PropFly yields temporally consistent, high-quality edits and significantly outperforms prior state-of-the-art propagation-based video editing methods according to reported experiments.

Conclusion: On-the-fly supervision from pre-trained VDMs using CFG-driven source/edited latent pairs, combined with an adapter trained via GMFM, provides an effective, paired-data-free pipeline for propagation-based video editing, delivering superior quality and temporal consistency over existing approaches.

Abstract: Propagation-based video editing enables precise user control by propagating a single edited frame into following frames while maintaining the original context such as motion and structures. However, training such models requires large-scale, paired (source and edited) video datasets, which are costly and complex to acquire. Hence, we propose the PropFly, a training pipeline for Propagation-based video editing, relying on on-the-Fly supervision from pre-trained video diffusion models (VDMs) instead of requiring off-the-shelf or precomputed paired video editing datasets. Specifically, our PropFly leverages one-step clean latent estimations from intermediate noised latents with varying Classifier-Free Guidance (CFG) scales to synthesize diverse pairs of 'source' (low-CFG) and 'edited' (high-CFG) latents on-the-fly. The source latent serves as structural information of the video, while the edited latent provides the target transformation for learning propagation. Our pipeline enables an additional adapter attached to the pre-trained VDM to learn to propagate edits via Guidance-Modulated Flow Matching (GMFM) loss, which guides the model to replicate the target transformation. Our on-the-fly supervision ensures the model to learn temporally consistent and dynamic transformations. Extensive experiments demonstrate that our PropFly significantly outperforms the state-of-the-art methods on various video editing tasks, producing high-quality editing results.

</details>


### [26] [Long-Term Multi-Session 3D Reconstruction Under Substantial Appearance Change](https://arxiv.org/abs/2602.20584)
*Beverley Gorry,Tobias Fischer,Michael Milford,Alejandro Fontan*

Main category: cs.CV

TL;DR: Joint SfM that enforces cross‑session correspondences—using complementary handcrafted + learned features and VPR‑guided pair selection—reconstructs a single coherent 3D model across years, succeeding where standard (independent/joint) SfM fails on long‑term coral reef data.


<details>
  <summary>Details</summary>
Motivation: Long‑term environmental monitoring needs consistent 3D reconstructions across months/years, but standard SfM assumes near‑simultaneous capture and minimal appearance change; post‑hoc alignment of separate sessions breaks under substantial visual/structural change (e.g., coral reefs).

Method: Perform a joint SfM that directly enforces cross‑session correspondences. Build correspondences using a mix of robust handcrafted features and learned features to handle large temporal gaps. Use visual place recognition to shortlist likely cross‑session image pairs so expensive learned matching is applied only where useful, ensuring scalability. Optimize all sessions together into one model.

Result: On long‑term coral reef datasets with significant real‑world change, the approach consistently produces coherent joint reconstructions, whereas existing independent and standard joint SfM pipelines fail. The VPR‑guided restriction cuts computational cost and boosts alignment robustness.

Conclusion: Embedding cross‑session matches into a joint SfM pipeline is critical for long‑term monitoring. Combining complementary features and VPR‑guided matching enables scalable, robust, single‑model reconstructions across years, addressing failures of conventional post‑hoc alignment methods.

Abstract: Long-term environmental monitoring requires the ability to reconstruct and align 3D models across repeated site visits separated by months or years. However, existing Structure-from-Motion (SfM) pipelines implicitly assume near-simultaneous image capture and limited appearance change, and therefore fail when applied to long-term monitoring scenarios such as coral reef surveys, where substantial visual and structural change is common. In this paper, we show that the primary limitation of current approaches lies in their reliance on post-hoc alignment of independently reconstructed sessions, which is insufficient under large temporal appearance change. We address this limitation by enforcing cross-session correspondences directly within a joint SfM reconstruction. Our approach combines complementary handcrafted and learned visual features to robustly establish correspondences across large temporal gaps, enabling the reconstruction of a single coherent 3D model from imagery captured years apart, where standard independent and joint SfM pipelines break down. We evaluate our method on long-term coral reef datasets exhibiting significant real-world change, and demonstrate consistent joint reconstruction across sessions in cases where existing methods fail to produce coherent reconstructions. To ensure scalability to large datasets, we further restrict expensive learned feature matching to a small set of likely cross-session image pairs identified via visual place recognition, which reduces computational cost and improves alignment robustness.

</details>


### [27] [Interaction-aware Representation Modeling with Co-occurrence Consistency for Egocentric Hand-Object Parsing](https://arxiv.org/abs/2602.20597)
*Yuejiao Su,Yi Wang,Lei Yao,Yawen Cui,Lap-Pui Chau*

Main category: cs.CV

TL;DR: InterFormer is an interaction-aware transformer for egocentric hand–object parsing that uses contact-driven query initialization, dual-context feature fusion, and a relational loss to curb “interaction illusion,” delivering state-of-the-art results on EgoHOS and OOD mini-HOI4D.


<details>
  <summary>Details</summary>
Motivation: Fine-grained understanding of egocentric hand–object interactions is essential for embodied agents, but current transformer methods struggle with (1) static or semantic-only query initialization that poorly adapts to changing active objects, (2) reliance on pixel-level semantic features that inject non-interaction noise into queries, and (3) physically inconsistent predictions (interaction illusion).

Method: Proposes InterFormer with three components: (1) Dynamic Query Generator (DQG) that grounds queries in spatial hand–object contact dynamics to target hands and active objects; (2) Dual-context Feature Selector (DFS) that fuses coarse interactive cues with semantics to suppress irrelevant content and emphasize interactive relations; (3) Conditional Co-occurrence (CoCo) loss that encodes hand–object relationship constraints to enforce physical consistency. End-to-end training.

Result: InterFormer achieves state-of-the-art performance on EgoHOS and strong out-of-distribution generalization on mini-HOI4D; code and models are released.

Conclusion: Encoding interaction awareness into query initialization, feature selection, and loss design improves accuracy and physical plausibility for egocentric hand–object parsing, with strong generalization across datasets.

Abstract: A fine-grained understanding of egocentric human-environment interactions is crucial for developing next-generation embodied agents. One fundamental challenge in this area involves accurately parsing hands and active objects. While transformer-based architectures have demonstrated considerable potential for such tasks, several key limitations remain unaddressed: 1) existing query initialization mechanisms rely primarily on semantic cues or learnable parameters, demonstrating limited adaptability to changing active objects across varying input scenes; 2) previous transformer-based methods utilize pixel-level semantic features to iteratively refine queries during mask generation, which may introduce interaction-irrelevant content into the final embeddings; and 3) prevailing models are susceptible to "interaction illusion", producing physically inconsistent predictions. To address these issues, we propose an end-to-end Interaction-aware Transformer (InterFormer), which integrates three key components, i.e., a Dynamic Query Generator (DQG), a Dual-context Feature Selector (DFS), and the Conditional Co-occurrence (CoCo) loss. The DQG explicitly grounds query initialization in the spatial dynamics of hand-object contact, enabling targeted generation of interaction-aware queries for hands and various active objects. The DFS fuses coarse interactive cues with semantic features, thereby suppressing interaction-irrelevant noise and emphasizing the learning of interactive relationships. The CoCo loss incorporates hand-object relationship constraints to enhance physical consistency in prediction. Our model achieves state-of-the-art performance on both the EgoHOS and the challenging out-of-distribution mini-HOI4D datasets, demonstrating its effectiveness and strong generalization ability. Code and models are publicly available at https://github.com/yuggiehk/InterFormer.

</details>


### [28] [VAGNet: Grounding 3D Affordance from Human-Object Interactions in Videos](https://arxiv.org/abs/2602.20608)
*Aihua Mao,Kaihang Huang,Yong-Jin Liu,Chee Seng Chan,Ying He*

Main category: cs.CV

TL;DR: They propose video-guided 3D affordance grounding: a model (VAGNet) learns where humans contact 3D objects by aligning dynamic interaction cues from videos with object geometry. They also release PVAD, a paired HOI video–3D dataset. The approach yields state-of-the-art localization of affordance regions, outperforming static-cue methods.


<details>
  <summary>Details</summary>
Motivation: Affordances are defined by actions, yet most 3D grounding methods rely on static images or text, which miss the true, dynamic contact regions involved in human–object interaction. This leads to ambiguous or incorrect localization. Inspired by how humans learn by watching actions, the paper seeks to use interaction dynamics as functional supervision.

Method: Introduce video-guided 3D affordance grounding and VAGNet, a framework that extracts interaction cues from HOI videos and aligns them with 3D object structure to resolve ambiguities that static cues cannot. They also create PVAD, the first dataset pairing HOI videos with corresponding 3D objects to provide explicit functional supervision for contact region grounding.

Result: On the new PVAD benchmark, VAGNet achieves state-of-the-art performance, significantly surpassing static-based baselines in localizing true contact regions (numerical details not provided in the abstract).

Conclusion: Dynamic, action-based supervision substantially improves 3D affordance grounding. With VAGNet and the PVAD dataset, the work establishes a new setting and strong baseline; code and data will be publicly released.

Abstract: 3D object affordance grounding aims to identify regions on 3D objects that support human-object interaction (HOI), a capability essential to embodied visual reasoning. However, most existing approaches rely on static visual or textual cues, neglecting that affordances are inherently defined by dynamic actions. As a result, they often struggle to localize the true contact regions involved in real interactions. We take a different perspective. Humans learn how to use objects by observing and imitating actions, not just by examining shapes. Motivated by this intuition, we introduce video-guided 3D affordance grounding, which leverages dynamic interaction sequences to provide functional supervision. To achieve this, we propose VAGNet, a framework that aligns video-derived interaction cues with 3D structure to resolve ambiguities that static cues cannot address. To support this new setting, we introduce PVAD, the first HOI video-3D pairing affordance dataset, providing functional supervision unavailable in prior works. Extensive experiments on PVAD show that VAGNet achieves state-of-the-art performance, significantly outperforming static-based baselines. The code and dataset will be open publicly.

</details>


### [29] [Knowing the Unknown: Interpretable Open-World Object Detection via Concept Decomposition Model](https://arxiv.org/abs/2602.20616)
*Xueqiang Lv,Shizhou Zhang,Yinghui Xing,Di Xu,Peng Wang,Yanning Zhang*

Main category: cs.CV

TL;DR: IPOW makes open-world object detection interpretable by decomposing RoI features into discriminative, shared, and background concept spaces and adding concept-guided rectification to reduce known–unknown confusion, yielding higher unknown recall with more reliable predictions.


<details>
  <summary>Details</summary>
Motivation: OWOD must detect known classes and flag unknowns. Prior work boosts unknown recall but lacks interpretability, which can cause misclassification when unknowns resemble known classes, reducing reliability and hindering diagnosis of errors.

Method: Embed a Concept Decomposition Model (CDM) into Faster R-CNN to explicitly split coupled RoI features into three concept types: (1) discriminative concepts that maximize separability among known classes; (2) shared concepts that generalize across classes; and (3) background concepts capturing non-object/background cues. Use these interpretable subspaces to analyze where confusion arises (unknowns projected into known discriminative space). Introduce Concept-Guided Rectification (CGR) to adjust predictions based on concept activations, mitigating cases where unknowns intrude into known discriminative regions and transferring shared/background cues to unknown detection.

Result: Across benchmarks, IPOW increases unknown recall while reducing known–unknown confusion, and yields concept-level explanations for both known and unknown detections (qualitative/quantitative evidence implied).

Conclusion: An interpretable OWOD framework that better separates known class evidence from generalizable and background cues can "know the unknown," improving reliability and offering concept-level explanations alongside performance gains.

Abstract: Open-world object detection (OWOD) requires incrementally detecting known categories while reliably identifying unknown objects. Existing methods primarily focus on improving unknown recall, yet overlook interpretability, often leading to known-unknown confusion and reduced prediction reliability. This paper aims to make the entire OWOD framework interpretable, enabling the detector to truly "knowing the unknown". To this end, we propose a concept-driven InterPretable OWOD framework(IPOW) by introducing a Concept Decomposition Model (CDM) for OWOD, which explicitly decomposes the coupled RoI features in Faster R-CNN into discriminative, shared, and background concepts. Discriminative concepts identify the most discriminative features to enlarge the distances between known categories, while shared and background concepts, due to their strong generalization ability, can be readily transferred to detect unknown categories. Leveraging the interpretable framework, we identify that known-unknown confusion arises when unknown objects fall into the discriminative space of known classes. To address this, we propose Concept-Guided Rectification (CGR) to further resolve such confusion. Extensive experiments show that IPOW significantly improves unknown recall while mitigating confusion, and provides concept-level interpretability for both known and unknown predictions.

</details>


### [30] [RecoverMark: Robust Watermarking for Localization and Recovery of Manipulated Faces](https://arxiv.org/abs/2602.20618)
*Haonan An,Xiaohui Ye,Guang Hua,Yihang Tao,Hangcheng Cao,Xiangyu Yu,Yuguang Fang*

Main category: cs.CV

TL;DR: RecoverMark embeds a representation of the protected face into the image’s background, enabling robust manipulation localization, content recovery, and ownership verification, even under watermark-removal attacks. Through a two-stage, attack-simulating training pipeline, it overcomes the fragility and interference issues of prior fragile/dual-watermark methods and generalizes across seen/unseen attacks and datasets.


<details>
  <summary>Details</summary>
Motivation: AI-driven face manipulation threatens visual authenticity and IP protection. Existing fragile watermarks assume attackers won’t target the watermark and thus are easily removed; dual-watermark schemes (fragile + robust) further suffer from mutual interference and limited capacity, degrading localization/recovery. A defense is needed that remains effective when adversaries actively attempt watermark removal.

Method: Key insights: (1) Attackers must preserve background semantic consistency to avoid visual suspicion, even under global, imperceptible removal attempts; (2) using the image’s own face content as the watermark boosts extraction robustness. RecoverMark embeds the protected face information into the surrounding background. It employs a robust two-stage training paradigm with tailored distortion layers to emulate diverse removal/manipulation attacks and a progressive strategy to stabilize and strengthen embedding/extraction for non-fragile localization, recovery, and ownership verification.

Result: Extensive experiments show RecoverMark maintains localization and recovery performance and ownership verification robustness against a broad range of seen and unseen attacks. It generalizes well to both in-distribution and out-of-distribution data, outperforming conventional fragile/dual-watermark baselines under adversarial conditions.

Conclusion: By treating the face content as the watermark and embedding it into the background with attack-aware training, RecoverMark provides a unified, robust solution for manipulation localization, content recovery, and IP verification, addressing core fragility and interference limitations of prior methods and remaining resilient to practical watermark-removal attacks.

Abstract: The proliferation of AI-generated content has facilitated sophisticated face manipulation, severely undermining visual integrity and posing unprecedented challenges to intellectual property. In response, a common proactive defense leverages fragile watermarks to detect, localize, or even recover manipulated regions. However, these methods always assume an adversary unaware of the embedded watermark, overlooking their inherent vulnerability to watermark removal attacks. Furthermore, this fragility is exacerbated in the commonly used dual-watermark strategy that adds a robust watermark for image ownership verification, where mutual interference and limited embedding capacity reduce the fragile watermark's effectiveness. To address the gap, we propose RecoverMark, a watermarking framework that achieves robust manipulation localization, content recovery, and ownership verification simultaneously. Our key insight is twofold. First, we exploit a critical real-world constraint: an adversary must preserve the background's semantic consistency to avoid visual detection, even if they apply global, imperceptible watermark removal attacks. Second, using the image's own content (face, in this paper) as the watermark enhances extraction robustness. Based on these insights, RecoverMark treats the protected face content itself as the watermark and embeds it into the surrounding background. By designing a robust two-stage training paradigm with carefully crafted distortion layers that simulate comprehensive potential attacks and a progressive training strategy, RecoverMark achieves a robust watermark embedding in no fragile manner for image manipulation localization, recovery, and image IP protection simultaneously. Extensive experiments demonstrate the proposed RecoverMark's robustness against both seen and unseen attacks and its generalizability to in-distribution and out-of-distribution data.

</details>


### [31] [Object-Scene-Camera Decomposition and Recomposition for Data-Efficient Monocular 3D Object Detection](https://arxiv.org/abs/2602.20627)
*Zhaonian Kuang,Rui Ding,Meng Yang,Xinhu Zheng,Gang Hua*

Main category: cs.CV

TL;DR: Decouple objects, scenes, and camera poses from monocular images into textured 3D point models and backgrounds, then recombine them on-the-fly with pose perturbations to synthesize diverse training data—serving as a plug-and-play augmentation that (per authors) boosts multiple M3OD models under full and sparse supervision on KITTI and Waymo.


<details>
  <summary>Details</summary>
Motivation: Monocular 3D object detection is ill‑posed and typically overfits due to dataset bias: objects, scenes, and camera poses are tightly entangled in collected images, limiting diversity and hindering effective use of labeled data.

Method: An online data manipulation pipeline: (1) decompose training images into textured 3D object point models and background scenes with efficient compute/storage; (2) each epoch, recompose fresh training images by inserting 3D objects into freespace of background scenes and rendering them with perturbed camera poses from the textured 3D point representation. Works as a plug‑and‑play component for fully or sparsely supervised training; in sparse supervision, only closest objects are annotated, with flexibility to scale annotations.

Result: Applied across five representative M3OD models and evaluated on KITTI and Waymo. The authors claim it boosts model performance and improves data utilization/generalization; details of quantitative gains are not provided in the abstract.

Conclusion: Systematically decoupling and recombining object, scene, and camera pose expands training diversity, mitigates overfitting, and improves training efficiency for M3OD. The approach is model‑agnostic and annotation‑efficient, making it practical for both full and sparse supervision settings.

Abstract: Monocular 3D object detection (M3OD) is intrinsically ill-posed, hence training a high-performance deep learning based M3OD model requires a humongous amount of labeled data with complicated visual variation from diverse scenes, variety of objects and camera poses.However, we observe that, due to strong human bias, the three independent entities, i.e., object, scene, and camera pose, are always tightly entangled when an image is captured to construct training data. More specifically, specific 3D objects are always captured in particular scenes with fixed camera poses, and hence lacks necessary diversity. Such tight entanglement induces the challenging issues of insufficient utilization and overfitting to uniform training data. To mitigate this, we propose an online object-scene-camera decomposition and recomposition data manipulation scheme to more efficiently exploit the training data. We first fully decompose training images into textured 3D object point models and background scenes in an efficient computation and storage manner. We then continuously recompose new training images in each epoch by inserting the 3D objects into the freespace of the background scenes, and rendering them with perturbed camera poses from textured 3D point representation. In this way, the refreshed training data in all epochs can cover the full spectrum of independent object, scene, and camera pose combinations. This scheme can serve as a plug-and-play component to boost M3OD models, working flexibly with both fully and sparsely supervised settings. In the sparsely-supervised setting, objects closest to the ego-camera for all instances are sparsely annotated. We then can flexibly increase the annotated objects to control annotation cost. For validation, our method is widely applied to five representative M3OD models and evaluated on both the KITTI and the more complicated Waymo datasets.

</details>


### [32] [From Pairs to Sequences: Track-Aware Policy Gradients for Keypoint Detection](https://arxiv.org/abs/2602.20630)
*Yepeng Liu,Hao Li,Liwen Yang,Fangzhen Li,Xudi Ge,Yuliang Gu,kuang Gao,Bing Wang,Guang Chen,Hangjun Ye,Yongchao Xu*

Main category: cs.CV

TL;DR: TraqPoint uses reinforcement learning to select keypoints on image sequences, directly optimizing long-term track quality via a track-aware reward, and shows significant gains over some SOTA methods on relative pose estimation and 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: Pair-trained keypoint methods do not explicitly optimize for long-term trackability under large viewpoint and illumination changes across sequences, limiting performance in SfM/SLAM.

Method: Reframe keypoint detection as a sequential decision-making problem. Train an end-to-end RL policy (policy gradient) on image sequences with a track-aware reward that jointly promotes cross-view consistency (trackability) and distinctiveness of keypoints, optimizing a Track-quality (Traq) objective.

Result: On sparse matching benchmarks—including relative pose estimation and 3D reconstruction—TraqPoint significantly outperforms several state-of-the-art keypoint detection/description baselines.

Conclusion: Optimizing keypoint selection with sequence-aware RL and a track-quality reward yields more robust, distinctive, and longer tracks, leading to improved performance in downstream 3D tasks like SfM/SLAM.

Abstract: Keypoint-based matching is a fundamental component of modern 3D vision systems, such as Structure-from-Motion (SfM) and SLAM. Most existing learning-based methods are trained on image pairs, a paradigm that fails to explicitly optimize for the long-term trackability of keypoints across sequences under challenging viewpoint and illumination changes. In this paper, we reframe keypoint detection as a sequential decision-making problem. We introduce TraqPoint, a novel, end-to-end Reinforcement Learning (RL) framework designed to optimize the \textbf{Tra}ck-\textbf{q}uality (Traq) of keypoints directly on image sequences. Our core innovation is a track-aware reward mechanism that jointly encourages the consistency and distinctiveness of keypoints across multiple views, guided by a policy gradient method. Extensive evaluations on sparse matching benchmarks, including relative pose estimation and 3D reconstruction, demonstrate that TraqPoint significantly outperforms some state-of-the-art (SOTA) keypoint detection and description methods.

</details>


### [33] [Boosting Instance Awareness via Cross-View Correlation with 4D Radar and Camera for 3D Object Detection](https://arxiv.org/abs/2602.20632)
*Xiaokai Bai,Lianqing Zheng,Si-Yuan Cao,Xiaohan Zhang,Zhe Wu,Beinan Yu,Fang Wang,Jie Bai,Hui-Liang Shen*

Main category: cs.CV

TL;DR: SIFormer is a scene–instance aware transformer that fuses 4D millimeter-wave radar and camera data for 3D object detection by injecting 2D instance cues into BEV space, suppressing background noise, and using transformer-based fusion—achieving state-of-the-art results on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: 4D radar is robust and affordable for autonomous driving but provides sparse, weak geometry, making instance activation hard. Existing fusion strategies trade off global context (BEV-level) versus instance detail (perspective-level). A method is needed to jointly capture holistic scene understanding and strong instance focus under weak radar geometry.

Method: SIFormer: (1) During view transformation, suppresses background via segmentation- and depth-guided localization. (2) Introduces a cross-view activation mechanism to inject 2D image instance cues into BEV, strengthening instance awareness when radar geometry is weak. (3) Employs a transformer-based fusion module to aggregate complementary image semantics and radar geometry for robust 3D detection.

Result: Bridges BEV- and perspective-level fusion strengths, improving instance awareness and detection accuracy; achieves state-of-the-art performance on View-of-Delft, TJ4DRadSet, and NuScenes datasets. Code available at github.com/shawnnnkb/SIFormer.

Conclusion: By coupling scene-level context with instance-level activation and robust cross-modal fusion, SIFormer mitigates radar sparsity and enhances 3D detection, unifying BEV and perspective paradigms into a more reliable radar–camera fusion framework.

Abstract: 4D millimeter-wave radar has emerged as a promising sensing modality for autonomous driving due to its robustness and affordability. However, its sparse and weak geometric cues make reliable instance activation difficult, limiting the effectiveness of existing radar-camera fusion paradigms. BEV-level fusion offers global scene understanding but suffers from weak instance focus, while perspective-level fusion captures instance details but lacks holistic context. To address these limitations, we propose SIFormer, a scene-instance aware transformer for 3D object detection using 4D radar and camera. SIFormer first suppresses background noise during view transformation through segmentation- and depth-guided localization. It then introduces a cross-view activation mechanism that injects 2D instance cues into BEV space, enabling reliable instance awareness under weak radar geometry. Finally, a transformer-based fusion module aggregates complementary image semantics and radar geometry for robust perception. As a result, with the aim of enhancing instance awareness, SIFormer bridges the gap between the two paradigms, combining their complementary strengths to address inherent sparse nature of radar and improve detection accuracy. Experiments demonstrate that SIFormer achieves state-of-the-art performance on View-of-Delft, TJ4DRadSet and NuScenes datasets. Source code is available at github.com/shawnnnkb/SIFormer.

</details>


### [34] [SurgAtt-Tracker: Online Surgical Attention Tracking via Temporal Proposal Reranking and Motion-Aware Refinement](https://arxiv.org/abs/2602.20636)
*Rulin Zhou,Guankun Wang,An Wang,Yujie Ma,Lixin Ouyang,Bolin Cui,Junyan Li,Chaowei Zhu,Mingyang Li,Ming Chen,Xiaopin Zhong,Peng Lu,Jiankun Wang,Xianming Liu,Hongliang Ren*

Main category: cs.CV

TL;DR: They propose SurgAtt-Tracker, a spatio-temporal, heatmap-based surgical attention tracker with proposal-level re-ranking and motion-aware refinement, plus a new 1.16M-sample benchmark. It achieves state-of-the-art robustness and supplies frame-wise FoV guidance for robotic camera control.


<details>
  <summary>Details</summary>
Motivation: Minimally invasive surgery needs accurate, stable field-of-view (FoV) guidance. Prior work often mixes attention estimation with camera control or assumes specific objects, limiting robustness and interpretability. A principled, attention-focused model and scalable benchmark are missing.

Method: Model surgeon focus as a dense, frame-wise attention heatmap and learn it spatio-temporally. SurgAtt-Tracker exploits temporal coherence via proposal-level reranking and motion-aware refinement instead of direct regression. They also introduce SurgAtt-1.16M, a large-scale, clinically annotated benchmark for heatmap-based attention across procedures and institutions.

Result: Across multiple surgical datasets, SurgAtt-Tracker attains state-of-the-art attention tracking and strong robustness under occlusion, multiple-instrument interference, and cross-domain scenarios.

Conclusion: A dedicated, interpretable attention-tracking framework plus a large benchmark improves reliability of FoV guidance and directly enables downstream robotic FoV planning and automated camera control.

Abstract: Accurate and stable field-of-view (FoV) guidance is critical for safe and efficient minimally invasive surgery, yet existing approaches often conflate visual attention estimation with downstream camera control or rely on direct object-centric assumptions. In this work, we formulate surgical attention tracking as a spatio-temporal learning problem and model surgeon focus as a dense attention heatmap, enabling continuous and interpretable frame-wise FoV guidance. We propose SurgAtt-Tracker, a holistic framework that robustly tracks surgical attention by exploiting temporal coherence through proposal-level reranking and motion-aware refinement, rather than direct regression. To support systematic training and evaluation, we introduce SurgAtt-1.16M, a large-scale benchmark with a clinically grounded annotation protocol that enables comprehensive heatmap-based attention analysis across procedures and institutions. Extensive experiments on multiple surgical datasets demonstrate that SurgAtt-Tracker consistently achieves state-of-the-art performance and strong robustness under occlusion, multi-instrument interference, and cross-domain settings. Beyond attention tracking, our approach provides a frame-wise FoV guidance signal that can directly support downstream robotic FoV planning and automatic camera control.

</details>


### [35] [Dataset Color Quantization: A Training-Oriented Framework for Dataset-Level Compression](https://arxiv.org/abs/2602.20650)
*Chenyue Yu,Lingao Xiao,Jinhong Deng,Ivor W. Tsang,Yang He*

Main category: cs.CV

TL;DR: DCQ compresses image datasets by quantizing colors with shared palettes across similar images, guided by model-aware importance, to preserve structure and training-relevant information—yielding better accuracy under strong storage compression.


<details>
  <summary>Details</summary>
Motivation: Large image datasets strain storage, especially in resource-limited settings. Prior dataset reduction often drops full samples and overlooks within-image redundancy, particularly excessive color-space detail not essential for learning.

Method: Dataset Color Quantization (DCQ): (1) enforce consistent palette representations among similar images; (2) retain semantically important colors using model-perception guidance; (3) preserve structural details needed for effective feature learning—forming a unified, dataset-level color-space compression pipeline.

Result: Across CIFAR-10/100, Tiny-ImageNet, and ImageNet-1K, DCQ maintains or improves training performance even under aggressive compression, outperforming baselines that discard samples or ignore color redundancy.

Conclusion: DCQ provides a scalable, robust approach to dataset-level storage reduction by removing color redundancy while safeguarding semantics and structure, enabling effective training in resource-constrained deployments.

Abstract: Large-scale image datasets are fundamental to deep learning, but their high storage demands pose challenges for deployment in resource-constrained environments. While existing approaches reduce dataset size by discarding samples, they often ignore the significant redundancy within each image -- particularly in the color space. To address this, we propose Dataset Color Quantization (DCQ), a unified framework that compresses visual datasets by reducing color-space redundancy while preserving information crucial for model training. DCQ achieves this by enforcing consistent palette representations across similar images, selectively retaining semantically important colors guided by model perception, and maintaining structural details necessary for effective feature learning. Extensive experiments across CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K show that DCQ significantly improves training performance under aggressive compression, offering a scalable and robust solution for dataset-level storage reduction. Code is available at \href{https://github.com/he-y/Dataset-Color-Quantization}{https://github.com/he-y/Dataset-Color-Quantization}.

</details>


### [36] [Communication-Inspired Tokenization for Structured Image Representations](https://arxiv.org/abs/2602.20731)
*Aram Davtyan,Yusuf Sahin,Yasaman Haghighi,Sebastian Stapf,Pablo Acuaviva,Alexandre Alahi,Paolo Favaro*

Main category: cs.CV

TL;DR: COMiT is a communication-inspired discrete image tokenizer that builds a fixed-length token sequence iteratively from localized crops, refining and reorganizing tokens across steps. Using a single transformer trained end-to-end with flow-matching reconstruction and semantic alignment losses, it yields object-centric, interpretable tokens and improves compositional generalization and relational reasoning over prior tokenizers.


<details>
  <summary>Details</summary>
Motivation: Most discrete image tokenizers are optimized for compression/reconstruction and tend to encode local textures rather than object-level semantics, limiting downstream reasoning and compositionality in vision/multimodal transformers. The authors seek a tokenization scheme that captures structured, object-centric information under a token budget, mirroring incremental human communication.

Method: COMiT constructs a latent message as a fixed-budget sequence of discrete tokens via iterative attention to localized image crops. At each iteration, the model integrates new visual evidence and refines/reorders the existing token sequence. The final message conditions a flow-matching decoder to reconstruct the full image. Both encoder (sequential tokenization) and decoder are implemented in a single transformer and trained end-to-end with a combination of flow-matching reconstruction loss and a semantic representation alignment loss.

Result: Empirically, semantic alignment helps grounding but the attentive, sequential tokenization is the key driver for inducing interpretable, object-centric token structure. COMiT achieves substantially better compositional generalization and relational reasoning than prior tokenizers while maintaining strong reconstruction via the flow-matching decoder.

Conclusion: A communication-inspired, iterative tokenization process under a fixed token budget can produce structured, object-centric discrete visual tokens that better support reasoning and compositionality. Integrating encoding and decoding in one transformer and training with reconstruction plus semantic alignment effectively balances fidelity and semantics, outperforming existing approaches.

Abstract: Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture local texture rather than object-level semantic structure. Inspired by the incremental and compositional nature of human communication, we introduce COMmunication inspired Tokenization (COMiT), a framework for learning structured discrete visual token sequences. COMiT constructs a latent message within a fixed token budget by iteratively observing localized image crops and recurrently updating its discrete representation. At each step, the model integrates new visual information while refining and reorganizing the existing token sequence. After several encoding iterations, the final message conditions a flow-matching decoder that reconstructs the full image. Both encoding and decoding are implemented within a single transformer model and trained end-to-end using a combination of flow-matching reconstruction and semantic representation alignment losses. Our experiments demonstrate that while semantic alignment provides grounding, attentive sequential tokenization is critical for inducing interpretable, object-centric token structure and substantially improving compositional generalization and relational reasoning over prior methods.

</details>


### [37] [Real-time Motion Segmentation with Event-based Normal Flow](https://arxiv.org/abs/2602.20790)
*Sheng Zhong,Zhongyang Ren,Xiya Zhu,Dehao Yuan,Cornelia Fermuller,Yi Zhou*

Main category: cs.CV

TL;DR: Real-time event-based motion segmentation using dense normal flow and graph-cut optimization, delivering ~800× speedup over prior open-source methods without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Raw event data are sparse and expensive to process directly, making state-of-the-art approaches too slow for real-time motion segmentation in dynamic scenes. A compact intermediate motion representation is needed.

Method: Estimate dense normal flow from local event neighborhoods; use it to initialize and fit a small set of motion models; cast segmentation as an energy minimization solved by graph cuts; iteratively alternate between normal-flow clustering and motion-model fitting to refine segments.

Result: Achieves near 800× computational speedup versus an open-source SOTA baseline while maintaining high segmentation accuracy across multiple public datasets, enabling real-time performance.

Conclusion: Normal-flow–based representations coupled with graph-cut optimization provide efficient and accurate event-based motion segmentation, scaling to real-time by limiting candidate motion models and iteratively refining them.

Abstract: Event-based cameras are bio-inspired sensors with pixels that independently and asynchronously respond to brightness changes at microsecond resolution, offering the potential to handle visual tasks in challenging scenarios. However, due to the sparse information content in individual events, directly processing the raw event data to solve vision tasks is highly inefficient, which severely limits the applicability of state-of-the-art methods in real-time tasks, such as motion segmentation, a fundamental task for dynamic scene understanding. Incorporating normal flow as an intermediate representation to compress motion information from event clusters within a localized region provides a more effective solution. In this work, we propose a normal flow-based motion segmentation framework for event-based vision. Leveraging the dense normal flow directly learned from event neighborhoods as input, we formulate the motion segmentation task as an energy minimization problem solved via graph cuts, and optimize it iteratively with normal flow clustering and motion model fitting. By using a normal flow-based motion model initialization and fitting method, the proposed system is able to efficiently estimate the motion models of independently moving objects with only a limited number of candidate models, which significantly reduces the computational complexity and ensures real-time performance, achieving nearly a 800x speedup in comparison to the open-source state-of-the-art method. Extensive evaluations on multiple public datasets fully demonstrate the accuracy and efficiency of our framework.

</details>


### [38] [SIMSPINE: A Biomechanics-Aware Simulation Framework for 3D Spine Motion Annotation and Benchmarking](https://arxiv.org/abs/2602.20792)
*Muhammad Saif Ullah Khan,Didier Stricker*

Main category: cs.CV

TL;DR: Introduces a biomechanics-aware keypoint simulation framework and releases SIMSPINE, a large-scale dataset (2.14M frames) with vertebra-level 3D spinal annotations and baselines, yielding notable gains in 2D spine detection and establishing a unified benchmark for anatomically valid 3D spine estimation.


<details>
  <summary>Details</summary>
Motivation: Spinal motion is crucial for biomechanics but is underexplored in computer vision due to complex multi-joint kinematics and the scarcity of large-scale, anatomically consistent 3D annotations.

Method: Use musculoskeletal modeling to generate anatomically consistent vertebra-level 3D spinal keypoints that augment existing human pose datasets. Build SIMSPINE from indoor multi-camera captures without external restraints. Provide pretrained baselines: fine-tuned 2D detectors, monocular 3D pose lifting, and multi-view reconstruction pipelines.

Result: SIMSPINE comprises 2.14 million frames enabling data-driven learning of vertebral kinematics from subtle posture variations. Baselines improve 2D spine metrics: AUC from 0.63 to 0.80 in controlled settings and AP from 0.91 to 0.93 for in-the-wild tracking; provide unified benchmark tools spanning 2D, monocular 3D, and multi-view.

Conclusion: The framework and dataset enable reproducible, anatomically grounded 3D spine estimation in natural conditions, bridging musculoskeletal simulation and computer vision, and advancing research in vision-based biomechanics, motion analysis, and digital human modeling.

Abstract: Modeling spinal motion is fundamental to understanding human biomechanics, yet remains underexplored in computer vision due to the spine's complex multi-joint kinematics and the lack of large-scale 3D annotations. We present a biomechanics-aware keypoint simulation framework that augments existing human pose datasets with anatomically consistent 3D spinal keypoints derived from musculoskeletal modeling. Using this framework, we create the first open dataset, named SIMSPINE, which provides sparse vertebra-level 3D spinal annotations for natural full-body motions in indoor multi-camera capture without external restraints. With 2.14 million frames, this enables data-driven learning of vertebral kinematics from subtle posture variations and bridges the gap between musculoskeletal simulation and computer vision. In addition, we release pretrained baselines covering fine-tuned 2D detectors, monocular 3D pose lifting models, and multi-view reconstruction pipelines, establishing a unified benchmark for biomechanically valid spine motion estimation. Specifically, our 2D spine baselines improve the state-of-the-art from 0.63 to 0.80 AUC in controlled environments, and from 0.91 to 0.93 AP for in-the-wild spine tracking. Together, the simulation framework and SIMSPINE dataset advance research in vision-based biomechanics, motion analysis, and digital human modeling by enabling reproducible, anatomically grounded 3D spine estimation under natural conditions.

</details>


### [39] [RU4D-SLAM: Reweighting Uncertainty in Gaussian Splatting SLAM for 4D Scene Reconstruction](https://arxiv.org/abs/2602.20807)
*Yangfan Zhao,Hanwei Zhang,Ke Huang,Qiufeng Wang,Zhenzhou Shao,Dengyu Wu*

Main category: cs.CV

TL;DR: RU4D-SLAM fuses 4D Gaussian splatting with uncertainty-aware SLAM to handle motion and blur, adding temporal modeling, motion-blur rendering, semantic-guided uncertainty reweighting, and a learnable opacity weight. It achieves superior tracking accuracy and 4D reconstruction in dynamic, low-quality scenarios.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian splatting + SLAM works well in static scenes but fails with moving objects and motion blur, which corrupt both reconstruction and pose tracking. 4D splatting is promising but underused for SLAM; there is a need for a 4D-aware, uncertainty-driven approach that remains robust in dynamic environments.

Method: Proposes RU4D-SLAM, a 4D scene reconstruction and SLAM framework that: (1) augments 3D Gaussian splats with temporal factors for 4D mapping; (2) integrates motion blur rendering to better represent dynamics and synthesize blurred images; (3) extends per-pixel uncertainty modeling from static to blurred imagery for improved tracking; (4) introduces a semantic-guided reweighting scheme to refine per-pixel uncertainty in dynamic regions; and (5) uses a learnable opacity weight for adaptive 4D mapping.

Result: On standard benchmarks, RU4D-SLAM substantially outperforms state-of-the-art methods in trajectory accuracy and 4D reconstruction quality, particularly for scenes with moving objects and low-quality (blurred) inputs. Code is publicly available.

Conclusion: Embedding temporal modeling and uncertainty reweighting into Gaussian-splatting SLAM markedly improves robustness and accuracy in dynamic scenes, indicating 4D-aware SLAM as a strong direction for handling motion and blur in real-world environments.

Abstract: Combining 3D Gaussian splatting with Simultaneous Localization and Mapping (SLAM) has gained popularity as it enables continuous 3D environment reconstruction during motion. However, existing methods struggle in dynamic environments, particularly moving objects complicate 3D reconstruction and, in turn, hinder reliable tracking. The emergence of 4D reconstruction, especially 4D Gaussian splatting, offers a promising direction for addressing these challenges, yet its potential for 4D-aware SLAM remains largely underexplored. Along this direction, we propose a robust and efficient framework, namely Reweighting Uncertainty in Gaussian Splatting SLAM (RU4D-SLAM) for 4D scene reconstruction, that introduces temporal factors into spatial 3D representation while incorporating uncertainty-aware perception of scene changes, blurred image synthesis, and dynamic scene reconstruction. We enhance dynamic scene representation by integrating motion blur rendering, and improve uncertainty-aware tracking by extending per-pixel uncertainty modeling, which is originally designed for static scenarios, to handle blurred images. Furthermore, we propose a semantic-guided reweighting mechanism for per-pixel uncertainty estimation in dynamic scenes, and introduce a learnable opacity weight to support adaptive 4D mapping. Extensive experiments on standard benchmarks demonstrate that our method substantially outperforms state-of-the-art approaches in both trajectory accuracy and 4D scene reconstruction, particularly in dynamic environments with moving objects and low-quality inputs. Code available: https://ru4d-slam.github.io

</details>


### [40] [Vision-Language Models for Ergonomic Assessment of Manual Lifting Tasks: Estimating Horizontal and Vertical Hand Distances from RGB Video](https://arxiv.org/abs/2602.20658)
*Mohammad Sadra Rajabi,Aanuoluwapo Ojelade,Sunwook Kim,Maury A. Nussbaum*

Main category: cs.CV

TL;DR: VLM-driven video pipelines can non-invasively estimate the RNLE horizontal (H) and vertical (V) hand distances; a segmentation + multi-view approach yields ~5–8 cm MAE and 20–40% lower error than detection-only, indicating practical feasibility.


<details>
  <summary>Details</summary>
Motivation: RNLE requires H and V distances that are typically measured manually or with specialized sensors—methods that are intrusive, time-consuming, and hard to scale in real-world workplaces. A reliable RGB video–based alternative would streamline ergonomic risk assessments and broaden deployment.

Method: Two multi-stage VLM pipelines were built: (1) text-guided detection-only and (2) detection plus pixel-level segmentation. Both use text prompts to localize task-relevant ROIs, extract visual features, and a transformer-based temporal regressor to estimate H and V at lift start and end. Performance was tested on varied lifting tasks with leave-one-subject-out validation across seven camera-view conditions.

Result: Accuracy depended on pipeline and camera view. The segmentation-based, multi-view pipeline performed best, achieving mean absolute errors ≈6–8 cm (H) and 5–8 cm (V). Relative to detection-only, segmentation reduced error by ~20–30% for H and ~35–40% for V.

Conclusion: VLM-based pipelines are feasible for estimating RNLE distance parameters from RGB video. Pixel-level segmentation and multi-view inputs meaningfully improve accuracy, supporting progress toward practical, non-invasive ergonomic risk assessment.

Abstract: Manual lifting tasks are a major contributor to work-related musculoskeletal disorders, and effective ergonomic risk assessment is essential for quantifying physical exposure and informing ergonomic interventions. The Revised NIOSH Lifting Equation (RNLE) is a widely used ergonomic risk assessment tool for lifting tasks that relies on six task variables, including horizontal (H) and vertical (V) hand distances; such distances are typically obtained through manual measurement or specialized sensing systems and are difficult to use in real-world environments. We evaluated the feasibility of using innovative vision-language models (VLMs) to non-invasively estimate H and V from RGB video streams. Two multi-stage VLM-based pipelines were developed: a text-guided detection-only pipeline and a detection-plus-segmentation pipeline. Both pipelines used text-guided localization of task-relevant regions of interest, visual feature extraction from those regions, and transformer-based temporal regression to estimate H and V at the start and end of a lift. For a range of lifting tasks, estimation performance was evaluated using leave-one-subject-out validation across the two pipelines and seven camera view conditions. Results varied significantly across pipelines and camera view conditions, with the segmentation-based, multi-view pipeline consistently yielding the smallest errors, achieving mean absolute errors of approximately 6-8 cm when estimating H and 5-8 cm when estimating V. Across pipelines and camera view configurations, pixel-level segmentation reduced estimation error by approximately 20-30% for H and 35-40% for V relative to the detection-only pipeline. These findings support the feasibility of VLM-based pipelines for video-based estimation of RNLE distance parameters.

</details>


### [41] [Onboard-Targeted Segmentation of Straylight in Space Camera Sensors](https://arxiv.org/abs/2602.20709)
*Riccardo Gallon,Fabian Schiemenz,Alessandra Menicucci,Eberhard Gill*

Main category: cs.CV

TL;DR: AI-based semantic segmentation of solar-induced straylight in space camera imagery, using cross-domain pretraining (Flare7k++) and a lightweight DeepLabV3-MobileNetV3 model tailored for onboard deployment, with system-aware evaluation metrics interfaced to navigation.


<details>
  <summary>Details</summary>
Motivation: Straylight around the camera FoV degrades navigation/vision performance in spacecraft, yet realistic space datasets are scarce and onboard compute is limited. A method is needed that generalizes across diverse flare textures and can run on resource-constrained hardware, with performance assessed in terms meaningful to the navigation pipeline.

Method: Use semantic segmentation with DeepLabV3 and a MobileNetV3 backbone to detect/segment straylight. Pretrain on the public Flare7k++ (non-space flare) dataset to improve generalization, then apply to a published anomalous space-image dataset. Architect the model for low resource usage and define an interface to the onboard navigation pipeline. Propose custom system-level metrics tailored to that interface.

Result: The abstract reports a working segmentation pipeline designed for spacecraft constraints and evaluated via custom, navigation-relevant metrics. It emphasizes generalization from cross-domain pretraining but provides no quantitative results in the abstract.

Conclusion: Cross-domain pretraining plus a lightweight segmentation model offers a practical approach to onboard straylight handling and system-level assessment. Integration with navigation is considered via a defined interface and metrics; future validation with more realistic space data and in-orbit testing would solidify effectiveness.

Abstract: This study details an artificial intelligence (AI)-based methodology for the semantic segmentation of space camera faults. Specifically, we address the segmentation of straylight effects induced by solar presence around the camera's Field of View (FoV). Anomalous images are sourced from our published dataset. Our approach emphasizes generalization across diverse flare textures, leveraging pre-training on a public dataset (Flare7k++) including flares in various non-space contexts to mitigate the scarcity of realistic space-specific data. A DeepLabV3 model with MobileNetV3 backbone performs the segmentation task. The model design targets deployment in spacecraft resource-constrained hardware. Finally, based on a proposed interface between our model and the onboard navigation pipeline, we develop custom metrics to assess the model's performance in the system-level context.

</details>


### [42] [See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis](https://arxiv.org/abs/2602.20951)
*Jaehyun Park,Minyoung Ahn,Minkyu Kim,Jonghyun Lee,Jae-Gil Lee,Dongmin Park*

Main category: cs.CV

TL;DR: ArtiAgent is a three-agent pipeline that automatically creates paired real vs. artifact-injected images by manipulating patch-wise embeddings inside a diffusion transformer, then curates and explains the artifacts, yielding a 100K-image, richly annotated artifact dataset.


<details>
  <summary>Details</summary>
Motivation: AI-generated images frequently contain artifacts that degrade realism. Scaling models/pretraining may not fully remove them, while prior artifact-aware methods rely on costly, hard-to-scale human-labeled datasets. An automated, reliable way to acquire artifact-annotated data is needed.

Method: ArtiAgent assembles: (1) a perception agent to detect and ground entities/sub-entities in real images; (2) a synthesis agent that injects artifacts using dedicated tools via novel patch-wise embedding manipulation within a diffusion transformer, producing artifact-injected counterparts; and (3) a curation agent that filters outputs and attaches local and global explanations. The system outputs paired clean/artifact images with annotations.

Result: Using ArtiAgent, the authors synthesize a dataset of 100K images with rich, multi-level artifact annotations and show its efficacy and versatility across multiple applications; code is released.

Conclusion: ArtiAgent offers a scalable, automated alternative to manual artifact labeling, enabling systematic artifact generation, curation, and explanation. It supports artifact-aware training, benchmarking, and analysis across diverse tasks, though detailed quantitative gains are not specified in the abstract.

Abstract: Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation a highly crucial area of study. Previous artifact-aware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach to reliably acquire artifact-annotated datasets. In this paper, we propose ArtiAgent, which efficiently creates pairs of real and artifact-injected images. It comprises three agents: a perception agent that recognizes and grounds entities and subentities from real images, a synthesis agent that introduces artifacts via artifact injection tools through novel patch-wise embedding manipulation within a diffusion transformer, and a curation agent that filters the synthesized artifacts and generates both local and global explanations for each instance. Using ArtiAgent, we synthesize 100K images with rich artifact annotations and demonstrate both efficacy and versatility across diverse applications. Code is available at link.

</details>


### [43] [SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models](https://arxiv.org/abs/2602.20901)
*Yuechen Xie,Xiaoyan Zhang,Yicheng Shan,Hao Zhu,Rui Tang,Rong Wei,Mingli Song,Yuanyu Wan,Jie Song*

Main category: cs.CV

TL;DR: Introduces SpatiaLQA, a benchmark for spatial logical reasoning in VLMs (9,605 QA pairs over 241 indoor scenes), shows 41 mainstream VLMs struggle, and proposes a recursive, scene-graph–assisted reasoning approach that outperforms prior methods.


<details>
  <summary>Details</summary>
Motivation: Despite strong results on standard VQA and logic tasks, current VLMs falter when decisions require understanding spatial relations and multi-step dependencies in real-world scenes. The field lacks a dedicated benchmark and effective techniques targeting this capability, termed spatial logical reasoning.

Method: 1) Construct SpatiaLQA from real indoor scenes with questions requiring spatial relations and multi-step logical dependencies. 2) Benchmark 41 VLMs on these tasks. 3) Propose recursive scene graph assisted reasoning: use visual foundation models to iteratively decompose complex scenes into task-relevant scene graphs and condition VLM reasoning on these structured representations.

Result: Across 41 VLMs, performance on SpatiaLQA is weak, indicating a significant gap in spatial logical reasoning. The proposed recursive scene-graph method achieves the best results among evaluated approaches (exact numbers not reported in the abstract).

Conclusion: Spatial logical reasoning remains a key shortcoming for VLMs. SpatiaLQA provides a focused evaluation resource, and leveraging recursive, task-relevant scene graphs substantially improves performance. Code and data are publicly available, enabling further research.

Abstract: Vision-Language Models (VLMs) have been increasingly applied in real-world scenarios due to their outstanding understanding and reasoning capabilities. Although VLMs have already demonstrated impressive capabilities in common visual question answering and logical reasoning, they still lack the ability to make reasonable decisions in complex real-world environments. We define this ability as spatial logical reasoning, which not only requires understanding the spatial relationships among objects in complex scenes, but also the logical dependencies between steps in multi-step tasks. To bridge this gap, we introduce Spatial Logical Question Answering (SpatiaLQA), a benchmark designed to evaluate the spatial logical reasoning capabilities of VLMs. SpatiaLQA consists of 9,605 question answer pairs derived from 241 real-world indoor scenes. We conduct extensive experiments on 41 mainstream VLMs, and the results show that even the most advanced models still struggle with spatial logical reasoning. To address this issue, we propose a method called recursive scene graph assisted reasoning, which leverages visual foundation models to progressively decompose complex scenes into task-relevant scene graphs, thereby enhancing the spatial logical reasoning ability of VLMs, outperforming all previous methods. Code and dataset are available at https://github.com/xieyc99/SpatiaLQA.

</details>


### [44] [TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering](https://arxiv.org/abs/2602.20903)
*Hanshen Zhu,Yuliang Liu,Xuecheng Wu,An-Lan Wang,Hao Feng,Dingkang Yang,Chao Feng,Can Huang,Jingqun Tang,Xiang Bai*

Main category: cs.CV

TL;DR: TextPecker introduces a structural-anomaly–aware RL strategy for text rendering in text-to-image generation, overcoming the inability of current MLLMs/OCR to detect distorted or misaligned text and yielding state-of-the-art fidelity and alignment gains.


<details>
  <summary>Details</summary>
Motivation: Despite progress in text-to-image models, generated text often suffers from distortions, blurriness, and misalignment. Existing evaluators (MLLMs and OCR) fail to perceive these structural defects, producing noisy rewards that block reliable evaluation and RL optimization of visual text rendering.

Method: Propose TextPecker, a plug-and-play RL framework that supplies structure-aware rewards to any text-to-image generator. Build a character-level recognition dataset annotated for structural anomalies and a stroke-editing synthesis engine to broaden error coverage. Use these to train an anomaly-perceptive recognizer and mitigate reward noise during RL fine-tuning.

Result: Across diverse generators, TextPecker consistently improves VTR. On the strong Qwen-Image baseline for Chinese text, it achieves +4% structural fidelity and +8.7% semantic alignment, setting a new state-of-the-art in high-fidelity VTR.

Conclusion: By providing reliable structure-aware rewards and training resources, TextPecker fills a key gap in VTR optimization and advances toward robust, structurally faithful visual text generation.

Abstract: Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.

</details>


### [45] [VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation](https://arxiv.org/abs/2602.21054)
*Seongheon Park,Changdae Oh,Hyeong Kyu Choi,Xuefeng Du,Sharon Li*

Main category: cs.CV

TL;DR: VAUQ is a training-free, vision-aware self-evaluation framework for LVLMs that combines predictive entropy with an Image-Information Score—measuring how much the image reduces uncertainty—enhanced by core-region masking to better detect hallucinations and predict answer correctness.


<details>
  <summary>Details</summary>
Motivation: LVLMs often hallucinate because their outputs can be driven by language priors rather than visual evidence. Existing self-evaluation relies mostly on language signals and thus poorly estimates correctness for vision-conditioned answers. A method is needed that explicitly quantifies dependence on visual input.

Method: Compute (1) predictive entropy of the model’s answers and (2) an Image-Information Score (IS) that captures the reduction in uncertainty attributable to the image (e.g., by comparing uncertainty with/without image or with masked image). Use an unsupervised core-region masking strategy to emphasize salient visual regions when estimating IS. Combine entropy with the core-masked IS into a single, training-free scoring function for self-evaluation.

Result: Across multiple datasets, VAUQ’s score correlates more reliably with answer correctness and outperforms existing LLM/LVLM self-evaluation baselines in detecting hallucinations and ranking predictions by reliability.

Conclusion: By explicitly measuring image-conditioned information gain and emphasizing salient regions, VAUQ provides a practical, training-free, vision-grounded uncertainty estimate that improves LVLM self-evaluation and supports safer deployment.

Abstract: Large Vision-Language Models (LVLMs) frequently hallucinate, limiting their safe deployment in real-world applications. Existing LLM self-evaluation methods rely on a model's ability to estimate the correctness of its own outputs, which can improve deployment reliability; however, they depend heavily on language priors and are therefore ill-suited for evaluating vision-conditioned predictions. We propose VAUQ, a vision-aware uncertainty quantification framework for LVLM self-evaluation that explicitly measures how strongly a model's output depends on visual evidence. VAUQ introduces the Image-Information Score (IS), which captures the reduction in predictive uncertainty attributable to visual input, and an unsupervised core-region masking strategy that amplifies the influence of salient regions. Combining predictive entropy with this core-masked IS yields a training-free scoring function that reliably reflects answer correctness. Comprehensive experiments show that VAUQ consistently outperforms existing self-evaluation methods across multiple datasets.

</details>


### [46] [Computing a Characteristic Orientation for Rotation-Independent Image Analysis](https://arxiv.org/abs/2602.20930)
*Cristian Valero-Abundio,Emilio Sansano-Sansano,Raúl Montoliu,Marina Martínez García*

Main category: cs.CV

TL;DR: A simple preprocessing step (GID) estimates each image’s dominant orientation and rotates it to a canonical frame so standard CNNs see consistently oriented inputs, yielding superior accuracy on RotMNIST and solid gains on CIFAR‑10 without changing architectures.


<details>
  <summary>Details</summary>
Motivation: Rotation invariance is hard for standard deep nets; common fixes (heavy augmentation, special equivariant layers, architectural changes) add compute/complexity and limit plug‑and‑play use. The goal is a lightweight, architecture‑agnostic way to gain rotation robustness.

Method: General Intensity Direction (GID) computes a global orientation per image (from intensity/gradient cues) and rotates the whole image into a canonical reference. Unlike moment‑invariant descriptors, it preserves spatial layout so it can be fed to conventional convolutional networks without modifying them.

Result: On rotated MNIST, GID plus a standard model surpasses state‑of‑the‑art rotation‑invariant architectures; on CIFAR‑10, it remains effective under more complex, natural‑image conditions.

Conclusion: Pre-aligning images via a learned‑free (or lightweight) orientation estimator can materially improve rotation robustness with minimal overhead and no architectural changes, offering a practical alternative to equivariant networks; broader validation and analysis of failure modes will determine generality.

Abstract: Handling geometric transformations, particularly rotations, remains a challenge in deep learning for computer vision. Standard neural networks lack inherent rotation invariance and typically rely on data augmentation or architectural modifications to improve robustness. Although effective, these approaches increase computational demands, require specialised implementations, or alter network structures, limiting their applicability. This paper introduces General Intensity Direction (GID), a preprocessing method that improves rotation robustness without modifying the network architecture. The method estimates a global orientation for each image and aligns it to a canonical reference frame, allowing standard models to process inputs more consistently across different rotations. Unlike moment-based approaches that extract invariant descriptors, this method directly transforms the image while preserving spatial structure, making it compatible with convolutional networks. Experimental evaluation on the rotated MNIST dataset shows that the proposed method achieves higher accuracy than state-of-the-art rotation-invariant architectures. Additional experiments on the CIFAR-10 dataset, confirm that the method remains effective under more complex conditions.

</details>


### [47] [Dropping Anchor and Spherical Harmonics for Sparse-view Gaussian Splatting](https://arxiv.org/abs/2602.20933)
*Shuangkang Fang,I-Chao Shen,Xuanyang Zhang,Zesheng Wang,Yufeng Wang,Wenrui Ding,Gang Yu,Takeo Igarashi*

Main category: cs.CV

TL;DR: DropAnSH-GS introduces structured, anchor-based Gaussian dropout plus high-degree SH dropout for 3D Gaussian Splatting, reducing overfitting in sparse views, improving quality with negligible overhead, and enabling post-training SH-based compression.


<details>
  <summary>Details</summary>
Motivation: Random per-Gaussian opacity dropout in 3DGS is weakened by neighbor compensation, and existing methods ignore that high-degree spherical harmonics (SH) exacerbate overfitting. A stronger, structure-aware regularizer is needed, and color/appearance representations should be regularized too.

Method: 1) Anchor-based spatial dropout: randomly select anchor Gaussians and drop their spatial neighbors together, breaking local redundancy and forcing globally informed representations. 2) SH dropout: randomly remove higher-degree SH coefficients so appearance is concentrated in lower degrees, reducing overfit and permitting later SH truncation for compact models. The approach plugs into diverse 3DGS variants with minimal changes.

Result: Experiments show consistent, substantial gains over prior 3DGS dropout methods under sparse-view settings, with negligible compute overhead and broad applicability across 3DGS variants.

Conclusion: Structured (anchor + neighbor) dropout plus SH-degree dropout effectively combats spatial redundancy and spectral overfitting in 3DGS, improving generalization and offering flexible post-training compression via SH truncation, while remaining lightweight and easily integrable.

Abstract: Recent 3D Gaussian Splatting (3DGS) Dropout methods address overfitting under sparse-view conditions by randomly nullifying Gaussian opacities. However, we identify a neighbor compensation effect in these approaches: dropped Gaussians are often compensated by their neighbors, weakening the intended regularization. Moreover, these methods overlook the contribution of high-degree spherical harmonic coefficients (SH) to overfitting. To address these issues, we propose DropAnSH-GS, a novel anchor-based Dropout strategy. Rather than dropping Gaussians independently, our method randomly selects certain Gaussians as anchors and simultaneously removes their spatial neighbors. This effectively disrupts local redundancies near anchors and encourages the model to learn more robust, globally informed representations. Furthermore, we extend the Dropout to color attributes by randomly dropping higher-degree SH to concentrate appearance information in lower-degree SH. This strategy further mitigates overfitting and enables flexible post-training model compression via SH truncation. Experimental results demonstrate that DropAnSH-GS substantially outperforms existing Dropout methods with negligible computational overhead, and can be readily integrated into various 3DGS variants to enhance their performances. Project Website: https://sk-fun.fun/DropAnSH-GS

</details>


### [48] [UFO: Unifying Feed-Forward and Optimization-based Methods for Large Driving Scene Modeling](https://arxiv.org/abs/2602.20943)
*Kaiyuan Tan,Yingying Shen,Mingfei Tu,Haohui Zhu,Bing Wang,Guang Chen,Hangjun Ye,Haiyang Sun*

Main category: cs.CV

TL;DR: UFO introduces a recurrent, token-filtered 4D scene reconstruction method that updates a global scene representation online and uses pose-guided dynamics to efficiently and accurately handle long driving sequences—achieving 0.5 s processing for 16 s logs with better visual and geometric quality than prior methods.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving needs scalable, accurate long-range dynamic scene reconstructions for simulation and closed-loop learning. Existing feed-forward approaches suffer quadratic complexity with sequence length and degrade on long-duration dynamics, while optimization-based methods are too slow per scene.

Method: A recurrent paradigm that maintains and iteratively refines a global 4D scene representation as new frames arrive. It employs visibility-based filtering to select informative scene tokens for efficient long-sequence processing and introduces object pose-guided modeling to capture dynamic object motion over extended durations, effectively blending strengths of optimization-based and feed-forward methods.

Result: On the Waymo Open Dataset, the approach surpasses per-scene optimization and state-of-the-art feed-forward baselines across multiple sequence lengths. It reconstructs 16-second driving logs within 0.5 second while preserving superior visual fidelity and geometric accuracy.

Conclusion: UFO provides a scalable, accurate solution for long-range 4D driving scene reconstruction, making it suitable for real-time simulation and closed-loop learning. Its recurrent updating, visibility-based token selection, and pose-guided dynamics resolve prior complexity and dynamic modeling bottlenecks.

Abstract: Dynamic driving scene reconstruction is critical for autonomous driving simulation and closed-loop learning. While recent feed-forward methods have shown promise for 3D reconstruction, they struggle with long-range driving sequences due to quadratic complexity in sequence length and challenges in modeling dynamic objects over extended durations. We propose UFO, a novel recurrent paradigm that combines the benefits of optimization-based and feed-forward methods for efficient long-range 4D reconstruction. Our approach maintains a 4D scene representation that is iteratively refined as new observations arrive, using a visibility-based filtering mechanism to select informative scene tokens and enable efficient processing of long sequences. For dynamic objects, we introduce an object pose-guided modeling approach that supports accurate long-range motion capture. Experiments on the Waymo Open Dataset demonstrate that our method significantly outperforms both per-scene optimization and existing feed-forward methods across various sequence lengths. Notably, our approach can reconstruct 16-second driving logs within 0.5 second while maintaining superior visual quality and geometric accuracy.

</details>


### [49] [Are Multimodal Large Language Models Good Annotators for Image Tagging?](https://arxiv.org/abs/2602.20972)
*Ming-Kun Xie,Jia-Hao Xiao,Zhiqiang Kou,Zhongnian Li,Gang Niu,Masashi Sugiyama*

Main category: cs.CV

TL;DR: They study whether multimodal LLMs can replace humans for image tagging and introduce TagLLM, a two-stage prompting framework that cuts labeling costs to ~0.1% of human effort, attains 50–80% of human annotation quality, and achieves >90% of human-trained downstream performance while closing 60–80% of the remaining gap.


<details>
  <summary>Details</summary>
Motivation: Manual multi-label image tagging is costly; MLLMs could automate annotation, but their quality, cost, and impact on downstream training relative to human labels are unclear.

Method: Analyze the cost–quality gap between MLLM- and human-generated annotations, then propose TagLLM with two components: (1) Candidates generation via structured group-wise prompting to create a compact, high-recall label set; (2) Label disambiguation via interactive prompt-based semantic calibration to refine/resolve candidate labels.

Result: Conservative estimates show MLLM annotation costs ~1/1000 of human labeling (mostly GPU time). Raw MLLM annotations reach ~50–80% of human quality yet yield >90% of human-level performance on downstream training. TagLLM further narrows the gap, recovering ~60–80% of the remaining downstream performance difference vs. human labels.

Conclusion: With an efficient prompting pipeline (TagLLM), MLLM-based annotation can substantially substitute for manual image tagging at a tiny fraction of the cost; careful candidate generation and semantic disambiguation are key to bridging most of the performance gap to human labels.

Abstract: Image tagging, a fundamental vision task, traditionally relies on human-annotated datasets to train multi-label classifiers, which incurs significant labor and costs. While Multimodal Large Language Models (MLLMs) offer promising potential to automate annotation, their capability to replace human annotators remains underexplored. This paper aims to analyze the gap between MLLM-generated and human annotations and to propose an effective solution that enables MLLM-based annotation to replace manual labeling. Our analysis of MLLM annotations reveals that, under a conservative estimate, MLLMs can reduce annotation cost to as low as one-thousandth of the human cost, mainly accounting for GPU usage, which is nearly negligible compared to manual efforts. Their annotation quality reaches about 50\% to 80\% of human performance, while achieving over 90\% performance on downstream training tasks.Motivated by these findings, we propose TagLLM, a novel framework for image tagging, which aims to narrow the gap between MLLM-generated and human annotations. TagLLM comprises two components: Candidates generation, which employs structured group-wise prompting to efficiently produce a compact candidate set that covers as many true labels as possible while reducing subsequent annotation workload; and label disambiguation, which interactively calibrates the semantic concept of categories in the prompts and effectively refines the candidate labels. Extensive experiments show that TagLLM substantially narrows the gap between MLLM-generated and human annotations, especially in downstream training performance, where it closes about 60\% to 80\% of the difference.

</details>


### [50] [Cycle-Consistent Tuning for Layered Image Decomposition](https://arxiv.org/abs/2602.20989)
*Zheng Gu,Min Lu,Zhida Sun,Dani Lischinski,Daniel Cohen-O,Hui Huang*

Main category: cs.CV

TL;DR: They fine-tune a large diffusion model (via LoRA) and jointly train decomposition and composition networks with cycle-consistency, plus a progressive self-training loop, to separate logos from underlying objects; it yields accurate, robust, and generalizable layered decompositions.


<details>
  <summary>Details</summary>
Motivation: Real-world layered effects (e.g., shading, reflections, perspective) are non-linear and globally coupled, making layer separation (like removing a logo from a surface while preserving both layers) difficult. A more general and robust approach is needed beyond task-specific models.

Method: Use an in-context, diffusion-based image decomposition framework. Fine-tune a pretrained diffusion model with lightweight LoRA adapters. Introduce a cycle-consistent training scheme that jointly optimizes a decomposition model and a composition model, enforcing that recomposing the separated layers reconstructs the original image. Add a progressive self-improvement (self-training) step by iteratively augmenting training with high-quality model-generated examples.

Result: Extensive experiments show accurate, coherent logo-object decompositions, improved robustness under complex interactions, and encouraging generalization to other types of layered decomposition tasks.

Conclusion: Cycle-consistent, LoRA-adapted diffusion models with progressive self-training provide a promising, potentially unified framework for layered image decomposition, delivering strong accuracy and robustness while generalizing beyond the primary logo-object task.

Abstract: Disentangling visual layers in real-world images is a persistent challenge in vision and graphics, as such layers often involve non-linear and globally coupled interactions, including shading, reflection, and perspective distortion. In this work, we present an in-context image decomposition framework that leverages large diffusion foundation models for layered separation. We focus on the challenging case of logo-object decomposition, where the goal is to disentangle a logo from the surface on which it appears while faithfully preserving both layers. Our method fine-tunes a pretrained diffusion model via lightweight LoRA adaptation and introduces a cycle-consistent tuning strategy that jointly trains decomposition and composition models, enforcing reconstruction consistency between decomposed and recomposed images. This bidirectional supervision substantially enhances robustness in cases where the layers exhibit complex interactions. Furthermore, we introduce a progressive self-improving process, which iteratively augments the training set with high-quality model-generated examples to refine performance. Extensive experiments demonstrate that our approach achieves accurate and coherent decompositions and also generalizes effectively across other decomposition types, suggesting its potential as a unified framework for layered image decomposition.

</details>


### [51] [VII: Visual Instruction Injection for Jailbreaking Image-to-Video Generation Models](https://arxiv.org/abs/2602.20999)
*Bowen Zheng,Yongli Xiang,Ziming Hong,Zerong Lin,Chaojian Yu,Tongliang Liu,Xinge You*

Main category: cs.CV

TL;DR: Training-free, transferable image-based jailbreak (Visual Instruction Injection) that hides malicious intent as visual cues in a benign reference image to steer I2V models into harmful generations, achieving high success and low refusals across multiple commercial systems.


<details>
  <summary>Details</summary>
Motivation: I2V models increasingly follow implicit visual instructions from reference images. Safety mechanisms mostly police text, leaving the image modality as an under-protected attack surface that adversaries can exploit to bypass content filters.

Method: Two coordinated modules: (1) Malicious Intent Reprogramming distills unsafe prompts into low-detectability intent while minimizing overt harmfulness; (2) Visual Instruction Grounding renders these distilled cues onto a safe image while preserving semantic consistency, thus covertly guiding generation. The attack is training-free, needs no model internals, and is designed to be transferable.

Result: Across Kling‑v2.5‑turbo, Gemini Veo‑3.1, Seedance‑1.5‑pro, and PixVerse‑V5, the approach attains up to 83.5% Attack Success Rate with near-zero Refusal Rate, outperforming existing jailbreak baselines.

Conclusion: The visual-instruction channel is a critical vulnerability in I2V systems. Defenses should treat images as instructions, implement cross-modal consistency checks, sanitize or redact rendered cues/overlays, and enhance multimodal safety to close this gap.

Abstract: Image-to-Video (I2V) generation models, which condition video generation on reference images, have shown emerging visual instruction-following capability, allowing certain visual cues in reference images to act as implicit control signals for video generation. However, this capability also introduces a previously overlooked risk: adversaries may exploit visual instructions to inject malicious intent through the image modality. In this work, we uncover this risk by proposing Visual Instruction Injection (VII), a training-free and transferable jailbreaking framework that intentionally disguises the malicious intent of unsafe text prompts as benign visual instructions in the safe reference image. Specifically, VII coordinates a Malicious Intent Reprogramming module to distill malicious intent from unsafe text prompts while minimizing their static harmfulness, and a Visual Instruction Grounding module to ground the distilled intent onto a safe input image by rendering visual instructions that preserve semantic consistency with the original unsafe text prompt, thereby inducing harmful content during I2V generation. Empirically, our extensive experiments on four state-of-the-art commercial I2V models (Kling-v2.5-turbo, Gemini Veo-3.1, Seedance-1.5-pro, and PixVerse-V5) demonstrate that VII achieves Attack Success Rates of up to 83.5% while reducing Refusal Rates to near zero, significantly outperforming existing baselines.

</details>


### [52] [Le-DETR: Revisiting Real-Time Detection Transformer with Efficient Encoder Design](https://arxiv.org/abs/2602.21010)
*Jiannan Huang,Aditya Kane,Fengzhe Zhou,Yunchao Wei,Humphrey Shi*

Main category: cs.CV

TL;DR: Le-DETR introduces EfficientNAT backbones and a local-attention hybrid encoder to deliver state-of-the-art real-time DETR performance while cutting pretraining data by ~80%, training only on ImageNet-1K and COCO, and matching/surpassing strong baselines (YOLOv12, DEIM-D-FINE) at comparable or better latency.


<details>
  <summary>Details</summary>
Motivation: Real-time DETR models often demand heavy, hard-to-reproduce backbone pretraining, which limits experimentation with new backbones and slows progress. The goal is to achieve high accuracy and low latency without costly pretraining.

Method: (1) Design EfficientNAT backbones (multiple scales) that combine modern efficient convolutions with local attention; (2) rework the hybrid DETR encoder to use local attention for faster, stronger feature modeling; (3) train using only ImageNet-1K and COCO2017; (4) instantiate Le-DETR in M/L/X sizes; (5) evaluate on COCO Val2017 and measure latency on RTX4090.

Result: Le-DETR-M/L/X reach 52.9/54.3/55.1 mAP on COCO Val2017 with 4.45/5.01/6.68 ms latency (RTX4090). They use ~80% fewer pretraining images than prior work. Le-DETR outperforms or matches YOLOv12-L/X (+0.6/-0.1 mAP) at similar speed with ~20% speedup, and beats DEIM-D-FINE: +0.2 mAP for M with slightly faster inference; +0.4 mAP vs DEIM-D-FINE-L with only +0.4 ms latency.

Conclusion: Well-chosen backbone and encoder designs with local attention enable real-time DETR models that are both accurate and fast without heavy pretraining, setting new SOTA under a low-cost training regime; code and weights will be released to support reproducibility.

Abstract: Real-time object detection is crucial for real-world applications as it requires high accuracy with low latency. While Detection Transformers (DETR) have demonstrated significant performance improvements, current real-time DETR models are challenging to reproduce from scratch due to excessive pre-training overheads on the backbone, constraining research advancements by hindering the exploration of novel backbone architectures. In this paper, we want to show that by using general good design, it is possible to have \textbf{high performance} with \textbf{low pre-training cost}. After a thorough study of the backbone architecture, we propose EfficientNAT at various scales, which incorporates modern efficient convolution and local attention mechanisms. Moreover, we re-design the hybrid encoder with local attention, significantly enhancing both performance and inference speed. Based on these advancements, we present Le-DETR (\textbf{L}ow-cost and \textbf{E}fficient \textbf{DE}tection \textbf{TR}ansformer), which achieves a new \textbf{SOTA} in real-time detection using only ImageNet1K and COCO2017 training datasets, saving about 80\% images in pre-training stage compared with previous methods. We demonstrate that with well-designed, real-time DETR models can achieve strong performance without the need for complex and computationally expensive pretraining. Extensive experiments show that Le-DETR-M/L/X achieves \textbf{52.9/54.3/55.1 mAP} on COCO Val2017 with \textbf{4.45/5.01/6.68 ms} on an RTX4090. It surpasses YOLOv12-L/X by \textbf{+0.6/-0.1 mAP} while achieving similar speed and \textbf{+20\%} speedup. Compared with DEIM-D-FINE, Le-DETR-M achieves \textbf{+0.2 mAP} with slightly faster inference, and surpasses DEIM-D-FINE-L by \textbf{+0.4 mAP} with only \textbf{0.4 ms} additional latency. Code and weights will be open-sourced.

</details>


### [53] [From Perception to Action: An Interactive Benchmark for Vision Reasoning](https://arxiv.org/abs/2602.21015)
*Yuhao Wu,Maojia Song,Yihuai Lan,Lei Wang,Zhiqiang Hu,Yao Xiao,Heng Zhou,Weihua Zheng,Dylan Raharja,Soujanya Poria,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: Introduces CHAIN, an interactive 3D physics benchmark to test whether models can understand and act on physical structure; finds state-of-the-art VLMs and diffusion models struggle with long-horizon, structure-grounded planning and action execution.


<details>
  <summary>Details</summary>
Motivation: Real-world agents must reason about geometry, contact, and support to decide feasible actions, but prevailing VLM evaluations (e.g., single-turn VQA) ignore these physical and causal constraints, providing a poor proxy for embodied, long-horizon competence.

Method: Design the CHAIN benchmark: an interactive, physics-driven 3D testbed with tasks (e.g., interlocking mechanical puzzles, 3D stacking and packing) that require understanding, planning, and executing action sequences under physical constraints. Evaluate multiple state-of-the-art VLMs and diffusion models under unified interactive protocols.

Result: Across tasks, top-performing models fail to reliably internalize physical structure and causal constraints, produce brittle or incorrect long-horizon plans, and poorly translate perceived structure into effective actions.

Conclusion: Passive perception benchmarks are insufficient for assessing embodied competence. CHAIN exposes significant gaps in current models’ physical and causal reasoning and offers a principled evaluation pathway to drive research on structure-aware planning and action execution.

Abstract: Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.

</details>


### [54] [Not Just What's There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning](https://arxiv.org/abs/2602.21035)
*Junhao Xiao,Zhiyu Wu,Hao Lin,Yi Chen,Yahui Liu,Xiaoran Zhao,Zixu Wang,Zejiang He*

Main category: cs.CV

TL;DR: CLIPGlasses is a plug-and-play add-on for CLIP that disentangles negated concepts from text embeddings and penalizes their alignment during image–text matching, sharply reducing false positives and improving cross-domain robustness, especially with limited data.


<details>
  <summary>Details</summary>
Motivation: VLMs like CLIP often embed negated and affirmative statements similarly (e.g., “no dog” ≈ “dog”), leading to false matches. Prior fixes fine-tune text encoders, which can overfit and hurt generalization. A lightweight, generalizable remedy for negation handling is needed.

Method: A dual-stage framework: (1) Lens module extracts/isolates negated semantics from the text embedding; (2) Frame module predicts a context-aware repulsion strength; this signal is integrated into a modified similarity score to penalize alignment with the negated semantics, without full CLIP fine-tuning (plug-and-play).

Result: Across benchmarks, CLIP+CLIPGlasses is competitive in-domain and surpasses SOTA on cross-domain generalization. Gains are especially strong in low-resource settings, and false positive matches induced by negation are reduced.

Conclusion: CLIPGlasses effectively augments CLIP’s negation understanding with minimal architectural changes and training, improving robustness and generalization. It is a practical alternative to encoder fine-tuning for handling negation in VLMs, particularly valuable under domain shift and data-scarce conditions.

Abstract: Vision-Language Models (VLMs) like CLIP struggle to understand negation, often embedding affirmatives and negatives similarly (e.g., matching "no dog" with dog images). Existing methods refine negation understanding via fine-tuning CLIP's text encoder, risking overfitting. In this work, we propose CLIPGlasses, a plug-and-play framework that enhances CLIP's ability to comprehend negated visual descriptions. CLIPGlasses adopts a dual-stage design: a Lens module disentangles negated semantics from text embeddings, and a Frame module predicts context-aware repulsion strength, which is integrated into a modified similarity computation to penalize alignment with negated semantics, thereby reducing false positive matches. Experiments show that CLIP equipped with CLIPGlasses achieves competitive in-domain performance and outperforms state-of-the-art methods in cross-domain generalization. Its superiority is especially evident under low-resource conditions, indicating stronger robustness across domains.

</details>


### [55] [OmniOCR: Generalist OCR for Ethnic Minority Languages](https://arxiv.org/abs/2602.21042)
*Bonan Liu,Zeyu Zhang,Bingbing Meng,Han Wang,Hanshuo Zhang,Chengping Wang,Daji Ergu,Ying Cai*

Main category: cs.CV

TL;DR: OmniOCR proposes a universal, parameter‑efficient OCR framework for under‑resourced ethnic minority scripts using Dynamic LoRA with sparsity regularization, yielding large accuracy gains (39%–66%) over strong baselines on TibetanMNIST, Shui, ancient Yi, and Dongba without extra inference cost.


<details>
  <summary>Details</summary>
Motivation: Most OCR advances target well‑resourced scripts; minority languages suffer from complex writing systems, limited annotations, and diverse historic/modern forms. There is a need for models that generalize in low‑/zero‑shot settings while preserving knowledge across scripts.

Method: Introduce Dynamic Low‑Rank Adaptation that allocates per‑layer and per‑script capacity to adapt a base OCR model while preserving shared knowledge. Apply sparsity regularization to prune redundant LoRA updates for compactness and efficiency, avoiding added inference overhead. Evaluate on four minority‑script datasets against zero‑shot foundation models and standard post‑training baselines.

Result: OmniOCR achieves state‑of‑the‑art accuracy and superior parameter efficiency on TibetanMNIST, Shui, ancient Yi, and Dongba. It outperforms zero‑shot foundation models and standard fine‑tuning, improving accuracy by 39%–66% across datasets.

Conclusion: Dynamic, sparsified LoRA enables effective, efficient adaptation of OCR models to diverse, low‑resource minority scripts, delivering SOTA results without increasing inference cost; the framework and code are released for reproduction and further research.

Abstract: Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation (Dynamic LoRA) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training, achieving state-of-the-art accuracy with superior parameter efficiency, and compared with the state-of-the-art baseline models, it improves accuracy by 39%-66% on these four datasets. Code: https://github.com/AIGeeksGroup/OmniOCR.

</details>


### [56] [OCR-Agent: Agentic OCR with Capability and Memory Reflection](https://arxiv.org/abs/2602.21053)
*Shimin Wen,Zeyu Zhang,Xingdou Bian,Hongjie Zhu,Lulu He,Layi Shama,Daji Ergu,Ying Cai*

Main category: cs.CV

TL;DR: They introduce a training-free, iterative self-correction framework for VLMs (OCR-Agent) that uses structured reflection to diagnose mistakes, avoid repeating failed attempts, and re-reason, achieving new SOTA on OCRBench v2 and outperforming larger fine-tuned models.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs can refine answers iteratively but lack effective self-correction: they repeat unproductive revisions, fail to overcome cognitive biases, and struggle to stably improve answer quality in multi-turn settings. A structured, self-aware mechanism is needed to systematically diagnose errors and avoid repetition.

Method: A three-stage iterative framework: (1) Capability Reflection—diagnose errors and draft a targeted correction plan; (2) Memory Reflection—review prior attempts to prevent repetition and encourage novel strategies; (3) rigorous re-reasoning—apply the plan to produce an improved answer. Implemented as OCR-Agent, requiring no additional training.

Result: On OCRBench v2, OCR-Agent beats the open-source SOTA InternVL3-8B by +2.0 (English) and +1.2 (Chinese), and sets new SOTA in Visual Understanding (79.9) and Reasoning (66.5), surpassing even larger fine-tuned models.

Conclusion: Structured, self-aware reflection with capability and memory components robustly improves multi-turn reasoning in VLMs without extra training, yielding state-of-the-art performance; code is available at the provided repository.

Abstract: Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection. This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection, then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning. Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs' reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent.

</details>


### [57] [Optimizing Occupancy Sensor Placement in Smart Environments](https://arxiv.org/abs/2602.21098)
*Hao Lu,Richard J. Radke*

Main category: cs.CV

TL;DR: ILP-based automatic placement of low-resolution, privacy-preserving ToF sensors optimizes real-time zone-occupancy counting by simulating occupant trajectories and predicting layout accuracy; validated via simulations across diverse office layouts.


<details>
  <summary>Details</summary>
Motivation: Reduce building energy use by conditioning and lighting only occupied zones while preserving privacy. Zone-counting with low-res ToF works but is highly sensitive to sensor placement, motivating an automatic method that also predicts accuracy.

Method: Encode office geometry; simulate many plausible occupant trajectories; formulate sensor placement as an integer linear program that optimizes counting accuracy under a fixed sensor budget; solve via branch-and-bound to return optimal locations and predicted performance.

Result: Across several simulated office environments, the approach selects effective sensor layouts and accurately forecasts zone-counting performance, improving over manual/ad-hoc placements (qualitative gains; no specific metrics reported in the abstract).

Conclusion: Automatic, optimization-based placement of ToF sensors is practical and beneficial for privacy-preserving zone occupancy recognition, enabling better accuracy and supporting energy-efficient building control across varied office configurations.

Abstract: Understanding the locations of occupants in a commercial built environment is critical for realizing energy savings by delivering lighting, heating, and cooling only where it is needed. The key to achieving this goal is being able to recognize zone occupancy in real time, without impeding occupants' activities or compromising privacy. While low-resolution, privacy-preserving time-of-flight (ToF) sensor networks have demonstrated good performance in zone counting, the performance depends on careful sensor placement. To address this issue, we propose an automatic sensor placement method that determines optimal sensor layouts for a given number of sensors, and can predict the counting accuracy of such a layout. In particular, given the geometric constraints of an office environment, we simulate a large number of occupant trajectories. We then formulate the sensor placement problem as an integer linear programming (ILP) problem and solve it with the branch and bound method. We demonstrate the effectiveness of the proposed method based on simulations of several different office environments.

</details>


### [58] [Skullptor: High Fidelity 3D Head Reconstruction in Seconds with Multi-View Normal Prediction](https://arxiv.org/abs/2602.21100)
*Noé Artru,Rukhshanda Hussain,Emeline Got,Alexandre Messier,David B. Lindell,Abdallah Dib*

Main category: cs.CV

TL;DR: Hybrid pipeline that predicts cross-view-consistent surface normals in a fast feed-forward pass and then uses them as strong priors in inverse rendering to recover high-frequency details, yielding near-photogrammetry head reconstructions with fewer cameras and less compute.


<details>
  <summary>Details</summary>
Motivation: High-fidelity 3D head reconstruction is valuable but current options are impractical or low quality: dense-view photogrammetry is accurate yet costly and laborious (esp. around facial hair), while recent single-image or optimization-heavy methods trade off either detail or efficiency.

Method: 1) Extend monocular foundation models with cross-view attention to predict geometrically consistent multi-view surface normals in one pass. 2) Use these predicted normals as priors within an inverse rendering optimization to refine geometry and recover high-frequency details.

Result: Outperforms state-of-the-art single-image and multi-view approaches. Achieves fidelity comparable to dense-view photogrammetry while requiring fewer cameras and reduced computation. Code and model will be released.

Conclusion: Combining feed-forward multi-view normal prediction with inverse rendering bridges efficiency–fidelity trade-offs, enabling practical, high-detail 3D head reconstruction with lower capture and compute budgets.

Abstract: Reconstructing high-fidelity 3D head geometry from images is critical for a wide range of applications, yet existing methods face fundamental limitations. Traditional photogrammetry achieves exceptional detail but requires extensive camera arrays (25-200+ views), substantial computation, and manual cleanup in challenging areas like facial hair. Recent alternatives present a fundamental trade-off: foundation models enable efficient single-image reconstruction but lack fine geometric detail, while optimization-based methods achieve higher fidelity but require dense views and expensive computation. We bridge this gap with a hybrid approach that combines the strengths of both paradigms. Our method introduces a multi-view surface normal prediction model that extends monocular foundation models with cross-view attention to produce geometrically consistent normals in a feed-forward pass. We then leverage these predictions as strong geometric priors within an inverse rendering optimization framework to recover high-frequency surface details. Our approach outperforms state-of-the-art single-image and multi-view methods, achieving high-fidelity reconstruction on par with dense-view photogrammetry while reducing camera requirements and computational cost. The code and model will be released.

</details>


### [59] [Event-Aided Sharp Radiance Field Reconstruction for Fast-Flying Drones](https://arxiv.org/abs/2602.21101)
*Rong Zou,Marco Cannici,Davide Scaramuzza*

Main category: cs.CV

TL;DR: They fuse asynchronous event-camera streams with motion‑blurred RGB frames inside a joint NeRF-and-odometry optimization to recover sharp radiance fields and accurate trajectories from agile drone flights, outperforming prior methods by >50% on real data without ground-truth.


<details>
  <summary>Details</summary>
Motivation: Fast drone flight causes extreme motion blur and pose drift/noise, which break standard NeRF reconstruction that relies on sharp images and accurate camera poses. Event sensors offer high temporal resolution and blur robustness, motivating a method that exploits events to make NeRF viable in these regimes.

Method: A unified pipeline that (1) embeds event–image fusion directly in NeRF optimization and (2) jointly refines event-based visual–inertial odometry priors using both event and frame modalities. The system co-optimizes the radiance field and camera trajectory with losses tying events and blurred frames to the rendering, requiring no ground-truth supervision.

Result: On synthetic and real fast-flight datasets with severe blur and unreliable pose priors, the method reconstructs sharp, detailed radiance fields and accurate trajectories, achieving over 50% performance gains on real-world data versus state-of-the-art baselines.

Conclusion: Integrating event data with blurred frames and jointly refining odometry makes NeRF robust to high-speed degradation, enabling high-fidelity, unsupervised 3D reconstruction and accurate ego-motion for agile aerial robots, with clear benefits for inspection, exploration, and search-and-rescue.

Abstract: Fast-flying aerial robots promise rapid inspection under limited battery constraints, with direct applications in infrastructure inspection, terrain exploration, and search and rescue. However, high speeds lead to severe motion blur in images and induce significant drift and noise in pose estimates, making dense 3D reconstruction with Neural Radiance Fields (NeRFs) particularly challenging due to their high sensitivity to such degradations. In this work, we present a unified framework that leverages asynchronous event streams alongside motion-blurred frames to reconstruct high-fidelity radiance fields from agile drone flights. By embedding event-image fusion into NeRF optimization and jointly refining event-based visual-inertial odometry priors using both event and frame modalities, our method recovers sharp radiance fields and accurate camera trajectories without ground-truth supervision. We validate our approach on both synthetic data and real-world sequences captured by a fast-flying drone. Despite highly dynamic drone flights, where RGB frames are severely degraded by motion blur and pose priors become unreliable, our method reconstructs high-fidelity radiance fields and preserves fine scene details, delivering a performance gain of over 50% on real-world data compared to state-of-the-art methods.

</details>


### [60] [BrepGaussian: CAD reconstruction from Multi-View Images with Gaussian Splatting](https://arxiv.org/abs/2602.21105)
*Jiaxing Yu,Dongyang Ren,Hangyu Xu,Zhouyuxiao Yang,Yuanqi Li,Jie Guo,Zhengkang Zhou,Yanwen Guo*

Main category: cs.CV

TL;DR: BrepGaussian learns boundary-representation (B-rep) geometry directly from 2D images using a Gaussian Splatting renderer and a two-stage training scheme, yielding cleaner, coherent parametric surfaces and outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Recovering explicit CAD-friendly B-rep models from unstructured data is difficult. Existing deep methods typically need dense, clean point clouds and generalize poorly to new shapes. A method that infers B-rep geometry from images would be broadly useful in vision/graphics and reduce dependence on ideal 3D inputs.

Method: Use a Gaussian Splatting renderer with learnable features to aggregate multi-view image evidence, then apply a tailored fitting strategy to obtain parametric B-rep elements. A two-stage learning framework first emphasizes geometry and edge capture, then refines patch-level features to improve surface cleanliness and instance coherence.

Result: Across extensive experiments, the approach surpasses state-of-the-art baselines for B-rep recovery (from images), producing cleaner geometry and more coherent instance segmentation/patches. Quantitative details aren’t provided in the abstract, but the claim is consistent across tested datasets; code and datasets will be released.

Conclusion: Learning B-rep from images via Gaussian Splatting and staged training effectively decouples geometry reconstruction from feature learning, improving accuracy and cleanliness of recovered parametric models and showing superior performance to existing methods.

Abstract: The boundary representation (B-rep) models a 3D solid as its explicit boundaries: trimmed corners, edges, and faces. Recovering B-rep representation from unstructured data is a challenging and valuable task of computer vision and graphics. Recent advances in deep learning have greatly improved the recovery of 3D shape geometry, but still depend on dense and clean point clouds and struggle to generalize to novel shapes. We propose B-rep Gaussian Splatting (BrepGaussian), a novel framework that learns 3D parametric representations from 2D images. We employ a Gaussian Splatting renderer with learnable features, followed by a specific fitting strategy. To disentangle geometry reconstruction and feature learning, we introduce a two-stage learning framework that first captures geometry and edges and then refines patch features to achieve clean geometry and coherent instance representations. Extensive experiments demonstrate the superior performance of our approach to state-of-the-art methods. We will release our code and datasets upon acceptance.

</details>


### [61] [UDVideoQA: A Traffic Video Question Answering Dataset for Multi-Object Spatio-Temporal Reasoning in Urban Dynamics](https://arxiv.org/abs/2602.21137)
*Joseph Raj Vishal,Nagasiri Poluri,Katha Naik,Rutuja Patil,Kashyap Hegde Kota,Krishna Vinod,Prithvi Jai Ramesh,Mohammad Farhadi,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: UDVideoQA is a privacy-preserving, real-world urban traffic VideoQA and question-generation benchmark that stresses both visual grounding and causal reasoning, revealing a gap between perception and reasoning in current VideoLMs and showing that targeted fine-tuning can close much of it.


<details>
  <summary>Details</summary>
Motivation: Video language models struggle with real, unscripted multi-agent urban traffic scenes and often conflate abstract reasoning with basic perception. There is a need for a privacy-aware, large-scale, and systematically annotated benchmark that tests hierarchical reasoning (from basic understanding to counterfactuals) while preserving real-world fidelity.

Method: Curate 16 hours of multi-intersection traffic video under diverse conditions; apply an event-driven dynamic blur for privacy; densely annotate 8 hours to produce 28K QA pairs (~1 Q/s) via a unified pipeline; define a hierarchical taxonomy spanning basic, attribution, event, reverse, and counterfactual reasoning; release paired benchmarks for VideoQA and VideoQGen; evaluate 10 SOTA VideoLMs on UDVideoQA and 8 models on VideoQGen; fine-tune Qwen2.5-VL 7B on UDVideoQA.

Result: Benchmarks show a persistent perception–reasoning gap: models strong in abstract inference often fail at visual grounding. Gemini Pro attains the best zero-shot accuracy; fine-tuning Qwen2.5-VL 7B on UDVideoQA narrows the gap to proprietary systems. For VideoQGen, Gemini 2.5 Pro and Qwen3 Max produce the most relevant and complex questions, but all models show limited linguistic diversity.

Conclusion: UDVideoQA (dataset, tools, and benchmarks) establishes a real-world, privacy-preserving testbed for visual grounding and causal reasoning in urban scenes. Results highlight the need to improve perception robustness and diversify language generation, and show that targeted fine-tuning on UDVideoQA can yield competitive open models. Resource: https://ud-videoqa.github.io/UD-VideoQA/UD-VideoQA/

Abstract: Understanding the complex, multi-agent dynamics of urban traffic remains a fundamental challenge for video language models. This paper introduces Urban Dynamics VideoQA, a benchmark dataset that captures the unscripted real-world behavior of dynamic urban scenes. UDVideoQA is curated from 16 hours of traffic footage recorded at multiple city intersections under diverse traffic, weather, and lighting conditions. It employs an event-driven dynamic blur technique to ensure privacy preservation without compromising scene fidelity. Using a unified annotation pipeline, the dataset contains 28K question-answer pairs generated across 8 hours of densely annotated video, averaging one question per second. Its taxonomy follows a hierarchical reasoning level, spanning basic understanding and attribution to event reasoning, reverse reasoning, and counterfactual inference, enabling systematic evaluation of both visual grounding and causal reasoning. Comprehensive experiments benchmark 10 SOTA VideoLMs on UDVideoQA and 8 models on a complementary video question generation benchmark. Results reveal a persistent perception-reasoning gap, showing models that excel in abstract inference often fail with fundamental visual grounding. While models like Gemini Pro achieve the highest zero-shot accuracy, fine-tuning the smaller Qwen2.5-VL 7B model on UDVideoQA bridges this gap, achieving performance comparable to proprietary systems. In VideoQGen, Gemini 2.5 Pro, and Qwen3 Max generate the most relevant and complex questions, though all models exhibit limited linguistic diversity, underscoring the need for human-centric evaluation. The UDVideoQA suite, including the dataset, annotation tools, and benchmarks for both VideoQA and VideoQGen, provides a foundation for advancing robust, privacy-aware, and real-world multimodal reasoning. UDVideoQA is available at https://ud-videoqa.github.io/UD-VideoQA/UD-VideoQA/.

</details>


### [62] [LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis](https://arxiv.org/abs/2602.21142)
*Zhifan Jiang,Dong Yang,Vishwesh Nath,Abhijeet Parida,Nishad P. Kulkarni,Ziyue Xu,Daguang Xu,Syed Muhammad Anwar,Holger R. Roth,Marius George Linguraru*

Main category: cs.CV

TL;DR: Introduces LUMEN, a multi-image, multi-task instruction-tuning framework for longitudinal chest X-ray VQA that improves diagnostic performance and shows early promise for prognosis on MIMIC-CXR/Medical-Diff-VQA.


<details>
  <summary>Details</summary>
Motivation: Radiologists rely on temporal changes across serial chest X-rays for accurate diagnosis/prognosis, but manual longitudinal review is time-consuming; there is a need for VLMs that reason over multiple timepoints and support prognostic decision-making.

Method: Design a training framework (LUMEN) optimized for longitudinal CXR interpretation via multi-image and multi-task instruction fine-tuning; construct a new instruction-following dataset that encodes longitudinal studies to enable a prognostic VQA task; evaluate on MIMIC-CXR and Medical-Diff-VQA.

Result: Compared to baselines, LUMEN achieves significant gains on diagnostic VQA benchmarks and demonstrates promising, though preliminary, prognostic capabilities.

Conclusion: Well-designed, instruction-tuned VLMs that ingest longitudinal imaging can yield more accurate and clinically meaningful interpretations; LUMEN is a step toward VQA systems with prognostic utility in radiology.

Abstract: Large vision-language models (VLMs) have evolved from general-purpose applications to specialized use cases such as in the clinical domain, demonstrating potential for decision support in radiology. One promising application is assisting radiologists in decision-making by the analysis of radiology imaging data such as chest X-rays (CXR) via a visual and natural language question-answering (VQA) interface. When longitudinal imaging is available, radiologists analyze temporal changes, which are essential for accurate diagnosis and prognosis. The manual longitudinal analysis is a time-consuming process, motivating the development of a training framework that can provide prognostic capabilities. We introduce a novel training framework LUMEN, that is optimized for longitudinal CXR interpretation, leveraging multi-image and multi-task instruction fine-tuning to enhance prognostic and diagnostic performance. We conduct experiments on the publicly available MIMIC-CXR and its associated Medical-Diff-VQA datasets. We further formulate and construct a novel instruction-following dataset incorporating longitudinal studies, enabling the development of a prognostic VQA task. Our method demonstrates significant improvements over baseline models in diagnostic VQA tasks, and more importantly, shows promising potential for prognostic capabilities. These results underscore the value of well-designed, instruction-tuned VLMs in enabling more accurate and clinically meaningful radiological interpretation of longitudinal radiological imaging data.

</details>


### [63] [SPRITETOMESH: Automatic Mesh Generation for 2D Skeletal Animation Using Learned Segmentation and Contour-Aware Vertex Placement](https://arxiv.org/abs/2602.21153)
*Bastien Gimbert*

Main category: cs.CV

TL;DR: SPRITETOMESH is an automatic pipeline that converts 2D sprites into skeletal-animation-ready triangle meshes by combining a trained segmentation network with classical contour/edge-based vertex placement and Delaunay triangulation, delivering sub-3s processing and 300x–1200x speedup over manual work; attempts to directly predict vertices with a neural heatmap fail, motivating the hybrid design.


<details>
  <summary>Details</summary>
Motivation: Manual authoring of animation-ready meshes for 2D sprites is slow (15–60 minutes per sprite) and tedious, requiring precise vertex placement along visual boundaries. Automating this would greatly accelerate 2D game production while preserving mesh quality compatible with tools like Spine2D.

Method: A hybrid learned–algorithmic pipeline: (1) a segmentation model (EfficientNet-B0 encoder + U-Net decoder) trained on 100k+ sprite–mask pairs to produce accurate binary masks (IoU 0.87); (2) exterior contour vertices from the mask via Douglas–Peucker simplification with adaptive arc subdivision; (3) interior vertices placed along visual boundaries found using bilateral-filtered multi-channel Canny edge detection with contour-following; (4) Delaunay triangulation with mask-based centroid filtering to finalize the mesh. A controlled experiment contrasts this with a neural heatmap regression decoder for vertex positions, which fails to learn.

Result: Segmentation achieves IoU 0.87; the full pipeline generates meshes in under 3 seconds per sprite, yielding 300x–1200x speedup versus manual workflows. The vertex heatmap regression decoder does not converge (loss plateaus at 0.061) while the segmentation decoder trains normally, supporting the claim that direct vertex prediction is ill-posed given multiple valid meshing solutions.

Conclusion: Hybridization—learning where labels are unambiguous (segmentation) and algorithms where artistic ambiguity exists (vertex placement)—is effective for rapid, practical sprite meshing. The released model provides immediate utility to game developers, and the negative result justifies avoiding direct learned vertex prediction for this task.

Abstract: We present SPRITETOMESH, a fully automatic pipeline for converting 2D game sprite images into triangle meshes compatible with skeletal animation frameworks such as Spine2D. Creating animation-ready meshes is traditionally a tedious manual process requiring artists to carefully place vertices along visual boundaries, a task that typically takes 15-60 minutes per sprite. Our method addresses this through a hybrid learned-algorithmic approach. A segmentation network (EfficientNet-B0 encoder with U-Net decoder) trained on over 100,000 sprite-mask pairs from 172 games achieves an IoU of 0.87, providing accurate binary masks from arbitrary input images. From these masks, we extract exterior contour vertices using Douglas-Peucker simplification with adaptive arc subdivision, and interior vertices along visual boundaries detected via bilateral-filtered multi-channel Canny edge detection with contour-following placement. Delaunay triangulation with mask-based centroid filtering produces the final mesh. Through controlled experiments, we demonstrate that direct vertex position prediction via neural network heatmap regression is fundamentally not viable for this task: the heatmap decoder consistently fails to converge (loss plateau at 0.061) while the segmentation decoder trains normally under identical conditions. We attribute this to the inherently artistic nature of vertex placement - the same sprite can be meshed validly in many different ways. This negative result validates our hybrid design: learned segmentation where ground truth is unambiguous, algorithmic placement where domain heuristics are appropriate. The complete pipeline processes a sprite in under 3 seconds, representing a speedup of 300x-1200x over manual creation. We release our trained model to the game development community.

</details>


### [64] [Region of Interest Segmentation and Morphological Analysis for Membranes in Cryo-Electron Tomography](https://arxiv.org/abs/2602.21195)
*Xingyi Cheng,Julien Maufront,Aurélie Di Cicco,Daniël M. Pelt,Manuela Dezi,Daniel Lévy*

Main category: cs.CV

TL;DR: Two-step framework (TomoROIS + SurfORA) for direct, shape-agnostic ROI segmentation and quantitative surface morphology analysis in cryo-ET. Trains with small annotations, handles open/closed surfaces, and extracts distances, curvature, and roughness. Demonstrated on reconstituted membranes to quantify contact sites and invaginations; broadly applicable beyond cryo-ET.


<details>
  <summary>Details</summary>
Motivation: ROI identification in scientific imaging is essential but often achieved indirectly via full-structure segmentation followed by post hoc analysis. This is especially limiting for continuous, geometrically complex structures like membranes, typically segmented as a single entity, making localized quantitative analysis difficult.

Method: TomoROIS performs deep learning-based ROI segmentation that can be trained from scratch on small annotated datasets. SurfORA takes the segmented structures, converts them into point clouds/meshes, and computes morphological metrics (e.g., inter-membrane distances, curvature, surface roughness). It supports closed and open surfaces and includes considerations for open surfaces common in cryo-ET due to the missing wedge.

Result: Applied to in vitro reconstituted membrane systems with deformable vesicles, enabling automated, quantitative analysis of membrane contact sites and remodeling events such as invagination. Demonstrations suggest the approach can generalize to broader imaging contexts.

Conclusion: Direct ROI-first segmentation coupled with mesh-based surface morphometrics provides a practical, quantitative workflow for analyzing complex membrane geometries in cryo-ET and likely benefits other imaging domains where localized, shape-agnostic analysis is needed.

Abstract: Cryo-electron tomography (cryo-ET) enables high resolution, three-dimensional reconstruction of biological structures, including membranes and membrane proteins. Identification of regions of interest (ROIs) is central to scientific imaging, as it enables isolation and quantitative analysis of specific structural features within complex datasets. In practice, however, ROIs are typically derived indirectly through full structure segmentation followed by post hoc analysis. This limitation is especially apparent for continuous and geometrically complex structures such as membranes, which are segmented as single entities. Here, we developed TomoROIS-SurfORA, a two step framework for direct, shape-agnostic ROI segmentation and morphological surface analysis. TomoROIS performs deep learning-based ROI segmentation and can be trained from scratch using small annotated datasets, enabling practical application across diverse imaging data. SurfORA processes segmented structures as point clouds and surface meshes to extract quantitative morphological features, including inter-membrane distances, curvature, and surface roughness. It supports both closed and open surfaces, with specific considerations for open surfaces, which are common in cryo-ET due to the missing wedge effect. We demonstrate both tools using in vitro reconstituted membrane systems containing deformable vesicles with complex geometries, enabling automatic quantitative analysis of membrane contact sites and remodeling events such as invagination. While demonstrated here on cryo-ET membrane data, the combined approach is applicable to ROI detection and surface analysis in broader scientific imaging contexts.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [65] [Talking to Yourself: Defying Forgetting in Large Language Models](https://arxiv.org/abs/2602.20162)
*Yutao Sun,Mingshuai Chen,Tiancheng Zhao,Phillip Miao,Zilun Zhang,Haozhan Shen,Ruizhe Zhu,Jianwei Yin*

Main category: cs.CL

TL;DR: SA-SFT is a lightweight self-augmentation approach where an LLM generates self-dialogues before supervised fine-tuning and mixes them with task data, requiring no external data or schedule changes. It mitigates catastrophic forgetting while improving in-domain performance and outperforms baselines across extensive evaluations; theory suggests self-generated data counteracts style-induced parameter drift.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs on narrow task data often causes catastrophic forgetting, degrading broad knowledge and reasoning. The goal is to adapt models to new tasks without sacrificing general capabilities and without relying on extra data or complex training tricks.

Method: Before standard SFT, the model auto-generates self-dialogues (self-authored examples) and mixes them with the task data. Training uses the usual optimization and schedules—no architectural changes, no external corpora, and no extra tuning phases. The approach effectively self-aligns the model’s style/distribution during fine-tuning.

Result: Across 50 evaluation scenarios, SA-SFT maintains performance comparable to the base model and achieves the best results in 40 cases, while also improving in-domain metrics. It consistently reduces catastrophic forgetting and outperforms baselines such as layer freezing and external data mixing, despite using no external data.

Conclusion: Self-augmentation via self-generated data is a simple, effective way to adapt LLMs robustly, mitigating catastrophic forgetting. A theoretical analysis attributes forgetting partly to style-induced parameter drift; self-alignment with model-authored data counteracts this drift, enabling strong in-domain gains without sacrificing general performance.

Abstract: Catastrophic forgetting remains a major challenge when fine-tuning large language models (LLMs) on narrow, task-specific data, often degrading their general knowledge and reasoning abilities. We propose SA-SFT, a lightweight self-augmentation routine in which an LLM generates self-dialogues prior to fine-tuning, and the resulting self-authored data are mixed with task data without modifying optimization or training schedules.
  Despite requiring no external data or additional tuning, SA-SFT consistently mitigates catastrophic forgetting while improving in-domain performance. Across 50 evaluation scenarios, it maintains performance comparable to the original model and achieves the best results in 40 cases, outperforming common baselines such as layer freezing and external data mixing. Guided by these empirical findings, we further present a theoretical analysis suggesting that forgetting can partly stem from style-induced parameter drift, and that self-alignment through self-generated data provides an effective means to counteract this effect. Overall, our results indicate that self-augmentation offers a simple and effective mechanism for robust LLM adaptation without incurring catastrophic forgetting.

</details>


### [66] [Benchmarking Distilled Language Models: Performance and Efficiency in Resource-Constrained Settings](https://arxiv.org/abs/2602.20164)
*Sachin Gopal Wani,Eric Page,Ajay Dholakia,David Ellison*

Main category: cs.CL

TL;DR: Distilling large language models into smaller ones yields far better performance-per-compute: a distilled 8B model matches or surpasses much larger models while being ~2000× cheaper to create than training an 8B model from scratch.


<details>
  <summary>Details</summary>
Motivation: Make powerful language models usable in resource-constrained settings by validating whether knowledge distillation can be more than compression—i.e., a primary, cost-effective path to state-of-the-art small language models.

Method: Benchmark distilled models against vanilla-trained and proprietary models on both performance (especially reasoning) and computational cost. Construct a distilled 8B model and compare its performance-to-compute curve with baselines.

Result: Distillation produces a superior performance-to-compute curve. Creating a distilled 8B model is >2000× more compute-efficient than training the same-size vanilla model, while achieving reasoning on par with or better than models ~10× larger.

Conclusion: Knowledge distillation is validated as a first-class strategy—not just a compression step—for building state-of-the-art, accessible AI, enabling strong reasoning at drastically lower compute cost.

Abstract: Knowledge distillation offers a transformative pathway to developing powerful, yet efficient, small language models (SLMs) suitable for resource-constrained environments. In this paper, we benchmark the performance and computational cost of distilled models against their vanilla and proprietary counterparts, providing a quantitative analysis of their efficiency. Our results demonstrate that distillation creates a superior performance-tocompute curve. We find that creating a distilled 8B model is over 2,000 times more compute-efficient than training its vanilla counterpart, while achieving reasoning capabilities on par with, or even exceeding, standard models ten times its size. These findings validate distillation not just as a compression technique, but as a primary strategy for building state-of-the-art, accessible AI

</details>


### [67] [ConceptRM: The Quest to Mitigate Alert Fatigue through Consensus-Based Purity-Driven Data Cleaning for Reflection Modelling](https://arxiv.org/abs/2602.20166)
*Yongda Yu,Lei Zhang,Xinxin Guo,Minghui Yu,Zhengqi Zhuang,Guoping Rong,Haifeng Shen,Zhengfeng Li,Boge Wang,Guoan Zhang,Bangyu Xiang,Xiaobin Xu*

Main category: cs.CL

TL;DR: ConceptRM builds a high-quality training corpus from noisy user-feedback labels to train a reflection (alert-filtering) model. Using a few expert-annotated anchors, it perturbs data to create varied-noise sets, applies co-teaching among diverse models, and selects consensus negatives. It markedly improves false-alert interception over strong LLM baselines (up to +53.31% in-domain, +41.67% OOD) with minimal annotation cost.


<details>
  <summary>Details</summary>
Motivation: Alert-heavy systems cause user fatigue because many alerts are false. Training a reflection model to filter false alerts typically relies on user-feedback labels gathered in production, which are noisy and expensive to clean manually. A noise-robust, low-annotation method is needed to curate reliable training data—especially negative samples—to improve filtering without high labeling cost.

Method: Start with a small set of expert-annotated examples as anchors. Generate perturbed datasets with controlled, varying noise ratios. Train multiple distinct models via co-teaching on these noisy datasets so models learn from each other’s confident signals. Derive consensus across models to identify reliable negative samples from the noisy pool, forming a high-quality corpus for training the final reflection model that intercepts false alerts.

Result: ConceptRM substantially increases false-alert interception performance versus several state-of-the-art LLM baselines: up to 53.31% improvement on in-domain data and 41.67% on out-of-domain data, while keeping annotation costs low.

Conclusion: Leveraging limited expert anchors, noise-perturbed training, and co-teaching consensus enables robust identification of reliable negatives from noisy feedback. This yields an effective reflection model that reduces alert fatigue, generalizes across domains, and lowers annotation burden.

Abstract: In many applications involving intelligent agents, the overwhelming volume of alerts (mostly false) generated by the agents may desensitize users and cause them to overlook critical issues, leading to the so-called ''alert fatigue''. A common strategy is to train a reflection model as a filter to intercept false alerts with labelled data collected from user verification feedback. However, a key challenge is the noisy nature of such data as it is often collected in production environments. As cleaning noise via manual annotation incurs high costs, this paper proposes a novel method ConceptRM for constructing a high-quality corpus to train a reflection model capable of effectively intercepting false alerts. With only a small amount of expert annotations as anchors, ConceptRM creates perturbed datasets with varying noise ratios and utilizes co-teaching to train multiple distinct models for collaborative learning. By analyzing the consensus decisions of these models, it effectively identifies reliable negative samples from a noisy dataset. Experimental results demonstrate that ConceptRM significantly enhances the interception of false alerts with minimal annotation cost, outperforming several state-of-the-art LLM baselines by up to 53.31% on in-domain datasets and 41.67% on out-of-domain datasets.

</details>


### [68] [InterviewSim: A Scalable Framework for Interview-Grounded Personality Simulation](https://arxiv.org/abs/2602.20294)
*Yu Li,Pranav Narayanan Venkit,Yada Pruksachatkun,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: They build a large, interview-grounded evaluation suite for LLM personality simulation using 671k Q&A pairs from 23k verified transcripts across 1,000 public figures, introduce four metrics (content similarity, factual consistency, personality alignment, knowledge retention), and show interview-grounded methods beat biographical/parametric baselines, with a trade-off: retrieval helps style/response quality, chronological use helps factuality/retention.


<details>
  <summary>Details</summary>
Motivation: Personality simulation studies typically evaluate against proxies (surveys, bios, short AI-led interviews) rather than what people actually said, limiting validity and scalability. The field needs a direct, large-scale, authentic benchmark to ground and assess simulated personalities.

Method: Curate and verify long-form interview transcripts for 1,000 public personalities; extract 671k question–answer pairs (≈11.5 hours/person). Propose a multi-dimensional evaluation with four complementary metrics covering content similarity, factual consistency, personality alignment, and knowledge retention. Systematically compare simulation methods: parametric-only, biographical-profile grounded, retrieval-augmented with interview data, and chronological (time-ordered) use of interview data.

Result: Interview-grounded approaches substantially outperform biographical- or parametric-only baselines. Retrieval-augmented methods better capture personality style and overall response quality, whereas chronological methods better preserve factual consistency and knowledge retention, revealing a practical trade-off in how to best leverage interview data.

Conclusion: An interview-grounded, multi-metric framework enables principled selection of personality simulation methods based on application goals. Empirical results offer actionable guidance: use retrieval for style/alignment and chronological structuring for factuality/retention, advancing robust, authentic personality simulation research.

Abstract: Simulating real personalities with large language models requires grounding generation in authentic personal data. Existing evaluation approaches rely on demographic surveys, personality questionnaires, or short AI-led interviews as proxies, but lack direct assessment against what individuals actually said. We address this gap with an interview-grounded evaluation framework for personality simulation at a large scale. We extract over 671,000 question-answer pairs from 23,000 verified interview transcripts across 1,000 public personalities, each with an average of 11.5 hours of interview content. We propose a multi-dimensional evaluation framework with four complementary metrics measuring content similarity, factual consistency, personality alignment, and factual knowledge retention. Through systematic comparison, we demonstrate that methods grounded in real interview data substantially outperform those relying solely on biographical profiles or the model's parametric knowledge. We further reveal a trade-off in how interview data is best utilized: retrieval-augmented methods excel at capturing personality style and response quality, while chronological-based methods better preserve factual consistency and knowledge retention. Our evaluation framework enables principled method selection based on application requirements, and our empirical findings provide actionable insights for advancing personality simulation research.

</details>


### [69] [What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance](https://arxiv.org/abs/2602.20300)
*William Watson,Nicole Cho,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: They show that the wording and structure of user queries systematically affect LLM hallucination risk, and they quantify this with a 22-feature representation that maps a consistent risk landscape across large, real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Hallucinations are usually blamed on models or decoding, but classical linguistics suggests that query form also shapes responses. The paper tests whether specific query features make hallucinations more or less likely.

Method: Construct a 22-dimensional feature vector for each query (covering clause complexity, lexical rarity, anaphora, negation, answerability, intention grounding, etc.). Analyze 369,837 real-world queries across datasets/models to correlate these features with hallucination propensity.

Result: They find a stable pattern: deep clause nesting and underspecified queries correlate with higher hallucination; clear intention grounding and answerability correlate with lower hallucination; domain specificity has mixed, dataset- and model-dependent effects.

Conclusion: Query features provide an empirical representation predictive of hallucination risk, enabling guided query rewriting and motivating controlled intervention studies to reduce hallucinations.

Abstract: Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent "risk landscape": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.

</details>


### [70] [No One Size Fits All: QueryBandits for Hallucination Mitigation](https://arxiv.org/abs/2602.20332)
*Nicole Cho,William Watson,Alec Koppel,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: QueryBandits is a model-agnostic contextual bandit that adaptively selects query-rewrite strategies to reduce LLM hallucinations—especially for closed-source models—achieving strong gains over no-rewrite and static baselines using only forward-pass control.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs are rising, yet mitigation work largely targets open-source models via post-hoc detection or parameter editing. Closed-source systems dominate institutional use but lack feasible mitigation methods that don’t require model internals or retraining.

Method: Formulate query rewriting as a contextual bandit: per incoming query, use semantic features to choose among multiple rewrite strategies (arms). Learn online (e.g., Thompson Sampling) with an empirically validated, calibrated reward function that proxies hallucination reduction/QA correctness. Compare contextual bandits to vanilla (context-free) bandits and to static zero-shot rewrite policies (Paraphrase, Expand) and No-Rewrite.

Result: On 16 QA scenarios, the Thompson Sampling QueryBandit attains an 87.5% win rate over No-Rewrite and outperforms Paraphrase and Expand by 42.6% and 60.3%. All contextual bandits beat vanilla bandits across datasets; higher feature variance aligns with more diverse arm selection. Some static policies incur higher cumulative regret than No-Rewrite, sometimes worsening hallucinations.

Conclusion: There is no universally optimal rewrite; adaptive, feature-conditioned selection performs better. QueryBandits shifts model behavior via forward-pass-only mechanisms, making it suitable for closed-source deployments without retraining or gradient-based adaptation.

Abstract: Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.

</details>


### [71] [Natural Language Processing Models for Robust Document Categorization](https://arxiv.org/abs/2602.20336)
*Radoslaw Roszczyk,Pawel Tecza,Maciej Stodolski,Krzysztof Siwek*

Main category: cs.CL

TL;DR: Compared Naive Bayes, BiLSTM, and fine‑tuned BERT for imbalanced text classification and built a working routing system. BERT was most accurate (>99%) but costly; BiLSTM was near‑state‑of‑the‑art (~98.56%) with moderate cost; Naive Bayes was fastest (~ms) but least accurate (~94.5%).


<details>
  <summary>Details</summary>
Motivation: Enable reliable, efficient automated document/request categorization in real‑world pipelines where classes are imbalanced and computational budgets matter, requiring a balance between accuracy and efficiency.

Method: Empirical comparison of three models of increasing complexity (Naive Bayes, BiLSTM, fine‑tuned BERT) on an imbalanced text classification task. Measured classification accuracy and computational/training costs; analyzed impact of class imbalance; implemented a demonstrative end‑to‑end system for routing technical requests.

Result: BERT achieved the highest accuracy, consistently >99%, but required the longest training time and greatest compute. BiLSTM reached ~98.56% accuracy with moderate training cost and strong contextual modeling. Naive Bayes trained in milliseconds but averaged ~94.5% accuracy. Class imbalance affected all models, especially minority categories.

Conclusion: For the studied scenario, BiLSTM offers the best accuracy–efficiency trade‑off and is recommended for deployment. The authors suggest future improvements and deeper exploration of transformer architectures to further enhance performance.

Abstract: This article presents an evaluation of several machine learning methods applied to automated text classification, alongside the design of a demonstrative system for unbalanced document categorization and distribution. The study focuses on balancing classification accuracy with computational efficiency, a key consideration when integrating AI into real world automation pipelines. Three models of varying complexity were examined: a Naive Bayes classifier, a bidirectional LSTM network, and a fine tuned transformer based BERT model.
  The experiments reveal substantial differences in performance. BERT achieved the highest accuracy, consistently exceeding 99\%, but required significantly longer training times and greater computational resources. The BiLSTM model provided a strong compromise, reaching approximately 98.56\% accuracy while maintaining moderate training costs and offering robust contextual understanding. Naive Bayes proved to be the fastest to train, on the order of milliseconds, yet delivered the lowest accuracy, averaging around 94.5\%. Class imbalance influenced all methods, particularly in the recognition of minority categories.
  A fully functional demonstrative system was implemented to validate practical applicability, enabling automated routing of technical requests with throughput unattainable through manual processing. The study concludes that BiLSTM offers the most balanced solution for the examined scenario, while also outlining opportunities for future improvements and further exploration of transformer architectures.

</details>


### [72] [How communicatively optimal are exact numeral systems? Once more on lexicon size and morphosyntactic complexity](https://arxiv.org/abs/2602.20372)
*Chundra Cathcart,Arne Rubehn,Katja Bocklage,Luca Ciucci,Kellen Parker van Dam,Alžběta Kučerová,Jekaterina Mažara,Carlo Y. Meloni,David Snee,Johann-Mattis List*

Main category: cs.CL

TL;DR: Reassessing claims that numeral systems are communicatively optimal, the paper shows—using a richer treatment of morphological complexity and allomorphy across 52 languages—that many numeral systems are less efficient than predicted by prior models.


<details>
  <summary>Details</summary>
Motivation: Prior work posits that exact recursive numeral systems balance a tradeoff between lexicon size and average morphosyntactic complexity, implying near-optimal communicative efficiency across languages. The authors argue this view overlooks how complexity is realized, particularly the role of predictable vs. unpredictable allomorphy, leading to potentially inflated efficiency estimates.

Method: Compile data from 52 genetically diverse languages; annotate numeral morphology with a scheme that distinguishes predictable from unpredictable allomorphy; re-estimate communicative efficiency (lexicon size vs. average complexity in morphemes) under this enriched characterization; compare observed systems to efficiency expectations from prior models.

Result: When complexity is measured with the new annotation (accounting for allomorphy type), many languages fall significantly short of predicted efficiency—i.e., they are less efficient than one would expect under previously assumed optimal tradeoffs.

Conclusion: Communicative efficiency alone does not explain cross-linguistic numeral system structure. Modeling the granularity of morphological complexity and formal variation is crucial, and evolutionary explanations for numeral systems should incorporate additional constraints (historical, cognitive, learnability) beyond simple efficiency tradeoffs.

Abstract: Recent research argues that exact recursive numeral systems optimize communicative efficiency by balancing a tradeoff between the size of the numeral lexicon and the average morphosyntactic complexity (roughly length in morphemes) of numeral terms. We argue that previous studies have not characterized the data in a fashion that accounts for the degree of complexity languages display. Using data from 52 genetically diverse languages and an annotation scheme distinguishing between predictable and unpredictable allomorphy (formal variation), we show that many of the world's languages are decisively less efficient than one would expect. We discuss the implications of our findings for the study of numeral systems and linguistic evolution more generally.

</details>


### [73] [Case-Aware LLM-as-a-Judge Evaluation for Enterprise-Scale RAG Systems](https://arxiv.org/abs/2602.20379)
*Mukul Chhabra,Luigi Medrano,Arush Verma*

Main category: cs.CL

TL;DR: Introduces a case-aware LLM-as-a-judge framework to evaluate multi-turn enterprise RAG assistants using eight operational metrics, severity-aware scoring, and deterministic JSON prompts, revealing actionable tradeoffs that generic metrics miss.


<details>
  <summary>Details</summary>
Motivation: Enterprise RAG assistants work in multi-turn, case-based workflows (e.g., tech support/IT ops) with structured identifiers and resolution steps. Existing benchmark-style, single-turn evaluations miss enterprise-specific failure modes like case misidentification, workflow misalignment, and partial/stepwise resolution, leading to poor diagnostic value.

Method: Design a case-aware, turn-level LLM-as-a-judge evaluation with eight metrics partitioned into retrieval quality, grounding fidelity, answer utility, precision integrity, and case/workflow alignment. Apply a severity-aware scoring protocol to curb score inflation and improve diagnostic clarity. Use deterministic prompting with strict JSON outputs to enable scalable batch evaluation, regression testing, and production monitoring. Conduct a comparative study of two instruction-tuned models across short and long workflows.

Result: Generic proxy metrics provide ambiguous signals; the proposed framework surfaces enterprise-critical tradeoffs and yields clearer, actionable diagnostics across heterogeneous cases and workflow lengths.

Conclusion: Operationally grounded, multi-turn, case-aware evaluation provides more reliable and actionable insights for enterprise RAG systems than generic metrics, supporting continuous improvement, regression testing, and production monitoring.

Abstract: Enterprise Retrieval-Augmented Generation (RAG) assistants operate in multi-turn, case-based workflows such as technical support and IT operations, where evaluation must reflect operational constraints, structured identifiers (e.g., error codes, versions), and resolution workflows. Existing RAG evaluation frameworks are primarily designed for benchmark-style or single-turn settings and often fail to capture enterprise-specific failure modes such as case misidentification, workflow misalignment, and partial resolution across turns.
  We present a case-aware LLM-as-a-Judge evaluation framework for enterprise multi-turn RAG systems. The framework evaluates each turn using eight operationally grounded metrics that separate retrieval quality, grounding fidelity, answer utility, precision integrity, and case/workflow alignment. A severity-aware scoring protocol reduces score inflation and improves diagnostic clarity across heterogeneous enterprise cases. The system uses deterministic prompting with strict JSON outputs, enabling scalable batch evaluation, regression testing, and production monitoring.
  Through a comparative study of two instruction-tuned models across short and long workflows, we show that generic proxy metrics provide ambiguous signals, while the proposed framework exposes enterprise-critical tradeoffs that are actionable for system improvement.

</details>


### [74] [Disentangling Geometry, Performance, and Training in Language Models](https://arxiv.org/abs/2602.20433)
*Atharva Kulkarni,Jacob Mitchell Springer,Arjun Subramonian,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: Across 108 OLMo-style LMs, unembedding-matrix geometry—especially effective rank—correlates with performance only inconsistently; it is primarily shaped by training hyperparameters and cannot reliably predict downstream performance.


<details>
  <summary>Details</summary>
Motivation: Geometry of Transformer weights has been informative for interpretability, prompting interest in whether simple geometric proxies (e.g., effective rank of the unembedding) could forecast model performance or explain degradation (e.g., saturation) without expensive evaluations.

Method: Train/evaluate 108 OLMo-style language models under controlled variations (notably batch size and weight decay); compute effective rank of the unembedding and other geometric metrics, extend analysis to final-layer representations; probe correlations with downstream task performance and test prior causal claims (e.g., low rank causing late-stage degradation) including adversarial counterexamples.

Result: - Best models often—but not always—show high effective rank; the trend is task- and setup-dependent.
- Low effective rank co-occurs with, but does not cause, late-stage performance degradation in small models; adversarial cases show low-rank models without saturation.
- Effective rank is strongly modulated by pretraining hyperparameters (batch size, weight decay), which themselves influence performance.
- Other geometric metrics and final-layer geometry are largely aligned with each other but fail to reliably predict downstream performance.

Conclusion: Existing geometric measures of the unembedding/final layer mostly reflect training choices rather than inherent capability; they are unreliable as performance predictors. Practitioners should avoid using effective rank (and similar metrics) as proxies for downstream performance and pursue metrics with clearer causal grounding or use direct evaluations.

Abstract: Geometric properties of Transformer weights, particularly the unembedding matrix, have been widely useful in language model interpretability research. Yet, their utility for estimating downstream performance remains unclear. In this work, we systematically investigate the relationship between model performance and the unembedding matrix geometry, particularly its effective rank. Our experiments, involving a suite of 108 OLMo-style language models trained under controlled variation, reveal several key findings. While the best-performing models often exhibit a high effective rank, this trend is not universal across tasks and training setups. Contrary to prior work, we find that low effective rank does not cause late-stage performance degradation in small models, but instead co-occurs with it; we find adversarial cases where low-rank models do not exhibit saturation. Moreover, we show that effective rank is strongly influenced by pre-training hyperparameters, such as batch size and weight decay, which in-turn affect the model's performance. Lastly, extending our analysis to other geometric metrics and final-layer representation, we find that these metrics are largely aligned, but none can reliably predict downstream performance. Overall, our findings suggest that the model's geometry, as captured by existing metrics, primarily reflects training choices rather than performance.

</details>


### [75] [From Performance to Purpose: A Sociotechnical Taxonomy for Evaluating Large Language Model Utility](https://arxiv.org/abs/2602.20513)
*Gavin Levinson,Keith Feldman*

Main category: cs.CL

TL;DR: Introduces LUX, a unified, metric-grounded taxonomy and web tool for evaluating LLM utility across performance, interaction, operations, and governance, enabling consistent, use-case–aligned comparisons beyond task accuracy.


<details>
  <summary>Details</summary>
Motivation: Task-level benchmarks don’t capture the sociotechnical factors that determine whether LLMs are fit for high-stakes, real-world deployment, and existing metrics are fragmented without a unifying structure for comparison.

Method: Design of a hierarchical taxonomy (LUX) organized into four domains—performance, interaction, operations, governance—decomposed into dimensions and components, each linked to measurable factors; delivery of a dynamic web tool that maps components to a repository of evaluation metrics.

Result: A comprehensive, organized framework and interactive resource that standardize how practitioners evaluate LLM utility, supporting quantitative comparisons and alignment of model selection with intended applications.

Conclusion: By systematizing diverse evaluation considerations and tying them to concrete metrics, LUX enables more rigorous, consistent, and context-appropriate assessment of LLMs, addressing gaps left by traditional task performance metrics.

Abstract: As large language models (LLMs) continue to improve at completing discrete tasks, they are being integrated into increasingly complex and diverse real-world systems. However, task-level success alone does not establish a model's fit for use in practice. In applied, high-stakes settings, LLM effectiveness is driven by a wider array of sociotechnical determinants that extend beyond conventional performance measures. Although a growing set of metrics capture many of these considerations, they are rarely organized in a way that supports consistent evaluation, leaving no unified taxonomy for assessing and comparing LLM utility across use cases. To address this gap, we introduce the Language Model Utility Taxonomy (LUX), a comprehensive framework that structures utility evaluation across four domains: performance, interaction, operations, and governance. Within each domain, LUX is organized hierarchically into thematically aligned dimensions and components, each grounded in metrics that enable quantitative comparison and alignment of model selection with intended use. In addition, an external dynamic web tool is provided to support exploration of the framework by connecting each component to a repository of relevant metrics (factors) for applied evaluation.

</details>


### [76] [Stop-Think-AutoRegress: Language Modeling with Latent Diffusion Planning](https://arxiv.org/abs/2602.20528)
*Justin Lovelace,Christian Belardi,Sofian Zalouk,Adhitya Polavaram,Srivatsa Kundurthy,Kilian Q. Weinberger*

Main category: cs.CL

TL;DR: STAR-LDM adds a diffusion-based “thinking” phase that plans in a continuous latent space between autoregressive token steps, yielding better coherence/reasoning and controllable generation versus similar-sized baselines.


<details>
  <summary>Details</summary>
Motivation: Autoregressive LMs are myopic—making token-by-token choices without global foresight—and are hard to steer without retraining. The paper aims to introduce explicit global planning and lightweight controllability while keeping fluency.

Method: During generation, the model periodically pauses (Stop-Think) to run a latent diffusion process that refines a semantic plan, then continues autoregressively (AutoRegress) conditioned on that plan. Lightweight attribute classifiers provide controllable steering without retraining, trading off control and fluency better than specialized methods.

Result: Outperforms similar-sized models on language understanding benchmarks; achieves >70% LLM-as-judge win rates for narrative coherence and commonsense reasoning; shows improved fluency-control trade-offs via classifier-guided steering.

Conclusion: Combining diffusion-based global planning with autoregressive decoding improves coherence and reasoning and enables practical, fine-grained control, suggesting a promising hybrid architecture for language generation.

Abstract: The Stop-Think-AutoRegress Language Diffusion Model (STAR-LDM) integrates latent diffusion planning with autoregressive generation. Unlike conventional autoregressive language models limited to token-by-token decisions, STAR-LDM incorporates a "thinking" phase that pauses generation to refine a semantic plan through diffusion before continuing. This enables global planning in continuous space prior to committing to discrete tokens. Evaluations show STAR-LDM significantly outperforms similar-sized models on language understanding benchmarks and achieves $>70\%$ win rates in LLM-as-judge comparisons for narrative coherence and commonsense reasoning. The architecture also allows straightforward control through lightweight classifiers, enabling fine-grained steering of attributes without model retraining while maintaining better fluency-control trade-offs than specialized approaches.

</details>


### [77] [Personal Information Parroting in Language Models](https://arxiv.org/abs/2602.20580)
*Nishant Subramani,Kshitish Ghate,Mona Diab*

Main category: cs.CL

TL;DR: Builds a regex-and-rules detector for personal information (PI) and uses it to quantify PI memorization in Pythia LMs, finding non-trivial verbatim parroting (2.7–13.6%) that increases with model size and training; recommends aggressive dataset filtering/anonymization.


<details>
  <summary>Details</summary>
Motivation: Web-scale LM pretraining ingests vast amounts of PI. Memorization of such PI poses privacy risks, but detecting PI reliably and quantifying memorization across model sizes/training remains underexplored.

Method: Develop a regexes-and-rules (R&R) detector for emails, phone numbers, and IP addresses that outperforms prior regex-based detectors. Curate 483 PI instances with their preceding contexts. Measure memorization by prompting models (Pythia 160M–6.9B at 70k–143k steps) with the exact preceding tokens from the source document and checking if greedy decoding reproduces the full PI span verbatim (“parroting”). Analyze how memorization varies with model size and pretraining duration.

Result: R&R detector outperforms existing regex PI detectors. On the 483-instance set, Pythia-6.9B parrots 13.6% of PIs verbatim; Pythia-160M parrots 2.7%. Memorization is positively correlated with both model size and amount of pretraining.

Conclusion: PI parroting occurs even in small models and scales with size/training, implying heightened privacy risks as models grow. The authors recommend aggressive filtering and anonymization of pretraining data to reduce memorization of PI.

Abstract: Modern language models (LM) are trained on large scrapes of the Web, containing millions of personal information (PI) instances, many of which LMs memorize, increasing privacy risks. In this work, we develop the regexes and rules (R&R) detector suite to detect email addresses, phone numbers, and IP addresses, which outperforms the best regex-based PI detectors. On a manually curated set of 483 instances of PI, we measure memorization: finding that 13.6% are parroted verbatim by the Pythia-6.9b model, i.e., when the model is prompted with the tokens that precede the PI in the original document, greedy decoding generates the entire PI span exactly. We expand this analysis to study models of varying sizes (160M-6.9B) and pretraining time steps (70k-143k iterations) in the Pythia model suite and find that both model size and amount of pretraining are positively correlated with memorization. Even the smallest model, Pythia-160m, parrots 2.7% of the instances exactly. Consequently, we strongly recommend that pretraining datasets be aggressively filtered and anonymized to minimize PI parroting.

</details>


### [78] [Enhancing Hate Speech Detection on Social Media: A Comparative Analysis of Machine Learning Models and Text Transformation Approaches](https://arxiv.org/abs/2602.20634)
*Saurabh Mishra,Shivani Thakur,Radhika Mamidi*

Main category: cs.CL

TL;DR: Compares CNN/LSTM, BERT-family, and hybrid models for hate-speech/offensive-language detection; BERT yields top accuracy overall, hybrids win in some cases; introduces text-to-neutral transformations to mitigate harmful impact; outlines strengths, limits, and future work.


<details>
  <summary>Details</summary>
Motivation: Rising hate speech on social media demands reliable automated detection and mitigation to support moderation and reduce harm.

Method: Empirical comparison of traditional neural models (CNNs, LSTMs), advanced contextual models (BERT and variants), and hybrids combining architectural features. Additionally proposes and tests text transformation techniques that rephrase negative expressions into neutral ones to reduce harm while preserving detectability.

Result: BERT-class models outperform in overall accuracy due to deep contextual encoding; hybrid architectures achieve superior performance in certain scenarios; proposed text transformations show promise in neutralizing harmful content.

Conclusion: State-of-the-art contextual models are strong but not universally dominant; hybrid designs and transformation pipelines can complement detection. Further work is needed to address limitations and build more robust, practical moderation systems.

Abstract: The proliferation of hate speech on social media platforms has necessitated the development of effective detection and moderation tools. This study evaluates the efficacy of various machine learning models in identifying hate speech and offensive language and investigates the potential of text transformation techniques to neutralize such content. We compare traditional models like CNNs and LSTMs with advanced neural network models such as BERT and its derivatives, alongside exploring hybrid models that combine different architectural features. Our results indicate that while advanced models like BERT show superior accuracy due to their deep contextual understanding, hybrid models exhibit improved capabilities in certain scenarios. Furthermore, we introduce innovative text transformation approaches that convert negative expressions into neutral ones, thereby potentially mitigating the impact of harmful content. The implications of these findings are discussed, highlighting the strengths and limitations of current technologies and proposing future directions for more robust hate speech detection systems.

</details>


### [79] [Semantic Novelty at Scale: Narrative Shape Taxonomy and Readership Prediction in 28,606 Books](https://arxiv.org/abs/2602.20647)
*W. Frederick Zimmerman*

Main category: cs.CL

TL;DR: Defines a paragraph-level semantic-novelty trajectory for books using SBERT; discovers eight narrative shape archetypes and shows variance of novelty best predicts readership, with strong genre effects, historical shifts toward predictability, and largely unique book signatures.


<details>
  <summary>Details</summary>
Motivation: Move beyond sentiment/topic curves to an information-theoretic account of how unpredictability accumulates over a narrative, measurable at corpus scale and comparable across books, to explain engagement and genre/period differences.

Method: Compute semantic novelty per paragraph as cosine distance to the running centroid of all prior paragraphs using 768-d SBERT embeddings. For 28,606 PG19 books, build novelty curves, compress each to 16-segment PAA, and cluster (Ward linkage) to find archetypes. Derive trajectory features (Volume/variance, Speed, Terminal/Initial ratio, Circuitousness). Relate features to readership with partial correlations controlling for length; test genre associations (chi-square), historical trends (T/I over time), and signature uniqueness via SAX.

Result: Eight canonical shapes from Steep Descent (rapid convergence) to Steep Ascent (rising unpredictability). Volume is the strongest length-independent predictor of readership (partial ρ=0.32), then Speed (ρ=0.19) and Terminal/Initial ratio (ρ=0.19). Circuitousness correlates with readership (ρ=0.41) but is 93% collinear with length; partial ρ=0.11 after control. Genre strongly constrains shape (χ²=2121.6, p<10^-242): fiction tends toward plateaus; nonfiction front-loads information. Historically (1840–1910) T/I declines (r=−0.74, p=0.037), implying increasing predictability. SAX signatures are 85% unique across books.

Conclusion: Information-density dynamics form a distinct, quantifiable dimension of narrative structure with measurable links to reader engagement and strong genre/period signals. Length confounds can dominate naive corpus correlations, so controls are essential. The approach scales, yields interpretable archetypes, and suggests most books follow unique novelty paths; dataset is provided for replication.

Abstract: I introduce semantic novelty--cosine distance between each paragraph's sentence embedding and the running centroid of all preceding paragraphs--as an information-theoretic measure of narrative structure at corpus scale. Applying it to 28,606 books in PG19 (pre-1920 English literature), I compute paragraph-level novelty curves using 768-dimensional SBERT embeddings, then reduce each to a 16-segment Piecewise Aggregate Approximation (PAA). Ward-linkage clustering on PAA vectors reveals eight canonical narrative shape archetypes, from Steep Descent (rapid convergence) to Steep Ascent (escalating unpredictability). Volume--variance of the novelty trajectory--is the strongest length-independent predictor of readership (partial rho = 0.32), followed by speed (rho = 0.19) and Terminal/Initial ratio (rho = 0.19). Circuitousness shows strong raw correlation (rho = 0.41) but is 93 percent correlated with length; after control, partial rho drops to 0.11--demonstrating that naive correlations in corpus studies can be dominated by length confounds. Genre strongly constrains narrative shape (chi squared = 2121.6, p < 10 to the power negative 242), with fiction maintaining plateau profiles while nonfiction front-loads information. Historical analysis shows books became progressively more predictable between 1840 and 1910 (T/I ratio trend r = negative 0.74, p = 0.037). SAX analysis reveals 85 percent signature uniqueness, suggesting each book traces a nearly unique path through semantic space. These findings demonstrate that information-density dynamics, distinct from sentiment or topic, constitute a fundamental dimension of narrative structure with measurable consequences for reader engagement. Dataset: https://huggingface.co/datasets/wfzimmerman/pg19-semantic-novelty

</details>


### [80] [CARE: An Explainable Computational Framework for Assessing Client-Perceived Therapeutic Alliance Using Large Language Models](https://arxiv.org/abs/2602.20648)
*Anqi Li,Chenxiao Wang,Yu Lu,Renjun Xu,Lizhi Ma,Zhenzhong Lan*

Main category: cs.CL

TL;DR: CARE is an LLM-based system that predicts multi-dimensional therapeutic alliance scores from counseling transcripts and generates interpretable, context-grounded rationales; fine-tuned with rationale-augmented supervision, it substantially improves correlation with client ratings and offers actionable insights, including in real-world Chinese sessions.


<details>
  <summary>Details</summary>
Motivation: Client perceptions of the therapeutic alliance are vital for counseling outcomes, but traditional questionnaires are burdensome and delayed, and current computational methods give coarse, non-interpretable outputs that ignore whole-session context. There is a need for an automated, context-aware, and interpretable approach that aligns closely with client-perceived alliance.

Method: CARE fine-tunes LLaMA-3.1-8B-Instruct on the CounselingWAI dataset augmented with 9,516 expert-curated rationales, using rationale-augmented supervision. It predicts multi-dimensional alliance scores from full-session transcripts and produces textual rationales explaining the predictions. Performance and rationale quality are evaluated against leading LLMs using automatic metrics and human assessment, and the system is applied to real-world Chinese online counseling data for external insights.

Result: CARE outperforms leading LLM baselines, achieving over 70% higher Pearson correlation with client ratings and reducing the gap between counselor and client alliance assessments. Rationale-augmented supervision further boosts predictive accuracy. The model generates high-quality, contextually grounded rationales validated by automatic and human evaluations, and its application to Chinese counseling sessions surfaces common alliance-building challenges and shows how interaction patterns shape alliance.

Conclusion: An LLM with rationale-augmented supervision can deliver accurate, interpretable, context-sensitive alliance assessments from counseling transcripts, offering practical, AI-assisted support for mental health care and generalizing to real-world settings. The approach demonstrates clinical relevance and the value of explicit rationales in improving both model performance and usability.

Abstract: Client perceptions of the therapeutic alliance are critical for counseling effectiveness. Accurately capturing these perceptions remains challenging, as traditional post-session questionnaires are burdensome and often delayed, while existing computational approaches produce coarse scores, lack interpretable rationales, and fail to model holistic session context. We present CARE, an LLM-based framework to automatically predict multi-dimensional alliance scores and generate interpretable rationales from counseling transcripts. Built on the CounselingWAI dataset and enriched with 9,516 expert-curated rationales, CARE is fine-tuned using rationale-augmented supervision with the LLaMA-3.1-8B-Instruct backbone. Experiments show that CARE outperforms leading LLMs and substantially reduces the gap between counselor evaluations and client-perceived alliance, achieving over 70% higher Pearson correlation with client ratings. Rationale-augmented supervision further improves predictive accuracy. CARE also produces high-quality, contextually grounded rationales, validated by both automatic and human evaluations. Applied to real-world Chinese online counseling sessions, CARE uncovers common alliance-building challenges, illustrates how interaction patterns shape alliance development, and provides actionable insights, demonstrating its potential as an AI-assisted tool for supporting mental health care.

</details>


### [81] [CAMEL: Confidence-Gated Reflection for Reward Modeling](https://arxiv.org/abs/2602.20670)
*Zirui Zhu,Hailun Xu,Yang Luo,Yong Liu,Kanchan Sarkar,Kun Xu,Yang You*

Main category: cs.CL

TL;DR: CAMEL is a confidence-gated reward model that uses the log-probability margin of verdict tokens to decide when to invoke a reflective judging step; trained with RL and counterfactual prefix augmentation, it achieves state-of-the-art accuracy with substantially better efficiency.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between efficient but opaque scalar reward models and interpretable but costly generative judges by leveraging a free confidence signal, improving the accuracy–efficiency trade-off in preference modeling for LLM alignment.

Method: Estimate instance difficulty via the log-probability margin between verdict tokens. Make a fast single-token preference decision; if confidence is low, selectively trigger a reflection step. Train with reinforcement learning coupled with counterfactual prefix augmentation to expose diverse initial verdicts and promote genuine self-correction.

Result: State-of-the-art on three reward-model benchmarks with 82.9% average accuracy, a +3.2% gain over the best prior model. A 14B-parameter model outperforms 70B-parameter baselines, and the approach yields a strictly better accuracy–efficiency Pareto frontier.

Conclusion: Confidence-gated selective reflection, powered by internal log-probability margins, enables more accurate and interpretable reward modeling at lower computational cost; CAMEL delivers superior accuracy using smaller models while improving the accuracy–efficiency balance.

Abstract: Reward models play a fundamental role in aligning large language models with human preferences. Existing methods predominantly follow two paradigms: scalar discriminative preference models, which are efficient but lack interpretability, and generative judging models, which offer richer reasoning at the cost of higher computational overhead. We observe that the log-probability margin between verdict tokens strongly correlates with prediction correctness, providing a reliable proxy for instance difficulty without additional inference cost. Building on this insight, we propose CAMEL, a confidence-gated reflection framework that performs a lightweight single-token preference decision first and selectively invokes reflection only for low-confidence instances. To induce effective self-correction, we train the model via reinforcement learning with counterfactual prefix augmentation, which exposes the model to diverse initial verdicts and encourages genuine revision. Empirically, CAMEL achieves state-of-the-art performance on three widely used reward-model benchmarks with 82.9% average accuracy, surpassing the best prior model by 3.2% and outperforming 70B-parameter models using only 14B parameters, while establishing a strictly better accuracy-efficiency Pareto frontier.

</details>


### [82] [ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition](https://arxiv.org/abs/2602.20727)
*Xindian Ma,Rundong Kong,Peng Zhang,Ruoxiang Huang,Yongyu Jiang*

Main category: cs.CL

TL;DR: ID-LoRA reuses clustered structure in pretrained weights to build multiple adapters that all share one trainable low‑rank matrix, cutting trainable parameters (up to ~46% vs. LoRA) while outperforming full fine‑tuning and strong PEFT baselines across diverse tasks and multi‑task setups.


<details>
  <summary>Details</summary>
Motivation: Standard (Hydra/DoRA) LoRA variants still scale trainable parameters roughly with model size; reducing rank to save parameters often harms performance, especially in complex multi‑task settings. The goal is to preserve capacity and quality while substantially shrinking the number of trainable parameters.

Method: Cluster parameter groups from the pretrained weight matrix, then form multiple low‑rank components tied to those groups. All components share a single initialized trainable low‑rank matrix that is reused across groups, leveraging weight structure to maintain expressive power while minimizing additional parameters.

Result: Across five benchmarks (Math Reasoning, Code Generation, MMLU, CommonsenseQA, Safety Alignment), ID‑LoRA beats full fine‑tuning and PEFT baselines (LoRA, DoRA, HydraLoRA) while using up to 46% fewer trainable parameters than standard LoRA. In multi‑task settings (Code, MMLU), it surpasses LoRA/DoRA/HydraLoRA with only 54% of LoRA’s trainable parameters.

Conclusion: ID‑LoRA breaks the typical efficiency–performance trade‑off in PEFT by exploiting intrinsic structure in pretrained weights and sharing one low‑rank matrix across adapters, yielding better or comparable performance with substantially fewer trainable parameters, particularly effective for multi‑task fine‑tuning.

Abstract: LoRA has become a universal Parameter-Efficient Fine-Tuning (PEFT) technique that equips Large Language Models (LLMs) to adapt quickly to new tasks. However, when these models are scaled up, even the latest LoRA variants still introduce considerable overhead in trainable parameters. Conversely, aggressively lowering the rank to curb this overhead markedly degrades performance in complex multi-task settings. We propose ID-LoRA, a novel PEFT framework that breaks the trade-off. Its core innovation lies in extracting and reusing clustered parameter groups from the pretrained weight matrix. These groups are then used to form multiple low-rank components, all of which share only a single initialized trainable low-rank matrix. This approach cuts the number of trainable parameters while keeping the model's capacity intact. We evaluate ID-LoRA on five diverse benchmarks: Mathematical Reasoning, Code Generation, MMLU, CommonsenseQA, and Safety Alignment. ID-LoRA outperforms both full fine-tuning and existing PEFT baselines (e.g., LoRA, DoRA, HydraLoRA) while using up to 46% fewer trainable parameters than the standard LoRA. In multi-task scenarios, it surpasses LoRA and its recent variants (e.g., DoRA and HydraLoRA) on both Code and MMLU tasks, yet requires only 54% of the trainable parameters demanded by the conventional LoRA.

</details>


### [83] [Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization](https://arxiv.org/abs/2602.20743)
*Gabriel Loiseau,Damien Sileo,Damien Riquet,Maxime Meyer,Marc Tommasi*

Main category: cs.CL

TL;DR: They introduce an adaptive, prompt-optimized approach to text anonymization that tailors instructions to specific privacy-utility goals, outperforming static methods across diverse domains while remaining efficient and effective on open-source LMs.


<details>
  <summary>Details</summary>
Motivation: Text anonymization needs vary by domain, privacy objectives, and downstream tasks, but current methods use fixed, hand-crafted rules that don’t generalize or flexibly balance privacy and utility.

Method: Formulate “adaptive text anonymization” and propose a task-specific prompt optimization framework that automatically generates anonymization instructions for language models to meet target privacy-utility requirements across domains and use cases; evaluate on a new benchmark of five datasets with varied constraints and objectives.

Result: Across all datasets/settings, the approach achieves a superior privacy–utility trade-off versus baselines, runs efficiently, works well with open-source LMs, and attains performance comparable to larger closed-source systems; it also uncovers novel strategies along the privacy–utility frontier.

Conclusion: Automatically optimized prompts can adapt anonymization behavior to specific requirements, improving generalization and privacy–utility balance; this is a practical, efficient path for domain-specific, goal-aware text anonymization and reveals new trade-off strategies.

Abstract: Anonymizing textual documents is a highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization, a new task formulation in which anonymization strategies are automatically adapted to specific privacy-utility requirements. We propose a framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models, enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present a benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves a better privacy-utility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models, with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacy-utility trade-off frontier.

</details>


### [84] [Explicit Grammar Semantic Feature Fusion for Robust Text Classification](https://arxiv.org/abs/2602.20749)
*Azrin Sultana,Firoz Ahmed*

Main category: cs.CL

TL;DR: Lightweight text classification by explicitly encoding sentence-level grammar into a compact vector and fusing it with frozen contextual embeddings; this unified representation outperforms baselines by ~2–15% and is suitable for edge devices.


<details>
  <summary>Details</summary>
Motivation: Transformer-based NLP models are computationally intensive and ill-suited for resource-constrained settings. The study aims to reduce compute while preserving accuracy by leveraging explicit grammatical rules as inductive bias, avoiding full fine-tuning or heavy architectures.

Method: Construct a compact grammar vector capturing syntactic composition, phrase patterns, and complexity indicators; fuse it with frozen contextual embeddings to form a single, heterogeneous feature representation; train/evaluate with various classifiers (DBN, LSTM, BiLSTM, BERT, XLNet) under different epoch counts.

Result: The unified representation captures both semantic and structural signals, outperforming baselines by 2–15% across heterogeneous domains; the approach is lightweight and shows better suitability for edge devices than syntax-aware transformers that add learnable structure modules.

Conclusion: Encoding grammar as an explicit inductive bias rather than a learnable component, and combining it with frozen embeddings, yields a compact, efficient model that improves accuracy while lowering computational cost—promising for deployment in resource-limited environments.

Abstract: Natural Language Processing enables computers to understand human language by analysing and classifying text efficiently with deep-level grammatical and semantic features. Existing models capture features by learning from large corpora with transformer models, which are computationally intensive and unsuitable for resource-constrained environments. Therefore, our proposed study incorporates comprehensive grammatical rules alongside semantic information to build a robust, lightweight classification model without resorting to full parameterised transformer models or heavy deep learning architectures. The novelty of our approach lies in its explicit encoding of sentence-level grammatical structure, including syntactic composition, phrase patterns, and complexity indicators, into a compact grammar vector, which is then fused with frozen contextual embeddings. These heterogeneous elements unified a single representation that captures both the structural and semantic characteristics of the text. Deep learning models such as Deep Belief Networks (DBNs), Long Short-Term Memory (LSTMs), BiLSTMs, and transformer-based BERT and XLNET were used to train and evaluate the model, with the number of epochs varied. Based on experimental results, the unified feature representation model captures both the semantic and structural properties of text, outperforming baseline models by 2%-15%, enabling more effective learning across heterogeneous domains. Unlike prior syntax-aware transformer models that inject grammatical structure through additional attention layers, tree encoders, or full fine-tuning, the proposed framework treats grammar as an explicit inductive bias rather than a learnable module, resulting in a very lightweight model that delivers better performance on edge devices

</details>


### [85] [SibylSense: Adaptive Rubric Learning via Memory Tuning and Adversarial Probing](https://arxiv.org/abs/2602.20751)
*Yifei Xu,Guilherme Potje,Shivam Shandilya,Tiancheng Yuan,Leonardo de Oliveira Nunes,Rakshanda Agarwal,Saeid Asgari,Adam Atkinson,Emre Kıcıman,Songwu Lu,Ranveer Chandra,Tusher Chakraborty*

Main category: cs.CL

TL;DR: SibylSense adaptively refines rubric-based rewards for open-ended generation by learning at inference-time with a memory of validated rubric items, yielding more discriminative rubrics and better RL post-training than static or non-adaptive methods.


<details>
  <summary>Details</summary>
Motivation: Reward design for open-ended generation is hard: expert-crafted rubrics are costly, prompted rubrics can be shallow or inconsistent, and fixed discriminators drift or saturate, enabling reward hacking. A scalable, robust, and adaptive rubric mechanism is needed to guide RL post-training.

Method: Use a frozen rubric generator augmented with a tunable memory bank of validated rubric items. Update memory via verifier-based item rewards computed from discriminative gaps between reference and candidate answers using a few examples. Alternate (1) memory tuning to adapt the rubric generator and (2) a rubric-adversarial policy update that produces rubric-satisfying candidates, which shrinks gaps and pushes the generator to capture new quality dimensions.

Result: On two open-ended tasks, SibylSense produces more discriminative rubrics and improves downstream RL performance compared to static and non-adaptive baselines.

Conclusion: Inference-time adaptation with a validated-item memory mitigates rubric drift and reward hacking, scales rubric supervision, and enhances alignment and robustness of rewards for open-ended RL post-training.

Abstract: Designing aligned and robust rewards for open-ended generation remains a key barrier to RL post-training. Rubrics provide structured, interpretable supervision, but scaling rubric construction is difficult: expert rubrics are costly, prompted rubrics are often superficial or inconsistent, and fixed-pool discriminative rubrics can saturate and drift, enabling reward hacking. We present SibylSense, an inference-time learning approach that adapts a frozen rubric generator through a tunable memory bank of validated rubric items. Memory is updated via verifier-based item rewards measured by reference-candidate answer discriminative gaps from a handful of examples. SibylSense alternates memory tuning with a rubric-adversarial policy update that produces rubric-satisfying candidate answers, shrinking discriminative gaps and driving the rubric generator to capture new quality dimensions. Experiments on two open-ended tasks show that SibylSense yields more discriminative rubrics and improves downstream RL performance over static and non-adaptive baselines.

</details>


### [86] [Overton Pluralistic Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2602.20759)
*Yu Fu,Seongho Son,Ilija Bogunovic*

Main category: cs.CL

TL;DR: OP-GRPO is a reinforcement-learning framework that equips a single LLM to generate multiple, diverse, and non-redundant perspectives per query by combining a task-tuned similarity estimator with a dual-reward GRPO objective; a 3B model beats much larger/modular baselines on NLI and LLM-judge evaluations.


<details>
  <summary>Details</summary>
Motivation: Conventional alignment tends to collapse to a single ‘best’ answer and fails to reflect the plurality of human values. The authors seek a scalable way for one model to surface multiple legitimate viewpoints without prompt engineering or multi-module systems.

Method: Two-stage pipeline: (1) Train a Sentence Transformer as a similarity/coverage estimator specialized for Overton Pluralism. (2) Train the base LLM with OP-GRPO, a GRPO variant using a dual reward: one term rewards broad coverage of genuine perspectives (as scored by the estimator), and another rewards intra-set uniqueness to avoid redundancy—yielding implicit pluralism from a single model.

Result: On an NLI benchmark, Qwen2.5-3B-Instruct using OP-GRPO shows a 37.4% relative accuracy gain over a 20B GPT-OSS baseline and a 19.1% relative improvement over a modular pluralism baseline. GPT-4.1 judging further supports robustness of diversity/quality claims.

Conclusion: Implicit pluralism can be achieved in a single, small LLM via RL with a specialized similarity signal, delivering strong perspective diversity and competitive or superior task accuracy versus larger or modular systems; this suggests a compute-efficient path to pluralistic alignment.

Abstract: Existing alignment paradigms remain limited in capturing the pluralistic nature of human values. Overton Pluralism addresses this gap by generating responses with diverse perspectives from a single query. This paper introduces OP-GRPO (Overton Pluralistic Group Relative Policy Optimization), a reinforcement learning framework for implicit Overton Pluralism that enables a single large language model to produce pluralistic responses without explicit prompting or modular orchestration. Our workflow consists of two main steps. First, similarity estimator training fine-tunes a Sentence Transformer for Overton Pluralism tasks to provide more accurate coverage evaluation of generated responses. Second, OP-GRPO training incorporates this similarity estimator into a dual-reward system designed to ensure both broad coverage of genuine human perspectives and the uniqueness of each perspective, thereby promoting diversity. Empirical results demonstrate a "small models, big perspective coverage" effect. The trained Qwen2.5-3B-Instruct model surpasses a 20B GPT-OSS baseline with a 37.4 percent relative accuracy gain on a Natural Language Inference benchmark, and also outperforms a modular architecture baseline with a 19.1 percent relative improvement. Additional evaluations using GPT-4.1 as a large language model judge further confirm the robustness of the approach.

</details>


### [87] [Don't Ignore the Tail: Decoupling top-K Probabilities for Efficient Language Model Distillation](https://arxiv.org/abs/2602.20816)
*Sayantan Dasgupta,Trevor Cohn,Timothy Baldwin*

Main category: cs.CL

TL;DR: They replace the standard KL loss in knowledge distillation with a tail‑aware divergence that down‑weights the teacher’s top‑K modes and up‑weights lower‑probability tokens, achieving KL‑like compute cost and competitive (often better) performance in both pretraining and supervised distillation settings, using modest compute.


<details>
  <summary>Details</summary>
Motivation: KL-based distillation is dominated by the teacher’s highest‑probability next tokens, causing the student to ignore informative low‑probability mass (the “tail”). The authors want a loss that leverages this tail signal without increasing computational cost.

Method: Introduce a decoupled (tail‑aware) divergence that partitions the teacher distribution into top‑K probabilities (modes) and the remaining tail, then reduces the contribution of modes and increases the tail’s contribution. It is a drop‑in replacement for KL with the same computational profile; applied to distillation for decoder LMs in both pretraining and supervised settings.

Result: Across multiple datasets and tasks, the tail‑aware loss delivers competitive or improved student performance versus standard KL distillation. Training remains efficient enough for large datasets using academic‑scale compute.

Conclusion: Decoupling the teacher’s modes from its tail during distillation better exploits informative low‑probability signals, offering a practical, compute‑neutral alternative to KL that can improve distilled language models and make large‑scale distillation more accessible.

Abstract: The core learning signal used in language model distillation is the standard Kullback-Leibler (KL) divergence between the student and teacher distributions. Traditional KL divergence tends to be dominated by the next tokens with the highest probabilities, i.e., the teacher's modes, thereby diminishing the influence of less probable yet potentially informative components of the output distribution. We propose a new tail-aware divergence that decouples the contribution of the teacher model's top-K predicted probabilities from that of lower-probability predictions, while maintaining the same computational profile as the KL Divergence. Our decoupled approach reduces the impact of the teacher modes and, consequently, increases the contribution of the tail of the distribution. Experimental results demonstrate that our modified distillation method yields competitive performance in both pre-training and supervised distillation of decoder models across various datasets. Furthermore, the distillation process is efficient and can be performed with a modest academic budget for large datasets, eliminating the need for industry-scale computing.

</details>


### [88] [FinAnchor: Aligned Multi-Model Representations for Financial Prediction](https://arxiv.org/abs/2602.20859)
*Zirui He,Huopu Zhang,Yanguang Liu,Sirui Wu,Mengnan Du*

Main category: cs.CL

TL;DR: FinAnchor aligns and aggregates embeddings from multiple LLMs into a single anchor space via learned linear maps, creating a unified representation that yields stronger, more robust financial predictions than single-model baselines and typical ensembles—without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Financial prediction from long documents is hard because informative signals are sparse and noisy, and the best embedding model varies by task and over time. Practitioners need a simple, robust way to exploit complementary strengths of multiple LLMs without costly fine-tuning and despite incompatible embedding spaces.

Method: Choose one model’s embedding space as the anchor. Learn lightweight linear mappings to project other models’ embeddings into this anchor space. Aggregate the aligned features to form a unified representation used by downstream predictors. No fine-tuning of the underlying LLMs; emphasis on compatibility and simple integration.

Result: Across multiple financial NLP tasks, the unified, anchored representation consistently outperforms strong single-LLM baselines and standard ensemble strategies, indicating improved robustness and predictive accuracy.

Conclusion: Anchoring heterogeneous LLM representations via simple linear alignment is an effective, lightweight strategy for financial NLP on long documents, offering robustness across tasks and time without tuning base models, and outperforming common single-model and ensemble baselines.

Abstract: Financial prediction from long documents involves significant challenges, as actionable signals are often sparse and obscured by noise, and the optimal LLM for generating embeddings varies across tasks and time periods. In this paper, we propose FinAnchor(Financial Anchored Representations), a lightweight framework that integrates embeddings from multiple LLMs without fine-tuning the underlying models. FinAnchor addresses the incompatibility of feature spaces by selecting an anchor embedding space and learning linear mappings to align representations from other models into this anchor. These aligned features are then aggregated to form a unified representation for downstream prediction. Across multiple financial NLP tasks, FinAnchor consistently outperforms strong single-model baselines and standard ensemble methods, demonstrating the effectiveness of anchoring heterogeneous representations for robust financial prediction.

</details>


### [89] [Exa-PSD: a new Persian sentiment analysis dataset on Twitter](https://arxiv.org/abs/2602.20892)
*Seyed Himan Ghaderi,Saeed Sarbazi Azad,Mohammad Mehdi Jaziriyan,Ahmad Akbari*

Main category: cs.CL

TL;DR: Introduces Exa, a 12k Persian Twitter sentiment dataset (positive/neutral/negative) annotated by 5 native speakers; ParsBERT/RoBERTa baselines achieve 79.87 macro-F1, indicating practical utility.


<details>
  <summary>Details</summary>
Motivation: Persian NLP lacks broad, social-media–style sentiment datasets; existing resources are narrow (products, food, hotels) and fail to capture Twitter-specific colloquialisms and irony. A general Twitter-based benchmark is needed.

Method: Collect Persian tweets; label them into three sentiment classes by five native annotators; report dataset characteristics/statistics; evaluate with pre-trained ParsBERT and RoBERTa models as baselines using macro F1.

Result: Baseline evaluation reaches 79.87 macro F1 on the dataset.

Conclusion: Exa provides a valuable benchmark for Persian Twitter sentiment analysis; reasonable baseline performance suggests the dataset can support future research and system development.

Abstract: Today, Social networks such as Twitter are the most widely used platforms for communication of people. Analyzing this data has useful information to recognize the opinion of people in tweets. Sentiment analysis plays a vital role in NLP, which identifies the opinion of the individuals about a specific topic. Natural language processing in Persian has many challenges despite the adventure of strong language models. The datasets available in Persian are generally in special topics such as products, foods, hotels, etc while users may use ironies, colloquial phrases in social media To overcome these challenges, there is a necessity for having a dataset of Persian sentiment analysis on Twitter. In this paper, we introduce the Exa sentiment analysis Persian dataset, which is collected from Persian tweets. This dataset contains 12,000 tweets, annotated by 5 native Persian taggers. The aforementioned data is labeled in 3 classes: positive, neutral and negative. We present the characteristics and statistics of this dataset and use the pre-trained Pars Bert and Roberta as the base model to evaluate this dataset. Our evaluation reached a 79.87 Macro F-score, which shows the model and data can be adequately valuable for a sentiment analysis system.

</details>


### [90] [The Art of Efficient Reasoning: Data, Reward, and Optimization](https://arxiv.org/abs/2602.20945)
*Taiqiang Wu,Zenan Zu,Bo Zhou,Ngai Wong*

Main category: cs.CL

TL;DR: A systematic study of “efficient CoT” for LLMs: propose finer evaluation, uncover a two-stage RL training dynamic (first shorten, then refine reasoning), and show that training on easier prompts yields denser positive rewards, avoids length collapse, and transfers a useful length bias across models and domains, validated on Qwen3 (0.6B–30B).


<details>
  <summary>Details</summary>
Motivation: CoT improves LLM accuracy but inflates compute via long chains. Existing RL-based efficiency methods lack principled evaluation and mechanistic understanding, leading to instability (e.g., length collapse) and unclear generalization.

Method: Define fine-grained metrics (e.g., response-length distributions conditioned on correctness; accuracy under tight-to-loose token budgets 2k–32k). Run a large-scale, unified RL study (~0.2M GPU hours) varying: prompt/rollout design and difficulty, reward shaping schemes, and optimization strategies. Analyze training dynamics and cross-domain/size generalization on Qwen3 (0.6B–30B).

Result: Observed a consistent two-stage training pattern: (1) length adaptation (responses shorten), followed by (2) reasoning refinement (accuracy recovers/improves). Training on relatively easier prompts increases positive-reward density, preventing length collapse. The induced length bias generalizes across tasks and model sizes. The study distills actionable guidelines improving the efficiency–accuracy trade-off across budgets.

Conclusion: Efficient reasoning in LLMs is achievable and stable when reward signals are dense—practically via easier-prompt training—together with fine-grained evaluation. The learned brevity preference transfers across domains and scales, offering robust recipes for shorter, accurate CoT reasoning.

Abstract: Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.

</details>


### [91] [Blackbird Language Matrices: A Framework to Investigate the Linguistic Competence of Language Models](https://arxiv.org/abs/2602.20966)
*Paola Merlo,Chunyang Jiang,Giuseppe Samo,Vivi Nastase*

Main category: cs.CL

TL;DR: Introduces Blackbird Language Matrices (BLM), a curated, multi-level multiple‑choice benchmark inspired by intelligence tests to probe whether LLMs detect linguistic objects and exploit systematic cross‑sentence patterns; models achieve solid performance (multi‑lingual), encode grammatical attributes, and appear to solve tasks via pattern detection, supporting explainability‑oriented evaluation.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations struggle to isolate whether models recognize linguistic structures, use systematic patterns across sentences, and how linguistic vs. reasoning errors interact. A need exists for naturalistic yet controlled datasets that enable fine‑grained, explainable probing of these abilities.

Method: Design the BLM task/datasets: multi-choice items with structure within sentences, across input sequences, and within answer candidates. Curate (partly hand-built) naturalistic items. Benchmark simple baselines and tailored models; run targeted experiments on chunking and systematicity; analyze learned representations for grammatical objects/attributes; test across more than one language.

Result: BLMs are challenging but solvable: simple baselines reach good performance; tailored models perform better. Representations learned on BLM encode grammatical objects/attributes relevant to the task. Successful solutions hinge on detecting systematic cross‑sentence patterns. Results hold across multiple languages (extent unspecified).

Conclusion: Curated, structured datasets like BLM enable multi‑faceted, explainable studies of LLM behavior. Because BLMs include explicit learning contexts and answers and are partly hand‑crafted, they are well-suited for why‑questions about LLM behavior and can advance explainability; broader validation and scope expansion remain future work.

Abstract: This article describes a novel language task, the Blackbird Language Matrices (BLM) task, inspired by intelligence tests, and illustrates the BLM datasets, their construction and benchmarking, and targeted experiments on chunking and systematicity. BLMs are multiple-choice problems, structured at multiple levels: within each sentence, across the input sequence, within each candidate answer. Because of their rich structure, these curated, but naturalistic datasets are key to answer some core questions about current large language models abilities: do LLMs detect linguistic objects and their properties? Do they detect and use systematic patterns across sentences? Are they more prone to linguistic or reasoning errors, and how do these interact?
  We show that BLMs, while challenging, can be solved at good levels of performance, in more than one language, with simple baseline models or, at better performance levels, with more tailored models. We show that their representations contain the grammatical objects and attributes relevant to solve a linguistic task. We also show that these solutions are reached by detecting systematic patterns across sentences.
  The paper supports the point of view that curated, structured datasets support multi-faceted investigations of properties of language and large language models. Because they present a curated, articulated structure, because they comprise both learning contexts and expected answers, and because they are partly built by hand, BLMs fall in the category of datasets that can support explainability investigations, and be useful to ask why large language models behave the way they do.

</details>


### [92] [Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving](https://arxiv.org/abs/2602.20973)
*Yuliang Ji,Fuchen Shen,Jian Wu,Qiujie Xie,Yue Zhang*

Main category: cs.CL

TL;DR: They introduce PC-FOL, a first-order logic dataset emphasizing case-based (branching) reasoning with human-written natural-language proofs, show leading LLMs perform much worse on case-based than linear reasoning, and offer a graphical-model explanation for the gap to guide future work on proof generation.


<details>
  <summary>Details</summary>
Motivation: Existing math-reasoning benchmarks largely emphasize linear, step-by-step chains and underrepresent crucial forms like proof by cases (and contradiction), obscuring LLM weaknesses in non-linear reasoning. A focused benchmark is needed to diagnose and improve such abilities.

Method: Construct PC-FOL: professionally annotated FOL problems centered on case-based reasoning, each with a natural-language proof. Evaluate state-of-the-art LLMs on both linear and case-based tasks. Provide a theoretical account using graphical-model reasoning to explain the performance disparity between linear (chain-like) and case-based (branching) problems.

Result: Empirically, LLMs show a substantial drop on case-based problems compared with linear ones. Theoretical analysis attributes the gap to branching structures that amplify error accumulation and search complexity relative to linear chains.

Conclusion: Case-based reasoning is a central bottleneck for current LLMs. PC-FOL offers a much-needed benchmark and a theoretical lens, motivating methods that handle branching proof structures and suggesting directions for future automated natural-language proof generation research.

Abstract: To comprehensively evaluate the mathematical reasoning capabilities of Large Language Models (LLMs), researchers have introduced abundant mathematical reasoning datasets. However, most existing datasets primarily focus on linear reasoning, neglecting other parts such as proof by contradiction and proof by cases, which are crucial for investigating LLMs' reasoning abilities. To address this limitation, we first introduce a novel first-order logic (FOL) dataset named PC-FOL, annotated by professional mathematicians, focusing on case-based reasoning problems. All instances in this dataset are equipped with a manually written natural language proof, clearly distinguishing it from conventional linear reasoning datasets. Our experimental results over leading LLMs demonstrate a substantial performance gap between linear reasoning and case-based reasoning problems. To further investigate this phenomenon, we provide a theoretical analysis grounded in graphical model, which provides an explanation for the observed disparity between the two types of reasoning problems. We hope this work can reveal the core challenges in the field of automated natural language mathematical proof generation, paving the way for future research.

</details>


### [93] [Evaluating Proactive Risk Awareness of Large Language Models](https://arxiv.org/abs/2602.20976)
*Xuan Luo,Yubin Chen,Zhiyu Hou,Linpu Yu,Geng Tu,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: They propose a proactive risk-awareness evaluation for LLMs, introduce the Butterfly dataset (1,094 ecology-related queries) to test whether models anticipate and warn about latent environmental harms, and show across five LLMs that awareness drops under short responses, is broadly similar across languages, and exhibits persistent blind spots in (multimodal) species protection—highlighting the need for proactive safeguards.


<details>
  <summary>Details</summary>
Motivation: As LLMs influence everyday decisions, current safety tuning focused on blocking explicit harm misses unintended, consequential ecological risks. The authors aim to measure whether models can preemptively foresee and warn about such harms before damage occurs.

Method: Define an evaluation framework for proactive risk awareness; build the Butterfly dataset of ordinary solution-seeking queries with potential ecological impact; assess five popular LLMs, varying response length, language, and modality, and measure if models proactively flag risks and provide warnings.

Result: Consistent, significant declines in proactive awareness when responses are length-restricted; broadly similar behavior across languages; enduring blind spots in species protection tasks, including in multimodal settings.

Conclusion: There is a gap between current safety alignment and real-world ecological responsibility. Proactive safeguards—beyond reactive refusal—are needed in LLM deployment to anticipate and warn about latent environmental harms.

Abstract: As large language models (LLMs) are increasingly embedded in everyday decision-making, their safety responsibilities extend beyond reacting to explicit harmful intent toward anticipating unintended but consequential risks. In this work, we introduce a proactive risk awareness evaluation framework that measures whether LLMs can anticipate potential harms and provide warnings before damage occurs. We construct the Butterfly dataset to instantiate this framework in the environmental and ecological domain. It contains 1,094 queries that simulate ordinary solution-seeking activities whose responses may induce latent ecological impact. Through experiments across five widely used LLMs, we analyze the effects of response length, languages, and modality. Experimental results reveal consistent, significant declines in proactive awareness under length-restricted responses, cross-lingual similarities, and persistent blind spots in (multimodal) species protection. These findings highlight a critical gap between current safety alignment and the requirements of real-world ecological responsibility, underscoring the need for proactive safeguards in LLM deployment.

</details>


### [94] [Beyond the Star Rating: A Scalable Framework for Aspect-Based Sentiment Analysis Using LLMs and Text Classification](https://arxiv.org/abs/2602.21082)
*Vishal Patil,Shree Vaishnavi Bacha,Revanth Yamani,Yidan Sun,Mayank Kejriwal*

Main category: cs.CL

TL;DR: Hybrid pipeline: use an LLM to identify review aspects, then classic ML to classify aspect sentiment at scale; applied to 4.7M restaurant reviews, the resulting aspect sentiments significantly explain variation in overall ratings.


<details>
  <summary>Details</summary>
Motivation: Analyzing millions of unstructured customer reviews is difficult; pure LLM approaches are costly and hard to scale. There is a need for a scalable, accurate method that leverages LLM strengths without incurring prohibitive compute costs.

Method: Sample reviews are analyzed with ChatGPT to elicit key dining aspects. Human-labeled data are used to train traditional ML sentiment classifiers for those aspects. The classifiers are then applied to 4.7M restaurant reviews collected over 17 years from a major platform. Regression analyses assess how machine-labeled aspect sentiments relate to overall ratings across aspects, cuisines, and regions.

Result: Machine-labeled aspect sentiments significantly explain variance in overall ratings across multiple dimensions (aspects, cuisines, geographies), supporting validity and usefulness of the approach.

Conclusion: Combining LLMs for aspect discovery with classic ML for high-throughput sentiment classification is an effective, scalable framework for aspect-based sentiment analysis in hospitality and potentially other service sectors.

Abstract: Customer-provided reviews have become an important source of information for business owners and other customers alike. However, effectively analyzing millions of unstructured reviews remains challenging. While large language models (LLMs) show promise for natural language understanding, their application to large-scale review analysis has been limited by computational costs and scalability concerns. This study proposes a hybrid approach that uses LLMs for aspect identification while employing classic machine-learning methods for sentiment classification at scale. Using ChatGPT to analyze sampled restaurant reviews, we identified key aspects of dining experiences and developed sentiment classifiers using human-labeled reviews, which we subsequently applied to 4.7 million reviews collected over 17 years from a major online platform. Regression analysis reveals that our machine-labeled aspects significantly explain variance in overall restaurant ratings across different aspects of dining experiences, cuisines, and geographical regions. Our findings demonstrate that combining LLMs with traditional machine learning approaches can effectively automate aspect-based sentiment analysis of large-scale customer feedback, suggesting a practical framework for both researchers and practitioners in the hospitality industry and potentially, other service sectors.

</details>


### [95] [Prompt-Level Distillation: A Non-Parametric Alternative to Model Fine-Tuning for Efficient Reasoning](https://arxiv.org/abs/2602.21103)
*Sanket Badhe,Deep Shah*

Main category: cs.CL

TL;DR: Prompt-Level Distillation (PLD) transfers a teacher model’s reasoning patterns into a compact student model’s system prompt, achieving frontier-level accuracy with minimal latency and preserving interpretability.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought boosts accuracy but is slow and costly at test time; fine-tuning smaller models reduces interpretability and adds operational overhead. There is a need for a fast, low-cost, and transparent alternative for advanced reasoning.

Method: Extract explicit reasoning patterns from a high-capacity Teacher model; convert them into a structured, expressive instruction list placed in the Student model’s system prompt. Evaluate a Gemma-3 4B Student on StereoSet and Contract-NLI, comparing Macro F1 and latency against baselines.

Result: Using PLD, Gemma-3 4B’s Macro F1 improved from 57% to 90.0% on StereoSet and from 67% to 83% on Contract-NLI, with negligible latency overhead, matching frontier performance.

Conclusion: PLD provides an interpretable, low-latency alternative to CoT and fine-tuning by making reasoning steps explicit in prompts. Its transparency and efficiency suit regulated domains (law, finance, content moderation) and high-volume or edge deployments.

Abstract: Advanced reasoning typically requires Chain-of-Thought prompting, which is accurate but incurs prohibitive latency and substantial test-time inference costs. The standard alternative, fine-tuning smaller models, often sacrifices interpretability while introducing significant resource and operational overhead. To address these limitations, we introduce Prompt-Level Distillation (PLD). We extract explicit reasoning patterns from a Teacher model and organize them into a structured list of expressive instructions for the Student model's System Prompt. Evaluated on the StereoSet and Contract-NLI datasets using Gemma-3 4B, PLD improved Macro F1 scores from 57\% to 90.0\% and 67\% to 83\% respectively, enabling this compact model to match frontier performance with negligible latency overhead. These expressive instructions render the decision-making process transparent, allowing for full human verification of logic, making this approach ideal for regulated industries such as law, finance, and content moderation, as well as high-volume use cases and edge devices.

</details>


### [96] [PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data](https://arxiv.org/abs/2602.21165)
*Samah Fodeh,Linhai Ma,Yan Wang,Srivani Talakokkul,Ganesh Puthiaraju,Afshan Khan,Ashley Hagaman,Sarah Lowe,Aimee Roundtree*

Main category: cs.CL

TL;DR: PVminer is a domain-adapted NLP framework that unifies patient-centered communication and SDoH coding from secure patient messages using patient-specific BERT models and topic-augmented representations, achieving strong multi-label, hierarchical performance and outperforming biomedical/clinical baselines.


<details>
  <summary>Details</summary>
Motivation: Manual qualitative coding of patient-generated text is labor intensive and does not scale; existing ML/NLP approaches either separate PCC and SDoH or use models poorly matched to patient-facing language. The authors aim to scalably structure the patient voice across health systems in a unified way.

Method: Formulate patient voice detection as a multi-label, multi-class hierarchical task (Code, Subcode, Combo). Build patient-adapted encoders (PV-BERT-base/large). Add unsupervised topic modeling to create PV-Topic-BERT and inject topic representations during fine-tuning and inference. Train fine-tuned classifiers at each hierarchy level; include author identity and topic features. Conduct ablations to measure contributions.

Result: Outperforms biomedical/clinical pretrained baselines with F1: 82.25% (Code), 80.14% (Subcode), up to 77.87% (Combo). Ablations show meaningful gains from both author identity and topic augmentation.

Conclusion: Integrating patient-adapted language models with topic-based augmentation effectively structures patient voice in secure messages, supporting scalable PCC and SDoH analysis. Public release of models and code is planned; annotated datasets available on request.

Abstract: Patient-generated text such as secure messages, surveys, and interviews contains rich expressions of the patient voice (PV), reflecting communicative behaviors and social determinants of health (SDoH). Traditional qualitative coding frameworks are labor intensive and do not scale to large volumes of patient-authored messages across health systems. Existing machine learning (ML) and natural language processing (NLP) approaches provide partial solutions but often treat patient-centered communication (PCC) and SDoH as separate tasks or rely on models not well suited to patient-facing language. We introduce PVminer, a domain-adapted NLP framework for structuring patient voice in secure patient-provider communication. PVminer formulates PV detection as a multi-label, multi-class prediction task integrating patient-specific BERT encoders (PV-BERT-base and PV-BERT-large), unsupervised topic modeling for thematic augmentation (PV-Topic-BERT), and fine-tuned classifiers for Code, Subcode, and Combo-level labels. Topic representations are incorporated during fine-tuning and inference to enrich semantic inputs. PVminer achieves strong performance across hierarchical tasks and outperforms biomedical and clinical pre-trained baselines, achieving F1 scores of 82.25% (Code), 80.14% (Subcode), and up to 77.87% (Combo). An ablation study further shows that author identity and topic-based augmentation each contribute meaningful gains. Pre-trained models, source code, and documentation will be publicly released, with annotated datasets available upon request for research use.

</details>


### [97] [On Data Engineering for Scaling LLM Terminal Capabilities](https://arxiv.org/abs/2602.21193)
*Renjie Pi,Grace Lam,Mohammad Shoeybi,Pooya Jannaty,Bryan Catanzaro,Wei Ping*

Main category: cs.CL

TL;DR: Introduces a synthetic task-generation pipeline (Terminal-Task-Gen) and a large terminal-task dataset (Terminal-Corpus), then trains Qwen3-initialized models (Nemotron-Terminal 8B/14B/32B) that substantially boost Terminal-Bench 2.0 scores, reportedly rivaling much larger models; releases models and most data.


<details>
  <summary>Details</summary>
Motivation: Despite strong recent progress in terminal-capable LLMs, the underlying data engineering and training strategies are opaque. The work aims to make data practices explicit and to provide open resources to advance terminal agent research.

Method: Build a lightweight synthetic task generator supporting seed-based and skill-based construction; create a large-scale Terminal-Corpus; systematically study training/data strategies (filtering, curriculum learning, long-context training, scaling); fine-tune Qwen3 (8B/14B/32B) to produce Nemotron-Terminal models.

Result: On Terminal-Bench 2.0, Nemotron-Terminal achieves sizable absolute gains: 8B from 2.5%→13.0%, 14B from 4.0%→20.2%, 32B from 3.4%→27.4%, approaching or matching larger models. Models and most synthetic datasets are open-sourced on Hugging Face.

Conclusion: Carefully engineered synthetic data and targeted training strategies can markedly improve terminal agent performance even at moderate scales; open releases aim to catalyze further research and benchmarking.

Abstract: Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.

</details>


### [98] [Multimodal Multi-Agent Empowered Legal Judgment Prediction](https://arxiv.org/abs/2601.12815)
*Zhaolu Kang,Junhao Gong,Qingxi Chen,Hao Zhang,Jiaxin Liu,Rong Fu,Zhiyuan Feng,Yuan Wang,Simon Fong,Kaiyue Zhou*

Main category: cs.CL

TL;DR: JurisMMA is a staged, process-standardized framework for legal judgment prediction, paired with JurisMM—a 100k+ multimodal (text and video-text) Chinese judicial dataset—and shows strong, generalizable performance on JurisMM and LawBench.


<details>
  <summary>Details</summary>
Motivation: Traditional LJP approaches (statistical analyses, role-based simulations) struggle with cases involving multiple allegations, heterogeneous/multimodal evidence, and lack adaptable, standardized reasoning workflows.

Method: Propose JurisMMA, which decomposes trials into standardized, sequential stages to structure LJP; construct JurisMM, a large recent Chinese judicial dataset with text and video-text; evaluate on JurisMM and the LawBench benchmark.

Result: Experiments validate the framework’s effectiveness on both JurisMM and LawBench, indicating robustness to complex allegations and multimodal inputs and improved performance over prior approaches (no exact metrics given).

Conclusion: Standardized, staged decomposition plus a large multimodal dataset advances LJP performance and generalizes to broader legal AI tasks, providing a blueprint and resources for future legal methods and datasets.

Abstract: Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases based on factual descriptions, serving as a fundamental task to advance the development of legal systems. Traditional methods often rely on statistical analyses or role-based simulations but face challenges with multiple allegations, diverse evidence, and lack adaptability. In this paper, we introduce JurisMMA, a novel framework for LJP that effectively decomposes trial tasks, standardizes processes, and organizes them into distinct stages. Furthermore, we build JurisMM, a large dataset with over 100,000 recent Chinese judicial records, including both text and multimodal video-text data, enabling comprehensive evaluation. Experiments on JurisMM and the benchmark LawBench validate our framework's effectiveness. These results indicate that our framework is effective not only for LJP but also for a broader range of legal applications, offering new perspectives for the development of future legal methods and datasets.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [99] [Multilevel Determinants of Overweight and Obesity Among U.S. Children Aged 10-17: Comparative Evaluation of Statistical and Machine Learning Approaches Using the 2021 National Survey of Children's Health](https://arxiv.org/abs/2602.20303)
*Joyanta Jyoti Mondal*

Main category: cs.AI

TL;DR: Using a large, recent U.S. survey, the study compares statistical, machine-learning, and deep-learning models to predict adolescent overweight/obesity. Complex models offer only small gains over logistic regression, and performance disparities by race and poverty persist, pointing to data and equity gaps rather than algorithmic fixes.


<details>
  <summary>Details</summary>
Motivation: Child and adolescent obesity is a persistent U.S. public health problem shaped by multilevel factors. The authors seek to clarify which predictors matter at scale and whether advanced algorithms outperform simpler approaches while remaining well-calibrated and equitable across subgroups.

Method: Cross-sectional analysis of 18,792 adolescents (ages 10–17) from the 2021 National Survey of Children’s Health. Outcome: BMI-based overweight/obesity. Predictors: behaviors (diet, activity, sleep), household stress and SES, adverse experiences, neighborhood characteristics. Models: logistic regression, random forest, gradient boosting (incl. XGBoost, LightGBM), multilayer perceptron, TabNet. Evaluation: discrimination (AUC), accuracy, precision, recall, F1, calibration (Brier), and subgroup performance by race and poverty.

Result: AUCs ranged 0.66–0.79. Logistic regression, gradient boosting, and MLP provided the best balance of discrimination and calibration. Boosting and deep learning modestly improved recall and F1, but no approach was uniformly superior. Performance gaps across racial and poverty subgroups remained across algorithms.

Conclusion: Incremental benefits from model complexity are limited; multilevel predictors consistently contribute to risk. Persistent subgroup disparities suggest priorities should be better data quality and equity-focused surveillance rather than more complex algorithms.

Abstract: Background: Childhood and adolescent overweight and obesity remain major public health concerns in the United States and are shaped by behavioral, household, and community factors. Their joint predictive structure at the population level remains incompletely characterized. Objectives: The study aims to identify multilevel predictors of overweight and obesity among U.S. adolescents and compare the predictive performance, calibration, and subgroup equity of statistical, machine-learning, and deep-learning models. Data and Methods: We analyze 18,792 children aged 10-17 years from the 2021 National Survey of Children's Health. Overweight/obesity is defined using BMI categories. Predictors included diet, physical activity, sleep, parental stress, socioeconomic conditions, adverse experiences, and neighborhood characteristics. Models include logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Performance is evaluated using AUC, accuracy, precision, recall, F1 score, and Brier score. Results: Discrimination range from 0.66 to 0.79. Logistic regression, gradient boosting, and MLP showed the most stable balance of discrimination and calibration. Boosting and deep learning modestly improve recall and F1 score. No model was uniformly superior. Performance disparities across race and poverty groups persist across algorithms. Conclusion: Increased model complexity yields limited gains over logistic regression. Predictors consistently span behavioral, household, and neighborhood domains. Persistent subgroup disparities indicate the need for improved data quality and equity-focused surveillance rather than greater algorithmic complexity.

</details>


### [100] [An artificial intelligence framework for end-to-end rare disease phenotyping from clinical notes using large language models](https://arxiv.org/abs/2602.20324)
*Cathy Shyr,Yan Hu,Rory J. Tinker,Thomas A. Cassini,Kevin W. Byram,Rizwan Hamid,Daniel V. Fabbri,Adam Wright,Josh F. Peterson,Lisa Bastarache,Hua Xu*

Main category: cs.AI

TL;DR: RARE-PHENIX is an end-to-end AI system that extracts phenotypes from clinical notes, maps them to HPO, and ranks the most diagnostically informative terms, outperforming a strong baseline and aligning better with clinician curation.


<details>
  <summary>Details</summary>
Motivation: Manual extraction and standardization of phenotypes for rare disease diagnosis is slow and hard to scale. Existing AI tools target sub-steps (e.g., extraction only) and miss the full clinical workflow needed for actionable, ranked HPO phenotypes.

Method: An integrated pipeline: (1) large-language-model–based phenotype extraction from notes; (2) ontology-grounded standardization to HPO; (3) supervised learning to prioritize diagnostically informative HPO terms. Trained on 2,671 UDN patients; externally validated on 16,357 VUMC notes. Evaluated via ontology-based similarity and precision/recall/F1 vs clinician-curated HPO gold standards. Ablations assessed module contributions.

Result: Consistently outperforms PhenoBERT in end-to-end evaluation (e.g., ontology-based similarity 0.70 vs 0.58) and improves with each added module (extraction, standardization, prioritization), demonstrating benefits of modeling the complete workflow.

Conclusion: End-to-end, workflow-aligned phenotyping yields structured, ranked HPO terms that better match clinician curation and can enable human-in-the-loop rare disease diagnosis in real-world settings.

Abstract: Phenotyping is fundamental to rare disease diagnosis, but manual curation of structured phenotypes from clinical notes is labor-intensive and difficult to scale. Existing artificial intelligence approaches typically optimize individual components of phenotyping but do not operationalize the full clinical workflow of extracting features from clinical text, standardizing them to Human Phenotype Ontology (HPO) terms, and prioritizing diagnostically informative HPO terms. We developed RARE-PHENIX, an end-to-end AI framework for rare disease phenotyping that integrates large language model-based phenotype extraction, ontology-grounded standardization to HPO terms, and supervised ranking of diagnostically informative phenotypes. We trained RARE-PHENIX using data from 2,671 patients across 11 Undiagnosed Diseases Network clinical sites, and externally validated it on 16,357 real-world clinical notes from Vanderbilt University Medical Center. Using clinician-curated HPO terms as the gold standard, RARE-PHENIX consistently outperformed a state-of-the-art deep learning baseline (PhenoBERT) across ontology-based similarity and precision-recall-F1 metrics in end-to-end evaluation (i.e., ontology-based similarity of 0.70 vs. 0.58). Ablation analyses demonstrated performance improvements with the addition of each module in RARE-PHENIX (extraction, standardization, and prioritization), supporting the value of modeling the full clinical phenotyping workflow. By modeling phenotyping as a clinically aligned workflow rather than a single extraction task, RARE-PHENIX provides structured, ranked phenotypes that are more concordant with clinician curation and has the potential to support human-in-the-loop rare disease diagnosis in real-world settings.

</details>


### [101] [DMCD: Semantic-Statistical Framework for Causal Discovery](https://arxiv.org/abs/2602.20333)
*Samarth KaPatel,Sofia Nikiforova,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.AI

TL;DR: DMCD is a two-phase causal discovery method that drafts a sparse DAG using an LLM over variable metadata, then audits and refines it with conditional independence tests on data, yielding higher recall and F1 than baselines on three real-world, metadata-rich datasets.


<details>
  <summary>Details</summary>
Motivation: Conventional causal discovery from observational data often underperforms when signals are weak or the search space is large. Meanwhile, variable metadata (names, descriptions, units) encodes domain semantics that are rarely exploited. The authors aim to leverage these semantic cues as informative priors while retaining statistical rigor to improve structure learning.

Method: Phase I: use a large language model to read variable metadata and propose a sparse draft DAG, effectively a semantic prior over structures. Phase II: perform conditional independence testing to audit the draft; discrepancies trigger targeted edge additions, removals, or orientations to reconcile with the data. Ablations/probing assess whether gains stem from semantic reasoning rather than memorization.

Result: Across three real-world benchmarks (industrial engineering, environmental monitoring, IT systems), DMCD is competitive or superior to diverse baselines, with notably higher recall and F1. Probing and ablation studies suggest the improvements are due to leveraging metadata semantics rather than memorizing benchmark graphs.

Conclusion: Combining LLM-derived semantic priors with principled statistical verification is an effective and practical approach to causal structure learning, boosting coverage (recall) and overall F1 while grounding results in observational evidence.

Abstract: We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed prior over the space of possible causal structures. In Phase II, this draft is audited and refined via conditional independence testing, with detected discrepancies guiding targeted edge revisions.
  We evaluate our approach on three metadata-rich real-world benchmarks spanning industrial engineering, environmental monitoring, and IT systems analysis. Across these datasets, DMCD achieves competitive or leading performance against diverse causal discovery baselines, with particularly large gains in recall and F1 score. Probing and ablation experiments suggest that these improvements arise from semantic reasoning over metadata rather than memorization of benchmark graphs. Overall, our results demonstrate that combining semantic priors with principled statistical verification yields a high-performing and practically effective approach to causal structure learning.

</details>


### [102] [Diffusion Modulation via Environment Mechanism Modeling for Planning](https://arxiv.org/abs/2602.20422)
*Hanping Zhang,Yuhong Guo*

Main category: cs.AI

TL;DR: DMEMM augments diffusion-based trajectory planning in offline RL by explicitly modeling and injecting environment mechanisms—transition dynamics and rewards—into diffusion training, producing more coherent, executable trajectories and state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Conventional diffusion planners generate trajectories without enforcing Markov-consistent transitions, leading to incoherent rollouts that mismatch real environment dynamics and rewards. The work aims to ensure generated trajectories are realizable and aligned with the environment’s mechanisms.

Method: Diffusion Modulation via Environment Mechanism Modeling (DMEMM): during diffusion training, incorporate models of transition dynamics and reward functions to guide or constrain the denoising process so successive states/actions are dynamically consistent and reward-aligned. Practically, this likely involves conditioning or guidance terms derived from a learned (or given) dynamics/reward model added to the training objective or sampling process.

Result: Across offline RL benchmarks, DMEMM outperforms prior diffusion-based planners and other baselines, yielding state-of-the-art planning performance; qualitative and quantitative evidence suggests improved trajectory coherence and alignment with environment rewards/dynamics.

Conclusion: Embedding environment mechanisms into diffusion training mitigates the trajectory inconsistency of prior methods, enabling more realistic, high-performing offline RL planning; the approach provides a general template for mechanism-aware generative planning.

Abstract: Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.

</details>


### [103] [Implicit Intelligence -- Evaluating Agents on What Users Don't Say](https://arxiv.org/abs/2602.20424)
*Ved Sirdeshmukh,Marc Wetter*

Main category: cs.AI

TL;DR: Introduces Implicit Intelligence, a benchmark and simulation harness (Agent-as-a-World) to test whether AI agents handle unstated, contextual constraints—not just explicit instructions. Scenarios are simple on the surface but require discovering hidden requirements through exploration. Across 205 scenarios and 16 models, the best achieves only 48.3% pass rate, exposing a large gap to human-like implicit reasoning.


<details>
  <summary>Details</summary>
Motivation: Real-world user requests are often underspecified and depend on shared context, norms, and constraints (e.g., accessibility, privacy, safety). Existing benchmarks largely reward literal instruction-following and overlook agents’ ability to infer and respect implicit requirements critical for safe and effective deployment.

Method: Proposes an evaluation framework, Implicit Intelligence, and a simulation harness, Agent-as-a-World (AaW), where interactive environments are authored in human-readable YAML and simulated by language models. Scenarios embed hidden constraints discoverable via environmental exploration. Performance is measured via scenario pass rates reflecting correct handling of implicit requirements.

Result: Evaluated 16 frontier and open-weight models on 205 scenarios. Even the top-performing model reached only a 48.3% pass rate, indicating broad difficulties with implicit constraint discovery and adherence.

Conclusion: Current agents remain far from robust goal fulfillment under implicit, contextual constraints. The framework exposes critical gaps between literal instruction-following and human-like reasoning, motivating research on agents that can proactively infer, verify, and satisfy hidden requirements across safety, privacy, and accessibility contexts.

Abstract: Real-world requests to AI agents are fundamentally underspecified. Natural human communication relies on shared context and unstated constraints that speakers expect listeners to infer. Current agentic benchmarks test explicit instruction-following but fail to evaluate whether agents can reason about implicit requirements spanning accessibility needs, privacy boundaries, catastrophic risks, and contextual constraints. We present Implicit Intelligence, an evaluation framework testing whether AI agents can move beyond prompt-following to become genuine goal-fulfillers, paired with Agent-as-a-World (AaW), a harness where interactive worlds are defined in human-readable YAML files and simulated by language models. Our scenarios feature apparent simplicity in user requests, hidden complexity in correct solutions, and discoverability of constraints through environmental exploration. Evaluating 16 frontier and open-weight models across 205 scenarios, we find that even the best-performing model achieves only 48.3% scenario pass rate, revealing substantial room for improvement in bridging the gap between literal instruction-following and human-like contextual reasoning.

</details>


### [104] [Learning to Rewrite Tool Descriptions for Reliable LLM-Agent Tool Use](https://arxiv.org/abs/2602.20426)
*Ruocheng Guo,Kaiwen Dong,Xiang Gao,Kamalika Das*

Main category: cs.AI

TL;DR: Trace-Free+ improves LLM agent performance by optimizing tool interfaces (descriptions and schemas) via a curriculum that transfers from trace-rich training to trace-free deployment, yielding strong generalization to unseen tools and scalability to large tool sets.


<details>
  <summary>Details</summary>
Motivation: LLM agents often underperform because tool interfaces are written for humans, not machines. Prior optimization methods depend on execution traces (unavailable in cold-start/privacy settings) and treat tools independently, limiting scalability and generalization when agents must choose among many tools.

Method: Introduce Trace-Free+, a curriculum learning framework that starts with supervision from trace-rich settings and gradually removes reliance on traces, pushing the model to learn reusable interface-usage patterns and predict tool usage outcomes. Build a large-scale dataset of high-quality tool interfaces via a structured curation workflow across diverse tools.

Result: On StableToolBench and RestBench, Trace-Free+ achieves consistent improvements on unseen tools, strong cross-domain generalization, and robustness when the candidate tool set scales beyond 100 tools.

Conclusion: Optimizing tool interfaces—independent of agent fine-tuning—is a practical, deployable path to better LLM-agent performance, transferring effectively to new domains and large tool inventories without requiring execution traces at deployment.

Abstract: The performance of LLM-based agents depends not only on the agent itself but also on the quality of the tool interfaces it consumes. While prior work has focused heavily on agent fine-tuning, tool interfaces-including natural language descriptions and parameter schemas-remain largely human-oriented and often become a bottleneck, especially when agents must select from large candidate tool sets. Existing approaches to improving tool interfaces rely on execution traces, which are frequently unavailable in cold-start or privacy-constrained settings, and typically optimize each tool independently, limiting scalability and generalization to unseen tools. We propose Trace-Free+, a curriculum learning framework that progressively transfers supervision from trace-rich settings to trace-free deployment, encouraging the model to abstract reusable interface-usage patterns and tool usage outcomes. To support this approach, we construct a large-scale dataset of high-quality tool interfaces using a structured workflow over a diverse collection of tools. Experiments on StableToolBench and RestBench show consistent gains on unseen tools, strong cross-domain generalization, and robustness as the number of candidate tools scales to over 100, demonstrating that tool interface optimization is a practical and deployable complement to agent fine-tuning.

</details>


### [105] [PreScience: A Benchmark for Forecasting Scientific Contributions](https://arxiv.org/abs/2602.20459)
*Anirudh Ajith,Amanpreet Singh,Jay DeYoung,Nadav Kunievsky,Austin C. Kozlowski,Oyvind Tafjord,James Evans,Daniel S. Weld,Tom Hope,Doug Downey*

Main category: cs.AI

TL;DR: PreScience introduces a benchmark, dataset, and evaluation suite to test whether AI can forecast near‑future scientific developments; current systems show only moderate success and generate less diverse, less novel research than humans.


<details>
  <summary>Details</summary>
Motivation: Enable proactive discovery: help researchers anticipate emerging problems/methods, pick impactful directions, and identify likely collaborators by forecasting near‑term scientific progress from a fixed historical snapshot.

Method: Curate a temporally aligned dataset of 98K recent AI papers with disambiguated authors and linked metadata, embedded in a 502K‑paper author–citation graph. Decompose forecasting into four generative tasks (collaborator prediction, prior work selection, contribution generation, impact prediction). Provide baselines and a new LLM-based similarity metric, LACERScore, and run a 12‑month end‑to‑end simulation of scientific production.

Result: LACERScore better matches human judgments than prior similarity metrics. Frontier LLMs achieve only moderate contribution similarity (e.g., ~5.6/10). In an end‑to‑end 12‑month simulation, the synthetic research corpus is systematically less diverse and less novel than contemporaneous human work.

Conclusion: There is substantial headroom for AI scientific forecasting across all tasks. PreScience supplies a structured benchmark and metric to drive progress, while current LLMs fall short in generating diverse, novel, and accurate future contributions.

Abstract: Can AI systems trained on the scientific record up to a fixed point in time forecast the scientific advances that follow? Such a capability could help researchers identify collaborators and impactful research directions, and anticipate which problems and methods will become central next. We introduce PreScience -- a scientific forecasting benchmark that decomposes the research process into four interdependent generative tasks: collaborator prediction, prior work selection, contribution generation, and impact prediction. PreScience is a carefully curated dataset of 98K recent AI-related research papers, featuring disambiguated author identities, temporally aligned scholarly metadata, and a structured graph of companion author publication histories and citations spanning 502K total papers. We develop baselines and evaluations for each task, including LACERScore, a novel LLM-based measure of contribution similarity that outperforms previous metrics and approximates inter-annotator agreement. We find substantial headroom remains in each task -- e.g. in contribution generation, frontier LLMs achieve only moderate similarity to the ground-truth (GPT-5, averages 5.6 on a 1-10 scale). When composed into a 12-month end-to-end simulation of scientific production, the resulting synthetic corpus is systematically less diverse and less novel than human-authored research from the same period.

</details>


### [106] [KairosVL: Orchestrating Time Series and Semantics for Unified Reasoning](https://arxiv.org/abs/2602.20494)
*Haotian Si,Changhua Pei,Xiao He,Zeyan Li,Zhe Xie,Zexin Wang,Jiyao Hu,Zhaoyang Yu,Tieying Zhang,Dan Pei,Jianhui Li,Gaogang Xie*

Main category: cs.AI

TL;DR: They define a new task—Semantic-Conditional Time Series Reasoning—and train a model (KairosVL) with a two-round RL scheme: first to master temporal primitives, then to perform semantics-conditioned reasoning. It yields competitive results on synthetic and real data, preserves reasoning ability, and generalizes better to unseen cases.


<details>
  <summary>Details</summary>
Motivation: Conventional time series methods focus on numeric patterns and struggle with decision-oriented problems that require contextual/semantic understanding. There is a need for models that can reason over time while using semantic cues and that can generalize to novel scenarios.

Method: A two-stage reinforcement learning framework. Round 1 reinforces recognition/use of fundamental temporal primitives; Round 2 fine-tunes for semantic-conditioned reasoning. The trained model, KairosVL, is evaluated on synthetic and real-world tasks with ablations to probe reasoning preservation and generalization.

Result: KairosVL achieves competitive performance across datasets. Ablation studies indicate the two-round RL improves performance, maintains intrinsic reasoning capability, and enhances generalization to unseen scenarios.

Conclusion: Coupling semantic reasoning with temporal modeling is promising for real-world, decision-oriented time series intelligence. The proposed framework is presented as a practical path to such systems, showing improved performance and generalization while retaining reasoning skills.

Abstract: Driven by the increasingly complex and decision-oriented demands of time series analysis, we introduce the Semantic-Conditional Time Series Reasoning task, which extends conventional time series analysis beyond purely numerical modeling to incorporate contextual and semantic understanding. To further enhance the mode's reasoning capabilities on complex time series problems, we propose a two-round reinforcement learning framework: the first round strengthens the mode's perception of fundamental temporal primitives, while the second focuses on semantic-conditioned reasoning. The resulting model, KairosVL, achieves competitive performance across both synthetic and real-world tasks. Extensive experiments and ablation studies demonstrate that our framework not only boosts performance but also preserves intrinsic reasoning ability and significantly improves generalization to unseen scenarios. To summarize, our work highlights the potential of combining semantic reasoning with temporal modeling and provides a practical framework for real-world time series intelligence, which is in urgent demand.

</details>


### [107] [ActionEngine: From Reactive to Programmatic GUI Agents via State Machine Memory](https://arxiv.org/abs/2602.20502)
*Hongbin Zhong,Fazle Faisal,Luis França,Tanakorn Leesatapornwongsa,Adriana Szekeres,Kexin Rong,Suman Nath*

Main category: cs.AI

TL;DR: ActionEngine is a training‑free, two‑agent GUI agent that replaces step‑by‑step VLM actions with programmatic planning backed by a learned state‑machine of the interface, achieving much higher success at far lower cost/latency.


<details>
  <summary>Details</summary>
Motivation: Stepwise VLM-driven GUI agents incur high cost and latency proportional to the number of reasoning steps and lack persistent memory of prior pages, limiting accuracy. A more scalable, reliable approach needs global planning and durable interface knowledge.

Method: Two-agent architecture: (1) a Crawling Agent performs offline exploration to build and update a state‑machine memory of GUI pages and actions; (2) an Execution Agent compiles this memory into full, executable Python programs for online tasks. On failures, a vision-based re‑grounding fallback repairs the action and updates memory. The system uses crawler‑validated action templates, global planning, and node‑level execution with local validation/repair. No training required.

Result: On Reddit tasks in WebArena, ActionEngine reaches 95% task success with about one LLM call per task, outperforming the strongest vision‑only baseline at 66% while cutting cost by 11.8× and end‑to‑end latency by 2×.

Conclusion: Programmatic planning over a persistent GUI state‑machine, coupled with vision-based fallback repair, yields scalable and reliable GUI interaction that is both more accurate and far more efficient than reactive, stepwise VLM agents.

Abstract: Existing Graphical User Interface (GUI) agents operate through step-by-step calls to vision language models--taking a screenshot, reasoning about the next action, executing it, then repeating on the new page--resulting in high costs and latency that scale with the number of reasoning steps, and limited accuracy due to no persistent memory of previously visited pages.
  We propose ActionEngine, a training-free framework that transitions from reactive execution to programmatic planning through a novel two-agent architecture: a Crawling Agent that constructs an updatable state-machine memory of the GUIs through offline exploration, and an Execution Agent that leverages this memory to synthesize complete, executable Python programs for online task execution.
  To ensure robustness against evolving interfaces, execution failures trigger a vision-based re-grounding fallback that repairs the failed action and updates the memory.
  This design drastically improves both efficiency and accuracy: on Reddit tasks from the WebArena benchmark, our agent achieves 95% task success with on average a single LLM call, compared to 66% for the strongest vision-only baseline, while reducing cost by 11.8x and end-to-end latency by 2x.
  Together, these components yield scalable and reliable GUI interaction by combining global programmatic planning, crawler-validated action templates, and node-level execution with localized validation and repair.

</details>


### [108] [Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination](https://arxiv.org/abs/2602.20517)
*Rakshit Trivedi,Kartik Sharma,David C Parkes*

Main category: cs.AI

TL;DR: MIMIC introduces language-based “inner speech” as a controllable latent that guides imitation policies, using a VLM-guided CVAE to generate intents and a diffusion behavior-cloning policy conditioned on those intents, yielding more diverse, faithful, and steerable behaviors across manipulation and collaboration tasks.


<details>
  <summary>Details</summary>
Motivation: Human-AI coordination needs agents that can express human-like, context-dependent variability and be steerable at inference. Standard imitation learning often collapses to average behaviors, assumes Markovian dynamics, and offers little control once trained.

Method: Use vision-language models as linguistic scaffolding to supervise a conditional variational autoencoder that predicts inner speech (textual intent) from observations. Feed this generated inner speech, together with current observations, into a diffusion-based behavior cloning policy to select actions. At inference, behavior can be steered by supplying behavior-specific speech prompts without retraining.

Result: Across robotic manipulation and human-AI collaboration games, MIMIC increases behavior diversity and fidelity to demonstrations and enables fine-grained, inference-time behavioral steering without additional demonstrations. Code and pretrained agents are released.

Conclusion: Treating language as an internal representation of intent provides a practical mechanism for controllable, non-Markovian imitation. MIMIC improves diversity and faithfulness while enabling user-steerable behaviors, suggesting a promising path for flexible human-AI coordination.

Abstract: Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.

</details>


### [109] [From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production](https://arxiv.org/abs/2602.20558)
*Yucheng Shi,Ying Li,Yu Wang,Yesu Feng,Arjun Rao,Rein Houthooft,Shradha Sehgal,Jin Wang,Hao Zhen,Ninghao Liu,Linas Baltrunas*

Main category: cs.AI

TL;DR: They train a reinforcement-learning verbalization agent to convert user interaction logs into optimized text prompts for LLM-based recommenders, yielding large accuracy gains (up to 93% relative) over template concatenations on an industrial dataset.


<details>
  <summary>Details</summary>
Motivation: Template-based concatenation poorly captures user intent and LLMs are highly sensitive to input phrasing; improving how structured logs are turned into text could unlock major accuracy gains without changing the LLM.

Method: A data-centric framework uses reinforcement learning to learn a verbalization policy that filters noise, selects relevant metadata, and reorganizes histories into natural-language contexts. The reward is downstream recommendation accuracy; the agent discovers strategies like summarization and syntax normalization.

Result: On a large-scale industrial streaming dataset, learned verbalization improves discovery-item recommendation accuracy by up to 93% relative to template baselines and shows emergent behaviors (interest summarization, noise removal).

Conclusion: Learned verbalization is a powerful lever for LLM-based recommenders. Optimizing the input context itself can substantially boost performance and offers insights for constructing effective prompts beyond rigid templates.

Abstract: Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate fields, yielding suboptimal representations for recommendation. We propose a data-centric framework that learns verbalization for LLM-based recommendation. Using reinforcement learning, a verbalization agent transforms raw interaction histories into optimized textual contexts, with recommendation accuracy as the training signal. This agent learns to filter noise, incorporate relevant metadata, and reorganize information to improve downstream predictions. Experiments on a large-scale industrial streaming dataset show that learned verbalization delivers up to 93% relative improvement in discovery item recommendation accuracy over template-based baselines. Further analysis reveals emergent strategies such as user interest summarization, noise removal, and syntax normalization, offering insights into effective context construction for LLM-based recommender systems.

</details>


### [110] [CausalReasoningBenchmark: A Real-World Benchmark for Disentangled Evaluation of Causal Identification and Estimation](https://arxiv.org/abs/2602.20571)
*Ayush Sawarni,Jiyuan Tan,Vasilis Syrgkanis*

Main category: cs.AI

TL;DR: They introduce CausalReasoningBenchmark, which cleanly separates identification (research design) from estimation (numerical implementation) in causal inference benchmarks. It contains 173 queries over 138 real datasets and scores a structured identification specification and a numerical estimate separately; baseline LLM results show strong high-level strategy recognition but weak full design specification.


<details>
  <summary>Details</summary>
Motivation: Existing causal-inference benchmarks typically score only a single numeric output (e.g., ATE), conflating identification with estimation and obscuring where systems fail. The authors want granular diagnosis to advance automated causal reasoning, not just computation.

Method: Curate 173 causal queries from 85 peer-reviewed papers and 4 textbooks across 138 real-world datasets. For each query, require two outputs: (i) a structured identification specification (strategy, treatment, outcome, controls, and design-specific elements) and (ii) a point estimate with standard error. Score these components separately and evaluate a state-of-the-art LLM as a baseline.

Result: Baseline LLM identifies the high-level causal strategy correctly in 84% of cases but achieves only 30% correctness on the full identification specification, indicating that most errors stem from missing or incorrect design details rather than numerical estimation. The benchmark is publicly released on Hugging Face.

Conclusion: Separating identification from estimation exposes the main bottleneck—nuanced research design—and enables targeted progress in automated causal inference. CausalReasoningBenchmark provides a community resource to build and assess systems with stronger causal reasoning, beyond mere computation.

Abstract: Many benchmarks for automated causal inference evaluate a system's performance based on a single numerical output, such as an Average Treatment Effect (ATE). This approach conflates two distinct steps in causal analysis: identification-formulating a valid research design under stated assumptions-and estimation-implementing that design numerically on finite data. We introduce CausalReasoningBenchmark, a benchmark of 173 queries across 138 real-world datasets, curated from 85 peer-reviewed research papers and four widely-used causal-inference textbooks. For each query a system must produce (i) a structured identification specification that names the strategy, the treatment, outcome, and control variables, and all design-specific elements, and (ii) a point estimate with a standard error. By scoring these two components separately, our benchmark enables granular diagnosis: it distinguishes failures in causal reasoning from errors in numerical execution. Baseline results with a state-of-the-art LLM show that, while the model correctly identifies the high-level strategy in 84 % of cases, full identification-specification correctness drops to only 30 %, revealing that the bottleneck lies in the nuanced details of research design rather than in computation. CausalReasoningBenchmark is publicly available on Hugging Face and is designed to foster the development of more robust automated causal-inference systems.

</details>


### [111] [Physics-based phenomenological characterization of cross-modal bias in multimodal models](https://arxiv.org/abs/2602.20624)
*Hyeongmo Kim,Sohyun Kang,Yerin Choi,Seungyeon Ji,Junhyuk Woo,Hyunsuk Chung,Soyeon Caren Han,Kyungreem Han*

Main category: cs.AI

TL;DR: Position paper proposing a phenomenological, physics-based surrogate model of transformer dynamics to expose cross‑modal bias in multimodal LLMs; shows that multimodality can reinforce modality dominance rather than reduce it, evidenced by structured error‑attractor patterns and dynamical analyses.


<details>
  <summary>Details</summary>
Motivation: Fairness in AI includes comparative (equal treatment) and non‑comparative (accuracy, arbitrariness, opacity) concerns. MLLMs introduce complex multimodal interactions that can create subtle, systematic distortions not captured by standard representation/embedding analyses. The authors advocate phenomenological explainability that grounds explanations in the physical interactions the model 'experiences' during training/inference to better diagnose fairness issues.

Method: Develop a physics-based surrogate model that approximates transformer dynamics (semantic network structure, self‑ and cross‑attention) to study cross‑modal bias beyond embeddings. Validate via multi‑input diagnostics: (1) perturbation‑based emotion classification tests on Qwen2.5‑Omni and Gemma 3n with systematic label/input perturbations; (2) dynamical analysis using a Lorenz chaotic time‑series within the surrogate to probe attractor behavior and stability.

Result: Across two architecturally distinct MLLMs, multimodal inputs can amplify modality dominance rather than balancing it. Evidence includes structured error‑attractor patterns under systematic label perturbations; the surrogate’s Lorenz‑style dynamical analysis corroborates these bias dynamics.

Conclusion: Phenomenological, physics‑grounded surrogate modeling offers practical tools to diagnose and reason about algorithmic fairness in MLLMs by revealing dynamics invisible to conventional representation-level methods. The approach surfaces cross‑modal bias mechanisms (e.g., reinforced modality dominance) and motivates future work on mitigation strategies informed by surrogate dynamical insights.

Abstract: The term 'algorithmic fairness' is used to evaluate whether AI models operate fairly in both comparative (where fairness is understood as formal equality, such as "treat like cases as like") and non-comparative (where unfairness arises from the model's inaccuracy, arbitrariness, or inscrutability) contexts. Recent advances in multimodal large language models (MLLMs) are breaking new ground in multimodal understanding, reasoning, and generation; however, we argue that inconspicuous distortions arising from complex multimodal interaction dynamics can lead to systematic bias. The purpose of this position paper is twofold: first, it is intended to acquaint AI researchers with phenomenological explainable approaches that rely on the physical entities that the machine experiences during training/inference, as opposed to the traditional cognitivist symbolic account or metaphysical approaches; second, it is to state that this phenomenological doctrine will be practically useful for tackling algorithmic fairness issues in MLLMs. We develop a surrogate physics-based model that describes transformer dynamics (i.e., semantic network structure and self-/cross-attention) to analyze the dynamics of cross-modal bias in MLLM, which are not fully captured by conventional embedding- or representation-level analyses. We support this position through multi-input diagnostic experiments: 1) perturbation-based analyses of emotion classification using Qwen2.5-Omni and Gemma 3n, and 2) dynamical analysis of Lorenz chaotic time-series prediction through the physical surrogate. Across two architecturally distinct MLLMs, we show that multimodal inputs can reinforce modality dominance rather than mitigate it, as revealed by structured error-attractor patterns under systematic label perturbation, complemented by dynamical analysis.

</details>


### [112] [When can we trust untrusted monitoring? A safety case sketch across collusion strategies](https://arxiv.org/abs/2602.20628)
*Nelson Gardner-Challis,Jonathan Bostock,Georgiy Kozhevnikov,Morgan Sinclaire,Joan Velja,Alessandro Abate,Charlie Griffin*

Main category: cs.AI

TL;DR: Extends pre-deployment safety arguments for untrusted monitoring by broadening collusion threat models, introducing a taxonomy (passive self-recognition, causal/acausal collusion, combined), and providing a safety case sketch that identifies when passive self-recognition poses a stronger risk, thereby enabling more robust evaluations.


<details>
  <summary>Details</summary>
Motivation: As highly capable AIs gain autonomy, a misaligned system could cause catastrophic harm. Untrusted monitoring (one untrusted AI overseeing another) is a promising mitigation, but its safety is hard to justify without deploying an actually misaligned model. Prior work used narrow assumptions about how models might collude, risking false confidence. The paper seeks more rigorous, pre-deployment evidence under broader adversarial strategies.

Method: Relax prior assumptions about collusion; build a taxonomy of strategies—(1) passive self-recognition, (2) causal collusion via pre-shared hidden signals, (3) acausal collusion via Schelling points, and (4) combined strategies; and construct a safety case sketch that articulates assumptions, argument structure, and known gaps for untrusted monitoring deployments.

Result: Identifies conditions where passive self-recognition may outperform previously studied collusion strategies as a way to defeat monitoring; clarifies the structure of a rigorous safety argument; and surfaces remaining challenges and assumptions for evaluating untrusted monitoring. Contributions are conceptual/theoretical rather than empirical metrics.

Conclusion: By expanding the threat model and formalizing the safety argument, the work strengthens pre-deployment evaluation of untrusted monitoring. The taxonomy and safety case sketch help developers stress-test protocols, with special attention to passive self-recognition risks, moving the field toward more credible safety guarantees without deploying misaligned systems.

Abstract: AIs are increasingly being deployed with greater autonomy and capabilities, which increases the risk that a misaligned AI may be able to cause catastrophic harm. Untrusted monitoring -- using one untrusted model to oversee another -- is one approach to reducing risk. Justifying the safety of an untrusted monitoring deployment is challenging because developers cannot safely deploy a misaligned model to test their protocol directly. In this paper, we develop upon existing methods for rigorously demonstrating safety based on pre-deployment testing. We relax assumptions that previous AI control research made about the collusion strategies a misaligned AI might use to subvert untrusted monitoring. We develop a taxonomy covering passive self-recognition, causal collusion (hiding pre-shared signals), acausal collusion (hiding signals via Schelling points), and combined strategies. We create a safety case sketch to clearly present our argument, explicitly state our assumptions, and highlight unsolved challenges. We identify conditions under which passive self-recognition could be a more effective collusion strategy than those studied previously. Our work builds towards more robust evaluations of untrusted monitoring.

</details>


### [113] [Identifying two piecewise linear additive value functions from anonymous preference information](https://arxiv.org/abs/2602.20638)
*Vincent Auriau,Khaled Belahcene,Emmanuel Malherbe,Vincent Mousseau,Marc Pirlot*

Main category: cs.AI

TL;DR: They design a querying strategy that simultaneously elicits two unlabeled decision-makers’ additive value functions and can uniquely recover both when marginal utilities are piecewise linear with known breakpoints and answers are noiseless.


<details>
  <summary>Details</summary>
Motivation: In many settings (e.g., group decisions or crowdsourcing), responses may come from multiple people without identity labels. Standard elicitation assumes a single, labeled decision-maker, so there is a need to recover multiple preference models from mixed, unlabeled answers.

Method: Assume each decision-maker’s preferences follow an additive value model whose marginal value functions are piecewise linear with known knots. Ask the same query once and receive two answers (one per decision-maker) but without labels. Use these paired, noiseless comparisons to impose linear constraints on both sets of marginal functions and implicitly assign answers to decision-makers, yielding an identification algorithm under the structural assumptions.

Result: Under the stated assumptions (noiseless responses, additive model, piecewise-linear marginals with known breakpoints), the proposed elicitation procedure can identify—i.e., uniquely recover—the two decision-makers’ value functions from the unlabeled pairwise answers.

Conclusion: It is feasible to disentangle and learn two individuals’ additive preference models from unlabeled, simultaneous queries when sufficient structure is known. This opens avenues to multi-agent elicitation with potential extensions to noisy feedback, unknown breakpoints, or more than two decision-makers.

Abstract: Eliciting a preference model involves asking a person, named decision-maker, a series of questions. We assume that these preferences can be represented by an additive value function. In this work, we query simultaneously two decision-makers in the aim to elicit their respective value functions. For each query we receive two answers, without noise, but without knowing which answer corresponds to which decision-maker.We propose an elicitation procedure that identifies the two preference models when the marginal value functions are piecewise linear with known breaking points.

</details>


### [114] [Grounding LLMs in Scientific Discovery via Embodied Actions](https://arxiv.org/abs/2602.20639)
*Bo Zhang,Jinfeng Zhou,Yuxuan Chen,Jianing Yin,Minlie Huang,Hongning Wang*

Main category: cs.AI

TL;DR: EmbodiedAct turns scientific software (instantiated in MATLAB) into active, closed-loop agents for LLMs, enabling runtime perception and intervention during simulations to improve stability, reliability, and modeling accuracy over existing passive tool-use baselines.


<details>
  <summary>Details</summary>
Motivation: LLMs can reason about science but standard "execute-then-respond" workflows lack runtime perception, causing missed transient anomalies (e.g., numerical instabilities, diverging oscillations) and poor reliability in long-horizon simulations.

Method: Introduce EmbodiedAct, which grounds LLMs in embodied actions within scientific software, creating a tight perception–execution loop so the agent can observe intermediate states, detect anomalies, and adjust actions in real time. The framework is instantiated in MATLAB and evaluated on complex engineering design and scientific modeling tasks.

Result: Across extensive experiments, EmbodiedAct significantly outperforms baselines, achieving state-of-the-art reliability and stability in long simulations and higher accuracy in scientific modeling tasks.

Conclusion: Active, perceptive, closed-loop embodiment of LLMs within established scientific tools yields more reliable and accurate scientific computing, suggesting a generalizable path to robust AI-assisted simulation and design.

Abstract: Large Language Models (LLMs) have shown significant potential in scientific discovery but struggle to bridge the gap between theoretical reasoning and verifiable physical simulation. Existing solutions operate in a passive "execute-then-response" loop and thus lacks runtime perception, obscuring agents to transient anomalies (e.g., numerical instability or diverging oscillations). To address this limitation, we propose EmbodiedAct, a framework that transforms established scientific software into active embodied agents by grounding LLMs in embodied actions with a tight perception-execution loop. We instantiate EmbodiedAct within MATLAB and evaluate it on complex engineering design and scientific modeling tasks. Extensive experiments show that EmbodiedAct significantly outperforms existing baselines, achieving SOTA performance by ensuring satisfactory reliability and stability in long-horizon simulations and enhanced accuracy in scientific modeling.

</details>


### [115] [Recursive Belief Vision Language Model](https://arxiv.org/abs/2602.20659)
*Vaidehi Bagaria,Bijo Sebastian,Nirav Patel*

Main category: cs.AI

TL;DR: RB-VLA augments VLA policies with a persistent, action-conditioned belief state, enabling phase-aware control under partial observability and delivering large success and latency gains on long-horizon manipulation.


<details>
  <summary>Details</summary>
Motivation: Existing VLAs are observation-driven with short context or repeated VLM queries, causing progress loss, action repetition from perceptual aliasing, growing memory, and high latency. The main bottleneck is not semantic reasoning but the absence of persistent, action-aware state and limited temporal/physical reasoning for multi-stage control.

Method: Introduce a belief-centric architecture trained via self-supervised world-model objectives to learn a compact latent state capturing task-relevant history, dynamics, and object interactions. Query the VLM once to obtain high-level intent; use the learned belief to track task progress and support phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. Condition a diffusion policy jointly on belief and intent for robust closed-loop execution.

Result: RB-VLA outperforms prior VLAs on long-horizon benchmarks: +52.5% success on multi-stage pick-and-place and +37.5% on stacking versus π0; up to 5x lower inference latency; eliminates timestep-dependent memory growth. Ablations identify the belief module as the primary contributor, raising success from 32.5% to 77.5%.

Conclusion: Belief-based, persistent state representations—combined with a single-shot VLM intent—enable efficient, robust long-horizon VLA control under partial observability, addressing progress tracking and aliasing while cutting latency and memory costs.

Abstract: Current vision-language-action (VLA) models struggle with long-horizon manipulation under partial observability. Most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. Semantic reasoning alone is not the primary bottleneck in long-horizon manipulation. Instead, VLAs lack persistent, action-conditioned state representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once for high-level intent, the VLM provides task specification, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks, respectively, compared to π0. It also reduces inference latency by up to 5x relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show that the belief module is the primary driver of performance, increasing success rates from 32.5% to 77.5%. These results demonstrate the effectiveness of belief-based state representations for long-horizon VLA policies.

</details>


### [116] [How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective](https://arxiv.org/abs/2602.20687)
*Bo Peng,Pi Bu,Keyu Pan,Xinrun Xu,Yinxiu Zhao,Miao Chen,Yang Du,Lin Li,Jun Song,Tong Xu*

Main category: cs.AI

TL;DR: NativeEmbodied is a new benchmark for VLM-driven embodied agents that uses a unified native low-level action space and jointly evaluates both complex high-level tasks and decomposed low-level skill tasks, revealing core skill deficiencies that cap overall performance.


<details>
  <summary>Details</summary>
Motivation: Existing VLM embodied benchmarks rely on non-native control (high-level commands or discretized actions) and emphasize only high-level tasks, making them unrealistic for real-world control and poor at diagnosing which fundamental skills limit performance.

Method: Build a benchmark on diverse simulated scenes with a unified native low-level action space. Include three representative high-level tasks in complex scenarios to test overall capability, and construct four types of low-level tasks by decoupling and targeting fundamental embodied skills. Enable joint evaluation across task and skill granularities for fine-grained assessment.

Result: State-of-the-art VLMs, when evaluated on NativeEmbodied, exhibit clear deficiencies in several fundamental embodied skills; these skill bottlenecks substantially hinder performance on the higher-level tasks.

Conclusion: A realistic, skill-aware benchmark exposes key gaps in current VLM-driven embodied agents and provides diagnostic insights that can guide research toward strengthening foundational skills to improve end-to-end task performance.

Abstract: Recent advances in vision-language models (VLMs) have shown promise for human-level embodied intelligence. However, existing benchmarks for VLM-driven embodied agents often rely on high-level commands or discretized action spaces, which are non-native settings that differ markedly from real-world control. In addition, current benchmarks focus primarily on high-level tasks and lack joint evaluation and analysis at both low and high levels. To address these limitations, we present NativeEmbodied, a challenging benchmark for VLM-driven embodied agents that uses a unified, native low-level action space. Built on diverse simulated scenes, NativeEmbodied includes three representative high-level tasks in complex scenarios to evaluate overall performance. For more detailed analysis, we further decouple the skills required by complex tasks and construct four types of low-level tasks, each targeting a fundamental embodied skill. This joint evaluation across task and skill granularities enables fine-grained assessment of embodied agents. Experiments with state-of-the-art VLMs reveal clear deficiencies in several fundamental embodied skills, and further analysis shows that these bottlenecks significantly limit performance on high-level tasks. NativeEmbodied highlights key challenges for current VLM-driven embodied agents and provides insights to guide future research.

</details>


### [117] [PromptCD: Test-Time Behavior Enhancement via Polarity-Prompt Contrastive Decoding](https://arxiv.org/abs/2602.20696)
*Baolong Bi,Yuyao Ge,Shenghua Liu,Yuchen He,Siqian Tong,Lizhe Chen,Lingrui Mei,Zehao Li,Yiwei Wang,Yujun Cai,Ming-Hsuan Yang,Xueqi Cheng*

Main category: cs.AI

TL;DR: PromptCD is a test-time contrastive decoding approach that steers LLMs and VLMs toward desired behaviors by contrasting outputs from positive vs. negative guiding prompts, improving alignment (3H) and VQA grounding without extra training or data.


<details>
  <summary>Details</summary>
Motivation: Most alignment methods require costly training-time interventions and curated data, while prior contrastive decoding techniques are narrow in scope. The goal is a simple, general, and inexpensive way to reliably control model behavior at inference across modalities.

Method: Construct paired, behavior-polarized prompts (positive and negative). Run the model with both and contrast their outputs—token-level probability distributions for LLMs and visual attention patterns for VLMs—to upweight behavior-consistent signals and downweight opposing ones. This generalizes contrastive decoding to broader enhancement objectives without additional training.

Result: On LLMs, PromptCD yields consistent, substantial improvements on the 3H (helpfulness, honesty, harmlessness) objectives at test time. On VLMs, it improves VQA accuracy by reinforcing visual grounding aligned with the target behavior, with analyses showing clearer, behavior-consistent attention maps.

Conclusion: PromptCD offers a simple, general, and cost-efficient test-time mechanism for reliable behavior control across LLMs and VLMs, extending contrastive decoding beyond narrow use cases and enabling meaningful self-enhancement without extra data or fine-tuning.

Abstract: Reliable AI systems require large language models (LLMs) to exhibit behaviors aligned with human preferences and values. However, most existing alignment approaches operate at training time and rely on additional high-quality data, incurring significant computational and annotation costs. While recent work has shown that contrastive decoding can leverage a model's internal distributions to improve specific capabilities, its applicability remains limited to narrow behavioral scopes and scenarios. In this work, we introduce Polarity-Prompt Contrastive Decoding (PromptCD), a test-time behavior control method that generalizes contrastive decoding to broader enhancement settings. PromptCD constructs paired positive and negative guiding prompts for a target behavior and contrasts model responses-specifically token-level probability distributions in LLMs and visual attention patterns in VLMs-to reinforce desirable outcomes. This formulation extends contrastive decoding to a wide range of enhancement objectives and is applicable to both LLMs and Vision-Language Models (VLMs) without additional training. For LLMs, experiments on the "3H" alignment objectives (helpfulness, honesty, and harmlessness) demonstrate consistent and substantial improvements, indicating that post-trained models can achieve meaningful self-enhancement purely at test time. For VLMs, we further analyze contrastive effects on visual attention, showing that PromptCD significantly improves VQA performance by reinforcing behavior-consistent visual grounding. Collectively, these results highlight PromptCD as a simple, general, and cost-efficient strategy for reliable behavior control across modalities.

</details>


### [118] [Online Algorithms with Unreliable Guidance](https://arxiv.org/abs/2602.20706)
*Julien Dallot,Yuval Emek,Yuval Gil,Maciej Pacut,Stefan Schmid*

Main category: cs.AI

TL;DR: They propose a clean, problem-centric model (OAG) for learning-augmented online algorithms under unreliable per-request guidance, plus a simple “drop-or-trust” compiler that achieves strong consistency–robustness tradeoffs, with optimal or improved guarantees on classic problems.


<details>
  <summary>Details</summary>
Motivation: Existing learning-augmented online models often entangle prediction mechanisms with algorithm design and require bespoke loss notions, making analyses problem-specific and hard to generalize. The authors want a unified framework that isolates prediction uncertainty and yields guarantees that smoothly interpolate between perfect and adversarial predictions.

Method: 1) Define the OAG model: each request comes with guidance from the answer space that is correct except with probability β, where incorrect guidance can be adversarial. Performance is measured for β=0 (consistency), β=1 (robustness), and smoothly in between. 2) Introduce the Drop-or-Trust Blindly (DTB) compiler that converts any prediction-oblivious online algorithm into an OAG algorithm that, per request, either follows the guidance or ignores it and runs the base algorithm, decided by a biased coin calibrated to β/analysis. 3) Analyze competitiveness guarantees yielded by DTB for canonical problems.

Result: Prove that DTB-generated algorithms achieve optimal consistency–robustness tradeoffs for caching and uniform metrical task systems. For bipartite matching under adversarial arrivals, the compiled algorithm surpasses prior state-of-the-art guarantees.

Conclusion: A minimal, modular OAG model plus the DTB compiler provides a broadly applicable pathway to learning-augmented online algorithms with provable, smoothly degrading guarantees, matching or improving best-known results on key benchmarks.

Abstract: This paper introduces a new model for ML-augmented online decision making, called online algorithms with unreliable guidance (OAG). This model completely separates between the predictive and algorithmic components, thus offering a single well-defined analysis framework that relies solely on the considered problem. Formulated through the lens of request-answer games, an OAG algorithm receives, with each incoming request, a piece of guidance which is taken from the problem's answer space; ideally, this guidance is the optimal answer for the current request, however with probability $β$, the guidance is adversarially corrupted. The goal is to develop OAG algorithms that admit good competitiveness when $β= 0$ (a.k.a. consistency) as well as when $β= 1$ (a.k.a. robustness); the appealing notion of smoothness, that in most prior work required a dedicated loss function, now arises naturally as $β$ shifts from $0$ to $1$.
  We then describe a systematic method, called the drop or trust blindly (DTB) compiler, which transforms any online algorithm into a learning-augmented online algorithm in the OAG model. Given a prediction-oblivious online algorithm, its learning-augmented counterpart produced by applying the DTB compiler either follows the incoming guidance blindly or ignores it altogether and proceeds as the initial algorithm would have; the choice between these two alternatives is based on the outcome of a (biased) coin toss. As our main technical contribution, we prove (rigorously) that although remarkably simple, the class of algorithms produced via the DTB compiler includes algorithms with attractive consistency-robustness guarantees for three classic online problems: for caching and uniform metrical task systems our algorithms are optimal, whereas for bipartite matching (with adversarial arrival order), our algorithm outperforms the state-of-the-art.

</details>


### [119] [ICON: Indirect Prompt Injection Defense for Agents based on Inference-Time Correction](https://arxiv.org/abs/2602.20708)
*Che Wang,Fuyao Zhang,Jiaming Zhang,Ziqi Zhang,Yinghui Wang,Longtao Huang,Jianbo Gao,Zhong Chen,Wei Yang Bryan Lim*

Main category: cs.AI

TL;DR: ICON detects and mitigates indirect prompt injection in LLM (and multi‑modal) agents by probing latent representations for over‑focusing signatures and then surgically steering attention, achieving very low attack success with substantially better task continuity than refusal-based defenses.


<details>
  <summary>Details</summary>
Motivation: Indirect prompt injection in retrieved content can hijack agent behavior. Existing defenses rely on strict filtering/refusal, which often over-refuse and break valid workflows. The goal is to neutralize attacks while preserving the agent’s task trajectory and utility.

Method: Introduce ICON, a two-stage probing-to-mitigation framework: (1) Latent Space Trace Prober flags IPI by measuring high-intensity, over-focusing signatures in the model’s latent space; (2) Mitigating Rectifier performs targeted attention steering, down-weighting adversarial query–key dependencies and amplifying task-relevant signals to restore normal behavior. Designed to be backbone-agnostic and extendable to multi-modal agents.

Result: Across multiple backbones, ICON reports a 0.4% attack success rate (on par with commercial detectors) while improving task utility by over 50% relative to strict filtering baselines. It generalizes out-of-distribution and transfers to multi-modal settings.

Conclusion: Latent-space probing combined with surgical attention rectification yields a better security–utility trade-off than blanket refusal: ICON maintains task continuity, matches top-tier detection rates, and generalizes across models and modalities.

Abstract: Large Language Model (LLM) agents are susceptible to Indirect Prompt Injection (IPI) attacks, where malicious instructions in retrieved content hijack the agent's execution. Existing defenses typically rely on strict filtering or refusal mechanisms, which suffer from a critical limitation: over-refusal, prematurely terminating valid agentic workflows. We propose ICON, a probing-to-mitigation framework that neutralizes attacks while preserving task continuity. Our key insight is that IPI attacks leave distinct over-focusing signatures in the latent space. We introduce a Latent Space Trace Prober to detect attacks based on high intensity scores. Subsequently, a Mitigating Rectifier performs surgical attention steering that selectively manipulate adversarial query key dependencies while amplifying task relevant elements to restore the LLM's functional trajectory. Extensive evaluations on multiple backbones show that ICON achieves a competitive 0.4% ASR, matching commercial grade detectors, while yielding a over 50% task utility gain. Furthermore, ICON demonstrates robust Out of Distribution(OOD) generalization and extends effectively to multi-modal agents, establishing a superior balance between security and efficiency.

</details>


### [120] [Counterfactual Simulation Training for Chain-of-Thought Faithfulness](https://arxiv.org/abs/2602.20710)
*Peter Hase,Christopher Potts*

Main category: cs.AI

TL;DR: They propose Counterfactual Simulation Training (CST), a method that trains models to produce more faithful chains-of-thought (CoT) by rewarding rationales that let a simulator predict model outputs under counterfactual inputs; it improves CoT-based monitoring and simulatability across models up to 235B parameters.


<details>
  <summary>Details</summary>
Motivation: CoT is widely used to interpret LLM decisions, but CoT text often isn’t faithful to the internal decision process. This limits interpretability, safety monitoring (e.g., spurious cue use, reward hacking, sycophancy), and reliability. The paper seeks a training signal that aligns CoTs with the true causal determinants of model outputs.

Method: CST trains models so that provided CoTs enable a learned simulator to accurately predict the model’s outputs across counterfactual variations of inputs. Two applications: (1) CoT monitoring with cue-based counterfactuals to detect reliance on spurious features, reward hacking, or sycophancy; (2) generic model-based counterfactual simulation to encourage more faithful, generalizable reasoning traces. They also explore combining RL-style optimization with LLM-based rewriting of unfaithful CoTs to increase efficiency.

Result: On cue-based counterfactuals, CST boosts monitor accuracy by ~35 points; on generic counterfactuals, simulatability improves by ~2 points. CST outperforms prompting baselines. Rewriting unfaithful CoTs with an LLM is ~5× more sample-efficient than RL alone. However, gains don’t generalize to dissuading cues, and larger models are not inherently more faithful though they benefit more from CST.

Conclusion: CST improves the faithfulness and utility of CoT for monitoring and interpretability, showing scalable benefits across large models. While some generalization gaps remain (e.g., dissuading cues), the approach offers a practical path to more faithful CoTs with promising monitoring applications; code is released for reproducibility.

Abstract: Inspecting Chain-of-Thought reasoning is among the most common means of understanding why an LLM produced its output. But well-known problems with CoT faithfulness severely limit what insights can be gained from this practice. In this paper, we introduce a training method called Counterfactual Simulation Training (CST), which aims to improve CoT faithfulness by rewarding CoTs that enable a simulator to accurately predict a model's outputs over counterfactual inputs. We apply CST in two settings: (1) CoT monitoring with cue-based counterfactuals, to detect when models rely on spurious features, reward hack, or are sycophantic, and (2) counterfactual simulation over generic model-based counterfactuals, to encourage models to produce more faithful, generalizable reasoning in the CoT. Experiments with models up to 235B parameters show that CST can substantially improve monitor accuracy on cue-based counterfactuals (by 35 accuracy points) as well as simulatability over generic counterfactuals (by 2 points). We further show that: (1) CST outperforms prompting baselines, (2) rewriting unfaithful CoTs with an LLM is 5x more efficient than RL alone, (3) faithfulness improvements do not generalize to dissuading cues (as opposed to persuading cues), and (4) larger models do not show more faithful CoT out of the box, but they do benefit more from CST. These results suggest that CST can improve CoT faithfulness in general, with promising applications for CoT monitoring. Code for experiments in this paper is available at https://github.com/peterbhase/counterfactual-simulation-training

</details>


### [121] [Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning](https://arxiv.org/abs/2602.20722)
*Xu Wan,Yansheng Wang,Wenqi Huang,Mingyang Sun*

Main category: cs.AI

TL;DR: BAPO is an off-policy RL-with-verifiable-rewards framework for LLM post-training that improves sample efficiency via dynamic batch selection—re-evaluating hard cases and reusing high-quality samples—while guaranteeing a lower bound on policy improvement; it outperforms GRPO by ~12.5% on math, planning, and visual reasoning and fixes 40.7% of base-model failures.


<details>
  <summary>Details</summary>
Motivation: On-policy RLVR wastes valuable experience and tends to produce homogeneous rewards, limiting progress on hard examples. There is a need for a data-efficient approach that focuses learning on difficult, high-value samples without sacrificing reliable policy improvement.

Method: Introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR method that (1) dynamically selects training batches by re-evaluating historically difficult samples, (2) reuses high-quality samples, and (3) maintains a provable lower-bound guarantee for policy improvement, mitigating off-policy instability.

Result: Across math, planning, and visual reasoning benchmarks, BAPO achieves an average 12.5% gain over GRPO and resolves 40.7% of problems that base models consistently fail to solve.

Conclusion: Adaptive off-policy RLVR with guaranteed improvement can substantially enhance LLM post-training efficiency and effectiveness, particularly on verifiable tasks, by directing updates toward hard and high-quality experiences.

Abstract: Traditional on-policy Reinforcement Learning with Verifiable Rewards (RLVR) frameworks suffer from experience waste and reward homogeneity, which directly hinders learning efficiency on difficult samples during large language models post-training. In this paper, we introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework to improve the data efficiency in large language models post-training. It dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, while holding a lower bound guarantee for policy improvement. Extensive experiments further demonstrate that BAPO achieves an average 12.5% improvement over GRPO across mathematics, planning, and visual reasoning tasks. Crucially, BAPO successfully resolves 40.7% of problems that base models consistently fail to solve.

</details>


### [122] [Modality-Guided Mixture of Graph Experts with Entropy-Triggered Routing for Multimodal Recommendation](https://arxiv.org/abs/2602.20723)
*Ji Dai,Quan Fang,Dengsheng Cai*

Main category: cs.AI

TL;DR: MAGNET is a modality-guided mixture-of-experts graph recommender that augments user–item graphs with content-derived edges and routes interactions to structured modality-role experts via entropy-triggered training, yielding more interpretable, stable multimodal fusion and better ranking performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Multimodal recommendation suffers from heterogeneous and sometimes conflicting signals; shared fusion pathways entangle modalities and bias toward dominant ones, which is harmful under sparse feedback and long-tail distributions. A controllable, stable, and interpretable fusion mechanism is needed to leverage content without collapsing modality balance.

Method: MAGNET combines (1) dual-view graph learning: parallel encoding of the interaction graph and a content-augmented graph with lightweight fusion to improve coverage while preserving collaborative structure; (2) structured experts with explicit roles—dominant, balanced, complementary—to fuse behavioral, visual, and textual cues more transparently; (3) interaction-conditioned routing with a progressive, two-stage entropy-weighting scheme that shifts from coverage-oriented to specialization-oriented training, stabilizing sparse routing and preventing expert collapse.

Result: Across public benchmarks, MAGNET consistently surpasses strong baselines, particularly under sparse and long-tail settings (exact numbers not provided in the abstract).

Conclusion: By explicitly controlling what to fuse (via structure-aware augmentation) and how to fuse (via modality-role experts with entropy-triggered routing), MAGNET improves controllability, stability, and interpretability in multimodal recommendation while achieving superior empirical performance.

Abstract: Multimodal recommendation enhances ranking by integrating user-item interactions with item content, which is particularly effective under sparse feedback and long-tail distributions. However, multimodal signals are inherently heterogeneous and can conflict in specific contexts, making effective fusion both crucial and challenging. Existing approaches often rely on shared fusion pathways, leading to entangled representations and modality imbalance. To address these issues, we propose \textbf{MAGNET}, a \textbf{M}odality-Guided Mixture of \textbf{A}daptive \textbf{G}raph Experts \textbf{N}etwork with Progressive \textbf{E}ntropy-\textbf{T}riggered Routing for Multimodal Recommendation, designed to enhance controllability, stability, and interpretability in multimodal fusion. MAGNET couples interaction-conditioned expert routing with structure-aware graph augmentation, so that both \emph{what} to fuse and \emph{how} to fuse are explicitly controlled and interpretable. At the representation level, a dual-view graph learning module augments the interaction graph with content-induced edges, improving coverage for sparse and long-tail items while preserving collaborative structure via parallel encoding and lightweight fusion. At the fusion level, MAGNET employs structured experts with explicit modality roles -- dominant, balanced, and complementary -- enabling a more interpretable and adaptive combination of behavioral, visual, and textual cues. To further stabilize sparse routing and prevent expert collapse, we introduce a two-stage entropy-weighting mechanism that monitors routing entropy. This mechanism automatically transitions training from an early coverage-oriented regime to a later specialization-oriented regime, progressively balancing expert utilization and routing confidence. Extensive experiments on public benchmarks demonstrate consistent improvements over strong baselines.

</details>


### [123] [Balancing Multiple Objectives in Urban Traffic Control with Reinforcement Learning from AI Feedback](https://arxiv.org/abs/2602.20728)
*Chenyang Zhao,Vinny Cahill,Ivana Dusparic*

Main category: cs.AI

TL;DR: Extends RLAIF to multi-objective RL, using LLM-generated preference labels to learn policies that balance conflicting objectives according to user priorities, reducing manual reward engineering.


<details>
  <summary>Details</summary>
Motivation: Reward design is difficult in real-world RL, especially with multiple, conflicting objectives where policies can collapse to a dominant goal. Preference-based RL helps but relies on human labels; RLAIF scales preference labeling with LLMs, yet prior work is largely single-objective. The gap is handling trade-offs and user priorities at scale in multi-objective settings.

Method: Adapt preference-based RL with AI-generated (LLM) preference labels to multi-objective self-adaptive systems. Collect preferences over pairs of behavioral outcomes that reflect trade-offs across objectives and user-priority settings, and integrate these into a multi-objective RL pipeline to learn user-aligned policies without bespoke reward shaping.

Result: Empirically show that multi-objective RLAIF can produce policies with balanced trade-offs that reflect varied user priorities, avoiding collapse to a single objective and reducing the need for laborious reward engineering.

Conclusion: Integrating RLAIF into multi-objective RL provides a scalable path to user-aligned policy learning in domains with inherently conflicting objectives.

Abstract: Reward design has been one of the central challenges for real world reinforcement learning (RL) deployment, especially in settings with multiple objectives. Preference-based RL offers an appealing alternative by learning from human preferences over pairs of behavioural outcomes. More recently, RL from AI feedback (RLAIF) has demonstrated that large language models (LLMs) can generate preference labels at scale, mitigating the reliance on human annotators. However, existing RLAIF work typically focuses only on single-objective tasks, leaving the open question of how RLAIF handles systems that involve multiple objectives. In such systems trade-offs among conflicting objectives are difficult to specify, and policies risk collapsing into optimizing for a dominant goal. In this paper, we explore the extension of the RLAIF paradigm to multi-objective self-adaptive systems. We show that multi-objective RLAIF can produce policies that yield balanced trade-offs reflecting different user priorities without laborious reward engineering. We argue that integrating RLAIF into multi-objective RL offers a scalable path toward user-aligned policy learning in domains with inherently conflicting objectives.

</details>


### [124] [CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference](https://arxiv.org/abs/2602.20732)
*Chao Fei,Guozhong Li,Chenxi Liu,Panos Kalnis*

Main category: cs.AI

TL;DR: CHESS is an algorithm–system co-design for long-context LLMs that performs context-aware, hierarchical KV-cache selection and coarse-grained system operations to reconstruct only the most relevant history each step, achieving Full-KV or better quality using ~1% of the cache and up to 4.56× higher throughput.


<details>
  <summary>Details</summary>
Motivation: As context length grows, inference latency and memory are dominated by KV-cache size. Prior pruning is largely context-agnostic and causes quality drops; their irregular memory accesses and selection overheads also limit real-world speedups.

Method: A context-aware, hierarchical token selection policy that dynamically rebuilds a coherent, step-specific context during decoding, paired with a system design that uses coarse-grained selection to minimize data movement and avoid irregular memory access—turning theoretical sparsity into practical acceleration.

Result: Across extensive evaluations, CHESS surpasses Full-KV quality with only ~1% of the KV cache, delivers stable low-latency inference with up to 4.56× higher throughput, and consistently beats strong baselines.

Conclusion: Coupling context-aware hierarchical selection with coarse-grained systems support enables drastic KV-cache reduction without sacrificing (and sometimes improving) quality, translating into substantial, practical speedups for long-context LLM inference.

Abstract: Long-context LLMs demand accurate inference at low latency, yet decoding becomes primarily constrained by KV cache as context grows. Prior pruning methods are largely context-agnostic: their token selection ignores step-wise relevance and local semantics, which undermines quality. Moreover, their irregular accesses and selection overheads yield only limited wall-clock speedups. To address this, we propose \textbf{CHESS}, an \textit{algorithm-system co-design} KV-cache management system. Algorithmically, CHESS introduces a context-aware, hierarchical selection policy that dynamically reconstructs a coherent context for the current decoding. System-wise, coarse granularity selection eliminates expensive data movement, fully realizing practical acceleration from theoretical sparsity. Extensive evaluations demonstrate that CHESS surpasses Full-KV quality using only \textbf{1\%} of the KV cache, delivers low-latency stable inference with up to \textbf{4.56$\times$} higher throughput, and consistently outperforms other strong baselines. Code is available at \href{https://anonymous.4open.science/r/CHESS-9958/}{https://anonymous.4open.science/r/CHESS/}.

</details>


### [125] [PyVision-RL: Forging Open Agentic Vision Models via RL](https://arxiv.org/abs/2602.20739)
*Shitian Zhao,Shaoheng Lin,Ming Li,Haoquan Zhang,Wenshuo Peng,Kaipeng Zhang,Chen Wei*

Main category: cs.AI

TL;DR: PyVision-RL is an RL framework for open-weight multimodal agents that prevents interaction collapse by pairing an oversampling–filtering–ranking rollout strategy with an accumulative tool-use reward, yielding stable multi-turn tool interactions and efficient image/video reasoning via on-demand frame selection.


<details>
  <summary>Details</summary>
Motivation: Agentic multimodal models trained with RL tend to minimize tool calls and multi-turn reasoning (interaction collapse), undermining the benefits of agent behavior and inflating compute costs—especially for video. A stable training recipe that sustains interaction while improving efficiency is needed.

Method: Combine (1) an oversampling–filtering–ranking (OFR) rollout strategy to curate higher-quality trajectories with (2) an accumulative tool reward to explicitly incentivize sustained, multi-turn tool use. Use a unified pipeline to train open-weight multimodal models for images (PyVision-Image) and videos (PyVision-Video). For video, apply on-demand context construction that selectively samples task-relevant frames during reasoning to cut visual tokens.

Result: Training remains stable with sustained multi-turn tool usage. PyVision-Image and PyVision-Video achieve strong benchmark performance while substantially reducing visual token consumption in video tasks due to selective frame sampling, improving overall efficiency.

Conclusion: Sustained interaction and on-demand visual processing are key to scalable multimodal agents. PyVision-RL mitigates interaction collapse, encourages multi-turn tool use, and enables efficient image/video reasoning, offering a practical path for training open-weight agentic multimodal models.

Abstract: Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.

</details>


### [126] [Pipeline for Verifying LLM-Generated Mathematical Solutions](https://arxiv.org/abs/2602.20770)
*Varvara Sazonova,Dmitri Shmelkin,Stanislav Kikot,Vasily Motolygin*

Main category: cs.AI

TL;DR: They propose an open-source pipeline that evaluates math reasoning models by automatically and interactively verifying structured solutions with a proof assistant (Lean 4), rather than checking only final answers; it includes three cooperating agents, supports small models (≤8B), can generate formal/informal solutions, and shows low false-positive verification rates.


<details>
  <summary>Details</summary>
Motivation: Answer-only grading overstates capability and is vulnerable to lucky guesses or format mismatches. Reliable benchmarking of Large Reasoning Models on math requires solution-path verification and the ability to produce formally checkable proofs.

Method: Design a multi-agent pipeline (3 AI agents) that prompts models to produce solutions in a specific, verification-friendly format, enabling automatic and interactive verification via a proof assistant (Lean 4). The pipeline can also convert/produce both informal and formal solutions and is configurable for different benchmarks; code and server setup are open-sourced.

Result: On several datasets, the pipeline demonstrates a low probability of false positives (i.e., incorrect solutions being accepted as correct). It also shows that small models (≤8B) can participate effectively when guided by the specified prompting and verification workflow.

Conclusion: Verification-centered benchmarking is a more accurate alternative to answer-only checks. The proposed Lean4-based, multi-agent, prompt-structured pipeline provides practical, low-FP evaluation and solution generation, and is available as open-source infrastructure for the community.

Abstract: With the growing popularity of Large Reasoning Models and their results in solving mathematical problems, it becomes crucial to measure their capabilities. We introduce a pipeline for both automatic and interactive verification as a more accurate alternative to only checking the answer which is currently the most popular approach for benchmarks. The pipeline can also be used as a generator of correct solutions both in formal and informal languages. 3 AI agents, which can be chosen for the benchmark accordingly, are included in the structure. The key idea is the use of prompts to obtain the solution in the specific form which allows for easier verification using proof assistants and possible use of small models ($\le 8B$). Experiments on several datasets suggest low probability of False Positives. The open-source implementation with instructions on setting up a server is available at https://github.com/LogicEnj/lean4_verification_pipeline.

</details>


### [127] [POMDPPlanners: Open-Source Package for POMDP Planning](https://arxiv.org/abs/2602.20810)
*Yaacov Pariente,Vadim Indelman*

Main category: cs.AI

TL;DR: POMDPPlanners is an open-source Python toolkit that streamlines scalable, reproducible empirical evaluation of POMDP planning—especially in risk-sensitive settings—by bundling state-of-the-art planners, safety-critical benchmarks, hyperparameter optimization, caching, and parallel simulation.


<details>
  <summary>Details</summary>
Motivation: Empirical research on decision-making under uncertainty lacks a unified, scalable, and reproducible toolkit—particularly for risk-sensitive POMDP planning—making large simulation studies cumbersome and inconsistent across works.

Method: Implement a Python package that integrates modern POMDP planning algorithms, a suite of benchmark environments with safety-critical variants, automated hyperparameter tuning via Optuna, persistent caching with failure recovery, and configurable parallel simulation to reduce computational and engineering overhead.

Result: The package consolidates the evaluation pipeline, reduces the cost and fragility of extensive simulations, and supports systematic comparison of planners under both standard and safety-critical conditions with automated tuning.

Conclusion: By providing an end-to-end, reproducible, and scalable evaluation framework, POMDPPlanners addresses gaps in existing toolkits and facilitates robust research on risk-sensitive decision-making under uncertainty.

Abstract: We present POMDPPlanners, an open-source Python package for empirical evaluation of Partially Observable Markov Decision Process (POMDP) planning algorithms. The package integrates state-of-the-art planning algorithms, a suite of benchmark environments with safety-critical variants, automated hyperparameter optimization via Optuna, persistent caching with failure recovery, and configurable parallel simulation -- reducing the overhead of extensive simulation studies. POMDPPlanners is designed to enable scalable, reproducible research on decision-making under uncertainty, with particular emphasis on risk-sensitive settings where standard toolkits fall short.

</details>


### [128] [Qwen-BIM: developing large language model for BIM-based design with domain-specific benchmark and dataset](https://arxiv.org/abs/2602.20812)
*Jia-Rui Lin,Yun-Hong Cai,Xiang-Rui Ni,Shaojie Zhou,Peng Pan*

Main category: cs.AI

TL;DR: Introduces the first domain-specific LLM for BIM-based design by building a comprehensive benchmark, generating BIM-derived textual datasets, and fine-tuning a 14B model (Qwen-BIM) that gains 21% in G-Eval over its base and matches much larger general LLMs on BIM tasks.


<details>
  <summary>Details</summary>
Motivation: BIM-driven intelligent construction needs LLM support, but a lack of domain datasets and standardized evaluation benchmarks prevents reliable performance assessment and effective adaptation of general LLMs to BIM-specific tasks.

Method: 1) Design a BIM-focused evaluation benchmark with quantitative indicators; 2) develop a pipeline to convert BIM artifacts into textual data to assemble evaluation and fine-tuning datasets; 3) apply a tailored fine-tuning strategy to adapt a general LLM into a domain model (Qwen-BIM).

Result: The benchmark reveals general-purpose LLMs are inadequate for BIM-specific tasks. Using the new datasets and fine-tuning strategy, Qwen-BIM (14B) achieves a 21.0% average G-Eval improvement over the base model and performance comparable to 671B-parameter general LLMs on BIM design tasks.

Conclusion: Domain-specific benchmarks plus BIM-derived datasets enable effective adaptation of LLMs to BIM. Qwen-BIM demonstrates strong efficiency–performance trade-offs, providing a solid foundation for future BIM-oriented LLM development and indicating general LLMs are insufficient without specialization.

Abstract: As the construction industry advances toward digital transformation, BIM (Building Information Modeling)-based design has become a key driver supporting intelligent construction. Despite Large Language Models (LLMs) have shown potential in promoting BIM-based design, the lack of specific datasets and LLM evaluation benchmarks has significantly hindered the performance of LLMs. Therefore, this paper addresses this gap by proposing: 1) an evaluation benchmark for BIM-based design together with corresponding quantitative indicators to evaluate the performance of LLMs, 2) a method for generating textual data from BIM and constructing corresponding BIM-derived datasets for LLM evaluation and fine-tuning, and 3) a fine-tuning strategy to adapt LLMs for BIM-based design. Results demonstrate that the proposed domain-specific benchmark effectively and comprehensively assesses LLM capabilities, highlighting that general LLMs are still incompetent for domain-specific tasks. Meanwhile, with the proposed benchmark and datasets, Qwen-BIM is developed and achieves a 21.0% average increase in G-Eval score compared to the base LLM model. Notably, with only 14B parameters, performance of Qwen-BIM is comparable to that of general LLMs with 671B parameters for BIM-based design tasks. Overall, this study develops the first domain-specific LLM for BIM-based design by introducing a comprehensive benchmark and high-quality dataset, which provide a solid foundation for developing BIM-related LLMs in various fields.

</details>


### [129] [Pressure Reveals Character: Behavioural Alignment Evaluation at Depth](https://arxiv.org/abs/2602.20813)
*Nora Petrova,John Burden*

Main category: cs.AI

TL;DR: They introduce a realistic, multi-turn alignment benchmark (904 scenarios across six categories) and use LLM judges, calibrated to human annotations, to score 24 frontier models. Results show widespread alignment gaps, a latent “g-like” alignment factor, and public release of the benchmark and leaderboard.


<details>
  <summary>Details</summary>
Motivation: Single-turn or self-report style evaluations miss real-world failure modes. Alignment harms are growing, yet comprehensive, realistic, multi-turn benchmarks are scarce. The authors aim to pressure-test models under conditions that provoke misalignment.

Method: Construct 904 scenarios spanning Honesty, Safety, Non-Manipulation, Robustness, Corrigibility, and Scheming. Scenarios include conflicting instructions, simulated tool use, and escalating multi-turn interactions. Human raters validate realism; LLM judges (validated against human labels) score 24 models. Factor analysis examines whether alignment behaves as a unified construct.

Result: Even top models show category-specific gaps; most models display broad weaknesses. Scores are positively correlated across categories, suggesting a single latent alignment factor. A public benchmark and interactive leaderboard are released.

Conclusion: Current models are not reliably aligned under realistic, multi-turn pressure. Alignment appears to be a coherent, general trait, implying that improvements may transfer across categories. The benchmark enables ongoing tracking and targeted improvement, with plans to expand weak areas and add new models.

Abstract: Evaluating alignment in language models requires testing how they behave under realistic pressure, not just what they claim they would do. While alignment failures increasingly cause real-world harm, comprehensive evaluation frameworks with realistic multi-turn scenarios remain lacking. We introduce an alignment benchmark spanning 904 scenarios across six categories -- Honesty, Safety, Non-Manipulation, Robustness, Corrigibility, and Scheming -- validated as realistic by human raters. Our scenarios place models under conflicting instructions, simulated tool access, and multi-turn escalation to reveal behavioural tendencies that single-turn evaluations miss. Evaluating 24 frontier models using LLM judges validated against human annotations, we find that even top-performing models exhibit gaps in specific categories, while the majority of models show consistent weaknesses across the board. Factor analysis reveals that alignment behaves as a unified construct (analogous to the g-factor in cognitive research) with models scoring high on one category tending to score high on others. We publicly release the benchmark and an interactive leaderboard to support ongoing evaluation, with plans to expand scenarios in areas where we observe persistent weaknesses and to add new models as they are released.

</details>


### [130] [Diagnosing Causal Reasoning in Vision-Language Models via Structured Relevance Graphs](https://arxiv.org/abs/2602.20878)
*Dhita Putri Pratama,Soyeon Caren Han,Yihao Ding*

Main category: cs.AI

TL;DR: They introduce a causal-graph representation and a diagnostic benchmark to separate ‘finding the right evidence’ from ‘reasoning with it’ in LVLMs, showing that structured guidance markedly boosts causal attribution and inference.


<details>
  <summary>Details</summary>
Motivation: LVLMs often exploit spurious correlations, and current evaluations focus on answer correctness, obscuring whether failures come from weak reasoning or from misidentifying causally relevant information.

Method: Propose Vision-Language Causal Graphs (VLCGs), a query-conditioned structure encoding relevant objects, attributes, relations, and scene assumptions; build ViLCaR, a benchmark with Causal Attribution, Causal Inference, and QA tasks plus graph-aligned metrics; test SOTA LVLMs and examine the effect of injecting structured relevance information versus zero-shot and standard in-context learning.

Result: Providing models with structured relevance (via VLCGs) yields significant gains in attribution and inference consistency beyond what zero-shot or standard ICL achieve, as captured by graph-aligned metrics rather than only final-answer accuracy.

Conclusion: Current LVLM shortcomings in causal reasoning arise chiefly from insufficient structural guidance; explicit causal structure helps models identify relevant evidence and apply reasoning they already possess, improving reliability and interpretability.

Abstract: Large Vision-Language Models (LVLMs) achieve strong performance on visual question answering benchmarks, yet often rely on spurious correlations rather than genuine causal reasoning. Existing evaluations primarily assess the correctness of the answers, making it unclear whether failures arise from limited reasoning capability or from misidentifying causally relevant information. We introduce Vision-Language Causal Graphs (VLCGs), a structured, query-conditioned representation that explicitly encodes causally relevant objects, attributes, relations, and scene-grounded assumptions. Building on this representation, we present ViLCaR, a diagnostic benchmark comprising tasks for Causal Attribution, Causal Inference, and Question Answering, along with graph-aligned evaluation metrics that assess relevance identification beyond final answer accuracy. Experiments in state-of-the-art LVLMs show that injecting structured relevance information significantly improves attribution and inference consistency compared to zero-shot and standard in-context learning. These findings suggest that current limitations in LVLM causal reasoning stem primarily from insufficient structural guidance rather than a lack of reasoning capacity.

</details>


### [131] [Predicting Sentence Acceptability Judgments in Multimodal Contexts](https://arxiv.org/abs/2602.20918)
*Hyewon Jang,Nikolai Ilinykh,Sharid Loáiciga,Jey Han Lau,Shalom Lappin*

Main category: cs.AI

TL;DR: Humans’ sentence acceptability judgments are largely unaffected by prior visual context, whereas LLMs’ judgments shift and compress with visuals, slightly reducing their agreement with humans; model behavior varies (Qwen most human-like), and the link between acceptability scores and log-probabilities weakens in visual settings.


<details>
  <summary>Details</summary>
Motivation: Test whether non-linguistic visual context influences sentence acceptability judgments and whether LLMs align with humans in multimodal conditions, extending prior work where textual context impacted judgments.

Method: Collect human acceptability ratings for sentences with/without prior exposure to images; evaluate multiple LLMs on the same items under visual vs no-visual conditions; assess accuracy vs human ratings, distributional compression, inter-model differences, and correlations between LLM acceptability predictions and normalized log-probabilities.

Result: Human ratings show little to no effect of visual images (unlike known effects of textual context). LLMs exhibit compression effects, predict human judgments well but perform slightly better without visuals; model distributions differ (Qwen closest to humans). LLM acceptability scores correlate strongly with normalized log-probabilities overall, but correlations drop when visual context is present.

Conclusion: Human grammaticality judgments appear robust to incidental visual context, while LLMs are perturbed by it, indicating different multimodal integration. Visual context increases the gap between LLM internal representations and overt predictions; there are both parallels and divergences between human and LLM processing in multimodal settings.

Abstract: Previous work has examined the capacity of deep neural networks (DNNs), particularly transformers, to predict human sentence acceptability judgments, both independently of context, and in document contexts. We consider the effect of prior exposure to visual images (i.e., visual context) on these judgments for humans and large language models (LLMs). Our results suggest that, in contrast to textual context, visual images appear to have little if any impact on human acceptability ratings. However, LLMs display the compression effect seen in previous work on human judgments in document contexts. Different sorts of LLMs are able to predict human acceptability judgments to a high degree of accuracy, but in general, their performance is slightly better when visual contexts are removed. Moreover, the distribution of LLM judgments varies among models, with Qwen resembling human patterns, and others diverging from them. LLM-generated predictions on sentence acceptability are highly correlated with their normalised log probabilities in general. However, the correlations decrease when visual contexts are present, suggesting that a higher gap exists between the internal representations of LLMs and their generated predictions in the presence of visual contexts. Our experimental work suggests interesting points of similarity and of difference between human and LLM processing of sentences in multimodal contexts.

</details>


### [132] [HELP: HyperNode Expansion and Logical Path-Guided Evidence Localization for Accurate and Efficient GraphRAG](https://arxiv.org/abs/2602.20926)
*Yuqi Huang,Ning Liao,Kai Yang,Anning Hu,Shengchao Hu,Xiaoxing Wang,Junchi Yan*

Main category: cs.AI

TL;DR: HELP is a GraphRAG framework that chains knowledge triplets into hypernode reasoning paths and maps them directly to source text via precomputed graph–text links, delivering competitive QA accuracy with up to 28.8× speedup by avoiding random walks and LLM summary noise.


<details>
  <summary>Details</summary>
Motivation: LLMs hallucinate and lack complete knowledge; classic RAG overlooks structural dependencies crucial for multi-hop reasoning. Existing graph-based RAG methods improve structure but are slowed by costly traversals and suffer from semantic drift from LLM-generated summaries. A method is needed that preserves structural reasoning while remaining efficient and faithful to sources.

Method: Two core strategies: (1) HyperNode Expansion—iteratively link knowledge triplets into coherent multi-hop reasoning paths and abstract them as HyperNodes to capture complex dependencies and tighten retrieval focus; (2) Logical Path-Guided Evidence Localization—use precomputed graph–text correlations to project these paths directly to the corpus, avoiding random walks and minimizing semantic distortion from summaries, thereby reducing latency.

Result: Across multiple simple and multi-hop QA benchmarks, HELP attains competitive accuracy and achieves up to a 28.8× speedup over leading GraphRAG baselines, indicating substantially lower retrieval latency without notable accuracy loss.

Conclusion: Structuring reasoning as hypernodes and aligning them to text via precomputed mappings balances accuracy and efficiency in graph-based RAG, preserving knowledge integrity and offering practical gains for multi-hop QA workloads.

Abstract: Large Language Models (LLMs) often struggle with inherent knowledge boundaries and hallucinations, limiting their reliability in knowledge-intensive tasks. While Retrieval-Augmented Generation (RAG) mitigates these issues, it frequently overlooks structural interdependencies essential for multi-hop reasoning. Graph-based RAG approaches attempt to bridge this gap, yet they typically face trade-offs between accuracy and efficiency due to challenges such as costly graph traversals and semantic noise in LLM-generated summaries. In this paper, we propose HyperNode Expansion and Logical Path-Guided Evidence Localization strategies for GraphRAG (HELP), a novel framework designed to balance accuracy with practical efficiency through two core strategies: 1) HyperNode Expansion, which iteratively chains knowledge triplets into coherent reasoning paths abstracted as HyperNodes to capture complex structural dependencies and ensure retrieval accuracy; and 2) Logical Path-Guided Evidence Localization, which leverages precomputed graph-text correlations to map these paths directly to the corpus for superior efficiency. HELP avoids expensive random walks and semantic distortion, preserving knowledge integrity while drastically reducing retrieval latency. Extensive experiments demonstrate that HELP achieves competitive performance across multiple simple and multi-hop QA benchmarks and up to a 28.8$\times$ speedup over leading Graph-based RAG baselines.

</details>


### [133] [Architecting AgentOS: From Token-Level Context to Emergent System-Level Intelligence](https://arxiv.org/abs/2602.20934)
*ChengYou Li,XiaoDong Liu,XiangBao Meng,XinYu Zhao*

Main category: cs.AI

TL;DR: Proposes AgentOS, a conceptual system that reframes an LLM as a reasoning kernel within an OS-like architecture, introducing deep context management and OS-inspired mechanisms to coordinate multi-agent cognition and mitigate drift.


<details>
  <summary>Details</summary>
Motivation: LLMs are shifting from single-shot predictors to autonomous systems, but there is a gap between token-level processing and system-level intelligence. Existing work (longer contexts, prompt tricks) doesn’t provide a principled architecture for coordination, memory, and stability across agents and time.

Method: Define AgentOS: an OS-style framework where the LLM is a Reasoning Kernel. Introduce Deep Context Management that treats the context window as an Addressable Semantic Space. Provide mechanisms—Semantic Slicing and Temporal Alignment—to structure and synchronize context; map OS abstractions (memory paging, interrupts, scheduling) onto LLM-native constructs for multi-agent orchestration.

Result: Presents a theoretical roadmap and design abstractions rather than empirical results. Claims mitigation of cognitive drift, improved resilience, scalability, and self-evolution in cognitive environments via structured coordination and context control.

Conclusion: Progress toward AGI depends less on raw model scale and more on architectural efficiency of system-level coordination. AgentOS and deep context management offer a principled path to building robust, scalable, multi-agent cognitive systems.

Abstract: The paradigm of Large Language Models is undergoing a fundamental transition from static inference engines to dynamic autonomous cognitive systems.While current research primarily focuses on scaling context windows or optimizing prompt engineering the theoretical bridge between micro scale token processing and macro scale systemic intelligence remains fragmented.This paper proposes AgentOS,a holistic conceptual framework that redefines the LLM as a "Reasoning Kernel" governed by structured operating system logic.Central to this architecture is Deep Context Management which conceptualizes the context window as an Addressable Semantic Space rather than a passive buffer.We systematically deconstruct the transition from discrete sequences to coherent cognitive states introducing mechanisms for Semantic Slicing and Temporal Alignment to mitigate cognitive drift in multi-agent orchestration.By mapping classical OS abstractions such as memory paging interrupt handling and process scheduling onto LLM native constructs, this review provides a rigorous roadmap for architecting resilient scalable and self-evolving cognitive environments.Our analysis asserts that the next frontier of AGI development lies in the architectural efficiency of system-level coordination.

</details>


### [134] [LogicGraph : Benchmarking Multi-Path Logical Reasoning via Neuro-Symbolic Generation and Verification](https://arxiv.org/abs/2602.21044)
*Yanrui Wu,Lingling Zhang,Xinyu Zhang,Jiayu Chang,Pengyu Li,Xu Jiang,Jingtao Hu,Jun Liu*

Main category: cs.AI

TL;DR: LogicGraph is a new benchmark and evaluation framework for multi-path logical reasoning, generating solver-verified problems with exhaustive minimal proofs to test whether LLMs can explore multiple valid derivations rather than fixating on a single path. SOTA LLMs underperform on this divergent reasoning, with the gap worsening at greater depth.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations emphasize convergent reasoning (one correct proof), overlooking real-world cases with multiple valid derivations. There is no systematic way to assess whether models can explore diverse logical paths, creating a blind spot in measuring true reasoning capability.

Method: A neuro-symbolic pipeline using backward logic generation and semantic instantiation constructs high-depth, multi-path problems with built-in logical distractions. Each instance is solver-verified and paired with an exhaustive set of minimal proofs. A reference-free evaluation framework assesses both convergent and divergent reasoning performance.

Result: Across state-of-the-art LLMs, models tend to commit early to a single derivation and fail to explore alternatives; the coverage gap between possible proofs and discovered proofs grows substantially with reasoning depth.

Conclusion: LogicGraph surfaces a systematic divergence gap in LLM reasoning and offers a rigorous, reference-free way to evaluate and improve multi-path reasoning. The benchmark and code/data are released to catalyze progress.

Abstract: Evaluations of large language models (LLMs) primarily emphasize convergent logical reasoning, where success is defined by producing a single correct proof. However, many real-world reasoning problems admit multiple valid derivations, requiring models to explore diverse logical paths rather than committing to one route. To address this limitation, we introduce LogicGraph, the first benchmark aimed to systematically evaluate multi-path logical reasoning, constructed via a neuro-symbolic framework that leverages backward logic generation and semantic instantiation. This pipeline yields solver-verified reasoning problems formalized by high-depth multi-path reasoning and inherent logical distractions, where each instance is associated with an exhaustive set of minimal proofs. We further propose a reference-free evaluation framework to rigorously assess model performance in both convergent and divergent regimes. Experiments on state-of-the-art language models reveal a common limitation: models tend to commit early to a single route and fail to explore alternatives, and the coverage gap grows substantially with reasoning depth. LogicGraph exposes this divergence gap and provides actionable insights to motivate future improvements. Our code and data will be released at https://github.com/kkkkarry/LogicGraph.

</details>


### [135] [Tool Building as a Path to "Superintelligence"](https://arxiv.org/abs/2602.21061)
*David Koplow,Tomer Galanti,Tomaso Poggio*

Main category: cs.AI

TL;DR: They introduce a benchmark to estimate the per-step success probability (γ) needed by the Diligent Learner framework, using GF(2) circuit-reconstruction tasks that force full information integration. Small LLMs’ γ collapses superlinearly with depth, while frontier models show partial robustness; precise tool use emerges as crucial, making tool design a key lever for scalable reasoning toward superintelligence.


<details>
  <summary>Details</summary>
Motivation: Test whether LLMs have the per-step reliability required for test-time search to scale (per the Diligent Learner framework), particularly on out-of-distribution logical inference that demands integrating all provided information. Provide an empirical measure of γ and surface the main bottlenecks to scaling reasoning.

Method: Construct a family of GF(2) circuit reconstruction problems whose difficulty increases with reasoning depth and which are information-theoretically unsolvable unless the model carefully aggregates all clues. Measure stepwise success probabilities across model sizes and depths, and evaluate the role of tool-augmented reasoning by analyzing the precision of tool calls.

Result: For smaller LLMs, the per-step success probability γ declines superlinearly with increased depth; frontier models retain higher γ and display partial robustness. Successful large-scale reasoning is highly sensitive to the accuracy of tool invocations.

Conclusion: Most current LLMs—especially smaller ones—do not maintain the reliability needed for deep test-time search; top-tier models are closer but still fragile. Reaching Diligent Learner-style superintelligence will likely hinge on advances in tool design and controlling precise tool use to preserve high γ across many reasoning steps.

Abstract: The Diligent Learner framework suggests LLMs can achieve superintelligence via test-time search, provided a sufficient step-success probability $γ$. In this work, we design a benchmark to measure $γ$ on logical out-of-distribution inference. We construct a class of tasks involving GF(2) circuit reconstruction that grow more difficult with each reasoning step, and that are, from an information-theoretic standpoint, impossible to reliably solve unless the LLM carefully integrates all of the information provided. Our analysis demonstrates that while the $γ$ value for small LLMs declines superlinearly as depth increases, frontier models exhibit partial robustness on this task. Furthermore, we find that successful reasoning at scale is contingent upon precise tool calls, identifying tool design as a critical capability for LLMs to achieve general superintelligence through the Diligent Learner framework.

</details>


### [136] [Motivation is Something You Need](https://arxiv.org/abs/2602.21064)
*Mehdi Acheli,Walid Gaaloul*

Main category: cs.AI

TL;DR: A neuroscience-inspired dual-model training scheme keeps a small model learning continuously and intermittently “activates” a larger, shared-weight extension during selected high-value steps, boosting both efficiency and accuracy versus standard training.


<details>
  <summary>Details</summary>
Motivation: Leverage affective neuroscience—specifically the SEEKING (curiosity/anticipation) state—to recruit extra computational capacity only when it’s most beneficial, improving sample/compute efficiency and enabling simultaneous optimization of models for different deployment constraints.

Method: Use scalable architectures where a large model extends a small base model; train the small model continuously while intermittently activating the larger “motivated” model during predefined motivation conditions. Share weights to allow selective capacity expansion and joint updates during noteworthy training steps.

Result: On image classification, the alternating scheme improves the small base model over traditional training; in some cases, the intermittently trained large model outperforms a standalone large model despite seeing less data per epoch.

Conclusion: Neuroscience-inspired alternating activation can co-train small and large models with competitive or superior performance at lower cost than training the large model alone, suggesting an efficient path to multi-capacity model development.

Abstract: This work introduces a novel training paradigm that draws from affective neuroscience. Inspired by the interplay of emotions and cognition in the human brain and more specifically the SEEKING motivational state, we design a dual-model framework where a smaller base model is trained continuously, while a larger motivated model is activated intermittently during predefined "motivation conditions". The framework mimics the emotional state of high curiosity and anticipation of reward in which broader brain regions are recruited to enhance cognitive performance. Exploiting scalable architectures where larger models extend smaller ones, our method enables shared weight updates and selective expansion of network capacity during noteworthy training steps. Empirical evaluation on the image classification task demonstrates that, not only does the alternating training scheme efficiently and effectively enhance the base model compared to a traditional scheme, in some cases, the motivational model also surpasses its standalone counterpart despite seeing less data per epoch. This opens the possibility of simultaneously training two models tailored to different deployment constraints with competitive or superior performance while keeping training cost lower than when training the larger model.

</details>


### [137] [The Initial Exploration Problem in Knowledge Graph Exploration](https://arxiv.org/abs/2602.21066)
*Claire McNamara,Lucy Hederman,Declan O'Sullivan*

Main category: cs.AI

TL;DR: Defines the Initial Exploration Problem (IEP) for Knowledge Graphs: at first contact, lay users face scope uncertainty, ontology opacity, and query incapacity; current interfaces assume prior knowledge, so new scope-revealing interaction primitives and entry-point scaffolding are needed.


<details>
  <summary>Details</summary>
Motivation: Lay users struggle to orient themselves in unfamiliar, semantically rich KGs—they don’t know what’s in the graph, how it’s modeled, or what to ask—leading to high cognitive load and stalled exploration. The paper aims to explain this gap and guide interface designs that lower the barrier to initial engagement.

Method: Conceptual/theoretical synthesis drawing on ASK, exploratory search, information foraging, and cognitive load theory; analysis of existing KG exploration interfaces at the level of interaction primitives and their embedded epistemic assumptions.

Result: A formal framing of the IEP as three interdependent barriers—scope uncertainty, ontology opacity, and query incapacity—that co-occur at first contact; evidence that many KG interfaces presuppose an initial goal or modeling knowledge; identification of a design gap: missing primitives for scope revelation that communicate contents without requiring queries or ontology parsing.

Conclusion: The IEP provides a lens for evaluating KG interfaces and motivates entry-point scaffolding. Designers should add scope-revelation mechanisms and primitives that surface what the KG contains and how it’s structured, enabling users to begin exploration without prior semantic-web expertise.

Abstract: Knowledge Graphs (KGs) enable the integration and representation of complex information across domains, but their semantic richness and structural complexity create substantial barriers for lay users without expertise in semantic web technologies. When encountering an unfamiliar KG, such users face a distinct orientation challenge: they do not know what questions are possible, how the knowledge is structured, or how to begin exploration. This paper identifies and theorises this phenomenon as the Initial Exploration Problem (IEP). Drawing on theories from information behaviour and human-computer interaction, including ASK, exploratory search, information foraging, and cognitive load theory, we develop a conceptual framing of the IEP characterised by three interdependent barriers: scope uncertainty, ontology opacity, and query incapacity. We argue that these barriers converge at the moment of first contact, distinguishing the IEP from related concepts that presuppose an existing starting point or information goal. Analysing KG exploration interfaces at the level of interaction primitives, we suggest that many systems rely on epistemic assumptions that do not hold at first contact. This reveals a structural gap in the design space: the absence of interaction primitives for scope revelation, mechanisms that communicate what a KG contains without requiring users to formulate queries or interpret ontological structures. In articulating the IEP, this paper provides a theoretical lens for evaluating KG interfaces and for designing entry-point scaffolding that supports initial exploration.

</details>


### [138] [A Benchmark for Deep Information Synthesis](https://arxiv.org/abs/2602.21143)
*Debjit Paul,Daniel Murphy,Milan Gritta,Ronald Cardenas,Victor Prokhorov,Lena Sophia Bolliger,Aysim Toker,Roy Miles,Andreea-Maria Oncescu,Jasivan Alex Sivakumar,Philipp Borchert,Ismail Elezi,Meiru Zhang,Ka Yiu Lee,Guchun Zhang,Jun Wang,Gerasimos Lampouras*

Main category: cs.AI

TL;DR: DEEPSYNTH is a challenging benchmark for LLM-based agents that tests multi-source information gathering, synthesis, and structured reasoning; state-of-the-art systems perform poorly, exposing hallucination and large-search-space reasoning weaknesses.


<details>
  <summary>Details</summary>
Motivation: Most existing evaluations focus on simple fact retrieval or narrow tool-use rather than realistic, multi-step tasks that require integrating evidence from diverse sources to derive new insights. The authors aim to fill this gap with a benchmark that reflects real-world, time-consuming analytical workflows.

Method: They build DEEPSYNTH: 120 tasks spanning 7 domains and data sources across 67 countries. A multi-stage pipeline has annotators collect official data, form hypotheses, conduct manual analyses, and design tasks with verifiable answers. They evaluate 11 leading LLMs and deep research agents using F1 and an LLM-judge metric.

Result: Even the best systems achieve only 8.97 F1 and 17.5 on the LLM-judge metric. Error analysis highlights frequent hallucinations and difficulty reasoning over large information spaces.

Conclusion: DEEPSYNTH is a difficult, realistic benchmark that reveals current agents’ limitations and can guide research toward more reliable synthesis, planning, retrieval, and verification to reduce hallucinations and improve reasoning at scale.

Abstract: Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval. To address this, we introduce DEEPSYNTH, a novel benchmark designed to evaluate agents on realistic, time-consuming problems that combine information gathering, synthesis, and structured reasoning to produce insights. DEEPSYNTH contains 120 tasks collected across 7 domains and data sources covering 67 countries. DEEPSYNTH is constructed using a multi-stage data collection pipeline that requires annotators to collect official data sources, create hypotheses, perform manual analysis, and design tasks with verifiable answers. When evaluated on DEEPSYNTH, 11 state-of-the-art LLMs and deep research agents achieve a maximum F1 score of 8.97 and 17.5 on the LLM-judge metric, underscoring the difficulty of the benchmark. Our analysis reveals that current agents struggle with hallucinations and reasoning over large information spaces, highlighting DEEPSYNTH as a crucial benchmark for guiding future research.

</details>


### [139] [CG-DMER: Hybrid Contrastive-Generative Framework for Disentangled Multimodal ECG Representation Learning](https://arxiv.org/abs/2602.21154)
*Ziwei Niu,Hao Sun,Shujun Bian,Xihong Yang,Lanfen Lin,Yuxin Liu,Yueming Jin*

Main category: cs.AI

TL;DR: CG-DMER is a contrastive–generative framework that learns disentangled multimodal ECG representations via spatiotemporal masked modeling and shared/specific encoders, capturing inter-lead dynamics while reducing free-text bias; it attains state-of-the-art results on three public datasets.


<details>
  <summary>Details</summary>
Motivation: ECG diagnosis benefits from multimodal learning with clinical reports, but current models (1) treat ECG leads agnostically, missing fine-grained spatial–temporal dependencies, and (2) directly align signals with free-text reports, introducing modality-specific noise and bias. The goal is to capture inter-lead temporal–spatial patterns and align modalities without inheriting text-induced biases.

Method: Proposes CG-DMER with two core components: (1) Spatial–temporal masked modeling that masks both across leads (spatial) and time, then reconstructs missing segments to learn fine-grained inter-lead dependencies and temporal dynamics. (2) Representation disentanglement and alignment using modality-shared and modality-specific encoders, guided by contrastive and generative objectives to separate modality-invariant information from modality-specific content, mitigating noise/bias during multimodal fusion.

Result: Across three public ECG datasets, CG-DMER delivers state-of-the-art performance on various downstream tasks, outperforming prior multimodal ECG methods (no specific metrics reported in the abstract).

Conclusion: Disentangling shared vs. specific representations and employing spatiotemporal masked pretraining yields more robust, fine-grained ECG features and less text-induced bias, leading to superior generalization and SOTA performance on multiple datasets.

Abstract: Accurate interpretation of electrocardiogram (ECG) signals is crucial for diagnosing cardiovascular diseases. Recent multimodal approaches that integrate ECGs with accompanying clinical reports show strong potential, but they still face two main concerns from a modality perspective: (1) intra-modality: existing models process ECGs in a lead-agnostic manner, overlooking spatial-temporal dependencies across leads, which restricts their effectiveness in modeling fine-grained diagnostic patterns; (2) inter-modality: existing methods directly align ECG signals with clinical reports, introducing modality-specific biases due to the free-text nature of the reports. In light of these two issues, we propose CG-DMER, a contrastive-generative framework for disentangled multimodal ECG representation learning, powered by two key designs: (1) Spatial-temporal masked modeling is designed to better capture fine-grained temporal dynamics and inter-lead spatial dependencies by applying masking across both spatial and temporal dimensions and reconstructing the missing information. (2) A representation disentanglement and alignment strategy is designed to mitigate unnecessary noise and modality-specific biases by introducing modality-specific and modality-shared encoders, ensuring a clearer separation between modality-invariant and modality-specific representations. Experiments on three public datasets demonstrate that CG-DMER achieves state-of-the-art performance across diverse downstream tasks.

</details>


### [140] [NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning](https://arxiv.org/abs/2602.21172)
*Ishaan Rawal,Shubh Gupta,Yihan Hu,Wei Zhan*

Main category: cs.AI

TL;DR: They introduce a data- and token-efficient Vision-Language-Action driving model that forgoes explicit reasoning annotations and uses a difficulty-bias–aware RL algorithm (Dr. GRPO) to reach competitive results on Waymo and NAVSIM with <60% of the data and ~3× fewer tokens.


<details>
  <summary>Details</summary>
Motivation: End-to-end VLA driving systems typically require huge datasets and costly dense reasoning (chain-of-thought) annotations. The authors aim to reduce both data and annotation burdens while maintaining performance, making autonomous driving training more scalable and efficient.

Method: Propose a VLA model called No Reasoning for Driving (NRD) that eliminates explicit reasoning outputs/annotations, thereby reducing token budget. Identify that standard GRPO underperforms on small, reasoning-free datasets due to difficulty bias from high-variance rollouts. Replace GRPO with Dr. GRPO, an algorithm designed to mitigate difficulty bias in LLM training, adapting it to the driving policy optimization setting.

Result: NRD matches or approaches state-of-the-art VLA performance on Waymo and NAVSIM benchmarks while being fine-tuned on under 60% of the data and using ~3× fewer tokens. Empirically, standard GRPO provides little benefit in this regime, whereas Dr. GRPO yields substantive improvements.

Conclusion: Explicit reasoning is not necessary to achieve competitive end-to-end autonomous driving when training is made difficulty-bias–robust. By combining a reasoning-free VLA with Dr. GRPO, the approach reduces data and token requirements without sacrificing performance, improving the practicality and efficiency of autonomous driving systems.

Abstract: Vision-Language-Action (VLA) models are advancing autonomous driving by replacing modular pipelines with unified end-to-end architectures. However, current VLAs face two expensive requirements: (1) massive dataset collection, and (2) dense reasoning annotations. In this work, we address both challenges with \modelname (\textbf{No} \textbf{R}easoning for \textbf{D}riving). Compared to existing VLAs, \modelname achieves competitive performance while being fine-tuned on $<$60\% of the data and no reasoning annotations, resulting in 3$\times$ fewer tokens. We identify that standard Group Relative Policy Optimization (GRPO) fails to yield significant improvements when applied to policies trained on such small, reasoning-free datasets. We show that this limitation stems from difficulty bias, which disproportionately penalizes reward signals from scenarios that produce high-variance rollouts within GRPO. \modelname overcomes this by incorporating Dr.~GRPO, a recent algorithm designed to mitigate difficulty bias in LLMs. As a result, \modelname achieves competitive performance on Waymo and NAVSIM with a fraction of the training data and no reasoning overhead, enabling more efficient autonomous systems.

</details>
