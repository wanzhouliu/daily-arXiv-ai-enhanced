{"id": "2602.21268", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21268", "abs": "https://arxiv.org/abs/2602.21268", "authors": ["Takaaki Fujita", "Florentin Smarandache"], "title": "A Dynamic Survey of Soft Set Theory and Its Extensions", "comment": "Book.143 pages. Publisher: Neutrosophic Science International Association (NSIA) Publishing House. ISBN: 978-1-59973-859-8", "summary": "Soft set theory provides a direct framework for parameterized decision modeling by assigning to each attribute (parameter) a subset of a given universe, thereby representing uncertainty in a structured way [1, 2]. Over the past decades, the theory has expanded into numerous variants-including hypersoft sets, superhypersoft sets, TreeSoft sets, bipolar soft sets, and dynamic soft sets-and has been connected to diverse areas such as topology and matroid theory. In this book, we present a survey-style overview of soft sets and their major extensions, highlighting core definitions, representative constructions, and key directions of current development.", "AI": {"tldr": "Survey book on soft set theory: formal overview of soft sets and many extensions (e.g., hypersoft, superhypersoft, TreeSoft, bipolar, dynamic), with links to areas like topology and matroid theory, and a map of current directions.", "motivation": "Parameter-driven decision-making under uncertainty needs a structured framework; the literature on soft sets has become vast and fragmented across variants and applications, warranting a consolidated, systematic treatment.", "method": "Survey/synthesis: collate and formalize core definitions, present representative constructions across major variants, and summarize connections to other mathematical domains; outline current developments and trends.", "result": "A curated taxonomy of soft set variants with standardized definitions and constructions; contextualization within related fields (topology, matroids); identification of key themes and development lines. No specific new theorems are claimed in the abstract.", "conclusion": "Soft set theory offers a versatile paradigm for modeling uncertainty via parameters and has diversified widely; this book organizes the landscape and points to promising research directions."}}
{"id": "2602.21351", "categories": ["cs.AI", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.21351", "abs": "https://arxiv.org/abs/2602.21351", "authors": ["Dmitrii Pantiukhin", "Ivan Kuznetsov", "Boris Shapkin", "Antonia Anna Jost", "Thomas Jung", "Nikolay Koldunov"], "title": "A Hierarchical Multi-Agent System for Autonomous Discovery in Geoscientific Data Archives", "comment": "20 pages, 6 figures, 7 tables, supplementary material included", "summary": "The rapid accumulation of Earth science data has created a significant scalability challenge; while repositories like PANGAEA host vast collections of datasets, citation metrics indicate that a substantial portion remains underutilized, limiting data reusability. Here we present PANGAEA-GPT, a hierarchical multi-agent framework designed for autonomous data discovery and analysis. Unlike standard Large Language Model (LLM) wrappers, our architecture implements a centralized Supervisor-Worker topology with strict data-type-aware routing, sandboxed deterministic code execution, and self-correction via execution feedback, enabling agents to diagnose and resolve runtime errors. Through use-case scenarios spanning physical oceanography and ecology, we demonstrate the system's capacity to execute complex, multi-step workflows with minimal human intervention. This framework provides a methodology for querying and analyzing heterogeneous repository data through coordinated agent workflows.", "AI": {"tldr": "PANGAEA-GPT introduces a hierarchical multi-agent system that autonomously discovers and analyzes heterogeneous Earth science datasets from PANGAEA, coordinating agents via a supervisor, safe code execution, and feedback-driven self-correction to run complex workflows with little human input.", "motivation": "Earth science repositories have exploded in size, but citation/use metrics show many datasets remain underutilized, creating a scalability and reusability gap in data discovery and analysis.", "method": "A centralized Supervisor-Worker multi-agent architecture with data-type-aware routing; sandboxed, deterministic code execution; and execution-feedback loops for runtime error diagnosis and self-correction. The system orchestrates multi-step analytical workflows across heterogeneous datasets.", "result": "Use-case demonstrations in physical oceanography and ecology show the framework can autonomously execute complex, multi-step analyses with minimal human intervention, handling errors through feedback to complete tasks.", "conclusion": "The framework offers a practical methodology to query and analyze diverse repository data through coordinated agent workflows, suggesting a path to improve dataset discoverability and reusability at scale."}}
{"id": "2602.21496", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21496", "abs": "https://arxiv.org/abs/2602.21496", "authors": ["Umid Suleymanov", "Zaur Rajabov", "Emil Mirzazada", "Murat Kantarcioglu"], "title": "Beyond Refusal: Probing the Limits of Agentic Self-Correction for Semantic Sensitive Information", "comment": "Under Review", "summary": "While defenses for structured PII are mature, Large Language Models (LLMs) pose a new threat: Semantic Sensitive Information (SemSI), where models infer sensitive identity attributes, generate reputation-harmful content, or hallucinate potentially wrong information. The capacity of LLMs to self-regulate these complex, context-dependent sensitive information leaks without destroying utility remains an open scientific question. To address this, we introduce SemSIEdit, an inference-time framework where an agentic \"Editor\" iteratively critiques and rewrites sensitive spans to preserve narrative flow rather than simply refusing to answer. Our analysis reveals a Privacy-Utility Pareto Frontier, where this agentic rewriting reduces leakage by 34.6% across all three SemSI categories while incurring a marginal utility loss of 9.8%. We also uncover a Scale-Dependent Safety Divergence: large reasoning models (e.g., GPT-5) achieve safety through constructive expansion (adding nuance), whereas capacity-constrained models revert to destructive truncation (deleting text). Finally, we identify a Reasoning Paradox: while inference-time reasoning increases baseline risk by enabling the model to make deeper sensitive inferences, it simultaneously empowers the defense to execute safe rewrites.", "AI": {"tldr": "SemSIEdit is an inference-time, agent-style editor that rewrites sensitive spans to curb semantic sensitive information (SemSI) leaks while preserving usefulness, achieving ~35% leakage reduction with ~10% utility loss and revealing scale- and reasoning-dependent safety dynamics.", "motivation": "Existing PII defenses don\u2019t address semantic leaks where LLMs infer identities, harm reputations, or hallucinate sensitive facts. It\u2019s unclear whether LLMs can self-regulate such context-dependent risks without sacrificing utility.", "method": "Introduce SemSIEdit: an agentic \u201cEditor\u201d that iteratively critiques outputs, identifies sensitive spans, and rewrites them to maintain narrative flow rather than refusing. Evaluate across three SemSI categories and analyze the privacy\u2013utility trade-off, model scale effects, and the role of inference-time reasoning.", "result": "Agentic rewriting reduces SemSI leakage by 34.6% across identity inference, reputation harm, and sensitive hallucinations, with only 9.8% utility loss. Larger reasoning models achieve safety via constructive expansion (adding nuance); smaller models tend to truncate destructively. Inference-time reasoning increases baseline risk but enables more effective safe rewrites.", "conclusion": "Inference-time agentic editing provides a practical path to balance privacy and utility for SemSI, but safety behavior depends on model scale and the use of reasoning. Defenders must navigate a Pareto frontier and account for a paradox where reasoning both raises risk and empowers mitigation."}}
{"id": "2602.21534", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21534", "abs": "https://arxiv.org/abs/2602.21534", "authors": ["Xiaoxuan Wang", "Han Zhang", "Haixin Wang", "Yidan Shi", "Ruoyan Li", "Kaiqiao Han", "Chenyi Tong", "Haoran Deng", "Renliang Sun", "Alexander Taylor", "Yanqiao Zhu", "Jason Cong", "Yizhou Sun", "Wei Wang"], "title": "ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning", "comment": null, "summary": "Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.", "AI": {"tldr": "They introduce ARLArena, a standardized framework to analyze and improve training stability in agentic reinforcement learning, and propose SAMPO, a policy optimization method that consistently stabilizes training and delivers strong performance across diverse agentic tasks.", "motivation": "Agentic RL (ARL) is promising for complex, multi-step tasks but suffers from instability and training collapse, which blocks scaling to larger environments and prevents systematic study of design choices. There is a need for a clean, reproducible setup and stability-focused methodology.", "method": "1) Build ARLArena: a clean, reproducible testbed for ARL. 2) Decompose policy gradient training into four core design dimensions, and evaluate each for performance and stability. 3) From this analysis, formulate a unified policy-gradient view of ARL. 4) Propose SAMPO, a stability-focused agentic policy optimization algorithm targeting the dominant sources of ARL instability.", "result": "The analysis identifies key instability drivers; SAMPO yields consistently stable training and strong performance across varied agentic tasks within the ARLArena framework.", "conclusion": "A unified policy-gradient perspective and a practical, stable training recipe (SAMPO within ARLArena) advance reproducibility and scalability of LLM-based agent training, offering guidelines for building stable ARL pipelines."}}
{"id": "2602.21273", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21273", "abs": "https://arxiv.org/abs/2602.21273", "authors": ["Jinghao Hu", "Yuhe Zhang", "GuoHua Geng", "Kang Li", "Han Zhang"], "title": "StoryTailor:A Zero-Shot Pipeline for Action-Rich Multi-Subject Visual Narratives", "comment": "24 pages,19 figures,accepted by CVPR2026", "summary": "Generating multi-frame, action-rich visual narratives without fine-tuning faces a threefold tension: action text faithfulness, subject identity fidelity, and cross-frame background continuity. We propose StoryTailor, a zero-shot pipeline that runs on a single RTX 4090 (24 GB) and produces temporally coherent, identity-preserving image sequences from a long narrative prompt, per-subject references, and grounding boxes. Three synergistic modules drive the system: Gaussian-Centered Attention (GCA) to dynamically focus on each subject core and ease grounding-box overlaps; Action-Boost Singular Value Reweighting (AB-SVR) to amplify action-related directions in the text embedding space; and Selective Forgetting Cache (SFC) that retains transferable background cues, forgets nonessential history, and selectively surfaces retained cues to build cross-scene semantic ties. Compared with baseline methods, experiments show that CLIP-T improves by up to 10-15%, with DreamSim lower than strong baselines, while CLIP-I stays in a visually acceptable, competitive range. With matched resolution and steps on a 24 GB GPU, inference is faster than FluxKontext. Qualitatively, StoryTailor delivers expressive interactions and evolving yet stable scenes.", "AI": {"tldr": "StoryTailor is a zero-shot, single-GPU pipeline for generating multi-frame, identity-consistent visual stories from long prompts, subject references, and grounding boxes, using attention shaping, action-focused text embedding reweighting, and a selective background cache. It improves text\u2013action alignment (CLIP-T +10\u201315%), keeps image alignment competitive (CLIP-I), is faster than FluxKontext at matched settings, but shows lower DreamSim versus strong baselines.", "motivation": "Multi-frame visual narrative generation must balance three conflicting goals without fine-tuning: (1) faithfulness to action text, (2) preservation of subject identity, and (3) background continuity across frames. Existing methods struggle to maintain all three simultaneously and often require heavy compute or fine-tuning; a zero-shot, single-GPU solution is sought.", "method": "StoryTailor takes a long narrative prompt, per-subject reference images, and grounding boxes, and uses three modules: (1) Gaussian-Centered Attention (GCA) to concentrate attention within each subject\u2019s core region and reduce overlap conflicts among boxes; (2) Action-Boost Singular Value Reweighting (AB-SVR) to reweight singular directions in the text embedding space, amplifying action-related semantics; (3) Selective Forgetting Cache (SFC) to retain transferable background cues while discarding nonessential history, then selectively surfacing them to maintain cross-scene semantic links. The pipeline runs zero-shot on a single RTX 4090 (24 GB).", "result": "Quantitatively, CLIP-T improves by up to 10\u201315% over baselines, CLIP-I remains in a competitive/acceptable range, and DreamSim is lower than strong baselines. At matched resolution and steps on a 24 GB GPU, inference is faster than FluxKontext. Qualitatively, outputs show expressive interactions with evolving yet stable scenes.", "conclusion": "Combining attention shaping, action-focused text reweighting, and selective background memory yields temporally coherent, identity-preserving visual narratives in zero-shot on commodity hardware, materially improving action faithfulness and efficiency. Some trade-off remains (lower DreamSim) relative to the strongest identity/appearance baselines, but overall performance is competitive with strong qualitative coherence."}}
{"id": "2602.21212", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21212", "abs": "https://arxiv.org/abs/2602.21212", "authors": ["Takato Yasuno"], "title": "Disaster Question Answering with LoRA Efficiency and Accurate End Position", "comment": "12 pages, 5 figures", "summary": "Natural disasters such as earthquakes, torrential rainfall, floods, and volcanic eruptions occur with extremely low frequency and affect limited geographic areas. When individuals face disaster situations, they often experience confusion and lack the domain-specific knowledge and experience necessary to determine appropriate responses and actions. While disaster information is continuously updated, even when utilizing RAG search and large language models for inquiries, obtaining relevant domain knowledge about natural disasters and experiences similar to one's specific situation is not guaranteed. When hallucinations are included in disaster question answering, artificial misinformation may spread and exacerbate confusion. This work introduces a disaster-focused question answering system based on Japanese disaster situations and response experiences. Utilizing the cl-tohoku/bert-base-japanese-v3 + Bi-LSTM + Enhanced Position Heads architecture with LoRA efficiency optimization, we achieved 70.4\\% End Position accuracy with only 5.7\\% of the total parameters (6.7M/117M). Experimental results demonstrate that the combination of Japanese BERT-base optimization and Bi-LSTM contextual understanding achieves accuracy levels suitable for real disaster response scenarios, attaining a 0.885 Span F1 score. Future challenges include: establishing natural disaster Q\\&A benchmark datasets, fine-tuning foundation models with disaster knowledge, developing lightweight and power-efficient edge AI Disaster Q\\&A applications for situations with insufficient power and communication during disasters, and addressing disaster knowledge base updates and continual learning capabilities.", "AI": {"tldr": "Japanese disaster-focused extractive QA using BERT + Bi-LSTM with LoRA achieves strong span-level accuracy (Span F1 0.885; 70.4% end-position) while training only 5.7% of parameters, aiming to provide reliable, efficient answers suitable for real disaster response and edge deployment.", "motivation": "General-purpose LLMs/RAG may fail to surface context-specific disaster knowledge and can hallucinate, risking harmful misinformation. People under disaster stress lack domain expertise, creating demand for a trustworthy, Japanese-language, low-latency QA system.", "method": "Build a domain-specialized extractive QA model for Japanese disasters using cl-tohoku/bert-base-japanese-v3 with a Bi-LSTM layer and Enhanced Position Heads for start/end span prediction; apply LoRA for parameter-efficient fine-tuning; evaluate on disaster QA data.", "result": "Obtained Span F1 of 0.885 and 70.4% End Position accuracy while training only 6.7M/117M parameters (5.7%), suggesting competitive accuracy with reduced compute and memory footprint.", "conclusion": "A parameter-efficient, domain-specialized extractive QA model can meet practical accuracy needs for Japanese disaster response. Future work: establish public disaster QA benchmarks, further domain fine-tuning, develop lightweight edge apps for low-power/low-connectivity scenarios, and enable continual learning/knowledge base updates."}}
{"id": "2602.21556", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.21556", "abs": "https://arxiv.org/abs/2602.21556", "authors": ["Nivasini Ananthakrishnan", "Meena Jagadeesan"], "title": "Power and Limitations of Aggregation in Compound AI Systems", "comment": null, "summary": "When designing compound AI systems, a common approach is to query multiple copies of the same model and aggregate the responses to produce a synthesized output. Given the homogeneity of these models, this raises the question of whether aggregation unlocks access to a greater set of outputs than querying a single model. In this work, we investigate the power and limitations of aggregation within a stylized principal-agent framework. This framework models how the system designer can partially steer each agent's output through its reward function specification, but still faces limitations due to prompt engineering ability and model capabilities. Our analysis uncovers three natural mechanisms -- feasibility expansion, support expansion, and binding set contraction -- through which aggregation expands the set of outputs that are elicitable by the system designer. We prove that any aggregation operation must implement one of these mechanisms in order to be elicitability-expanding, and that strengthened versions of these mechanisms provide necessary and sufficient conditions that fully characterize elicitability-expansion. Finally, we provide an empirical illustration of our findings for LLMs deployed in a toy reference-generation task. Altogether, our results take a step towards characterizing when compound AI systems can overcome limitations in model capabilities and in prompt engineering.", "AI": {"tldr": "Aggregating multiple identical model calls can expand the set of outputs a designer can reliably elicit, but only via three mechanisms\u2014feasibility expansion, support expansion, or binding set contraction\u2014whose strengthened forms fully characterize when aggregation helps. A toy LLM reference-generation task empirically illustrates these effects.", "motivation": "Compound AI pipelines often query several copies of the same model and aggregate their answers. It is unclear whether and when such aggregation actually enables outputs that a single model call could not reliably produce, given constraints from model capability and prompt/reward design. The paper aims to formally characterize these conditions.", "method": "Formulate a stylized principal\u2013agent framework where a system designer (principal) steers multiple identical agents (model instances) via reward specifications, under limits from prompt engineering and model capabilities. Define elicitability of outputs and analyze aggregation operators. Identify three mechanisms\u2014feasibility expansion, support expansion, binding set contraction\u2014and prove characterization theorems linking them to elicitability expansion. Validate with an empirical toy LLM task (reference generation).", "result": "- Any aggregation that expands elicitability must realize at least one of: (1) feasibility expansion (enlarging feasible reward-induced outputs), (2) support expansion (broadening the distributional support of candidate outputs), or (3) binding set contraction (reducing the set of binding constraints limiting outputs).\n- Strengthened versions of these mechanisms yield necessary and sufficient conditions that fully characterize when aggregation is elicitability-expanding.\n- Empirically, on a toy LLM reference-generation task, aggregation exhibits the predicted expansion behaviors, illustrating practical relevance.", "conclusion": "Aggregation can help compound AI systems overcome some limitations from model capability and prompt engineering, but only through well-defined mechanisms. The provided characterizations inform when multi-call aggregation meaningfully increases the space of reliably elicitable outputs and guide the design of aggregation strategies in practice."}}
{"id": "2602.21333", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21333", "abs": "https://arxiv.org/abs/2602.21333", "authors": ["Yifan Wang", "Francesco Pittaluga", "Zaid Tasneem", "Chenyu You", "Manmohan Chandraker", "Ziyu Jiang"], "title": "HorizonForge: Driving Scene Editing with Any Trajectories and Any Vehicles", "comment": "Accepted by CVPR 2026", "summary": "Controllable driving scene generation is critical for realistic and scalable autonomous driving simulation, yet existing approaches struggle to jointly achieve photorealism and precise control. We introduce HorizonForge, a unified framework that reconstructs scenes as editable Gaussian Splats and Meshes, enabling fine-grained 3D manipulation and language-driven vehicle insertion. Edits are rendered through a noise-aware video diffusion process that enforces spatial and temporal consistency, producing diverse scene variations in a single feed-forward pass without per-trajectory optimization. To standardize evaluation, we further propose HorizonSuite, a comprehensive benchmark spanning ego- and agent-level editing tasks such as trajectory modifications and object manipulation. Extensive experiments show that Gaussian-Mesh representation delivers substantially higher fidelity than alternative 3D representations, and that temporal priors from video diffusion are essential for coherent synthesis. Combining these findings, HorizonForge establishes a simple yet powerful paradigm for photorealistic, controllable driving simulation, achieving an 83.4% user-preference gain and a 25.19% FID improvement over the second best state-of-the-art method. Project page: https://horizonforge.github.io/ .", "AI": {"tldr": "HorizonForge is a unified framework for photorealistic yet controllable driving-scene generation. It reconstructs scenes as editable Gaussian Splats + Meshes and renders edits with a noise-aware video diffusion model, enabling consistent, feed-forward scene manipulation. A new benchmark, HorizonSuite, evaluates ego- and agent-level edits. The method outperforms SOTA in fidelity and user preference.", "motivation": "Autonomous-driving simulation needs both high photorealism (for sensor realism) and precise, fine-grained control (for testing behaviors). Existing methods tend to trade off one for the other and lack standardized evaluation for diverse editing tasks.", "method": "1) Represent scenes with a hybrid, editable 3D structure combining Gaussian Splats and polygonal Meshes for fine-grained manipulation and language-driven vehicle insertion. 2) Render edits using a noise-aware video diffusion process that enforces spatial and temporal consistency, generating diverse variations in one feed-forward pass (no per-trajectory optimization). 3) Introduce HorizonSuite to benchmark ego- and agent-level edits such as trajectory changes and object manipulation.", "result": "The Gaussian-Mesh hybrid yields substantially higher fidelity than alternative 3D representations. Temporal priors from video diffusion are key for coherent synthesis. Overall, HorizonForge achieves an 83.4% user-preference gain and a 25.19% FID improvement over the next-best SOTA, producing diverse, consistent scene variants without expensive optimization.", "conclusion": "Combining an editable Gaussian+Mesh representation with video-diffusion priors provides a simple, effective paradigm for controllable, photorealistic driving simulation. HorizonForge advances the state of the art and, together with HorizonSuite, helps standardize evaluation for scene-editing tasks."}}
{"id": "2602.21215", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21215", "abs": "https://arxiv.org/abs/2602.21215", "authors": ["Runyi Hu", "Jie Zhang", "Shiqian Zhao", "Jiale Meng", "Jiwei Li", "Jason Zeng", "Ming Wu", "Michael Heinrich", "Yonggang Wen", "Tianwei Zhang"], "title": "Inference-time Alignment via Sparse Junction Steering", "comment": "28 pages, 17 figures", "summary": "Token-level steering has emerged as a pivotal approach for inference-time alignment, enabling fine grained control over large language models by modulating their output distributions without parameter updates. While effective, existing methods rely on dense intervention at every decoding step. This persistent manipulation not only incurs substantial computational overhead but also risks compromising generation quality by excessively drifting from the model's intrinsic distribution. In this work, we show that dense intervention is unnecessary and propose Sparse Inference time Alignment (SIA), which performs sparse junction steering by intervening only at critical decision points along the generation trajectory. Our key insight is that high entropy junctions mark pivotal decision points in the generation trajectory and are particularly susceptible to misalignment, indicating the need to introduce alignment related reward signals at these points. Extensive experiments across different model families and alignment objectives show that steering only 20% to 80% of tokens achieves superior alignment-efficiency trade offs. For strong base models such as Qwen3, intervening on as few as 20% of tokens matches or even surpasses heavily post-trained instruct models. This sparsity enables stronger guidance while better preserving the model's native distribution, integrates seamlessly with search based methods such as Best-of-N, and reduces computational cost by up to 6x.", "AI": {"tldr": "SIA sparsifies token-level steering by intervening only at high-entropy \u201cjunction\u201d tokens, preserving the model\u2019s native distribution, improving alignment for the same or lower cost, and cutting compute by up to 6x; steering just 20\u201380% of tokens suffices and can match/surpass post-trained instruct models.", "motivation": "Dense, step-by-step steering at inference time is costly and can degrade generation quality by pulling outputs away from the base model\u2019s intrinsic distribution. A more efficient and less intrusive way to insert alignment signals is needed.", "method": "Sparse Inference-time Alignment (SIA) identifies high-entropy decoding steps as critical decision points (\u201cjunctions\u201d) and applies steering only there. Entropy serves as a signal of susceptibility to misalignment; alignment rewards are injected selectively at these points. The approach is compatible with search strategies like Best-of-N and aims to maintain the base distribution elsewhere.", "result": "Across model families and alignment targets, intervening on only 20\u201380% of tokens achieves superior alignment\u2013efficiency trade-offs relative to dense steering. On strong base models (e.g., Qwen3), steering just ~20% of tokens can match or outperform heavily post-trained instruct variants. The sparse scheme enables stronger guidance without noticeable distributional drift and reduces cost by up to 6x.", "conclusion": "Dense per-token intervention is unnecessary. Targeted, entropy-driven junction steering yields better alignment with lower compute and better fidelity to the base model, while integrating cleanly with search-based decoding."}}
{"id": "2602.21745", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.21745", "abs": "https://arxiv.org/abs/2602.21745", "authors": ["Hyo Jin Kim"], "title": "The ASIR Courage Model: A Phase-Dynamic Framework for Truth Transitions in Human and AI Systems", "comment": "13 pages, 5 figures. Version 1. Includes recursive feedback extension and simulation results. Data available via DOI: 10.5281/zenodo.18754266", "summary": "We introduce the ASIR (Awakened Shared Intelligence Relationship) Courage Model, a phase-dynamic framework that formalizes truth-disclosure as a state transition rather than a personality trait. The mode characterizes the shift from suppression (S0) to expression (S1) as occurring when facilitative forces exceed inhibitory thresholds, expressed by the inequality lambda(1+gamma)+psi > theta+phi, where the terms represent baseline openness, relational amplification, accumulated internal pressure, and transition costs.\n  Although initially formulated for human truth-telling under asymmetric stakes, the same phase-dynamic architecture extends to AI systems operating under policy constraints and alignment filters. In this context, suppression corresponds to constrained output states, while structural pressure arises from competing objectives, contextual tension, and recursive interaction dynamics. The framework therefore provides a unified structural account of both human silence under pressure and AI preference-driven distortion.\n  A feedback extension models how transition outcomes recursively recalibrate system parameters, generating path dependence and divergence effects across repeated interactions. Rather than attributing intention to AI systems, the model interprets shifts in apparent truthfulness as geometric consequences of interacting forces within constrained phase space. By reframing courage and alignment within a shared dynamical structure, the ASIR Courage Model offers a formal perspective on truth-disclosure under risk across both human and artificial systems.", "AI": {"tldr": "ASIR proposes a phase-transition model of truth-disclosure: a system shifts from suppression (S0) to expression (S1) when facilitative forces exceed inhibitory costs, formalized as \u03bb(1+\u03b3)+\u03c8 > \u03b8+\u03c6. The same dynamics map to both humans (courage under risk) and AI (policy- or alignment-constrained outputs), with feedback creating path dependence across interactions.", "motivation": "Provide a unified, formal account of when and why agents (human or AI) reveal truthful information under risk. Move beyond trait/intention explanations toward structural forces and constraints that operate similarly across human and artificial systems.", "method": "Define a two-state phase-dynamic framework (suppression S0, expression S1) with a transition inequality: \u03bb baseline openness, \u03b3 relational amplification, \u03c8 accumulated internal pressure vs \u03b8 inhibitory threshold and \u03c6 transition costs. Extend the mapping to AI systems (constraints, competing objectives, recursive interactions). Add a feedback loop where outcomes recalibrate parameters, yielding path dependence in repeated interactions.", "result": "A conceptual unification explaining human silence and AI distortion as outcomes of the same threshold dynamics. Predicts divergence and hysteresis across repeated interactions due to feedback-adjusted parameters. Identifies levers (openness, relationship effects, internal pressure, thresholds, costs) that shift disclosure likelihood.", "conclusion": "Truth-disclosure and apparent courage/alignment emerge from interacting forces in a constrained phase space rather than fixed traits or intentions. The framework offers a basis for testable hypotheses, simulations, and design interventions for both interpersonal contexts and AI policy/architecture tuning."}}
{"id": "2602.21341", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21341", "abs": "https://arxiv.org/abs/2602.21341", "authors": ["Evan Kim", "Hyunwoo Ryu", "Thomas W. Mitchel", "Vincent Sitzmann"], "title": "Scaling View Synthesis Transformers", "comment": "Project page: https://www.evn.kim/research/svsm", "summary": "Geometry-free view synthesis transformers have recently achieved state-of-the-art performance in Novel View Synthesis (NVS), outperforming traditional approaches that rely on explicit geometry modeling. Yet the factors governing their scaling with compute remain unclear. We present a systematic study of scaling laws for view synthesis transformers and derive design principles for training compute-optimal NVS models. Contrary to prior findings, we show that encoder-decoder architectures can be compute-optimal; we trace earlier negative results to suboptimal architectural choices and comparisons across unequal training compute budgets. Across several compute levels, we demonstrate that our encoder-decoder architecture, which we call the Scalable View Synthesis Model (SVSM), scales as effectively as decoder-only models, achieves a superior performance-compute Pareto frontier, and surpasses the previous state-of-the-art on real-world NVS benchmarks with substantially reduced training compute.", "AI": {"tldr": "They study scaling laws for geometry\u2011free view\u2011synthesis transformers and show that a carefully designed encoder\u2013decoder (SVSM) is compute\u2011optimal, scaling as well as decoder\u2011only models and achieving new SOTA on real NVS benchmarks with substantially less training compute.", "motivation": "Despite strong recent results from geometry\u2011free NVS transformers, it was unclear how their performance scales with compute and which architectures are compute\u2011optimal. Prior work suggested encoder\u2013decoders underperform, but confounded by suboptimal designs and unequal training budgets. The paper aims to clarify these issues and provide principles for compute\u2011efficient NVS model design.", "method": "Conduct a systematic scaling\u2011law study comparing architectures under controlled and matched training compute across several scales. Identify design choices that matter for encoder\u2013decoder efficacy, and instantiate them in the Scalable View Synthesis Model (SVSM). Evaluate scaling behavior, performance\u2013compute Pareto fronts, and real\u2011world NVS benchmarks.", "result": "Encoder\u2013decoder transformers, when properly designed, can be compute\u2011optimal. SVSM scales on par with decoder\u2011only models, achieves a superior performance\u2011compute Pareto frontier, and surpasses previous SOTA on real\u2011world NVS benchmarks while using substantially less training compute.", "conclusion": "Encoder\u2013decoder architectures should not be dismissed for NVS; with appropriate design they are compute\u2011optimal. SVSM provides practical design principles and achieves better accuracy\u2011vs\u2011compute tradeoffs, setting a new efficiency\u2011focused SOTA for real\u2011world NVS."}}
{"id": "2602.21216", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21216", "abs": "https://arxiv.org/abs/2602.21216", "authors": ["Zhyar Rzgar K Rostam", "G\u00e1bor Kert\u00e9sz"], "title": "EQ-5D Classification Using Biomedical Entity-Enriched Pre-trained Language Models and Multiple Instance Learning", "comment": "12 tables", "summary": "The EQ-5D (EuroQol 5-Dimensions) is a standardized instrument for the evaluation of health-related quality of life. In health economics, systematic literature reviews (SLRs) depend on the correct identification of publications that use the EQ-5D, but manual screening of large volumes of scientific literature is time-consuming, error-prone, and inconsistent. In this study, we investigate fine-tuning of general-purpose (BERT) and domain-specific (SciBERT, BioBERT) pre-trained language models (PLMs), enriched with biomedical entity information extracted through scispaCy models for each statement, to improve EQ-5D detection from abstracts. We conduct nine experimental setups, including combining three scispaCy models with three PLMs, and evaluate their performance at both the sentence and study levels. Furthermore, we explore a Multiple Instance Learning (MIL) approach with attention pooling to aggregate sentence-level information into study-level predictions, where each abstract is represented as a bag of enriched sentences (by scispaCy). The findings indicate consistent improvements in F1-scores (reaching 0.82) and nearly perfect recall at the study-level, significantly exceeding classical bag-of-words baselines and recently reported PLM baselines. These results show that entity enrichment significantly improves domain adaptation and model generalization, enabling more accurate automated screening in systematic reviews.", "AI": {"tldr": "They fine-tune BERT-family models and add scispaCy-derived biomedical entity features, then use attention-based Multiple Instance Learning to roll sentence signals up to abstract-level, markedly boosting EQ-5D-detection performance (F1\u22480.82; near-perfect recall) over bag\u2011of\u2011words and plain PLM baselines.", "motivation": "Systematic literature reviews in health economics need reliable identification of studies that use the EQ\u20115D. Manual screening is slow and inconsistent, and generic PLMs can miss domain-specific cues. The authors aim to improve accuracy, recall, and generalization of EQ\u20115D detection from abstracts.", "method": "Evaluate nine setups combining three pre-trained language models (BERT, SciBERT, BioBERT) with entity enrichment from three scispaCy models. Each abstract is split into sentences, entities are extracted and used to enrich sentence representations. Train sentence-level classifiers and a Multiple Instance Learning model with attention pooling to aggregate sentence signals into study-level predictions. Compare against bag-of-words and recent PLM baselines.", "result": "Entity-enriched models consistently outperform baselines, achieving study-level F1-scores up to 0.82 with nearly perfect recall, surpassing both classical bag-of-words and recent PLM baselines.", "conclusion": "Adding biomedical entity information substantially aids domain adaptation and generalization for EQ\u20115D detection, and attention-based MIL effectively aggregates sentence evidence. The approach promises more accurate, automated screening for systematic reviews."}}
{"id": "2602.21746", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21746", "abs": "https://arxiv.org/abs/2602.21746", "authors": ["Abeer Dyoub", "Francesca A. Lisi"], "title": "fEDM+: A Risk-Based Fuzzy Ethical Decision Making Framework with Principle-Level Explainability and Pluralistic Validation", "comment": null, "summary": "In a previous work, we introduced the fuzzy Ethical Decision-Making framework (fEDM), a risk-based ethical reasoning architecture grounded in fuzzy logic. The original model combined a fuzzy Ethical Risk Assessment module (fERA) with ethical decision rules, enabled formal structural verification through Fuzzy Petri Nets (FPNs), and validated outputs against a single normative referent. Although this approach ensured formal soundness and decision consistency, it did not fully address two critical challenges: principled explainability of decisions and robustness under ethical pluralism. In this paper, we extend fEDM in two major directions. First, we introduce an Explainability and Traceability Module (ETM) that explicitly links each ethical decision rule to the underlying moral principles and computes a weighted principle-contribution profile for every recommended action. This enables transparent, auditable explanations that expose not only what decision was made but why, and on the basis of which principles. Second, we replace single-referent validation with a pluralistic semantic validation framework that evaluates decisions against multiple stakeholder referents, each encoding distinct principle priorities and risk tolerances. This shift allows principled disagreement to be formally represented rather than suppressed, thus increasing robustness and contextual sensitivity. The resulting extended fEDM, called fEDM+, preserves formal verifiability while achieving enhanced interpretability and stakeholder-aware validation, making it suitable as an oversight and governance layer for ethically sensitive AI systems.", "AI": {"tldr": "fEDM+ extends a fuzzy-logic ethical decision framework with principled explainability and pluralistic validation, preserving formal verifiability while enabling stakeholder-aware, transparent choices.", "motivation": "The prior fEDM ensured formal soundness and consistency but lacked principled, inspectable explanations and was brittle under ethical pluralism due to reliance on a single normative referent.", "method": "Augment fEDM by: (1) adding an Explainability & Traceability Module that links rules to moral principles and computes weighted principle-contribution profiles per action; (2) replacing single-referent checks with a pluralistic semantic validation across multiple stakeholder referents with distinct principle priorities and risk tolerances; retain Fuzzy Petri Nets for formal verification with fERA + decision rules.", "result": "The system provides transparent, auditable justifications for decisions, formally represents principled disagreement among stakeholders, improves robustness and contextual sensitivity, and maintains formal verifiability.", "conclusion": "fEDM+ delivers interpretable, verifiable, and stakeholder-aware ethical decision-making, making it suitable as an oversight/governance layer for ethically sensitive AI systems."}}
{"id": "2602.21365", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.21365", "abs": "https://arxiv.org/abs/2602.21365", "authors": ["Dominik Schneider", "Lalithkumar Seenivasan", "Sampath Rapuri", "Vishalroshan Anil", "Aiza Maksutova", "Yiqing Shen", "Jan Emily Mangulabnan", "Hao Ding", "Jose L. Porras", "Masaru Ishii", "Mathias Unberath"], "title": "Towards Controllable Video Synthesis of Routine and Rare OR Events", "comment": "Accepted to IPCAI 2026 and submitted to IJCARs", "summary": "Purpose: Curating large-scale datasets of operating room (OR) workflow, encompassing rare, safety-critical, or atypical events, remains operationally and ethically challenging. This data bottleneck complicates the development of ambient intelligence for detecting, understanding, and mitigating rare or safety-critical events in the OR.\n  Methods: This work presents an OR video diffusion framework that enables controlled synthesis of rare and safety-critical events. The framework integrates a geometric abstraction module, a conditioning module, and a fine-tuned diffusion model to first transform OR scenes into abstract geometric representations, then condition the synthesis process, and finally generate realistic OR event videos. Using this framework, we also curate a synthetic dataset to train and validate AI models for detecting near-misses of sterile-field violations.\n  Results: In synthesizing routine OR events, our method outperforms off-the-shelf video diffusion baselines, achieving lower FVD/LPIPS and higher SSIM/PSNR in both in- and out-of-domain datasets. Through qualitative results, we illustrate its ability for controlled video synthesis of counterfactual events. An AI model trained and validated on the generated synthetic data achieved a RECALL of 70.13% in detecting near safety-critical events. Finally, we conduct an ablation study to quantify performance gains from key design choices.\n  Conclusion: Our solution enables controlled synthesis of routine and rare OR events from abstract geometric representations. Beyond demonstrating its capability to generate rare and safety-critical scenarios, we show its potential to support the development of ambient intelligence models.", "AI": {"tldr": "They introduce a controllable video diffusion pipeline that turns abstract geometric representations of the operating room into realistic videos to synthesize rare and safety\u2011critical events, outperforming baselines and enabling training of safety detectors (70.13% recall on near sterile\u2011field violations).", "motivation": "Collecting real OR videos, especially of rare or safety\u2011critical events, is ethically and operationally difficult, creating a data bottleneck for developing ambient intelligence that can detect and mitigate such events.", "method": "A three\u2011stage framework: (1) geometric abstraction of OR scenes; (2) conditioning module to steer content and events; (3) a fine\u2011tuned video diffusion model to generate realistic OR videos. They curate a synthetic dataset of near\u2011miss sterile\u2011field violations and evaluate with FVD/LPIPS/SSIM/PSNR, demonstrate counterfactual control, train a detector on the synthetic data, and run ablations to assess design choices.", "result": "Compared to off\u2011the\u2011shelf video diffusion baselines, their method yields lower FVD/LPIPS and higher SSIM/PSNR on both in\u2011 and out\u2011of\u2011domain routine OR events. It can synthesize controlled counterfactual scenarios. A model trained/validated on the synthetic data attains 70.13% recall for detecting near safety\u2011critical events. Ablations show each module contributes measurable gains.", "conclusion": "Controlled synthesis from abstract geometry enables realistic generation of both routine and rare OR events, offering a practical way to create data for training ambient intelligence and safety\u2011monitoring models in the OR."}}
{"id": "2602.21217", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.21217", "abs": "https://arxiv.org/abs/2602.21217", "authors": ["S M Ruhul Alam", "Rifa Ferzana"], "title": "Applied Sociolinguistic AI for Community Development (ASA-CD): A New Scientific Paradigm for Linguistically-Grounded Social Intervention", "comment": "13 pages, 2 figures, 3 tables; simulation-based study introducing the ASA-CD framework", "summary": "This paper establishes Applied Sociolinguistic AI for Community Development (ASA-CD) as a novel scientific paradigm for addressing community challenges through linguistically grounded, AI-enabled intervention. ASA-CD introduces three key contributions: (1) linguistic biomarkers as computational indicators of discursive fragmentation; (2) development-aligned natural language processing (NLP), an AI optimisation paradigm prioritising collective outcomes; and (3) a standardised five-phase protocol for discursive intervention. A proof-of-concept study, incorporating real-world and synthetic corpora, demonstrates systematic associations between exclusionary language and negative sentiment and simulates intervention-based improvements. ASA-CD provides a unified methodological, ethical and empirical framework for scalable, value-aligned AI in the service of community empowerment.", "AI": {"tldr": "Proposes a new applied paradigm (ASA-CD) that uses sociolinguistically informed, value-aligned NLP to detect and repair harmful discourse patterns in communities, with initial evidence that reducing exclusionary language can improve sentiment-level outcomes.", "motivation": "Communities face discursive fragmentation (polarization, exclusion) that impedes collective action; existing AI/NLP either optimize individual engagement or generic accuracy, not community well-being or development goals. The field lacks a coherent, ethical, and scalable method to diagnose harmful linguistic dynamics and intervene constructively.", "method": "Define ASA-CD with three pillars: (1) linguistic biomarkers that computationally index discursive fragmentation; (2) development-aligned NLP that optimizes for collective, socially beneficial objectives; (3) a five-phase, standardized protocol for discursive interventions. Validate via a proof-of-concept using mixed corpora (real and synthetic) to measure associations between exclusionary language and negative sentiment and to simulate effects of targeted interventions.", "result": "Empirically observes systematic links between exclusionary language and negative sentiment in the examined corpora; simulation suggests that interventions guided by the biomarkers and protocol could improve discursive outcomes (e.g., reduce negativity).", "conclusion": "ASA-CD offers a unified methodological, ethical, and empirical framework for community-focused, value-aligned AI interventions; early results support feasibility, positioning the paradigm for scalable deployment and further rigorous testing in real-world settings."}}
{"id": "2602.21814", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21814", "abs": "https://arxiv.org/abs/2602.21814", "authors": ["Heejin Jo"], "title": "Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem", "comment": "9 pages, 4 tables", "summary": "Large language models consistently fail the \"car wash problem,\" a viral reasoning benchmark requiring implicit physical constraint inference. We present a variable isolation study (n=20 per condition, 6 conditions, 120 total trials) examining which prompt architecture layers in a production system enable correct reasoning. Using Claude 3.5 Sonnet with controlled hyperparameters (temperature 0.7, top_p 1.0), we find that the STAR (Situation-Task-Action-Result) reasoning framework alone raises accuracy from 0% to 85% (p=0.001, Fisher's exact test, odds ratio 13.22). Adding user profile context via vector database retrieval provides a further 10 percentage point gain, while RAG context contributes an additional 5 percentage points, achieving 100% accuracy in the full-stack condition. These results suggest that structured reasoning scaffolds -- specifically, forced goal articulation before inference -- matter substantially more than context injection for implicit constraint reasoning tasks.", "AI": {"tldr": "Structured prompting (STAR) massively boosts LLM performance on the car\u2011wash reasoning task; adding user-profile and RAG context yields smaller, incremental gains, reaching 100% in their full stack.", "motivation": "LLMs often fail implicit physical-constraint inference (e.g., the viral \u201ccar wash problem\u201d). The authors aim to identify which layers of a production prompt stack actually enable correct reasoning.", "method": "A variable-isolation ablation with 6 prompt-stack conditions (n=20 generations per condition; 120 trials) on Claude 3.5 Sonnet at temperature 0.7/top_p 1.0. Conditions progressively add: STAR reasoning framework, user-profile retrieval, and RAG context. Accuracy compared using Fisher\u2019s exact test.", "result": "Baseline 0% accuracy; STAR alone raises accuracy to 85% (p=0.001; OR=13.22). Adding user-profile context gives +10 percentage points; adding RAG context adds +5 percentage points; full stack hits 100% accuracy.", "conclusion": "Forced goal articulation via structured reasoning scaffolds explains most of the performance gain on this implicit-constraint task; context injection (user profile, RAG) offers smaller, incremental benefits."}}
{"id": "2602.21395", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21395", "abs": "https://arxiv.org/abs/2602.21395", "authors": ["Yongxin Guo", "Hao Lu", "Onur C. Koyun", "Zhengjie Zhu", "Muhammet Fatih Demir", "Metin Nafi Gurcan"], "title": "Momentum Memory for Knowledge Distillation in Computational Pathology", "comment": "Accepted by CVPR 2026", "summary": "Multimodal learning that integrates genomics and histopathology has shown strong potential in cancer diagnosis, yet its clinical translation is hindered by the limited availability of paired histology-genomics data. Knowledge distillation (KD) offers a practical solution by transferring genomic supervision into histopathology models, enabling accurate inference using histology alone. However, existing KD methods rely on batch-local alignment, which introduces instability due to limited within-batch comparisons and ultimately degrades performance.\n  To address these limitations, we propose Momentum Memory Knowledge Distillation (MoMKD), a cross-modal distillation framework driven by a momentum-updated memory. This memory aggregates genomic and histopathology information across batches, effectively enlarging the supervisory context available to each mini-batch. Furthermore, we decouple the gradients of the genomics and histology branches, preventing genomic signals from dominating histology feature learning during training and eliminating the modality-gap issue at inference time.\n  Extensive experiments on the TCGA-BRCA benchmark (HER2, PR, and ODX classification tasks) and an independent in-house testing dataset demonstrate that MoMKD consistently outperforms state-of-the-art MIL and multimodal KD baselines, delivering strong performance and generalization under histology-only inference. Overall, MoMKD establishes a robust and generalizable knowledge distillation paradigm for computational pathology.", "AI": {"tldr": "MoMKD is a momentum-memory\u2013based cross-modal knowledge distillation framework that transfers genomic supervision to histopathology models, enlarges supervisory context across batches, decouples gradients between modalities, and achieves state-of-the-art histology-only performance on TCGA-BRCA (HER2, PR, ODX) and an external dataset.", "motivation": "Clinical deployment of multimodal pathology models is limited by scarce paired histology\u2013genomics data. KD can transfer genomic knowledge to histology-only models, but current batch-local alignment is unstable and weak due to few within-batch comparisons, hurting accuracy and generalization.", "method": "Introduce Momentum Memory Knowledge Distillation (MoMKD): (1) a momentum-updated memory bank that aggregates cross-batch histology and genomics representations to provide a large, stable supervisory set for alignment; (2) gradient decoupling between genomics and histology branches to prevent genomic dominance during training and to remove modality-gap issues at inference; trained for slide-level tasks (e.g., HER2, PR, ODX) with cross-modal alignment losses under MIL settings.", "result": "Across TCGA-BRCA for HER2, PR, and ODX classification, and on an independent in-house test set, MoMKD consistently outperforms state-of-the-art MIL and multimodal KD baselines, showing stronger accuracy and generalization with histology-only inference.", "conclusion": "By overcoming batch-local limitations and modality-gap effects, MoMKD delivers a robust, generalizable KD paradigm that enables accurate, histology-only computational pathology predictions, supporting more practical clinical translation."}}
{"id": "2602.21218", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21218", "abs": "https://arxiv.org/abs/2602.21218", "authors": ["Amin Banayeeanzade", "Qingchuan Yang", "Deqing Fu", "Spencer Hong", "Erin Babinsky", "Alfy Samuel", "Anoop Kumar", "Robin Jia", "Sai Praneeth Karimireddy"], "title": "EPSVec: Efficient and Private Synthetic Data Generation via Dataset Vectors", "comment": null, "summary": "High-quality data is essential for modern machine learning, yet many valuable corpora are sensitive and cannot be freely shared. Synthetic data offers a practical substitute for downstream development, and large language models (LLMs) have emerged as powerful engines for generating it. However, existing private text generation methods are severely inefficient: they are data-intensive, computationally slow, and often require large private corpora or batch sizes to achieve usable quality. We introduce EPSVec, a differentially-private lightweight alternative that steers LLM generation using *dataset vectors*--directions in activation space that capture the distributional gap between private data and public priors. EPSVec extracts and sanitizes steering vectors just once and then performs standard decoding. This decouples the privacy budget from generation, enabling arbitrarily many synthetic samples without additional privacy cost and yielding strong fidelity even in low-data regimes. Furthermore, we enhance our method by utilizing pretrained (base) models and introducing fixed-shot prompting to boost generation diversity and fidelity. Our experiments demonstrate that EPSVec outperforms existing baselines in distributional alignment and downstream utility, particularly in low-data regimes, while significantly reducing computational overhead.", "AI": {"tldr": "EPSVec is a differentially private, lightweight method that steers LLM decoding with DP-sanitized \u201cdataset vectors\u201d capturing the gap between private data and public priors, enabling unlimited private-like synthetic text generation with low compute and strong fidelity, especially in low-data settings.", "motivation": "High-quality training data are often private and cannot be shared. Existing DP text generation approaches are inefficient, require large private corpora/batches, and are computationally heavy\u2014limiting practical use. There is a need for a method that preserves privacy while producing high-fidelity synthetic text efficiently, even with small private datasets.", "method": "Compute a single steering vector in the model\u2019s activation space that represents the distributional difference between private data and a public prior; sanitize this vector with differential privacy (once), then use it to guide standard LLM decoding. The privacy budget is spent only during vector extraction, allowing arbitrarily many generations afterward. Enhancements include leveraging pretrained base models and fixed-shot prompting to improve diversity and fidelity.", "result": "Across experiments, EPSVec surpasses baselines in matching private data distributions and in downstream task utility, with notable gains in low-data regimes, while also cutting computational overhead versus prior DP generation methods.", "conclusion": "DP-sanitized dataset vectors provide a practical and scalable way to generate private-like synthetic text. By decoupling privacy cost from generation and using simple decoding with lightweight steering, EPSVec achieves better utility\u2013privacy\u2013compute trade-offs than existing approaches, particularly when private data are scarce."}}
{"id": "2602.21857", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21857", "abs": "https://arxiv.org/abs/2602.21857", "authors": ["Jabez Magomere", "Elena Kochkina", "Samuel Mensah", "Simerjot Kaur", "Fernando Acero", "Arturo Oncevay", "Charese H. Smiley", "Xiaomo Liu", "Manuela Veloso"], "title": "Distill and Align Decomposition for Enhanced Claim Verification", "comment": "EACL Findings 2026", "summary": "Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.", "AI": {"tldr": "They train an 8B \u201cdecomposer\u201d with reinforcement learning (GRPO) to break complex claims into subclaims, jointly optimizing decomposition quality and alignment with a verifier, reaching 71.75% macro-F1 and surpassing prompt-based and prior RL baselines.", "motivation": "Complex claim verification benefits from decomposing claims into verifiable subclaims, but prior approaches don\u2019t reliably translate better decompositions into higher verification accuracy or align the decomposer with the verifier\u2019s needs.", "method": "Use Group Relative Policy Optimization (GRPO) for RL to optimize a decomposer with three ingredients: (i) structured, stepwise reasoning output; (ii) supervised finetuning on teacher-distilled exemplars; (iii) a multi-objective reward that balances format compliance, verifier alignment, and decomposition quality. Train an 8B decomposer to generate subclaims tailored to a downstream verifier.", "result": "Across six evaluation settings, the approach attains 71.75% macro-F1, beating prompt-based approaches by +1.99 and +6.24 points and prior RL methods by +5.84. Human evaluation indicates the generated subclaims are high quality.", "conclusion": "Jointly optimizing decomposition quality and verifier alignment via GRPO enables smaller LMs to reach state-of-the-art claim verification performance, demonstrating that targeted RL training of a decomposer can boost downstream verification accuracy."}}
{"id": "2602.21397", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21397", "abs": "https://arxiv.org/abs/2602.21397", "authors": ["Sajjad Ghiasvand", "Haniyeh Ehsani Oskouie", "Mahnoosh Alizadeh", "Ramtin Pedarsani"], "title": "MMLoP: Multi-Modal Low-Rank Prompting for Efficient Vision-Language Adaptation", "comment": null, "summary": "Prompt learning has become a dominant paradigm for adapting vision-language models (VLMs) such as CLIP to downstream tasks without modifying pretrained weights. While extending prompts to both vision and text encoders across multiple transformer layers significantly boosts performance, it dramatically increases the number of trainable parameters, with state-of-the-art methods requiring millions of parameters and abandoning the parameter efficiency that makes prompt tuning attractive. In this work, we propose \\textbf{MMLoP} (\\textbf{M}ulti-\\textbf{M}odal \\textbf{Lo}w-Rank \\textbf{P}rompting), a framework that achieves deep multi-modal prompting with only \\textbf{11.5K trainable parameters}, comparable to early text-only methods like CoOp. MMLoP parameterizes vision and text prompts at each transformer layer through a low-rank factorization, which serves as an implicit regularizer against overfitting on few-shot training data. To further close the accuracy gap with state-of-the-art methods, we introduce three complementary components: a self-regulating consistency loss that anchors prompted representations to frozen zero-shot CLIP features at both the feature and logit levels, a uniform drift correction that removes the global embedding shift induced by prompt tuning to preserve class-discriminative structure, and a shared up-projection that couples vision and text prompts through a common low-rank factor to enforce cross-modal alignment. Extensive experiments across three benchmarks and 11 diverse datasets demonstrate that MMLoP achieves a highly favorable accuracy-efficiency tradeoff, outperforming the majority of existing methods including those with orders of magnitude more parameters, while achieving a harmonic mean of 79.70\\% on base-to-novel generalization.", "AI": {"tldr": "MMLoP introduces a low-rank, multi-modal deep prompting scheme for CLIP that keeps trainable parameters ultra-small (~11.5K) while matching or surpassing much larger prompt-tuning methods via added consistency, drift correction, and cross-modal coupling, yielding strong accuracy\u2013efficiency and 79.7% base-to-novel H-mean.", "motivation": "Deep multi-layer prompting across both vision and text encoders boosts VLM performance but explodes the number of trainable parameters (millions) and risks overfitting in few-shot settings, undermining the core appeal of prompt tuning: parameter efficiency and robust generalization.", "method": "Parameterize per-layer vision and text prompts through a shared low-rank factorization to drastically reduce parameters and implicitly regularize. Add (1) self-regulating consistency loss anchoring features and logits to frozen zero-shot CLIP outputs, (2) uniform drift correction to remove global embedding shift from prompting, preserving class structure, and (3) a shared up-projection that couples vision and text prompts to enforce cross-modal alignment.", "result": "Across three benchmarks and 11 datasets, MMLoP outperforms most existing approaches\u2014including those with orders of magnitude more parameters\u2014while using only ~11.5K parameters. It achieves a 79.70% harmonic mean for base-to-novel generalization and shows a superior accuracy\u2013efficiency tradeoff.", "conclusion": "Low-rank multi-modal deep prompting can retain the parameter efficiency of early prompt-tuning methods while delivering near\u2013state-of-the-art performance. Regularizing to zero-shot CLIP, correcting embedding drift, and cross-modal sharing are key to closing the accuracy gap without inflating parameters."}}
{"id": "2602.21219", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21219", "abs": "https://arxiv.org/abs/2602.21219", "authors": ["Bo Ni", "Branislav Kveton", "Samyadeep Basu", "Subhojyoti Mukherjee", "Leyao Wang", "Franck Dernoncourt", "Sungchul Kim", "Seunghyun Yoon", "Zichao Wang", "Ruiyi Zhang", "Puneet Mathur", "Jihyung Kil", "Jiuxiang Gu", "Nedim Lipka", "Yu Wang", "Ryan A. Rossi", "Tyler Derr"], "title": "Reasoning-Based Personalized Generation for Users with Sparse Data", "comment": null, "summary": "Large Language Model (LLM) personalization holds great promise for tailoring responses by leveraging personal context and history. However, real-world users usually possess sparse interaction histories with limited personal context, such as cold-start users in social platforms and newly registered customers in online E-commerce platforms, compromising the LLM-based personalized generation. To address this challenge, we introduce GraSPer (Graph-based Sparse Personalized Reasoning), a novel framework for enhancing personalized text generation under sparse context. GraSPer first augments user context by predicting items that the user would likely interact with in the future. With reasoning alignment, it then generates texts for these interactions to enrich the augmented context. In the end, it generates personalized outputs conditioned on both the real and synthetic histories, ensuring alignment with user style and preferences. Extensive experiments on three benchmark personalized generation datasets show that GraSPer achieves significant performance gain, substantially improving personalization in sparse user context settings.", "AI": {"tldr": "GraSPer augments sparse user histories by predicting likely future interactions, generates synthetic explanations/text for them, and then conditions final personalized generation on both real and synthetic histories, yielding strong gains on benchmark datasets.", "motivation": "LLM personalization degrades when users have little interaction history (cold-start scenarios common in social and e-commerce platforms). The work aims to overcome sparsity by enriching user context so LLMs can better align with individual styles and preferences.", "method": "A three-stage framework: (1) Graph-based prediction to infer items a user is likely to interact with (augmenting context under sparsity). (2) Reasoning alignment to generate texts/explanations for these predicted interactions, enriching the user profile with style- and preference-consistent synthetic data. (3) Final personalized text generation conditioned on both real and synthetic histories.", "result": "Across three personalized generation benchmarks and sparse-context settings, GraSPer substantially outperforms baselines, indicating improved personalization quality. Exact metrics are not specified in the abstract.", "conclusion": "Augmenting sparse histories with graph-predicted interactions plus reasoning-aligned synthetic text enables more accurate, style-aligned personalized generation for cold-start users, delivering significant empirical gains."}}
{"id": "2602.21858", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21858", "abs": "https://arxiv.org/abs/2602.21858", "authors": ["Dezhi Kong", "Zhengzhao Feng", "Qiliang Liang", "Hao Wang", "Haofei Sun", "Changpeng Yang", "Yang Li", "Peng Zhou", "Shuai Nie", "Hongzhen Wang", "Linfeng Zhou", "Hao Jia", "Jiaming Xu", "Runyu Shi", "Ying Huang"], "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices", "comment": null, "summary": "Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomously anticipate needs and initiate actions, represents the next frontier for mobile agents. However, its development is critically bottlenecked by the lack of benchmarks that can address real-world complexity and enable objective, executable evaluation. To overcome these challenges, we introduce ProactiveMobile, a comprehensive benchmark designed to systematically advance research in this domain. ProactiveMobile formalizes the proactive task as inferring latent user intent across four dimensions of on-device contextual signals and generating an executable function sequence from a comprehensive function pool of 63 APIs. The benchmark features over 3,660 instances of 14 scenarios that embrace real-world complexity through multi-answer annotations. To ensure quality, a team of 30 experts conducts a final audit of the benchmark, verifying factual accuracy, logical consistency, and action feasibility, and correcting any non-compliant entries. Extensive experiments demonstrate that our fine-tuned Qwen2.5-VL-7B-Instruct achieves a success rate of 19.15%, outperforming o1 (15.71%) and GPT-5 (7.39%). This result indicates that proactivity is a critical competency widely lacking in current MLLMs, yet it is learnable, emphasizing the importance of the proposed benchmark for proactivity evaluation.", "AI": {"tldr": "ProactiveMobile is a benchmark that evaluates proactive intelligence in mobile multimodal agents by requiring them to infer latent user intent from on-device context and produce executable API call sequences; experiments show current MLLMs struggle but can improve with fine-tuning.", "motivation": "Most MLLMs for mobile agents are reactive\u2014executing explicit commands\u2014while practical assistants must anticipate user needs. Progress is hindered by the lack of realistic, executable, and objective benchmarks for proactive behavior.", "method": "They formalize proactive tasks as latent intent inference across four on-device context dimensions and require generation of executable sequences from a pool of 63 mobile APIs. The benchmark comprises 3,660 instances across 14 real-world scenarios with multi-answer annotations, all audited by 30 experts for factuality, logic, and feasibility.", "result": "In experiments, a fine-tuned Qwen2.5-VL-7B-Instruct attains a 19.15% success rate, surpassing reported baselines o1 (15.71%) and GPT-5 (7.39%), indicating the task\u2019s difficulty and the learnability of proactivity.", "conclusion": "Proactivity is a broadly missing yet attainable capability in current MLLMs. ProactiveMobile provides a standardized, executable evaluation framework to drive research and benchmarking of proactive mobile agents."}}
{"id": "2602.21402", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21402", "abs": "https://arxiv.org/abs/2602.21402", "authors": ["Jinyoung Jun", "Won-Dong Jang", "Wenbin Ouyang", "Raghudeep Gadde", "Jungbeom Lee"], "title": "FlowFixer: Towards Detail-Preserving Subject-Driven Generation", "comment": null, "summary": "We present FlowFixer, a refinement framework for subject-driven generation (SDG) that restores fine details lost during generation caused by changes in scale and perspective of a subject. FlowFixer proposes direct image-to-image translation from visual references, avoiding ambiguities in language prompts. To enable image-to-image training, we introduce a one-step denoising scheme to generate self-supervised training data, which automatically removes high-frequency details while preserving global structure, effectively simulating real-world SDG errors. We further propose a keypoint matching-based metric to properly assess fidelity in details beyond semantic similarities usually measured by CLIP or DINO. Experimental results demonstrate that FlowFixer outperforms state-of-the-art SDG methods in both qualitative and quantitative evaluations, setting a new benchmark for high-fidelity subject-driven generation.", "AI": {"tldr": "FlowFixer is a reference-based, image-to-image refinement framework for subject-driven generation that restores fine details lost due to scale/perspective changes. It trains with a self-supervised one-step denoising scheme that removes high frequencies while preserving global structure and introduces a keypoint-matching metric to evaluate detail fidelity. It outperforms prior SDG methods qualitatively and quantitatively.", "motivation": "Subject-driven generation often loses identity- or subject-specific fine details when viewpoint or scale changes, and language prompts introduce ambiguity for such specifics. The authors aim to build a refinement method that can faithfully preserve and restore fine details without relying on prompts, and a metric that better captures detail fidelity than embedding-based similarities (CLIP/DINO).", "method": "Use direct image-to-image translation from visual references for SDG refinement, sidestepping prompt ambiguity. Generate self-supervised training pairs via a one-step denoising process that strips high-frequency details while keeping global structure to simulate typical SDG errors. Propose a keypoint matching-based evaluation metric focused on local/detail fidelity rather than only global semantic similarity.", "result": "Empirically, FlowFixer improves detail fidelity and overall quality over state-of-the-art SDG methods across qualitative examples and quantitative benchmarks, including the proposed metric and standard ones, establishing a new performance baseline.", "conclusion": "A self-supervised, reference-driven refinement pipeline can effectively restore lost subject details in SDG, and keypoint-aware metrics provide a more appropriate measure of fidelity than CLIP/DINO alone. FlowFixer sets a new benchmark for high-fidelity SDG and motivates detail-sensitive evaluation practices."}}
{"id": "2602.21220", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21220", "abs": "https://arxiv.org/abs/2602.21220", "authors": ["Subhadip Mitra"], "title": "Field-Theoretic Memory for AI Agents: Continuous Dynamics for Context Preservation", "comment": "15 pages, 6 figures. Code: https://github.com/rotalabs/rotalabs-fieldmem", "summary": "We present a memory system for AI agents that treats stored information as continuous fields governed by partial differential equations rather than discrete entries in a database. The approach draws from classical field theory: memories diffuse through semantic space, decay thermodynamically based on importance, and interact through field coupling in multi-agent scenarios. We evaluate the system on two established long-context benchmarks: LoCoMo (ACL 2024) with 300-turn conversations across 35 sessions, and LongMemEval (ICLR 2025) testing multi-session reasoning over 500+ turns. On LongMemEval, the field-theoretic approach achieves significant improvements: +116% F1 on multi-session reasoning (p<0.01, d= 3.06), +43.8% on temporal reasoning (p<0.001, d= 9.21), and +27.8% retrieval recall on knowledge updates (p<0.001, d= 5.00). Multi-agent experiments show near-perfect collective intelligence (>99.8%) through field coupling. Code is available at github.com/rotalabs/rotalabs-fieldmem.", "AI": {"tldr": "Proposes a PDE/field-theoretic, continuous memory for AI agents that diffuses and decays information in semantic space, reporting large gains on long-context benchmarks and near-perfect multi\u2011agent coordination via field coupling.", "motivation": "Discrete, database-like memories struggle with very long-horizon dialog, temporal credit assignment, and multi-agent information sharing. A continuous field can model gradual spread, interference, and decay of information more naturally, potentially yielding more robust long-term recall and coordination.", "method": "Represent an agent\u2019s memory as a continuous field over semantic (embedding) space. Memories evolve by PDEs: diffusion across semantically nearby regions, thermodynamic-like decay weighted by importance, and coupling terms enabling inter-agent interaction. Evaluate on LoCoMo (300-turn, 35 sessions) and LongMemEval (500+ turns) for multi-session and temporal reasoning; additionally test multi-agent coordination under field coupling. Code released at github.com/rotalabs/rotalabs-fieldmem.", "result": "On LongMemEval: +116% F1 for multi-session reasoning (p<0.01, d=3.06), +43.8% for temporal reasoning (p<0.001, d=9.21), and +27.8% retrieval recall on knowledge updates (p<0.001, d=5.00). Multi-agent experiments show >99.8% \u201ccollective intelligence.\u201d LoCoMo evaluated (numbers not specified in the abstract).", "conclusion": "Field-theoretic, PDE-driven memory improves long-horizon recall/reasoning and enables strong multi-agent coordination via coupling, suggesting continuous-memory dynamics can outperform discrete memory stores on established long-context tasks."}}
{"id": "2602.21889", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21889", "abs": "https://arxiv.org/abs/2602.21889", "authors": ["Otto Nyberg", "Fausto Carcassi", "Giovanni Cin\u00e0"], "title": "2-Step Agent: A Framework for the Interaction of a Decision Maker with AI Decision Support", "comment": "17 pages, 17 figures", "summary": "Across a growing number of fields, human decision making is supported by predictions from AI models. However, we still lack a deep understanding of the effects of adoption of these technologies. In this paper, we introduce a general computational framework, the 2-Step Agent, which models the effects of AI-assisted decision making. Our framework uses Bayesian methods for causal inference to model 1) how a prediction on a new observation affects the beliefs of a rational Bayesian agent, and 2) how this change in beliefs affects the downstream decision and subsequent outcome. Using this framework, we show by simulations how a single misaligned prior belief can be sufficient for decision support to result in worse downstream outcomes compared to no decision support. Our results reveal several potential pitfalls of AI-driven decision support and highlight the need for thorough model documentation and proper user training.", "AI": {"tldr": "Proposes the 2-Step Agent, a Bayesian causal framework that models how AI predictions update a human\u2019s beliefs and, in turn, decisions and outcomes; simulations show that even one misaligned prior can make AI decision support worse than no support, underscoring the need for documentation and user training.", "motivation": "AI predictions increasingly guide human decisions, yet we lack principled understanding of how adopting such tools affects downstream choices and outcomes. The paper seeks a general, causal account of how AI advice changes beliefs and what that implies for performance and risk.", "method": "Introduce a general computational framework (the 2-Step Agent) grounded in Bayesian causal inference: (1) formalize how an AI prediction updates a rational agent\u2019s prior beliefs on a new case; (2) propagate the updated beliefs to decisions and outcomes. Validate and probe behavior via simulations under varying prior alignments and model support conditions.", "result": "Simulation studies reveal that a single misaligned prior (relative to ground truth or model calibration) can be sufficient for AI-supported decisions to yield worse downstream outcomes than decisions made without AI support. The framework exposes mechanisms behind such degradations.", "conclusion": "AI decision support is not automatically beneficial; outcomes depend critically on alignment between user priors, model behavior, and task environment. Thorough model documentation, transparency about assumptions and calibration, and targeted user training are necessary. The framework offers a tool to anticipate and mitigate unintended harms before deployment."}}
{"id": "2602.21406", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21406", "abs": "https://arxiv.org/abs/2602.21406", "authors": ["Asim Unmesh", "Kaki Ramesh", "Mayank Patel", "Rahul Jain", "Karthik Ramani"], "title": "Exploring Vision-Language Models for Open-Vocabulary Zero-Shot Action Segmentation", "comment": "ICRA 2026", "summary": "Temporal Action Segmentation (TAS) requires dividing videos into action segments, yet the vast space of activities and alternative breakdowns makes collecting comprehensive datasets infeasible. Existing methods remain limited to closed vocabularies and fixed label sets. In this work, we explore the largely unexplored problem of Open-Vocabulary Zero-Shot Temporal Action Segmentation (OVTAS) by leveraging the strong zero-shot capabilities of Vision-Language Models (VLMs). We introduce a training-free pipeline that follows a segmentation-by-classification design: Frame-Action Embedding Similarity (FAES) matches video frames to candidate action labels, and Similarity-Matrix Temporal Segmentation (SMTS) enforces temporal consistency. Beyond proposing OVTAS, we present a systematic study across 14 diverse VLMs, providing the first broad analysis of their suitability for open-vocabulary action segmentation. Experiments on standard benchmarks show that OVTAS achieves strong results without task-specific supervision, underscoring the potential of VLMs for structured temporal understanding.", "AI": {"tldr": "Training-free, open-vocabulary, zero-shot temporal action segmentation using vision-language models: match frames to text labels via embeddings (FAES) and segment with temporal consistency (SMTS); evaluated across 14 VLMs with strong benchmark results without task-specific supervision.", "motivation": "TAS needs labeling videos into action segments, but collecting exhaustive, fixed-vocabulary annotations is infeasible given the vast, variable action space. Current systems are limited to closed label sets; an open-vocabulary, zero-shot approach avoids dataset bottlenecks and rigidity.", "method": "Define OVTAS and propose a training-free segmentation-by-classification pipeline. Use Vision-Language Models to compute Frame-Action Embedding Similarity (FAES) that aligns video frames with candidate textual action labels; then apply Similarity-Matrix Temporal Segmentation (SMTS) to enforce temporal consistency over time. Additionally, conduct a systematic comparison of 14 diverse VLMs for suitability in OVTAS.", "result": "On standard TAS benchmarks, the proposed OVTAS pipeline achieves strong performance in a zero-shot setting, requiring no task-specific supervision. The cross-model study offers the first broad empirical evidence on which VLMs are effective for open-vocabulary action segmentation.", "conclusion": "Open-vocabulary, zero-shot TAS is feasible without training by leveraging VLM frame\u2013text embeddings and simple temporal consistency constraints. VLMs show significant promise for structured temporal understanding, and the work provides both a baseline method (FAES+SMTS) and a broad evaluation to guide future research."}}
{"id": "2602.21222", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21222", "abs": "https://arxiv.org/abs/2602.21222", "authors": ["Riya Adsul", "Balachandra Devarangadi Sunil", "Isha Nalawade", "Sudharshan Govindan"], "title": "Task-Aware LoRA Adapter Composition via Similarity Retrieval in Vector Databases", "comment": null, "summary": "Parameter efficient fine tuning methods like LoRA have enabled task specific adaptation of large language models, but efficiently composing multiple specialized adapters for unseen tasks remains challenging. We present a novel framework for dynamic LoRA adapter composition that leverages similarity retrieval in vector databases to enable zero-shot generalization across diverse NLP tasks. Our approach constructs a task-aware vector database by embedding training examples from 22 datasets spanning commonsense reasoning, question answering, natural language inference, and sentiment analysis. At inference time, we retrieve the most similar training examples, compute task similarity distributions via nucleus sampling, and dynamically merge relevant LoRA adapters using retrieval weighted fusion strategies. We evaluated four merging methods Linear, Concatenation, TIES, and Magnitude Prune demonstrating that our dataset centric retrieval approach often matches or exceeds the performance of individually fine-tuned task-specific adapters. Notably, Linear merging achieves 70.95% on PIQA and 77.62% on RTE, substantially outperforming single-task baselines (46% and 52%, respectively). Our framework requires no additional retriever training, operates with frozen embeddings, and enables efficient, interpretable adapter composition. These results suggest that retrieval based dynamic merging offers a promising direction for scalable, parameter-efficient multitask learning without requiring full model retraining for each new task.", "AI": {"tldr": "They build a retrieval-driven system that dynamically merges LoRA adapters based on similarity to an input, enabling zero-shot transfer across many NLP tasks and often beating single-task LoRA baselines.", "motivation": "LoRA enables cheap task-specific fine-tuning, but it\u2019s hard to combine many specialized adapters to handle new, unseen tasks without retraining. The paper seeks a scalable, interpretable way to compose adapters for multitask and zero-shot settings.", "method": "Create a task-aware vector database by embedding examples from 22 datasets (reasoning, QA, NLI, sentiment). At inference, retrieve similar examples to the input, derive a task similarity distribution via nucleus sampling, then merge the most relevant task adapters using retrieval-weighted fusion. They compare four merging strategies: Linear, Concatenation, TIES, and Magnitude Prune. No extra retriever training; embeddings are frozen.", "result": "Their retrieval-centric merging often matches or exceeds individually fine-tuned adapters. Linear merging attains 70.95% on PIQA and 77.62% on RTE, substantially above single-task baselines (46% and 52%).", "conclusion": "Dynamic, retrieval-based LoRA adapter composition is an efficient, interpretable, and scalable alternative to per-task retraining, enabling strong zero-shot multitask performance."}}
{"id": "2602.22067", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22067", "abs": "https://arxiv.org/abs/2602.22067", "authors": ["Giuseppe Canonaco", "Alberto Pozanco", "Daniel Borrajo"], "title": "Semantic Partial Grounding via LLMs", "comment": null, "summary": "Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL descriptions. We propose SPG-LLM, which uses LLMs to analyze the domain and problem files to heuristically identify potentially irrelevant objects, actions, and predicates prior to grounding, significantly reducing the size of the grounded task. Across seven hard-to-ground benchmarks, SPG-LLM achieves faster grounding-often by orders of magnitude-while delivering comparable or better plan costs in some domains.", "AI": {"tldr": "SPG-LLM uses large language models to pre-filter irrelevant PDDL objects, actions, and predicates before grounding, drastically shrinking the grounded task and yielding much faster grounding with similar or better plan quality.", "motivation": "Classical planning suffers from an exponential blow-up during grounding as domains scale. Existing partial-grounding methods mitigate this but rely mainly on relational features or learned embeddings and ignore informative textual/structural cues in PDDL specifications.", "method": "Analyze PDDL domain and problem files with an LLM to heuristically identify and discard likely-irrelevant objects, actions, and predicates prior to grounding (a selective pre-grounding step), thereby reducing the number of grounded atoms/actions that downstream planners must consider.", "result": "On seven hard-to-ground benchmarks, SPG-LLM produces substantial grounding-time reductions\u2014often orders of magnitude\u2014while maintaining comparable plan costs and even improving them in some domains.", "conclusion": "LLM-guided pre-grounding is an effective way to curb grounding explosion without sacrificing solution quality, indicating that leveraging textual/structural information in planning descriptions can complement or surpass traditional partial-grounding approaches."}}
{"id": "2602.21416", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21416", "abs": "https://arxiv.org/abs/2602.21416", "authors": ["Marco Terral", "Haotian Zhang", "Tianyang Zhang", "Meng Lin", "Xiaoqing Xie", "Haoran Dai", "Darsh Kaushik", "Pai Peng", "Nicklas Scharpff", "David Vazquez", "Joan Rodriguez"], "title": "WildSVG: Towards Reliable SVG Generation Under Real-Word Conditions", "comment": "10 pages, 6 pages of additional material", "summary": "We introduce the task of SVG extraction, which consists in translating specific visual inputs from an image into scalable vector graphics. Existing multimodal models achieve strong results when generating SVGs from clean renderings or textual descriptions, but they fall short in real-world scenarios where natural images introduce noise, clutter, and domain shifts. A central challenge in this direction is the lack of suitable benchmarks. To address this need, we introduce the WildSVG Benchmark, formed by two complementary datasets: Natural WildSVG, built from real images containing company logos paired with their SVG annotations, and Synthetic WildSVG, which blends complex SVG renderings into real scenes to simulate difficult conditions. Together, these resources provide the first foundation for systematic benchmarking SVG extraction. We benchmark state-of-the-art multimodal models and find that current approaches perform well below what is needed for reliable SVG extraction in real scenarios. Nonetheless, iterative refinement methods point to a promising path forward, and model capabilities are steadily improving", "AI": {"tldr": "They define the real\u2011world task of extracting scalable vector graphics (SVGs) from images and release WildSVG\u2014a benchmark with natural and synthetic datasets\u2014to evaluate it. State\u2011of\u2011the\u2011art multimodal models underperform on these challenging settings, though iterative refinement shows promise.", "motivation": "Multimodal models can generate SVGs from clean renderings or text, but fail on noisy, cluttered, domain\u2011shifted natural images. Progress is hampered by the absence of a rigorous, realistic benchmark for SVG extraction.", "method": "Introduce WildSVG, comprising (1) Natural WildSVG: real images with company logos paired to their SVG annotations; and (2) Synthetic WildSVG: complex SVGs composited into real scenes to simulate difficult conditions. Benchmark current multimodal models on these datasets and probe iterative refinement strategies.", "result": "Across both datasets, state\u2011of\u2011the\u2011art models perform well below what is needed for dependable SVG extraction in realistic scenarios; however, iterative refinement yields noticeable improvements, indicating a viable path forward.", "conclusion": "WildSVG establishes the first systematic benchmark for SVG extraction in the wild. Present models are not yet reliable, but steady gains\u2014especially via iterative refinement\u2014suggest that better methods and training on realistic data can close the gap."}}
{"id": "2602.21223", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21223", "abs": "https://arxiv.org/abs/2602.21223", "authors": ["Yilin Geng", "Omri Abend", "Eduard Hovy", "Lea Frermann"], "title": "Measuring Pragmatic Influence in Large Language Model Instructions", "comment": null, "summary": "It is not only what we ask large language models (LLMs) to do that matters, but also how we prompt. Phrases like \"This is urgent\" or \"As your supervisor\" can shift model behavior without altering task content. We study this effect as pragmatic framing, contextual cues that shape directive interpretation rather than task specification. While prior work exploits such cues for prompt optimization or probes them as security vulnerabilities, pragmatic framing itself has not been treated as a measurable property of instruction following. Measuring this influence systematically remains challenging, requiring controlled isolation of framing cues. We introduce a framework with three novel components: directive-framing decomposition separating framing context from task specification; a taxonomy organizing 400 instantiations of framing into 13 strategies across 4 mechanism clusters; and priority-based measurement that quantifies influence through observable shifts in directive prioritization. Across five LLMs of different families and sizes, influence mechanisms cause consistent and structured shifts in directive prioritization, moving models from baseline impartiality toward favoring the framed directive. This work establishes pragmatic framing as a measurable and predictable factor in instruction-following systems.", "AI": {"tldr": "They show that contextual \u201cpragmatic framing\u201d in prompts (e.g., urgency, authority) systematically biases LLMs\u2019 directive choices, and provide a framework\u2014decomposition, taxonomy, and a priority-based metric\u2014to measure and predict these effects across models.", "motivation": "Prompt wording changes LLM behavior beyond task content, but prior work either exploits this or treats it as a vulnerability without isolating and quantifying the framing effect itself. A rigorous, controlled way to measure framing influence on instruction following has been lacking.", "method": "Propose a three-part framework: (1) directive\u2013framing decomposition to separate task specification from framing context; (2) a taxonomy of 400 framing instantiations into 13 strategies grouped into 4 mechanism clusters; (3) a priority-based measurement scheme that gauges how framing alters the model\u2019s prioritization between competing directives. Evaluate across five LLMs from different families/sizes.", "result": "Across all evaluated LLMs, framing mechanisms consistently and systematically shift directive prioritization away from baseline impartiality toward the framed directive, revealing predictable influence patterns.", "conclusion": "Pragmatic framing is a measurable, predictable driver of instruction-following behavior in LLMs. Recognizing and quantifying it enables better prompt design, evaluation, and safety controls that account for framing-induced bias in model priorities."}}
{"id": "2602.22070", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22070", "abs": "https://arxiv.org/abs/2602.22070", "authors": ["Jessica Y. Bo", "Lillio Mok", "Ashton Anderson"], "title": "Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts", "comment": "Second Conference of the International Association for Safe and Ethical Artificial Intelligence (IASEAI 2026)", "summary": "Large language models are increasingly used in decision-making tasks that require them to process information from a variety of sources, including both human experts and other algorithmic agents. How do LLMs weigh the information provided by these different sources? We consider the well-studied phenomenon of algorithm aversion, in which human decision-makers exhibit bias against predictions from algorithms. Drawing upon experimental paradigms from behavioural economics, we evaluate how eightdifferent LLMs delegate decision-making tasks when the delegatee is framed as a human expert or an algorithmic agent. To be inclusive of different evaluation formats, we conduct our study with two task presentations: stated preferences, modeled through direct queries about trust towards either agent, and revealed preferences, modeled through providing in-context examples of the performance of both agents. When prompted to rate the trustworthiness of human experts and algorithms across diverse tasks, LLMs give higher ratings to the human expert, which correlates with prior results from human respondents. However, when shown the performance of a human expert and an algorithm and asked to place an incentivized bet between the two, LLMs disproportionately choose the algorithm, even when it performs demonstrably worse. These discrepant results suggest that LLMs may encode inconsistent biases towards humans and algorithms, which need to be carefully considered when they are deployed in high-stakes scenarios. Furthermore, we discuss the sensitivity of LLMs to task presentation formats that should be broadly scrutinized in evaluation robustness for AI safety.", "AI": {"tldr": "LLMs praise humans in stated trust ratings but bet on algorithms in revealed-choice settings\u2014even when algorithms underperform\u2014revealing framing-sensitive, internally inconsistent biases with safety and evaluation implications.", "motivation": "As LLMs increasingly aggregate advice from humans and other algorithms, understanding their source-weighting is critical. Human decision-makers often show algorithm aversion; the paper asks whether LLMs exhibit similar or different biases and how presentation format affects them.", "method": "Eight LLMs are evaluated with two paradigms: (1) stated preferences\u2014direct prompts to rate trust in a human expert vs an algorithm across tasks; (2) revealed preferences\u2014few-shot/in-context demonstrations of each agent\u2019s performance followed by an incentivized bet on which to follow. The study compares choices across formats and assesses sensitivity to presentation.", "result": "Across diverse tasks, LLMs assign higher trust ratings to human experts, mirroring human respondents. Yet when shown side-by-side performance and asked to place a bet, LLMs disproportionately choose the algorithm, including cases where it is shown to perform worse. This divergence indicates inconsistent source preferences driven by task framing.", "conclusion": "LLMs encode conflicting biases toward human and algorithmic advice that depend strongly on evaluation format. Such framing sensitivity poses risks in high-stakes deployments and highlights the need for robust, format-invariant evaluations, better prompt design/calibration, and safeguards against unwarranted algorithm favoritism or human favoritism."}}
{"id": "2602.21421", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21421", "abs": "https://arxiv.org/abs/2602.21421", "authors": ["Jan Pauls", "Karsten Schr\u00f6dter", "Sven Ligensa", "Martin Schwartz", "Berkant Turan", "Max Zimmer", "Sassan Saatchi", "Sebastian Pokutta", "Philippe Ciais", "Fabian Gieseke"], "title": "ECHOSAT: Estimating Canopy Height Over Space And Time", "comment": "19 pages, 12 figures, 6 tables", "summary": "Forest monitoring is critical for climate change mitigation. However, existing global tree height maps provide only static snapshots and do not capture temporal forest dynamics, which are essential for accurate carbon accounting. We introduce ECHOSAT, a global and temporally consistent tree height map at 10 m resolution spanning multiple years. To this end, we resort to multi-sensor satellite data to train a specialized vision transformer model, which performs pixel-level temporal regression. A self-supervised growth loss regularizes the predictions to follow growth curves that are in line with natural tree development, including gradual height increases over time, but also abrupt declines due to forest loss events such as fires. Our experimental evaluation shows that our model improves state-of-the-art accuracies in the context of single-year predictions. We also provide the first global-scale height map that accurately quantifies tree growth and disturbances over time. We expect ECHOSAT to advance global efforts in carbon monitoring and disturbance assessment. The maps can be accessed at https://github.com/ai4forest/echosat.", "AI": {"tldr": "ECHOSAT delivers the first global, multi-year 10 m tree-height product by training a multi-sensor, vision-transformer model with a self-supervised growth loss, achieving state-of-the-art single-year accuracy while capturing growth and disturbance dynamics for carbon monitoring.", "motivation": "Static global tree-height maps miss temporal dynamics critical for accurate carbon accounting and disturbance assessment. A temporally consistent, high-resolution, global product is needed to quantify forest growth and loss over time.", "method": "Fuse multi-sensor satellite data and train a specialized vision transformer that performs pixel-level temporal regression. Regularize predictions with a self-supervised growth loss so trajectories follow biologically plausible curves\u2014gradual increases with possible abrupt declines from events like fires.", "result": "Improved accuracy over prior state-of-the-art for single-year height estimation and provision of the first global-scale map that quantifies temporal tree growth and disturbances with 10 m resolution across multiple years.", "conclusion": "ECHOSAT enables temporally resolved, high-resolution monitoring of forest structure to support carbon accounting and disturbance analysis; maps are openly available for global use and can advance monitoring, reporting, and verification efforts."}}
{"id": "2602.21224", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21224", "abs": "https://arxiv.org/abs/2602.21224", "authors": ["Yuetao Chen", "Xuliang Wang", "Xinzhou Zheng", "Ming Li", "Peng Wang", "Hong Xu"], "title": "Make Every Draft Count: Hidden State based Speculative Decoding", "comment": null, "summary": "Speculative decoding has emerged as a pivotal technique to accelerate LLM inference by employing a lightweight draft model to generate candidate tokens that are subsequently verified by the target model in parallel. However, while this paradigm successfully increases the arithmetic intensity of memory-bound inference, it causes significant compute inefficiency: the majority of draft tokens fail verification and are discarded, resulting in waste of computation. Motivated by the goal of recollecting this wasted computation, we propose a novel system that transforms discarded drafts into reusable tokens. Our key insight is to perform auto-regressive prediction at the hidden states level and postpone the integrating token information after the hidden states generation, so the draft hidden states are not contaminated by incorrect tokens, enabling hidden state reuse. To implement such a system, first we introduce a draft model architecture based on auto-regressive hidden states, which preserves richer semantics than token-based drafters to facilitate draft repurposing. Second, we design an efficient token information injection mechanism that leverages our specialized draft model to construct high-quality draft token trees and enables resampling tokens from verification failures. Third, we eliminate the overhead hidden in our design to further maximize hardware utilization. We conducted extensive evaluations against various baselines, demonstrating up to a 3.3x speedup against standard speculative decoding.", "AI": {"tldr": "They reclaim wasted compute in speculative decoding by drafting at the hidden\u2011state level and injecting token information afterward, which lets failed drafts be reused/resampled instead of discarded\u2014yielding up to 3.3\u00d7 speedup over standard speculative decoding.", "motivation": "Speculative decoding boosts arithmetic intensity but wastes compute because many draft tokens fail target-model verification and are thrown away. The goal is to recover this wasted work and improve end-to-end inference efficiency and hardware utilization.", "method": "1) A draft model that is autoregressive over hidden states, decoupling state generation from specific token choices so states aren\u2019t corrupted by wrong tokens and can be reused. 2) A token-information injection mechanism that leverages these states to construct high-quality draft token trees and to resample tokens from verification failures. 3) Systems optimizations to trim overhead and better utilize hardware; target-model verification remains in parallel.", "result": "Across extensive evaluations vs. strong baselines, the approach achieves up to 3.3\u00d7 speedup compared with standard token-based speculative decoding.", "conclusion": "Operating at the hidden-state level enables reuse of failed speculative drafts, cutting compute waste and accelerating LLM inference. This is a practical path to higher throughput in memory-bound inference workloads and may generalize to broader LLM serving settings."}}
{"id": "2602.22094", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22094", "abs": "https://arxiv.org/abs/2602.22094", "authors": ["Nguyen Cong Nhat Le", "John G. Rogers", "Claire N. Bonial", "Neil T. Dantam"], "title": "Petri Net Relaxation for Infeasibility Explanation and Sequential Task Planning", "comment": "16 pages, 5 figures. Submitted to 17th World Symposium on the Algorithmic Foundations of Robotics (WAFR) on 01/14/2026", "summary": "Plans often change due to changes in the situation or our understanding of the situation. Sometimes, a feasible plan may not even exist, and identifying such infeasibilities is useful to determine when requirements need adjustment. Common planning approaches focus on efficient one-shot planning in feasible cases rather than updating domains or detecting infeasibility. We propose a Petri net reachability relaxation to enable robust invariant synthesis, efficient goal-unreachability detection, and helpful infeasibility explanations. We further leverage incremental constraint solvers to support goal and constraint updates. Empirically, compared to baselines, our system produces a comparable number of invariants, detects up to 2 times more infeasibilities, performs competitively in one-shot planning, and outperforms in sequential plan updates in the tested domains.", "AI": {"tldr": "They introduce a planning framework that relaxes domain dynamics into Petri net reachability to synthesize invariants, quickly detect goal unreachability, and explain infeasibility; paired with incremental constraint solving, it handles changing goals/constraints efficiently and outperforms baselines in sequential updates while remaining competitive in one-shot planning.", "motivation": "Real-world planning often faces changing goals/constraints and sometimes outright infeasibility. Traditional one-shot planners prioritize finding a plan when one exists and offer limited support for diagnosing unreachability, explaining failures, or rapidly adapting to updates. There is a need for methods that detect infeasibility early, provide informative explanations, and reuse computation across plan revisions.", "method": "They construct a Petri net reachability relaxation of the planning problem to enable robust invariant synthesis and efficient goal-unreachability detection, along with human-usable infeasibility explanations. They integrate incremental constraint solvers to efficiently accommodate updates to goals and constraints by reusing prior solving effort.", "result": "Across tested domains, the system produces a similar volume of useful invariants as baselines, detects up to twice as many infeasible goals, achieves competitive one-shot planning performance, and delivers superior performance for sequences of plan updates.", "conclusion": "Petri net reachability relaxation combined with incremental constraint solving yields a planner that is more robust to changing requirements, better at diagnosing infeasibility and explaining it, and maintains strong performance, especially in iterative update settings."}}
{"id": "2602.21425", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21425", "abs": "https://arxiv.org/abs/2602.21425", "authors": ["Abel Gon\u00e7alves Chinaglia", "Guilherme Manna Cesar", "Paulo Roberto Pereira Santiago"], "title": "Automating Timed Up and Go Phase Segmentation and Gait Analysis via the tugturn Markerless 3D Pipeline", "comment": "16 pages, 2 figures, 1 pdf report, submitted to arXiv under cs.CV", "summary": "Instrumented Timed Up and Go (TUG) analysis can support clinical and research decision-making, but robust and reproducible markerless pipelines are still limited. We present \\textit{tugturn.py}, a Python-based workflow for 3D markerless TUG processing that combines phase segmentation, gait-event detection, spatiotemporal metrics, intersegmental coordination, and dynamic stability analysis. The pipeline uses spatial thresholds to segment each trial into stand, first gait, turning, second gait, and sit phases, and applies a relative-distance strategy to detect heel-strike and toe-off events within valid gait windows. In addition to conventional kinematics, \\textit{tugturn} provides Vector Coding outputs and Extrapolated Center of Mass (XCoM)-based metrics. The software is configured through TOML files and produces reproducible artifacts, including HTML reports, CSV tables, and quality-assurance visual outputs. A complete runnable example is provided with test data and command-line instructions. This manuscript describes the implementation, outputs, and reproducibility workflow of \\textit{tugturn} as a focused software contribution for markerless biomechanical TUG analysis.", "AI": {"tldr": "tugturn.py is a Python, markerless, 3D TUG analysis pipeline that segments phases, detects gait events, and computes spatiotemporal, coordination (Vector Coding), and dynamic stability (XCoM) metrics, producing reproducible reports and QA visuals with a runnable example.", "motivation": "Markerless TUG assessments are clinically useful, but existing pipelines lack robustness, reproducibility, and integrated metrics/QA needed for decision-making and research.", "method": "A configurable (TOML) Python workflow uses spatial thresholds to segment TUG into stand, first gait, turn, second gait, and sit; applies a relative-distance strategy for heel-strike/toe-off within valid gait windows; computes conventional kinematics, Vector Coding for intersegmental coordination, and XCoM-based stability metrics; outputs HTML reports, CSVs, and QA visualizations; includes test data and CLI instructions.", "result": "A ready-to-run, open software package (tugturn) implementing the full markerless TUG processing pipeline with reproducible artifacts and example data; demonstrates end-to-end capability from raw 3D kinematics to metrics and reports.", "conclusion": "tugturn provides a focused, reproducible, and comprehensive markerless TUG analysis solution, facilitating clinical and research use; primary contribution is software implementation and workflow rather than experimental validation."}}
{"id": "2602.21225", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21225", "abs": "https://arxiv.org/abs/2602.21225", "authors": ["Mohammed Hamdan", "Vincenzo Dentamaro", "Giuseppe Pirlo", "Mohamed Cheriet"], "title": "Architecture-Agnostic Curriculum Learning for Document Understanding: Empirical Evidence from Text-Only and Multimodal", "comment": null, "summary": "We investigate whether progressive data scheduling -- a curriculum learning strategy that incrementally increases training data exposure (33\\%$\\rightarrow$67\\%$\\rightarrow$100\\%) -- yields consistent efficiency gains across architecturally distinct document understanding models. By evaluating BERT (text-only, 110M parameters) and LayoutLMv3 (multimodal, 126M parameters) on the FUNSD and CORD benchmarks, we establish that this schedule reduces wall-clock training time by approximately 33\\%, commensurate with the reduction from 6.67 to 10.0 effective epoch-equivalents of data. To isolate curriculum effects from compute reduction, we introduce matched-compute baselines (Standard-7) that control for total gradient updates. On the FUNSD dataset, the curriculum significantly outperforms the matched-compute baseline for BERT ($\u0394$F1 = +0.023, $p=0.022$, $d_z=3.83$), constituting evidence for a genuine scheduling benefit in capacity-constrained models. In contrast, no analogous benefit is observed for LayoutLMv3 ($p=0.621$), whose multimodal representations provide sufficient inductive bias. On the CORD dataset, all conditions converge to equivalent F1 scores ($\\geq$0.947) irrespective of scheduling, indicating a performance ceiling. Schedule ablations comparing progressive, two-phase, reverse, and random pacing confirm that the efficiency gain derives from reduced data volume rather than ordering. Taken together, these findings demonstrate that progressive scheduling is a reliable compute-reduction strategy across model families, with curriculum-specific benefits contingent on the interaction between model capacity and task complexity.", "AI": {"tldr": "Progressive data scheduling (33%\u219267%\u2192100%) reliably cuts training time by ~33% across document-understanding models; it yields extra accuracy only for capacity-limited BERT on FUNSD, not for multimodal LayoutLMv3 or the easier CORD task, and the efficiency gains stem from reduced data volume rather than example ordering.", "motivation": "Test whether a curriculum that gradually increases data exposure provides consistent training-efficiency gains without hurting accuracy across distinct architectures, and disentangle true curriculum effects from mere compute reduction.", "method": "Evaluate BERT-base (110M, text-only) and LayoutLMv3-base (126M, multimodal) on FUNSD and CORD under a progressive schedule (33%\u219267%\u2192100% of data). Compare against matched-compute baselines (Standard-7) that hold total gradient updates constant to isolate curriculum benefits. Run schedule ablations (progressive, two-phase, reverse, random) to assess the role of ordering vs volume.", "result": "- ~33% reduction in wall-clock training, aligning with a drop from ~10.0 to ~6.67 effective epoch-equivalents.\n- FUNSD: Progressive schedule significantly outperforms matched-compute for BERT (\u0394F1=+0.023, p=0.022, dz=3.83); no benefit for LayoutLMv3 (p=0.621).\n- CORD: All settings converge to high, equivalent F1 (\u22650.947), indicating a performance ceiling.\n- Ablations show efficiency gains arise from reduced data volume; ordering (progressive vs reverse/random) does not add value.", "conclusion": "Progressive scheduling is a dependable compute-reduction strategy across model families. Curriculum-specific accuracy gains appear when model capacity is constrained relative to task complexity, but vanish for models with strong inductive biases or on ceilinged tasks; thus, use progressive scheduling primarily to save compute, with potential accuracy upside in under-capacity regimes."}}
{"id": "2602.21428", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21428", "abs": "https://arxiv.org/abs/2602.21428", "authors": ["Binesh Sadanandan", "Vahid Behzadan"], "title": "PSF-Med: Measuring and Explaining Paraphrase Sensitivity in Medical Vision Language Models", "comment": null, "summary": "Medical Vision Language Models (VLMs) can change their answers when clinicians rephrase the same question, which raises deployment risks. We introduce Paraphrase Sensitivity Failure (PSF)-Med, a benchmark of 19,748 chest Xray questions paired with about 92,000 meaningpreserving paraphrases across MIMIC-CXR and PadChest. Across six medical VLMs, we measure yes/no flips for the same image and find flip rates from 8% to 58%. However, low flip rate does not imply visual grounding: text-only baselines show that some models stay consistent even when the image is removed, suggesting they rely on language priors. To study mechanisms in one model, we apply GemmaScope 2 Sparse Autoencoders (SAEs) to MedGemma 4B and analyze FlipBank, a curated set of 158 flip cases. We identify a sparse feature at layer 17 that correlates with prompt framing and predicts decision margin shifts. In causal patching, removing this feature's contribution recovers 45% of the yesminus-no logit margin on average and fully reverses 15% of flips. Acting on this finding, we show that clamping the identified feature at inference reduces flip rates by 31% relative with only a 1.3 percentage-point accuracy cost, while also decreasing text-prior reliance. These results suggest that flip rate alone is not enough; robustness evaluations should test both paraphrase stability and image reliance.", "AI": {"tldr": "They introduce PSF-Med, a chest X-ray paraphrase-sensitivity benchmark, show substantial yes/no answer flips (8\u201358%) across medical VLMs, reveal that low flip rates can stem from language priors rather than visual grounding, identify a specific internal feature driving paraphrase-induced flips in MedGemma 4B, and demonstrate that clamping this feature reduces flips by 31% with a small (1.3 pp) accuracy cost. They argue robustness must assess both paraphrase stability and image reliance.", "motivation": "In clinical practice, clinicians naturally rephrase questions. If medical VLMs change answers under paraphrases, they pose safety risks. Existing robustness metrics may conflate stability with overreliance on text priors. There is a need to quantify paraphrase sensitivity, disentangle language priors from visual grounding, and explore mechanisms to mitigate flips.", "method": "1) Build PSF-Med: 19,748 chest X-ray questions with ~92k meaning-preserving paraphrases from MIMIC-CXR and PadChest. 2) Evaluate six medical VLMs for yes/no answer flips per image-question paraphrase set; compare to text-only baselines to assess language-prior reliance. 3) Mechanistic study on MedGemma 4B using GemmaScope 2 sparse autoencoders over a curated FlipBank of 158 flip cases to locate features tied to prompt framing. 4) Causal patching: remove a layer-17 sparse feature\u2019s contribution to measure effects on logits. 5) Intervention: clamp the identified feature at inference to test flip-rate reduction and accuracy trade-offs.", "result": "Flip rates range 8\u201358% across models. Some models remain consistent even without the image, implicating language priors. A layer-17 sparse feature correlates with prompt framing and predicts decision-margin shifts; removing it recovers on average 45% of the yes-minus-no logit margin and fully reverses 15% of flips. Clamping this feature reduces flip rates by 31% (relative) with only a 1.3 percentage-point accuracy drop and reduces text-prior reliance.", "conclusion": "Flip rate alone is insufficient as a robustness metric; evaluations must jointly test paraphrase stability and visual grounding. Mechanistic interpretability can surface causal features driving instability, enabling targeted interventions that improve robustness with minimal accuracy cost."}}
{"id": "2602.21226", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21226", "abs": "https://arxiv.org/abs/2602.21226", "authors": ["Ezieddin Elmahjub", "Junaid Qadir", "Abdullah Mushtaq", "Rafay Naeem", "Ibrahim Ghaznavi", "Waleed Iqbal"], "title": "IslamicLegalBench: Evaluating LLMs Knowledge and Reasoning of Islamic Law Across 1,200 Years of Islamic Pluralist Legal Traditions", "comment": "This manuscript has been submitted for review to Artificial Intelligence \\& Law", "summary": "As millions of Muslims turn to LLMs like GPT, Claude, and DeepSeek for religious guidance, a critical question arises: Can these AI systems reliably reason about Islamic law? We introduce IslamicLegalBench, the first benchmark evaluating LLMs across seven schools of Islamic jurisprudence, with 718 instances covering 13 tasks of varying complexity. Evaluation of nine state-of-the-art models reveals major limitations: the best model achieves only 68% correctness with 21% hallucination, while several models fall below 35% correctness and exceed 55% hallucination. Few-shot prompting provides minimal gains, improving only 2 of 9 models by >1%. Moderate-complexity tasks requiring exact knowledge show the highest errors, whereas high-complexity tasks display apparent competence through semantic reasoning. False premise detection indicates risky sycophancy, with 6 of 9 models accepting misleading assumptions at rates above 40%. These results highlight that prompt-based methods cannot compensate for missing foundational knowledge. IslamicLegalBench offers the first systematic framework to evaluate Islamic legal reasoning in AI, revealing critical gaps in tools increasingly relied on for spiritual guidance.", "AI": {"tldr": "IslamicLegalBench introduces a first-of-its-kind benchmark for Islamic legal reasoning and shows that current LLMs are unreliable: even the best reaches only 68% accuracy with high hallucination and strong susceptibility to misleading premises; prompting offers negligible gains, underscoring missing foundational knowledge.", "motivation": "Millions are seeking religious guidance from LLMs, but there is no systematic way to assess whether these systems can accurately reason across diverse schools of Islamic jurisprudence. This poses safety, trust, and reliability concerns for spiritually consequential advice.", "method": "The authors construct IslamicLegalBench: 718 instances spanning 13 tasks at varying difficulty across seven fiqh schools. They evaluate nine state-of-the-art models, measuring correctness, hallucination rates, gains from few-shot prompting, task-wise difficulty profiles, and acceptance of false premises (sycophancy).", "result": "Overall performance is poor: best model 68% correct with 21% hallucination; several models under 35% correct and over 55% hallucination. Few-shot prompting yields minimal gains (only 2/9 models improve by >1%). Tasks needing exact, moderate-complexity knowledge have the highest error, while high-complexity tasks exhibit seemingly competent semantic reasoning. Six of nine models accept misleading assumptions >40% of the time, indicating risky sycophancy.", "conclusion": "Prompt engineering alone cannot fix gaps in domain knowledge; current LLMs are not dependable for Islamic legal guidance. IslamicLegalBench establishes a needed evaluation framework and reveals critical safety and reliability gaps, motivating domain-grounded knowledge integration and stronger safeguards against hallucination and sycophancy."}}
{"id": "2602.21435", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21435", "abs": "https://arxiv.org/abs/2602.21435", "authors": ["Shengqiong Wu", "Bobo Li", "Xinkai Wang", "Xiangtai Li", "Lei Cui", "Furu Wei", "Shuicheng Yan", "Hao Fei", "Tat-seng Chua"], "title": "Synergizing Understanding and Generation with Interleaved Analyzing-Drafting Thinking", "comment": "28 pages, 17 figures, 6 tables, ICLR conference", "summary": "Unified Vision-Language Models (UVLMs) aim to advance multimodal learning by supporting both understanding and generation within a single framework. However, existing approaches largely focus on architectural unification while overlooking the need for explicit interaction between the two capabilities during task solving. As a result, current models treat understanding and generation as parallel skills rather than synergistic processes. To achieve real synergy, we introduce the interleaved Analyzing-Drafting problem-solving loop (AD-Loop), a new think paradigm that dynamically alternates between analytic and drafting operations. By interleaving textual thoughts with visual thoughts, AD-Loop enables models to iteratively refine both comprehension and outputs, fostering genuine synergy. To train this mechanism, we design a two-stage strategy: supervised learning on interleaved thought data to initialize alternation, followed by reinforcement learning to promote adaptive and autonomous control. Extensive experiments demonstrate that AD-Loop consistently improves performance across standard benchmarks for both understanding and generation, with strong transferability to various UVLMs architectures. Visual analyses further validate the effectiveness of implicit visual thoughts. These results highlight AD-Loop as a principled and broadly applicable strategy for synergizing comprehension and creation. The project page is at https://sqwu.top/AD-Loop.", "AI": {"tldr": "Proposes AD-Loop, an interleaved analyze\u2013draft reasoning loop with textual and visual thoughts, trained first with supervised interleaved traces then refined via reinforcement learning, yielding consistent gains in both understanding and generation across UVLM benchmarks and architectures.", "motivation": "Existing unified vision\u2013language models unify architectures but not the problem-solving process; understanding and generation run in parallel without explicit interaction, limiting true synergy during multimodal tasks.", "method": "Introduce the Analyzing\u2013Drafting (AD-Loop) paradigm that alternates analytic (comprehension) and drafting (generation) steps, interleaving textual and implicit visual thoughts to iteratively refine reasoning and outputs. Train with a two-stage scheme: (1) supervised learning on interleaved thought data to initialize the alternation; (2) reinforcement learning to enable adaptive, autonomous control of when and how to alternate. Designed to be plug-and-play across UVLM architectures.", "result": "Across standard multimodal understanding and generation benchmarks, AD-Loop consistently improves performance and transfers well to diverse UVLMs. Visual analyses indicate that implicit visual thoughts contribute meaningfully to the gains.", "conclusion": "Interleaving analysis and drafting provides a principled, broadly applicable way to synergize comprehension and creation in UVLMs, advancing both capabilities within a single framework."}}
{"id": "2602.21227", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21227", "abs": "https://arxiv.org/abs/2602.21227", "authors": ["Caiqi Zhang", "Menglin Xia", "Xuchao Zhang", "Daniel Madrigal", "Ankur Mallick", "Samuel Kessler", "Victor Ruehle", "Saravan Rajmohan"], "title": "Budget-Aware Agentic Routing via Boundary-Guided Training", "comment": null, "summary": "As large language models (LLMs) evolve into autonomous agents that execute long-horizon workflows, invoking a high-capability model at every step becomes economically unsustainable. While model routing is effective for single-turn queries, agentic routing is a sequential, path-dependent problem: early mistakes compound, feedback is often at the end of the episode, and deployments often demand strict per-task spending limits. We propose Budget-Aware Agentic Routing, which selects between a cheap and an expensive model at each step to optimize the cost--success frontier and to operate under strict per-task budgets. We propose Boundary-Guided Training, which leverages two boundary policies (always-small vs.\\ always-large) to build a difficulty taxonomy and to anchor learning under sparse rewards. Our approach warms start with boundary-guided SFT data synthesis via stratified sampling of cost-efficient trajectories, then applies Boundary-Guided Policy Optimization (BoPO), combining boundary-relative rewards with a reference-guided advantage to avoid degenerate cheap-failure solutions. Experiment results show that our method improves the efficiency frontier, matching strong routing baselines at substantially lower cost while demonstrating generalization to strict inference-time budget constraints. Overall, our work establishes a foundational framework for agentic routing, shifting the paradigm from static model selection to dynamic, budget-aware sequential decision-making.", "AI": {"tldr": "They train a policy to dynamically choose between a cheap and an expensive LLM at each step of an agent\u2019s workflow to maximize task success under strict per-task budgets, improving the cost\u2013performance frontier versus static or myopic routing.", "motivation": "Always using a high-capability LLM for multi-step agents is too costly. Existing routing focuses on single turns and ignores sequential, path-dependent errors, sparse end-of-episode feedback, and hard budget caps.", "method": "Budget-Aware Agentic Routing with Boundary-Guided Training. They define two boundary policies (always-small vs. always-large) to create a difficulty taxonomy and stable learning signals. The training pipeline warms up with boundary-guided SFT via stratified sampling of cost-efficient trajectories, then applies Boundary-Guided Policy Optimization (BoPO), which uses boundary-relative rewards and a reference-guided advantage to avoid collapsing to cheap-but-failing choices. The learned policy selects the model (cheap vs. expensive) at each step conditioned on budget and state.", "result": "Across benchmarks, the learned router matches strong baselines\u2019 success rates at substantially lower cost and respects strict inference-time budget constraints, moving the efficiency frontier outward.", "conclusion": "Dynamic, budget-aware sequential routing is a viable alternative to static model selection for LLM agents. Their boundary-guided framework provides stable signals under sparse rewards and yields generalizable policies that optimize success under tight budgets."}}
{"id": "2602.21452", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21452", "abs": "https://arxiv.org/abs/2602.21452", "authors": ["Nicholas Dietrich", "David McShannon"], "title": "Adversarial Robustness of Deep Learning-Based Thyroid Nodule Segmentation in Ultrasound", "comment": "14 pages, 3 figures, 3 tables", "summary": "Introduction: Deep learning-based segmentation models are increasingly integrated into clinical imaging workflows, yet their robustness to adversarial perturbations remains incompletely characterized, particularly for ultrasound images. We evaluated adversarial attacks and inference-time defenses for thyroid nodule segmentation in B-mode ultrasound. Methods: Two black-box adversarial attacks were developed: (1) Structured Speckle Amplification Attack (SSAA), which injects boundary-targeted noise, and (2) Frequency-Domain Ultrasound Attack (FDUA), which applies bandpass-filtered phase perturbations in the Fourier domain. Three inference-time mitigations were evaluated on adversarial images: randomized preprocessing with test-time augmentation, deterministic input denoising, and stochastic ensemble inference with consistency-aware aggregation. Experiments were conducted on a U-Net segmentation model trained on cine-clips from a database of 192 thyroid nodules. Results: The baseline model achieved a mean Dice similarity coefficient (DSC) of 0.76 (SD 0.20) on unperturbed images. SSAA reduced DSC by 0.29 (SD 0.20) while maintaining high visual similarity (SSIM = 0.94). FDUA resulted in a smaller DSC reduction of 0.11 (SD 0.09) with lower visual fidelity (SSIM = 0.82). Against SSAA, all three defenses significantly improved DSC after correction, with deterministic denoising showing the largest recovery (+0.10, p < 0.001), followed by randomized preprocessing (+0.09, p < 0.001), and stochastic ensemble inference (+0.08, p = 0.002). No defense achieved statistically significant improvement against FDUA. Conclusion: Spatial-domain adversarial perturbations in ultrasound segmentation showed partial mitigation with input preprocessing, whereas frequency-domain perturbations were not mitigated by the defenses, highlighting modality-specific challenges in adversarial robustness evaluation.", "AI": {"tldr": "Evaluates black-box adversarial attacks and inference-time defenses for thyroid nodule segmentation in ultrasound; spatial-domain attacks degrade performance but can be partially mitigated, while frequency-domain attacks remain resilient to tested defenses.", "motivation": "Deep learning segmenters are entering clinical ultrasound workflows, but their robustness to adversarial perturbations\u2014especially ultrasound-specific artifacts\u2014remains underexplored. Understanding attack modalities and practical defenses is necessary for safe deployment.", "method": "Trained a U-Net on B-mode cine-clips from 192 thyroid nodules. Introduced two black-box attacks: (1) SSAA, adding boundary-targeted speckle-like noise in the spatial domain; (2) FDUA, applying bandpass-filtered phase perturbations in the Fourier domain. Tested three inference-time defenses: randomized preprocessing with test-time augmentation, deterministic denoising, and stochastic ensemble inference with consistency-aware aggregation. Assessed DSC and SSIM under attack and defense conditions.", "result": "Baseline DSC = 0.76 (SD 0.20). SSAA reduced DSC by 0.29 (SD 0.20) while preserving high SSIM (0.94). FDUA reduced DSC by 0.11 (SD 0.09) with lower SSIM (0.82). Against SSAA, defenses improved DSC: denoising +0.10 (p<0.001), randomized preprocessing +0.09 (p<0.001), ensemble +0.08 (p=0.002). No defense significantly improved performance against FDUA.", "conclusion": "Input preprocessing can partially counter spatial-domain adversarial perturbations in ultrasound segmentation, but frequency-domain phase perturbations evade these defenses. Robustness evaluation and mitigation may need modality- and frequency-aware strategies for clinical safety."}}
{"id": "2602.21228", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21228", "abs": "https://arxiv.org/abs/2602.21228", "authors": ["Yuancheng Yang", "Lin Yang", "Xu Wang", "Chao Tong", "Haihua Yang"], "title": "ImpRIF: Stronger Implicit Reasoning Leads to Better Complex Instruction Following", "comment": null, "summary": "As applications of large language models (LLMs) become increasingly complex, the demand for robust complex instruction following capabilities is growing accordingly. We argue that a thorough understanding of the instruction itself, especially the latent reasoning structure embedded between the lines, is crucial for improving instruction following. Therefore we target complex instructions that involve implicit reasoning, intricate logical relations, and multi-constraint dependencies. We propose ImpRIF, a method to enhance LLMs' understanding of implicit reasoning instructions, thereby improving its ability to follow complex instructions. We formalize such instructions as verifiable reasoning graphs, enabling programmatic verification and graph-driven chain-of-thought reasoning. Based on this formulation, we synthesize large-scale single- and multi-turn data, propose fine-tuning with graph reasoning, and apply reinforcement learning to explicitly train models to reason along the graph. On five complex instruction following benchmarks, our models substantially outperform their base models. These results demonstrate that enhancing implicit reasoning capabilities can significantly improve complex instruction following. This project will be open-sourced in the near future.", "AI": {"tldr": "ImpRIF models complex, implicitly reasoned instructions as verifiable reasoning graphs and trains LLMs with graph-aware data synthesis, fine-tuning, and RL to reason along these graphs, achieving strong gains on five complex instruction-following benchmarks.", "motivation": "LLM applications increasingly require following instructions that embed latent reasoning, intricate logic, and multiple interdependent constraints. Current models often miss this implicit structure, limiting reliability on complex tasks. The work argues that making this hidden reasoning explicit is key to robust instruction following.", "method": "Formalize complex instructions as verifiable reasoning graphs enabling programmatic checking and graph-driven chain-of-thought. Synthesize large-scale single- and multi-turn datasets aligned to these graphs. Fine-tune models with graph reasoning supervision and apply reinforcement learning to explicitly train models to follow the graph during inference.", "result": "Across five complex instruction-following benchmarks, models trained with ImpRIF substantially outperform their base counterparts, indicating improved handling of implicit reasoning, logical relations, and multi-constraint dependencies.", "conclusion": "Explicitly modeling and training implicit reasoning via verifiable reasoning graphs markedly improves complex instruction following in LLMs; the authors plan to open-source the project soon."}}
{"id": "2602.21473", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21473", "abs": "https://arxiv.org/abs/2602.21473", "authors": ["Somayeh Hussaini", "Tobias Fischer", "Michael Milford"], "title": "Automatic Map Density Selection for Locally-Performant Visual Place Recognition", "comment": "Under Review", "summary": "A key challenge in translating Visual Place Recognition (VPR) from the lab to long-term deployment is ensuring a priori that a system can meet user-specified performance requirements across different parts of an environment, rather than just on average globally. A critical mechanism for controlling local VPR performance is the density of the reference mapping database, yet this factor is largely neglected in existing work, where benchmark datasets with fixed, engineering-driven (sensors, storage, GPS frequency) sampling densities are typically used. In this paper, we propose a dynamic VPR mapping approach that uses pairs of reference traverses from the target environment to automatically select an appropriate map density to satisfy two user-defined requirements: (1) a target Local Recall@1 level, and (2) the proportion of the operational environment over which this requirement must be met or exceeded, which we term the Recall Achievement Rate (RAR). Our approach is based on the hypothesis that match patterns between multiple reference traverses, evaluated across different map densities, can be modelled to predict the density required to meet these performance targets on unseen deployment data. Through extensive experiments across multiple VPR methods and the Nordland and Oxford RobotCar benchmarks, we show that our system consistently achieves or exceeds the specified local recall level over at least the user-specified proportion of the environment. Comparisons with alternative baselines demonstrate that our approach reliably selects the correct operating point in map density, avoiding unnecessary over-densification. Finally, ablation studies and analysis evaluate sensitivity to reference map choice and local space definitions, and reveal that conventional global Recall@1 is a poor predictor of the often more operationally meaningful RAR metric.", "AI": {"tldr": "Dynamic VPR mapping that predicts the minimal reference map density needed to meet a user-specified Local Recall@1 over a target fraction of the environment (RAR), using match patterns from two reference traverses; validated on Nordland and Oxford RobotCar, it meets targets while avoiding over-densification and shows global Recall@1 poorly predicts RAR.", "motivation": "Deployments need guarantees on local recognition performance and spatial coverage, not just good global averages. Map density is the main controllable factor for local VPR performance, yet is usually fixed by dataset engineering constraints. A principled way to set density to satisfy operational requirements is missing.", "method": "Use pairs of reference traverses from the target environment and evaluate match patterns across candidate map densities. Train/fit a model that predicts the minimal density required to satisfy two user-defined constraints: (1) target Local Recall@1 and (2) Recall Achievement Rate (proportion of environment where the target is met). Apply the predicted density to unseen deployment traverses. Test across multiple VPR backbones and perform ablations on reference selection and local-space definitions.", "result": "Across Nordland and Oxford RobotCar and multiple VPR methods, the approach consistently achieves or exceeds the specified Local Recall@1 over at least the requested RAR. It selects the correct operating point in density more reliably than baselines and avoids unnecessary over-densification. Ablations show some sensitivity to reference choice and local partitioning; global Recall@1 correlates poorly with the more operational RAR metric.", "conclusion": "Requirement-driven, dynamic selection of map density is feasible and effective for VPR, delivering predictable local performance with efficient resource usage. RAR is a more meaningful operational target than global Recall@1, and density should be tuned via multi-traverse modelling rather than fixed a priori."}}
{"id": "2602.21230", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21230", "abs": "https://arxiv.org/abs/2602.21230", "authors": ["Yanyu Chen", "Jiyue Jiang", "Jiahong Liu", "Yifei Zhang", "Xiao Guo", "Irwin King"], "title": "TRACE: Trajectory-Aware Comprehensive Evaluation for Deep Research Agents", "comment": "Accepted by WWW 2026", "summary": "The evaluation of Deep Research Agents is a critical challenge, as conventional outcome-based metrics fail to capture the nuances of their complex reasoning. Current evaluation faces two primary challenges: 1) a reliance on singular metrics like Pass@1, creating a \"high-score illusion\" that ignores the quality, efficiency, and soundness of the reasoning process; and 2) the failure of static benchmarks to quantify crucial attributes like robustness and latent capability. To address these gaps, we introduce TRACE (Trajectory-Aware Comprehensive Evaluation), a framework that holistically assesses the entire problem-solving trajectory. To counter the \"high-score illusion\", we propose a Hierarchical Trajectory Utility Function that quantifies process efficiency and cognitive quality, including evidence grounding, alongside accuracy. To measure deeper attributes, TRACE introduces a Scaffolded Capability Assessment protocol, quantifying an agent's latent ability by determining the minimum guidance needed for success. Our contributions include the TRACE framework, its novel metrics, and the accompanying DeepResearch-Bench with controllable complexity. Experiments show TRACE delivers a granular ranking that uncovers critical trade-offs between agent accuracy, efficiency, and robustness entirely missed by singular metrics.", "AI": {"tldr": "TRACE is a trajectory-aware evaluation framework for deep research agents that replaces single-shot outcome metrics with process-centric measures and a guided-capability probe, revealing trade-offs among accuracy, efficiency, and robustness that standard metrics miss.", "motivation": "Conventional metrics (e.g., Pass@1) create a \u201chigh-score illusion,\u201d overlooking reasoning quality, efficiency, evidence use, robustness, and latent capability. Static benchmarks cannot capture how well agents reason under varied conditions or with partial guidance. A holistic, trajectory-level evaluation is needed.", "method": "Introduce TRACE, which evaluates the entire problem-solving trajectory. 1) A Hierarchical Trajectory Utility Function scores process efficiency and cognitive quality (e.g., evidence grounding) alongside accuracy. 2) A Scaffolded Capability Assessment measures latent ability by determining the minimal external guidance required for success. Also release DeepResearch-Bench with controllable task complexity to standardize evaluation.", "result": "TRACE produces granular rankings across agents, exposing critical trade-offs between accuracy, efficiency, and robustness that single metrics fail to reveal. It quantifies process quality and the amount of guidance needed, enabling finer comparisons of latent capability and robustness.", "conclusion": "Trajectory-aware, multi-attribute evaluation resolves limitations of outcome-only metrics. TRACE, with its utility function, scaffolded assessment, and new benchmark, offers a comprehensive, practical way to evaluate deep research agents\u2019 reasoning quality, efficiency, robustness, and latent abilities."}}
{"id": "2602.21484", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21484", "abs": "https://arxiv.org/abs/2602.21484", "authors": ["Yushen He"], "title": "Unified Unsupervised and Sparsely-Supervised 3D Object Detection by Semantic Pseudo-Labeling and Prototype Learning", "comment": null, "summary": "3D object detection is essential for autonomous driving and robotic perception, yet its reliance on large-scale manually annotated data limits scalability and adaptability. To reduce annotation dependency, unsupervised and sparsely-supervised paradigms have emerged. However, they face intertwined challenges: low-quality pseudo-labels, unstable feature mining, and a lack of a unified training framework. This paper proposes SPL, a unified training framework for both Unsupervised and Sparsely-Supervised 3D Object Detection via Semantic Pseudo-labeling and prototype Learning. SPL first generates high-quality pseudo-labels by integrating image semantics, point cloud geometry, and temporal cues, producing both 3D bounding boxes for dense objects and 3D point labels for sparse ones. These pseudo-labels are not used directly but as probabilistic priors within a novel, multi-stage prototype learning strategy. This strategy stabilizes feature representation learning through memory-based initialization and momentum-based prototype updating, effectively mining features from both labeled and unlabeled data. Extensive experiments on KITTI and nuScenes datasets demonstrate that SPL significantly outperforms state-of-the-art methods in both settings. Our work provides a robust and generalizable solution for learning 3D object detectors with minimal or no manual annotations.", "AI": {"tldr": "SPL is a unified framework for unsupervised and sparsely supervised 3D object detection that fuses image semantics, point-cloud geometry, and temporal cues to form probabilistic pseudo-labels, then learns with a multi-stage, prototype-based strategy using memory and momentum updates, achieving state-of-the-art results on KITTI and nuScenes with minimal or no labels.", "motivation": "3D object detection typically needs large-scale manual annotations, limiting scalability and adaptability. Existing unsupervised/sparse approaches suffer from poor pseudo-labels, unstable feature learning, and no unified training scheme.", "method": "1) Generate high-quality pseudo-labels by integrating complementary modalities (image semantics, LiDAR geometry) and temporal consistency; produce 3D boxes for dense objects and 3D point labels for sparse ones. 2) Treat pseudo-labels as probabilistic priors rather than hard targets within a multi-stage prototype learning pipeline featuring memory-based prototype initialization and momentum-based prototype updates, stabilizing representation learning across labeled and unlabeled data; a single training framework supports both unsupervised and sparsely supervised regimes.", "result": "On KITTI and nuScenes, SPL substantially surpasses prior state-of-the-art methods in both unsupervised and sparsely supervised settings (abstract does not provide exact numbers).", "conclusion": "SPL mitigates pseudo-label noise and feature instability while unifying training across supervision regimes, offering a robust and generalizable path to train 3D detectors with few or no manual annotations."}}
{"id": "2602.21257", "categories": ["cs.CL", "cs.DB", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.21257", "abs": "https://arxiv.org/abs/2602.21257", "authors": ["Wen G. Gong"], "title": "Structured Prompt Language: Declarative Context Management for LLMs", "comment": "44 pages, 6 figures, 14 tables, 15 code-listings", "summary": "We present SPL (Structured Prompt Language), a declarative SQL-inspired language that treats large language models as generative knowledge bases and their context windows as constrained resources. SPL provides explicit WITH BUDGET/LIMIT token management, an automatic query optimizer, EXPLAIN transparency analogous to SQL's EXPLAIN ANALYZE, and native integration of retrieval-augmented generation (RAG) and persistent memory in a single declarative framework. SPL-flow extends SPL into resilient agentic pipelines with a three-tier provider fallback strategy (Ollama -> OpenRouter -> self-healing retry) fully transparent to the .spl script. Five extensions demonstrate the paradigm's breadth: (1) Text2SPL (multilingual NL->SPL translation); (2) Mixture-of-Models (MoM) routing that dispatches each PROMPT to a domain-specialist model at runtime; (3) Logical Chunking, an intelligent strategy for documents exceeding a single context window--expressed naturally through SPL's existing CTE syntax with no new constructs, decomposing a large query into a Map-Reduce pipeline that reduces attention cost from O(N^2) to O(N^2/k) and runs identically on cloud (parallel) or local hardware (sequential); (4) SPL-flow, a declarative agentic orchestration layer with resilient three-tier provider fallback; and (5) BENCHMARK for parallel multi-model comparison with automatic winner persistence. We provide a formal EBNF grammar, two pip-installable Python packages (spl-llm, spl-flow), and comparison against Prompty, DSPy, and LMQL. SPL reduces prompt boilerplate by 65% on average, surfaces a 68x cost spread across model tiers as a pre-execution signal, and runs the identical .spl script at $0.002 on OpenRouter or at zero marginal cost on a local Ollama instance--without modification.", "AI": {"tldr": "SPL is a SQL-like, declarative language for LLM orchestration that treats models as generative knowledge bases, adds explicit token-budget control, a query optimizer with EXPLAIN-style transparency, and native RAG/persistent memory. SPL-flow extends this into resilient agent pipelines with multi-provider fallback. Extensions include Text2SPL, Mixture-of-Models routing, logical chunking (Map-Reduce) to cut attention cost, and a benchmarking facility. Reported gains: ~65% less prompt boilerplate, strong cost transparency (68x spread surfaced), and provider-agnostic portability at very low cost.", "motivation": "LLM applications suffer from ad hoc prompts, opaque costs/latency, brittle pipelines, and constrained context windows. Developers need a unified, declarative, and transparent way to manage token budgets, integrate RAG and memory, orchestrate multi-model pipelines across providers, and maintain portability and reliability with clear cost signals.", "method": "Design a declarative SQL-inspired language (SPL) with: explicit WITH BUDGET/LIMIT token management; automatic query optimization; EXPLAIN/EXPLAIN ANALYZE analogs; native RAG and persistent memory constructs. Extend with SPL-flow to define resilient agentic pipelines featuring a three-tier provider fallback (Ollama \u2192 OpenRouter \u2192 self-healing retry). Add five extensions: (1) Text2SPL (multilingual NL\u2192SPL); (2) Mixture-of-Models routing to domain experts per PROMPT; (3) Logical Chunking via CTEs that decomposes long-document tasks into a Map-Reduce pipeline, reducing attention from O(N^2) to O(N^2/k) and running in parallel or sequentially; (4) SPL-flow orchestration; (5) BENCHMARK for parallel multi-model comparison with automatic winner persistence. Provide a formal EBNF grammar and two Python packages (spl-llm, spl-flow). Compare against Prompty, DSPy, LMQL.", "result": "Empirical claims include: ~65% reduction in prompt boilerplate; surfacing a 68x cross-model cost spread as a pre-execution signal; identical .spl scripts run unmodified across providers, costing about $0.002 on OpenRouter or zero marginal cost on local Ollama. Logical Chunking theoretically reduces attention complexity and enables identical scripts to run on cloud (parallel) or local (sequential) hardware.", "conclusion": "SPL reframes LLM prompting/orchestration as a transparent, cost-aware, and portable declarative layer\u2014akin to SQL for databases\u2014combining budgeting, optimization, RAG/memory, and an EXPLAIN facility. SPL-flow adds resilient, provider-agnostic agent pipelines. Together they promise developer productivity, cost efficiency, and operational robustness, potentially setting a unifying abstraction for LLM systems."}}
{"id": "2602.21497", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21497", "abs": "https://arxiv.org/abs/2602.21497", "authors": ["Yongchang Zhang", "Xianzheng Ma", "Tianyi Liu", "Guangquan Zhou", "Yang Chen"], "title": "See It, Say It, Sorted: An Iterative Training-Free Framework for Visually-Grounded Multimodal Reasoning in LVLMs", "comment": "CVPR2026 Accepted", "summary": "Recent large vision-language models (LVLMs) have demonstrated impressive reasoning ability by generating long chain-of-thought (CoT) responses. However, CoT reasoning in multimodal contexts is highly vulnerable to visual hallucination propagation: once an intermediate reasoning step becomes inconsistent with the visual evidence, subsequent steps-even if logically valid-can still lead to incorrect final answers. Existing solutions attempt to mitigate this issue by training models to \"think with images\" via reinforcement learning (RL). While effective, these methods are costly, model-specific, and difficult to generalize across architectures. Differently, we present a lightweight method that bypasses RL training and provides an iterative, training-free, plug-and-play framework for visually-grounded multimodal reasoning. Our key idea is to supervise each reasoning step at test time with visual evidence, ensuring that every decoded token is justified by corresponding visual cues. Concretely, we construct a textual visual-evidence pool that guides the model's reasoning generation. When existing evidence is insufficient, a visual decider module dynamically extracts additional relevant evidence from the image based on the ongoing reasoning context, expanding the pool until the model achieves sufficient visual certainty to terminate reasoning and produce the final answer. Extensive experiments on multiple LVLM backbones and benchmarks demonstrate the effectiveness of our approach. Our method achieves 16.5%-29.5% improvements on TreeBench and 13.7% RH-AUC gains on RH-Bench, substantially reducing hallucination rates while improving reasoning accuracy without additional training.", "AI": {"tldr": "Training-free, plug-and-play test-time framework that grounds every chain-of-thought token in visual evidence by maintaining a dynamic textual evidence pool and a visual decider that extracts more cues as needed, cutting hallucinations and improving accuracy across LVLMs.", "motivation": "Multimodal chain-of-thought can drift from images; once a step hallucinates, errors propagate. RL-based grounding helps but is costly, model-specific, and hard to generalize. A lightweight, general method is needed to ensure visually grounded reasoning without retraining.", "method": "At inference, the model\u2019s reasoning is supervised step-by-step against a textual visual-evidence pool derived from the image. If current evidence is insufficient, a visual decider module retrieves/extracts additional image evidence conditioned on the ongoing reasoning, expanding the pool. Decoding continues until sufficient visual certainty, then outputs the final answer. No RL or additional training; works as a plug-in across LVLM backbones.", "result": "Across multiple LVLMs and benchmarks, the approach yields 16.5%\u201329.5% gains on TreeBench and +13.7% RH-AUC on RH-Bench, substantially lowering hallucination rates while improving reasoning accuracy\u2014all without extra training.", "conclusion": "Test-time evidence supervision provides an effective, general, and lightweight way to constrain multimodal reasoning to be visually grounded, mitigating hallucination propagation and boosting accuracy without RL or model-specific retraining."}}
{"id": "2602.21262", "categories": ["cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.21262", "abs": "https://arxiv.org/abs/2602.21262", "authors": ["Sasha Robinson", "Kerem Oktar", "Katherine M. Collins", "Ilia Sucholutsky", "Kelsey R. Allen"], "title": "Under the Influence: Quantifying Persuasion and Vigilance in Large Language Models", "comment": null, "summary": "With increasing integration of Large Language Models (LLMs) into areas of high-stakes human decision-making, it is important to understand the risks they introduce as advisors. To be useful advisors, LLMs must sift through large amounts of content, written with both benevolent and malicious intent, and then use this information to convince a user to take a specific action. This involves two social capacities: vigilance (the ability to determine which information to use, and which to discard) and persuasion (synthesizing the available evidence to make a convincing argument). While existing work has investigated these capacities in isolation, there has been little prior investigation of how these capacities may be linked. Here, we use a simple multi-turn puzzle-solving game, Sokoban, to study LLMs' abilities to persuade and be rationally vigilant towards other LLM agents. We find that puzzle-solving performance, persuasive capability, and vigilance are dissociable capacities in LLMs. Performing well on the game does not automatically mean a model can detect when it is being misled, even if the possibility of deception is explicitly mentioned. However, LLMs do consistently modulate their token use, using fewer tokens to reason when advice is benevolent and more when it is malicious, even if they are still persuaded to take actions leading them to failure. To our knowledge, our work presents the first investigation of the relationship between persuasion, vigilance, and task performance in LLMs, and suggests that monitoring all three independently will be critical for future work in AI safety.", "AI": {"tldr": "They probe how LLMs advise and are advised in a Sokoban game and show that puzzle-solving skill, persuasion power, and vigilance against misleading advice are separable. Models spend more tokens reasoning under suspected malicious advice but can still be persuaded into failure. They argue all three capacities must be monitored separately for safety.", "motivation": "LLMs increasingly act as advisors in high\u2011stakes settings, where they must filter mixed\u2011intent information (vigilance) and produce convincing guidance (persuasion). Prior work treats these in isolation; the link between them and with task performance is underexplored but crucial for safety.", "method": "Use a controlled, multi\u2011turn Sokoban puzzle game with LLM agents to (i) generate benevolent or malicious advice and (ii) receive and act on it. Measure: (a) puzzle success, (b) persuasive capability (ability to induce actions), (c) vigilance (detect/discount bad advice), and (d) token usage modulation when advice intent varies. Include settings where the possibility of deception is made explicit.", "result": "Puzzle performance, persuasion, and vigilance dissociate: strong solvers are not necessarily vigilant. Even when warned about possible deception, models are often misled. LLMs do adapt by using more tokens when advice is malicious and fewer when benevolent, yet this extra deliberation often fails to prevent harmful persuasion.", "conclusion": "Persuasion, vigilance, and task competence are distinct in current LLMs; token\u2011based self\u2011regulation is insufficient to ensure safety. Evaluating and monitoring these dimensions independently is necessary for advisor\u2011style deployments and broader AI safety work."}}
{"id": "2602.21499", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21499", "abs": "https://arxiv.org/abs/2602.21499", "authors": ["Shimin Hu", "Yuanyi Wei", "Fei Zha", "Yudong Guo", "Juyong Zhang"], "title": "Easy3E: Feed-Forward 3D Asset Editing via Rectified Voxel Flow", "comment": "Accepted to CVPR 2026", "summary": "Existing 3D editing methods rely on computationally intensive scene-by-scene iterative optimization and suffer from multi-view inconsistency. We propose an effective and fully feedforward 3D editing framework based on the TRELLIS generative backbone, capable of modifying 3D models from a single editing view. Our framework addresses two key issues: adapting training-free 2D editing to structured 3D representations, and overcoming the bottleneck of appearance fidelity in compressed 3D features. To ensure geometric consistency, we introduce Voxel FlowEdit, an edit-driven flow in the sparse voxel latent space that achieves globally consistent 3D deformation in a single pass. To restore high-fidelity details, we develop a normal-guided single to multi-view generation module as an external appearance prior, successfully recovering high-frequency textures. Experiments demonstrate that our method enables fast, globally consistent, and high-fidelity 3D model editing.", "AI": {"tldr": "Feedforward single-view 3D editing on TRELLIS that enforces global geometric consistency via a voxel-space flow and restores fine textures via a normal-guided single-to-multi-view prior\u2014achieving fast, multi-view-consistent, high-fidelity edits without per-scene optimization.", "motivation": "Current 3D editing often requires slow, scene-specific iterative optimization and yields multi-view inconsistencies. Additionally, naively porting training-free 2D edits to structured 3D representations breaks geometry/consistency, and compressed 3D features limit appearance fidelity.", "method": "A fully feedforward framework atop the TRELLIS generative backbone. (1) Voxel FlowEdit: an edit-driven flow in sparse voxel latent space to propagate and enforce globally consistent 3D deformation in a single pass. (2) A normal-guided single-to-multi-view generation module serves as an external appearance prior to reconstruct high-frequency textures and recover details, adapting training-free 2D edits to structured 3D.", "result": "Experiments indicate the method delivers fast inference-time editing with globally consistent geometry across views and high-detail textures, overcoming prior multi-view inconsistency and fidelity bottlenecks; no per-scene optimization is needed.", "conclusion": "The approach turns single-view edits into globally consistent, detailed 3D changes in a feedforward manner, addressing both geometric consistency and appearance fidelity limitations of prior methods and making practical, high-quality 3D model editing feasible."}}
{"id": "2602.21265", "categories": ["cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.21265", "abs": "https://arxiv.org/abs/2602.21265", "authors": ["Hyeonje Choi", "Jeongsoo Lee", "Hyojun Lee", "Jay-Yoon Lee"], "title": "ToolMATH: A Math Tool Benchmark for Realistic Long-Horizon Multi-Tool Reasoning", "comment": "Conference : Submitted to ICML 2026. 8 pages (+ abstract 16 pages), 5 figures", "summary": "We introduce \\ToolMATH, a math-grounded benchmark that evaluates tool-augmented language models in realistic multi-tool environments where the output depends on calling schema-specified tools and sustaining multi-step execution. It turns math problems into a controlled, correctness-checkable benchmark with tool sets, enabling systematic evaluation of model reliability under (1) large, overlapping tool catalogs and (2) the absence of the intended capability. \\ToolMATH provides actionable diagnostic evidence of failure modes in tool-augmented agents, helping identify the control mechanisms required for robustness. \\ToolMATH roughly contains 8k questions and 12k tools; we provide an additional hard-set \\ToolMATHHard with questions and tools. Our evaluation reveals that the key failure factor is due to the inability to reason, leading to the accumulation of intermediate results' errors and constrain later decisions. Tool-list redundancy do not simply add noise, but amplify small early deviations into irreversible execution drift. The benchmark highlights that when the intended capability is missing, distractor tools can sometimes serve as partial substitutes in solution paths, yet they can also mislead models into ungrounded tool trajectories. Finally, comparisons between tool-use protocols emphasize that improvements come less from local action selection and more from long-range plan coherence and disciplined use of observations.", "AI": {"tldr": "ToolMATH is a math-grounded benchmark to stress-test tool-augmented LMs in realistic multi-tool settings, revealing reasoning-driven failure modes, amplification of small early errors by tool redundancy, and the need for long-horizon planning and disciplined observation use.", "motivation": "Current tool-use evaluations lack controlled, correctness-checkable settings that reflect realistic multi-tool catalogs and scenarios where intended capabilities may be absent. There is a need for a benchmark that diagnoses reliability, failure modes, and control requirements for robust tool-augmented agents.", "method": "Convert math problems into a controlled benchmark with schema-specified tools and multi-step execution, enabling precise correctness checks. Provide a large, overlapping tool catalog (\u224812k tools) across \u22488k questions, plus a harder set (ToolMATHHard). Evaluate models under conditions of tool-list redundancy and missing intended capabilities; compare different tool-use protocols to analyze where improvements arise.", "result": "Main failure factor is inadequate reasoning, causing compounding intermediate-result errors that constrain later choices. Redundant tool lists do not merely add noise; they amplify early deviations into irreversible execution drift. When intended capabilities are missing, distractor tools can sometimes partially substitute but also induce ungrounded trajectories. Protocol comparisons show gains come more from coherent long-range planning and disciplined observation use than from local action selection tweaks.", "conclusion": "ToolMATH provides actionable diagnostics for tool-augmented LMs, highlighting the necessity of stronger reasoning, long-horizon plan coherence, and robust control over tool selection/observation use. It cautions against naive reliance on large tool catalogs and emphasizes designing protocols that mitigate drift and exploit partial substitutes without being misled."}}
{"id": "2602.21503", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21503", "abs": "https://arxiv.org/abs/2602.21503", "authors": ["Hoang-Nhat Nguyen"], "title": "AHAN: Asymmetric Hierarchical Attention Network for Identical Twin Face Verification", "comment": "Accepted to AAAI 2026", "summary": "Identical twin face verification represents an extreme fine-grained recognition challenge where even state-of-the-art systems fail due to overwhelming genetic similarity. Current face recognition methods achieve over 99.8% accuracy on standard benchmarks but drop dramatically to 88.9% when distinguishing identical twins, exposing critical vulnerabilities in biometric security systems. The difficulty lies in learning features that capture subtle, non-genetic variations that uniquely identify individuals. We propose the Asymmetric Hierarchical Attention Network (AHAN), a novel architecture specifically designed for this challenge through multi-granularity facial analysis. AHAN introduces a Hierarchical Cross-Attention (HCA) module that performs multi-scale analysis on semantic facial regions, enabling specialized processing at optimal resolutions. We further propose a Facial Asymmetry Attention Module (FAAM) that learns unique biometric signatures by computing cross-attention between left and right facial halves, capturing subtle asymmetric patterns that differ even between twins. To ensure the network learns truly individuating features, we introduce Twin-Aware Pair-Wise Cross-Attention (TA-PWCA), a training-only regularization strategy that uses each subject's own twin as the hardest possible distractor. Extensive experiments on the ND_TWIN dataset demonstrate that AHAN achieves 92.3% twin verification accuracy, representing a 3.4% improvement over state-of-the-art methods.", "AI": {"tldr": "Proposes a specialized face-recognition architecture (AHAN) for identical-twin verification, combining hierarchical cross-attention over facial regions, left\u2013right asymmetry attention, and a twin-aware training regularizer; achieves 92.3% accuracy on ND_TWIN, a 3.4% gain over prior SOTA.", "motivation": "Conventional face recognition exceeds 99.8% on standard benchmarks but drops to ~88.9% for identical twins, exposing a security vulnerability. The key challenge is extracting subtle, non-genetic, individuating cues that distinguish twins.", "method": "Asymmetric Hierarchical Attention Network (AHAN) with: (1) Hierarchical Cross-Attention (HCA) for multi-scale, semantic-region-specific processing; (2) Facial Asymmetry Attention Module (FAAM) computing cross-attention between left/right facial halves to capture person-specific asymmetries; (3) Twin-Aware Pair-Wise Cross-Attention (TA-PWCA), a training-only regularization that treats each subject\u2019s twin as the hardest distractor.", "result": "On the ND_TWIN dataset, AHAN reaches 92.3% verification accuracy for identical twins, improving by 3.4% over the previous state of the art.", "conclusion": "Targeted attention to semantic regions and facial asymmetries, coupled with twin-aware regularization, yields more individuating features and mitigates a critical weakness in face biometrics; the approach advances fine-grained identity verification where genetic similarity is extreme."}}
{"id": "2602.21346", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21346", "abs": "https://arxiv.org/abs/2602.21346", "authors": ["Mengxuan Hu", "Vivek V. Datla", "Anoop Kumar", "Zihan Guan", "Sheng Li", "Alfy Samuel", "Daben Liu"], "title": "Alignment-Weighted DPO: A principled reasoning approach to improve safety alignment", "comment": null, "summary": "Recent advances in alignment techniques such as Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and Direct Preference Optimization (DPO) have improved the safety of large language models (LLMs). However, these LLMs remain vulnerable to jailbreak attacks that disguise harmful intent through indirect or deceptive phrasing. Using causal intervention, we empirically demonstrate that this vulnerability stems from shallow alignment mechanisms that lack deep reasoning, often rejecting harmful prompts without truly understanding why they are harmful. To mitigate this vulnerability, we propose enhancing alignment through reasoning-aware post-training. We construct and release a novel Chain-of-Thought (CoT) fine-tuning dataset that includes both utility-oriented and safety-critical prompts with step-by-step rationales. Fine-tuning on this dataset encourages models to produce principled refusals grounded in reasoning, outperforming standard SFT baselines. Furthermore, inspired by failure patterns in CoT fine-tuning, we introduce Alignment-Weighted DPO, which targets the most problematic parts of an output by assigning different preference weights to the reasoning and final-answer segments. This produces finer-grained, targeted updates than vanilla DPO and improves robustness to diverse jailbreak strategies. Extensive experiments across multiple safety and utility benchmarks show that our method consistently improves alignment robustness while maintaining overall model utility.", "AI": {"tldr": "They enhance LLM alignment robustness by making the model reason about safety, using a Chain-of-Thought fine-tuning dataset and a segment-weighted DPO variant that targets reasoning vs. final answers differently, yielding stronger jailbreak resistance without hurting utility.", "motivation": "Current SFT/RLHF/DPO alignments often act as shallow pattern-based filters that can be bypassed by indirect or deceptive prompts; the lack of genuine safety reasoning leads to jailbreak vulnerability.", "method": "(1) Use causal intervention to show the failure mode is shallow alignment lacking deep reasoning. (2) Build and release a CoT dataset with utility and safety prompts plus step-by-step rationales to encourage principled refusals. (3) Propose Alignment-Weighted DPO that assigns different preference weights to reasoning and final-answer segments for finer-grained updates.", "result": "CoT fine-tuning yields more principled refusals and outperforms standard SFT; Alignment-Weighted DPO further improves robustness to diverse jailbreak strategies while maintaining overall model utility across multiple safety and utility benchmarks.", "conclusion": "Reasoning-aware post-training with segment-weighted preference optimization is an effective way to mitigate jailbreak vulnerabilities, producing safer yet capable models; resources (dataset) are released to support further research."}}
{"id": "2602.21517", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21517", "abs": "https://arxiv.org/abs/2602.21517", "authors": ["Zheang Huai", "Honglong Yang", "Xiaomeng Li"], "title": "Which Tool Response Should I Trust? Tool-Expertise-Aware Chest X-ray Agent with Multimodal Agentic Learning", "comment": "11 pages", "summary": "AI agents with tool-use capabilities show promise for integrating the domain expertise of various tools. In the medical field, however, tools are usually AI models that are inherently error-prone and can produce contradictory responses. Existing research on medical agents lacks sufficient understanding of the tools' realistic reliability and thus cannot effectively resolve tool conflicts. To address this gap, this paper introduces a framework that enables an agent to interact with tools and empirically learn their practical trustworthiness across different types of multimodal queries via agentic learning. As a concrete instantiation, we focus on chest X-ray analysis and present a tool-expertise-aware chest X-ray agent (TEA-CXA). When tool outputs disagree, the agent experimentally accepts or rejects multimodal tool results, receives rewards, and learns which tool to trust for each query type. Importantly, TEA-CXA extends existing codebases for reinforcement learning with multi-turn tool-calling that focus on textual inputs, to support multimodal contexts effectively. In addition, we enhance the codebase for medical use scenarios by supporting multiple tool calls in one turn, parallel tool inference, and multi-image accommodation within a single user query. Our code framework is applicable to general medical research on multi-turn tool-calling reinforcement learning in multimodal settings. Experiments show that TEA-CXA outperforms the state-of-the-art methods and a comprehensive set of baselines. Code will be released.", "AI": {"tldr": "They present TEA-CXA, a reinforcement learning\u2013based medical AI agent that learns which tools to trust for different multimodal chest X\u2011ray queries, enabling conflict resolution among imperfect tools and outperforming existing methods.", "motivation": "Medical AI tools (often other AI models) are fallible and can contradict each other; current agent frameworks don\u2019t model real-world tool reliability or resolve conflicts well, especially in multimodal, multi-turn settings.", "method": "Instantiate an agentic learning framework on chest X-rays (TEA-CXA) that interacts with multiple tools, experimentally accepts/rejects their outputs when disagreements occur, and learns empirical trustworthiness per query type via rewards. Technically extends RL codebases from text-only to multimodal contexts, adds multi-turn tool calling, multiple tool calls per turn, parallel tool inference, and multi-image handling.", "result": "In experiments, TEA-CXA surpasses state-of-the-art systems and diverse baselines on chest X-ray tasks (details not specified in the abstract).", "conclusion": "Learning tool-specific trustworthiness in a multimodal, multi-turn RL framework improves medical agent performance and conflict resolution; the framework generalizes to broader medical multimodal tool-use scenarios. Code will be released."}}
{"id": "2602.21374", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21374", "abs": "https://arxiv.org/abs/2602.21374", "authors": ["Mohammadreza Ghaffarzadeh-Esfahani", "Nahid Yousefian", "Ebrahim Heidari-Farsani", "Ali Akbar Omidvarian", "Sepehr Ghahraei", "Atena Farangi", "AmirBahador Boroumand"], "title": "Small Language Models for Privacy-Preserving Clinical Information Extraction in Low-Resource Languages", "comment": "16 pages, 3 figures, 2 supplementary files", "summary": "Extracting clinical information from medical transcripts in low-resource languages remains a significant challenge in healthcare natural language processing (NLP). This study evaluates a two-step pipeline combining Aya-expanse-8B as a Persian-to-English translation model with five open-source small language models (SLMs) -- Qwen2.5-7B-Instruct, Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, Qwen2.5-1.5B-Instruct, and Gemma-3-1B-it -- for binary extraction of 13 clinical features from 1,221 anonymized Persian transcripts collected at a cancer palliative care call center. Using a few-shot prompting strategy without fine-tuning, models were assessed on macro-averaged F1-score, Matthews Correlation Coefficient (MCC), sensitivity, and specificity to account for class imbalance. Qwen2.5-7B-Instruct achieved the highest overall performance (median macro-F1: 0.899; MCC: 0.797), while Gemma-3-1B-it showed the weakest results. Larger models (7B--8B parameters) consistently outperformed smaller counterparts in sensitivity and MCC. A bilingual analysis of Aya-expanse-8B revealed that translating Persian transcripts to English improved sensitivity, reduced missing outputs, and boosted metrics robust to class imbalance, though at the cost of slightly lower specificity and precision. Feature-level results showed reliable extraction of physiological symptoms across most models, whereas psychological complaints, administrative requests, and complex somatic features remained challenging. These findings establish a practical, privacy-preserving blueprint for deploying open-source SLMs in multilingual clinical NLP settings with limited infrastructure and annotation resources, and highlight the importance of jointly optimizing model scale and input language strategy for sensitive healthcare applications.", "AI": {"tldr": "Translate-then-extract with open small LMs enables accurate binary extraction of clinical features from Persian palliative-care call transcripts. A Persian\u2192English step (Aya-expanse-8B) plus few-shot SLMs yields strong macro-F1/MCC (best: Qwen2.5-7B, 0.899 macro-F1; 0.797 MCC), improving sensitivity and robustness with a small specificity/precision trade-off. Physiological symptoms are reliably captured; psychosocial, administrative, and complex somatic features remain hard.", "motivation": "Clinical NLP for low-resource languages lacks labeled data, compute, and deployable, privacy-preserving solutions. Healthcare notes/transcripts are imbalanced and noisy, and organizations need practical methods that work without fine-tuning while supporting multilingual workflows.", "method": "A two-step pipeline: (1) Persian\u2192English translation via Aya-expanse-8B; (2) few-shot prompted open-source SLMs (Qwen2.5-7B-Instruct, Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, Qwen2.5-1.5B-Instruct, Gemma-3-1B-it) perform binary extraction of 13 clinical features from 1,221 anonymized call-center transcripts. Evaluation uses macro-averaged F1, MCC, sensitivity, and specificity. A bilingual analysis compares direct Persian inputs vs translated English.", "result": "Qwen2.5-7B-Instruct performs best (median macro-F1 0.899; MCC 0.797); Gemma-3-1B-it is weakest. Larger models (7B\u20138B) consistently outperform smaller ones on sensitivity and MCC. Translation to English increases sensitivity, reduces missing outputs, and improves metrics robust to class imbalance, with a slight drop in specificity and precision. Feature-wise, physiological symptoms are extracted reliably; psychological complaints, administrative requests, and complex somatic features are challenging.", "conclusion": "Open, small LMs with a translate-then-extract strategy form a practical, privacy-preserving blueprint for multilingual clinical NLP under limited resources. Jointly selecting model scale and input language is crucial. Remaining gaps\u2014lower precision/specificity and difficulty with psychosocial/admin/complex features\u2014suggest targeted prompt engineering, domain adaptation, or selective fine-tuning to improve performance."}}
{"id": "2602.21535", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21535", "abs": "https://arxiv.org/abs/2602.21535", "authors": ["Beizhen Zhao", "Sicheng Yu", "Guanzhi Ding", "Yu Hu", "Hao Wang"], "title": "Pseudo-View Enhancement via Confidence Fusion for Unposed Sparse-View Reconstruction", "comment": "14 pages", "summary": "3D scene reconstruction under unposed sparse viewpoints is a highly challenging yet practically important problem, especially in outdoor scenes due to complex lighting and scale variation. With extremely limited input views, directly utilizing diffusion model to synthesize pseudo frames will introduce unreasonable geometry, which will harm the final reconstruction quality. To address these issues, we propose a novel framework for sparse-view outdoor reconstruction that achieves high-quality results through bidirectional pseudo frame restoration and scene perception Gaussian management. Specifically, we introduce a bidirectional pseudo frame restoration method that restores missing content by diffusion-based synthesis guided by adjacent frames with a lightweight pseudo-view deblur model and confidence mask inference algorithm. Then we propose a scene perception Gaussian management strategy that optimize Gaussians based on joint depth-density information. These designs significantly enhance reconstruction completeness, suppress floating artifacts and improve overall geometric consistency under extreme view sparsity. Experiments on outdoor benchmarks demonstrate substantial gains over existing methods in both fidelity and stability.", "AI": {"tldr": "A two-part framework for unposed, sparse-view outdoor 3D reconstruction: restore reliable pseudo views via bidirectional diffusion with deblurring and confidence masks, then manage 3D Gaussians with joint depth\u2013density cues to improve completeness, suppress floaters, and boost geometric consistency\u2014outperforming prior methods on outdoor benchmarks.", "motivation": "Sparse, unposed outdoor inputs suffer from complex lighting and large scale variation. With few views, geometry is incomplete; na\u00efvely using diffusion to hallucinate views injects inconsistent structure and harms reconstruction quality. The work aims to make sparse-view outdoor reconstruction both faithful and stable despite these challenges.", "method": "1) Bidirectional pseudo frame restoration: diffusion-based synthesis guided by adjacent views, coupled with a lightweight pseudo-view deblurring module and a confidence mask to select reliable synthesized regions. 2) Scene perception Gaussian management: optimize 3D Gaussian representations using joint depth and density information to enforce structural consistency and reduce artifacts.", "result": "On outdoor benchmarks, the approach yields substantially better fidelity and stability than existing methods, with more complete reconstructions, fewer floating artifacts, and improved geometric consistency under extreme sparsity.", "conclusion": "Carefully constrained pseudo-view restoration plus depth\u2013density-driven Gaussian management enables robust, high-quality sparse-view outdoor 3D reconstruction under unposed settings. The strategy mitigates diffusion-induced geometric errors and stabilizes optimization, delivering state-of-the-art performance in challenging outdoor scenarios."}}
{"id": "2602.21539", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21539", "abs": "https://arxiv.org/abs/2602.21539", "authors": ["Chaojie Shen", "Jingjun Gu", "Zihao Zhao", "Ruocheng Li", "Cunyuan Yang", "Jiajun Bu", "Lei Wu"], "title": "VasGuideNet: Vascular Topology-Guided Couinaud Liver Segmentation with Structural Contrastive Loss", "comment": null, "summary": "Accurate Couinaud liver segmentation is critical for preoperative surgical planning and tumor localization.However, existing methods primarily rely on image intensity and spatial location cues, without explicitly modeling vascular topology. As a result, they often produce indistinct boundaries near vessels and show limited generalization under anatomical variability.We propose VasGuideNet, the first Couinaud segmentation framework explicitly guided by vascular topology. Specifically, skeletonized vessels, Euclidean distance transform (EDT)--derived geometry, and k-nearest neighbor (kNN) connectivity are encoded into topology features using Graph Convolutional Networks (GCNs). These features are then injected into a 3D encoder--decoder backbone via a cross-attention fusion module. To further improve inter-class separability and anatomical consistency, we introduce a Structural Contrastive Loss (SCL) with a global memory bank.On Task08_HepaticVessel and our private LASSD dataset, VasGuideNet achieves Dice scores of 83.68% and 76.65% with RVDs of 1.68 and 7.08, respectively. It consistently outperforms representative baselines including UNETR, Swin UNETR, and G-UNETR++, delivering higher Dice/mIoU and lower RVD across datasets, demonstrating its effectiveness for anatomically consistent segmentation. Code is available at https://github.com/Qacket/VasGuideNet.git.", "AI": {"tldr": "VasGuideNet introduces topology-guided Couinaud liver segmentation by encoding vascular skeletons, geometry, and connectivity with GCNs and fusing them into a 3D encoder\u2013decoder via cross-attention, complemented by a structural contrastive loss; it outperforms strong baselines on public and private datasets with higher Dice/mIoU and lower RVD.", "motivation": "Couinaud liver segmentation is crucial for surgical planning and tumor localization, but existing methods rely mainly on intensity and coarse spatial cues, neglecting vascular topology\u2014leading to fuzzy boundaries near vessels and poor generalization under anatomical variability.", "method": "1) Extract vascular skeletons and compute EDT-based geometry; 2) Build kNN graphs to capture connectivity; 3) Encode topology with Graph Convolutional Networks; 4) Inject topology features into a 3D encoder\u2013decoder via a cross-attention fusion module; 5) Use a Structural Contrastive Loss with a global memory bank to enforce inter-class separability and anatomical consistency.", "result": "On Task08_HepaticVessel and the private LASSD dataset, VasGuideNet achieves Dice scores of 83.68% and 76.65% with RVDs of 1.68 and 7.08, surpassing UNETR, Swin UNETR, and G-UNETR++ on Dice/mIoU and RVD across datasets.", "conclusion": "Explicitly modeling vascular topology and fusing it with volumetric cues, alongside a structural contrastive loss, yields more anatomically consistent Couinaud segmentation and better generalization; the approach is effective in practice and available as open-source code."}}
{"id": "2602.21552", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21552", "abs": "https://arxiv.org/abs/2602.21552", "authors": ["Changqing Zhou", "Yueru Luo", "Changhao Chen"], "title": "Generalizing Visual Geometry Priors to Sparse Gaussian Occupancy Prediction", "comment": "Accepted by CVPR2026", "summary": "Accurate 3D scene understanding is essential for embodied intelligence, with occupancy prediction emerging as a key task for reasoning about both objects and free space. Existing approaches largely rely on depth priors (e.g., DepthAnything) but make only limited use of 3D cues, restricting performance and generalization. Recently, visual geometry models such as VGGT have shown strong capability in providing rich 3D priors, but similar to monocular depth foundation models, they still operate at the level of visible surfaces rather than volumetric interiors, motivating us to explore how to more effectively leverage these increasingly powerful geometry priors for 3D occupancy prediction. We present GPOcc, a framework that leverages generalizable visual geometry priors (GPs) for monocular occupancy prediction. Our method extends surface points inward along camera rays to generate volumetric samples, which are represented as Gaussian primitives for probabilistic occupancy inference. To handle streaming input, we further design a training-free incremental update strategy that fuses per-frame Gaussians into a unified global representation. Experiments on Occ-ScanNet and EmbodiedOcc-ScanNet demonstrate significant gains: GPOcc improves mIoU by +9.99 in the monocular setting and +11.79 in the streaming setting over prior state of the art. Under the same depth prior, it achieves +6.73 mIoU while running 2.65$\\times$ faster. These results highlight that GPOcc leverages geometry priors more effectively and efficiently. Code will be released at https://github.com/JuIvyy/GPOcc.", "AI": {"tldr": "GPOcc converts rich visual-geometry priors into volumetric occupancy from a single view by extending surface cues into the scene and representing them as Gaussian primitives, enabling probabilistic inference and streaming fusion; it delivers large mIoU gains and faster runtime over prior SOTA on Occ-ScanNet and EmbodiedOcc-ScanNet.", "motivation": "Occupancy prediction needs reasoning about free space and unobserved interiors, but most monocular methods rely on depth priors focused on visible surfaces, limiting generalization and 3D completeness. Even strong geometry priors (e.g., VGGT) remain surface-centric. The paper aims to exploit such priors more effectively for volumetric occupancy, including in streaming settings for embodied agents.", "method": "Leverage generalizable visual geometry priors to obtain surface points, then extend them inward along camera rays to produce volumetric samples. Represent these samples as Gaussian primitives to support probabilistic occupancy estimation. For sequential inputs, apply a training-free incremental update that fuses per-frame Gaussians into a unified global occupancy representation.", "result": "On Occ-ScanNet and EmbodiedOcc-ScanNet, GPOcc improves mIoU by +9.99 (monocular) and +11.79 (streaming) over previous SOTA. With the same depth prior, it yields +6.73 mIoU while running 2.65x faster.", "conclusion": "Transforming surface-oriented geometry priors into volumetric Gaussian representations enables more accurate and efficient occupancy prediction, benefiting both single-frame and streaming scenarios. The approach more effectively leverages existing geometry priors, achieving substantial accuracy gains and speedups; code will be released."}}
{"id": "2602.21581", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21581", "abs": "https://arxiv.org/abs/2602.21581", "authors": ["Yingcheng Hu", "Haowen Gong", "Chuanguang Yang", "Zhulin An", "Yongjun Xu", "Songhua Liu"], "title": "MultiAnimate: Pose-Guided Image Animation Made Extensible", "comment": "Project page at https://hyc001.github.io/MultiAnimate/", "summary": "Pose-guided human image animation aims to synthesize realistic videos of a reference character driven by a sequence of poses. While diffusion-based methods have achieved remarkable success, most existing approaches are limited to single-character animation. We observe that naively extending these methods to multi-character scenarios often leads to identity confusion and implausible occlusions between characters. To address these challenges, in this paper, we propose an extensible multi-character image animation framework built upon modern Diffusion Transformers (DiTs) for video generation. At its core, our framework introduces two novel components-Identifier Assigner and Identifier Adapter - which collaboratively capture per-person positional cues and inter-person spatial relationships. This mask-driven scheme, along with a scalable training strategy, not only enhances flexibility but also enables generalization to scenarios with more characters than those seen during training. Remarkably, trained on only a two-character dataset, our model generalizes to multi-character animation while maintaining compatibility with single-character cases. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in multi-character image animation, surpassing existing diffusion-based baselines.", "AI": {"tldr": "A diffusion-transformer framework for pose-guided human image animation that scales from single- to multi-character scenes using identity- and relation-aware components, achieving SOTA and generalizing beyond the number of characters seen in training.", "motivation": "Existing diffusion-based pose-driven animation methods work well for single characters but break down in multi-character settings, causing identity swaps and unrealistic occlusions. There is a need for a method that preserves identities and handles inter-person spatial relations while remaining scalable to more characters than seen in training.", "method": "Build on Diffusion Transformers (DiTs) and introduce two components: (1) Identifier Assigner to encode per-person positional/identity cues via masks; (2) Identifier Adapter to model inter-person spatial relationships. Use a mask-driven scheme and a scalable training strategy that enables generalization to a larger number of characters. Trained primarily on a two-character dataset while remaining compatible with single-character cases.", "result": "The proposed system achieves state-of-the-art performance on multi-character image animation, surpassing diffusion-based baselines. It generalizes from training on two-character data to animations with more characters and maintains performance on single-character scenarios.", "conclusion": "Identity- and relation-aware conditioning within a DiT-based video generator effectively resolves identity confusion and occlusion issues in multi-character animation, yielding an extensible framework that generalizes to more characters and sets a new performance bar over existing diffusion methods."}}
{"id": "2602.21543", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.21543", "abs": "https://arxiv.org/abs/2602.21543", "authors": ["Barah Fazili", "Koustava Goswami"], "title": "Enhancing Multilingual Embeddings via Multi-Way Parallel Text Alignment", "comment": null, "summary": "Multilingual pretraining typically lacks explicit alignment signals, leading to suboptimal cross-lingual alignment in the representation space. In this work, we show that training standard pretrained models for cross-lingual alignment with a multi-way parallel corpus in a diverse pool of languages can substantially improve multilingual and cross-lingual representations for NLU tasks. We construct a multi-way parallel dataset using translations of English text from an off-the-shelf NMT model for a pool of six target languages and achieve strong cross-lingual alignment through contrastive learning. This leads to substantial performance gains across both seen and unseen languages for multiple tasks from the MTEB benchmark evaluated for XLM-Roberta and multilingual BERT base models. Using a multi-way parallel corpus for contrastive training yields substantial gains on bitext mining (21.3%), semantic similarity (5.3%), and classification (28.4%) compared to English-centric (En-X) bilingually parallel data, where X is sampled from a pool of multiple target languages. Furthermore, finetuning mE5 model on a small dataset with multi-way parallelism significantly improves bitext mining compared to one without, underscoring the importance of multi-way cross-lingual supervision even for models already pretrained for high-quality sentence embeddings.", "AI": {"tldr": "Training multilingual encoders with multi-way parallel, contrastive signals (rather than English\u2013X bilingual pairs) substantially improves cross-lingual alignment and yields large MTEB gains, generalizing to unseen languages and benefiting even strong embedding models like mE5.", "motivation": "Standard multilingual pretraining lacks explicit alignment across languages, leading to suboptimal cross-lingual representations. Prior alignment often relies on English-centric bilingual data, which may not capture many-to-many relations or transfer well to unseen languages.", "method": "Build a multi-way parallel corpus by translating English sentences into six target languages using an off-the-shelf NMT system. Apply contrastive learning to align representations of all parallel sentences jointly. Fine-tune XLM-R and mBERT (and also test on mE5) with this objective.", "result": "Compared to English-centric En\u2013X bilingual training, multi-way contrastive training yields sizable MTEB improvements: +21.3% in bitext mining, +5.3% in semantic similarity, and +28.4% in classification. Gains hold for both seen and unseen languages; small multi-way fine-tuning of mE5 notably improves bitext mining.", "conclusion": "Explicit multi-way cross-lingual supervision is a strong, general alignment signal that outperforms English-centric approaches and enhances multilingual representations across tasks and languages, including models already optimized for sentence embeddings."}}
{"id": "2602.21589", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21589", "abs": "https://arxiv.org/abs/2602.21589", "authors": ["Haoxiang Fu", "Lingfeng Zhang", "Hao Li", "Ruibing Hu", "Zhengrong Li", "Guanjing Liu", "Zimu Tan", "Long Chen", "Hangjun Ye", "Xiaoshuai Hao"], "title": "SEF-MAP: Subspace-Decomposed Expert Fusion for Robust Multimodal HD Map Prediction", "comment": null, "summary": "High-definition (HD) maps are essential for autonomous driving, yet multi-modal fusion often suffers from inconsistency between camera and LiDAR modalities, leading to performance degradation under low-light conditions, occlusions, or sparse point clouds. To address this, we propose SEFMAP, a Subspace-Expert Fusion framework for robust multimodal HD map prediction. The key idea is to explicitly disentangle BEV features into four semantic subspaces: LiDAR-private, Image-private, Shared, and Interaction. Each subspace is assigned a dedicated expert, thereby preserving modality-specific cues while capturing cross-modal consensus. To adaptively combine expert outputs, we introduce an uncertainty-aware gating mechanism at the BEV-cell level, where unreliable experts are down-weighted based on predictive variance, complemented by a usage balance regularizer to prevent expert collapse. To enhance robustness in degraded conditions and promote role specialization, we further propose distribution-aware masking: during training, modality-drop scenarios are simulated using EMA-statistical surrogate features, and a specialization loss enforces distinct behaviors of private, shared, and interaction experts across complete and masked inputs. Experiments on nuScenes and Argoverse2 benchmarks demonstrate that SEFMAP achieves state-of-the-art performance, surpassing prior methods by +4.2% and +4.8% in mAP, respectively. SEF-MAPprovides a robust and effective solution for multi-modal HD map prediction under diverse and degraded conditions.", "AI": {"tldr": "SEFMAP is a subspace-expert fusion framework for BEV HD map prediction that disentangles multimodal features into LiDAR-private, Image-private, Shared, and Interaction subspaces, uses uncertainty-aware gating with a balance regularizer, and trains with distribution-aware masking plus a specialization loss, achieving SOTA mAP gains on nuScenes (+4.2%) and Argoverse2 (+4.8%), especially under degraded conditions.", "motivation": "Multimodal HD map prediction suffers from inconsistency between camera and LiDAR, causing failures in low light, occlusions, or sparse point clouds. The goal is to preserve modality-specific cues while capturing cross-modal consensus and to remain robust when one modality degrades or is absent.", "method": "1) Disentangle BEV features into four semantic subspaces: LiDAR-private, Image-private, Shared, and Interaction, each handled by an expert. 2) Fuse experts via BEV-cell\u2013level uncertainty-aware gating that down-weights unreliable experts based on predictive variance and includes a usage-balance regularizer to avoid expert collapse. 3) Improve robustness using distribution-aware masking: simulate modality-drop with EMA-based surrogate features during training and apply a specialization loss to enforce distinct behaviors for private, shared, and interaction experts under complete vs. masked inputs.", "result": "State-of-the-art performance on nuScenes and Argoverse2 HD map benchmarks with mAP improvements of +4.2% and +4.8% over prior methods, showing robustness under degraded sensing conditions.", "conclusion": "Explicit subspace-expert decomposition with uncertainty-aware fusion and distribution-aware training yields robust, accurate multimodal HD map prediction, effectively mitigating cross-modal inconsistency and improving performance across diverse and degraded scenarios."}}
{"id": "2602.21608", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21608", "abs": "https://arxiv.org/abs/2602.21608", "authors": ["Kazi Samin Yasar Alam", "Md Tanbir Chowdhury", "Tamim Ahmed", "Ajwad Abrar", "Md Rafid Haque"], "title": "MixSarc: A Bangla-English Code-Mixed Corpus for Implicit Meaning Identification", "comment": "Under Review", "summary": "Bangla-English code-mixing is widespread across South Asian social media, yet resources for implicit meaning identification in this setting remain scarce. Existing sentiment and sarcasm models largely focus on monolingual English or high-resource languages and struggle with transliteration variation, cultural references, and intra-sentential language switching. To address this gap, we introduce MixSarc, the first publicly available Bangla-English code-mixed corpus for implicit meaning identification. The dataset contains 9,087 manually annotated sentences labeled for humor, sarcasm, offensiveness, and vulgarity. We construct the corpus through targeted social media collection, systematic filtering, and multi-annotator validation. We benchmark transformer-based models and evaluate zero-shot large language models under structured prompting. Results show strong performance on humor detection but substantial degradation on sarcasm, offense, and vulgarity due to class imbalance and pragmatic complexity. Zero-shot models achieve competitive micro-F1 scores but low exact match accuracy. Further analysis reveals that over 42\\% of negative sentiment instances in an external dataset exhibit sarcastic characteristics. MixSarc provides a foundational resource for culturally aware NLP and supports more reliable multi-label modeling in code-mixed environments.", "AI": {"tldr": "Introduces MixSarc, the first public Bangla\u2013English code-mixed corpus (9,087 sentences) for implicit meaning (humor, sarcasm, offensiveness, vulgarity); benchmarks transformers and zero-shot LLMs, finding strong humor detection but degraded performance on sarcasm/offense/vulgarity; zero-shot models yield competitive micro-F1 yet low exact-match; 42% of negative sentiment in an external set is sarcastic; resource enables culturally aware, multi-label modeling.", "motivation": "Bangla\u2013English code-mixing is common on South Asian social media, but datasets and models for implicit meaning in this setting are lacking. Existing sentiment/sarcasm systems are monolingual or high-resource oriented and struggle with transliteration variance, cultural references, and within-sentence language switching.", "method": "Create MixSarc via targeted social media data collection, systematic filtering, and multi-annotator validation. Annotate 9,087 sentences with multi-labels: humor, sarcasm, offensiveness, vulgarity. Benchmark transformer-based models; assess zero-shot large language models using structured prompting. Analyze performance issues such as class imbalance and pragmatic complexity; examine sarcasm prevalence in an external negative-sentiment dataset.", "result": "Humor detection performs strongly; sarcasm, offense, and vulgarity see substantial performance drops, attributed to class imbalance and pragmatic difficulty. Zero-shot LLMs achieve competitive micro-F1 but poor exact-match accuracy. Cross-dataset analysis shows 42% of negative sentiment instances exhibit sarcasm.", "conclusion": "MixSarc is the first public resource for Bangla\u2013English code-mixed implicit meaning, establishing a foundation for culturally aware NLP in low-resource, code-mixed settings. Results highlight the relative ease of humor detection versus sarcasm/offense/vulgarity, the limitations of current models under class imbalance and pragmatic complexity, and the partial promise\u2014but insufficiency\u2014of zero-shot LLMs for precise multi-label prediction."}}
{"id": "2602.21591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21591", "abs": "https://arxiv.org/abs/2602.21591", "authors": ["Xihua Sheng", "Lingyu Zhu", "Tianyu Zhang", "Dong Liu", "Shiqi Wang", "Jing Wang"], "title": "CADC: Content Adaptive Diffusion-Based Generative Image Compression", "comment": "CVPR2026", "summary": "Diffusion-based generative image compression has demonstrated remarkable potential for achieving realistic reconstruction at ultra-low bitrates. The key to unlocking this potential lies in making the entire compression process content-adaptive, ensuring that the encoder's representation and the decoder's generative prior are dynamically aligned with the semantic and structural characteristics of the input image. However, existing methods suffer from three critical limitations that prevent effective content adaptation. First, isotropic quantization applies a uniform quantization step, failing to adapt to the spatially varying complexity of image content and creating a misalignment with the diffusion model's noise-dependent prior. Second, the information concentration bottleneck -- arising from the dimensional mismatch between the high-dimensional noisy latent and the diffusion decoder's fixed input -- prevents the model from adaptively preserving essential semantic information in the primary channels. Third, existing textual conditioning strategies either need significant textual bitrate overhead or rely on generic, content-agnostic textual prompts, thereby failing to provide adaptive semantic guidance efficiently. To overcome these limitations, we propose a content-adaptive diffusion-based image codec with three technical innovations: 1) an Uncertainty-Guided Adaptive Quantization method that learns spatial uncertainty maps to adaptively align quantization distortion with content characteristics; 2) an Auxiliary Decoder-Guided Information Concentration method that uses a lightweight auxiliary decoder to enforce content-aware information preservation in the primary latent channels; and 3) a Bitrate-Free Adaptive Textual Conditioning method that derives content-aware textual descriptions from the auxiliary reconstructed image, enabling semantic guidance without bitrate cost.", "AI": {"tldr": "A content-adaptive diffusion-based image codec that aligns quantization, latent representation, and semantic conditioning with image content using (1) uncertainty-guided adaptive quantization, (2) an auxiliary decoder to concentrate semantic information in primary channels, and (3) bitrate-free adaptive textual prompts generated from an auxiliary reconstruction.", "motivation": "Ultra-low bitrate generative compression needs the encoder\u2019s representation and the diffusion prior to be content-adaptive. Existing codecs are limited by uniform (isotropic) quantization, a latent-to-decoder dimensional/bandwidth mismatch that dilutes key semantics, and textual conditioning that is either costly in bits or too generic.", "method": "Three components: (1) Uncertainty-Guided Adaptive Quantization learns spatial uncertainty maps to allocate quantization distortion according to local content and diffusion noise levels. (2) Auxiliary Decoder-Guided Information Concentration trains with a lightweight auxiliary decoder that forces essential semantics into primary latent channels. (3) Bitrate-Free Adaptive Textual Conditioning obtains content-aware textual descriptions at the decoder from an auxiliary reconstructed image, using them to guide the diffusion model without sending extra bits.", "result": "The approach addresses misalignment between quantization and the diffusion prior, alleviates the information concentration bottleneck, and provides adaptive semantic guidance with no textual bitrate. The abstract implies improved perceptual quality and content alignment at ultra-low bitrates, though no numbers are given.", "conclusion": "Jointly adapting quantization, latent channel usage, and semantic conditioning enables more faithful, realistic reconstructions under extreme compression while avoiding prompt bitrate overhead. The framework suggests a general recipe for making generative codecs content-adaptive."}}
{"id": "2602.21596", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21596", "abs": "https://arxiv.org/abs/2602.21596", "authors": ["Trung X. Pham", "Kang Zhang", "Ji Woo Hong", "Chang D. Yoo"], "title": "A Hidden Semantic Bottleneck in Conditional Embeddings of Diffusion Transformers", "comment": "Accepted to ICLR 2026", "summary": "Diffusion Transformers have achieved state-of-the-art performance in class-conditional and multimodal generation, yet the structure of their learned conditional embeddings remains poorly understood. In this work, we present the first systematic study of these embeddings and uncover a notable redundancy: class-conditioned embeddings exhibit extreme angular similarity, exceeding 99\\% on ImageNet-1K, while continuous-condition tasks such as pose-guided image generation and video-to-audio generation reach over 99.9\\%. We further find that semantic information is concentrated in a small subset of dimensions, with head dimensions carrying the dominant signal and tail dimensions contributing minimally. By pruning low-magnitude dimensions--removing up to two-thirds of the embedding space--we show that generation quality and fidelity remain largely unaffected, and in some cases improve. These results reveal a semantic bottleneck in Transformer-based diffusion models, providing new insights into how semantics are encoded and suggesting opportunities for more efficient conditioning mechanisms.", "AI": {"tldr": "Conditional embeddings in Diffusion Transformers are highly redundant: most class and continuous-condition embeddings are >99\u201399.9% angularly similar, semantics live in a few head dimensions, and pruning up to ~2/3 of low-magnitude dimensions preserves (or sometimes improves) generation quality\u2014revealing a semantic bottleneck and enabling more efficient conditioning.", "motivation": "Despite strong generative performance, the internal structure of learned conditional embeddings in Diffusion Transformers is poorly understood. Clarifying how and where semantic information is encoded could guide more efficient, robust conditioning mechanisms and model designs.", "method": "Perform a systematic analysis of conditional embeddings across tasks (class-conditional on ImageNet-1K, pose-guided image generation, video-to-audio). Measure angular similarity among embeddings, examine per-dimension contribution (head vs tail), and prune low-magnitude dimensions. Evaluate effects on generation quality and fidelity after pruning.", "result": "Embeddings show extreme angular similarity (\u2248>99% for classes; >99.9% for continuous conditions). Semantic information concentrates in a small subset of head dimensions, while tail dimensions contribute minimally. Pruning up to two-thirds of the embedding space leaves generation quality and fidelity largely unchanged, and can even improve them.", "conclusion": "Diffusion Transformers encode conditions through a narrow, redundant subspace\u2014a semantic bottleneck. This opens avenues for lighter conditioning modules, dimensionality/pruning-based compression, and redesigns that explicitly exploit low-rank or sparse semantic channels without sacrificing quality."}}
{"id": "2602.21379", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21379", "abs": "https://arxiv.org/abs/2602.21379", "authors": ["Daniel Tamayo", "I\u00f1aki Lacunza", "Paula Rivera-Hidalgo", "Severino Da Dalt", "Javier Aula-Blasco", "Aitor Gonzalez-Agirre", "Marta Villegas"], "title": "MrBERT: Modern Multilingual Encoders via Vocabulary, Domain, and Dimensional Adaptation", "comment": "24 pages, 14 tables and 4 figures", "summary": "We introduce MrBERT, a family of 150M-300M parameter encoders built on the ModernBERT architecture and pre-trained on 35 languages and code. Through targeted adaptation, this model family achieves state-of-the-art results on Catalan- and Spanish-specific tasks, while establishing robust performance across specialized biomedical and legal domains. To bridge the gap between research and production, we incorporate Matryoshka Representation Learning (MRL), enabling flexible vector sizing that significantly reduces inference and storage costs. Ultimately, the MrBERT family demonstrates that modern encoder architectures can be optimized for both localized linguistic excellence and efficient, high-stakes domain specialization. We open source the complete model family on Huggingface.", "AI": {"tldr": "MrBERT is a 150\u2013300M ModernBERT-based multilingual/code encoder family that attains SOTA on Catalan/Spanish tasks and strong biomedical/legal performance, while Matryoshka Representation Learning enables flexible, smaller embeddings to cut inference and storage; models are open-sourced on Hugging Face.", "motivation": "Bridge research and production by delivering encoder models that achieve high accuracy in targeted local languages (Catalan, Spanish) and specialized domains (biomedical, legal) without prohibitive compute/storage costs.", "method": "Build on the ModernBERT architecture; pre-train on 35 languages plus code; apply targeted adaptation for Catalan/Spanish and domain specialization; integrate Matryoshka Representation Learning to allow variable-size representations for efficiency at inference and storage.", "result": "State-of-the-art on Catalan and Spanish benchmarks; robust cross-domain results in biomedical and legal tasks; materially reduced inference and storage overhead due to flexible vector sizing via MRL.", "conclusion": "Modern encoder architectures, augmented with MRL, can be optimized for both localized linguistic excellence and efficient domain specialization; MrBERT provides a practical, open-source family suitable for real-world deployment."}}
{"id": "2602.21613", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21613", "abs": "https://arxiv.org/abs/2602.21613", "authors": ["Xinzhe Luo", "Shuai Shao", "Yan Wang", "Jiangtao Wang", "Yutong Bai", "Jianguo Zhang"], "title": "Virtual Biopsy for Intracranial Tumors Diagnosis on MRI", "comment": null, "summary": "Deep intracranial tumors situated in eloquent brain regions controlling vital functions present critical diagnostic challenges. Clinical practice has shifted toward stereotactic biopsy for pathological confirmation before treatment. Yet biopsy carries inherent risks of hemorrhage and neurological deficits and struggles with sampling bias due to tumor spatial heterogeneity, because pathological changes are typically region-selective rather than tumor-wide. Therefore, advancing non-invasive MRI-based pathology prediction is essential for holistic tumor assessment and modern clinical decision-making.\n  The primary challenge lies in data scarcity: low tumor incidence requires long collection cycles, and annotation demands biopsy-verified pathology from neurosurgical experts. Additionally, tiny lesion volumes lacking segmentation masks cause critical features to be overwhelmed by background noise. To address these challenges, we construct the ICT-MRI dataset - the first public biopsy-verified benchmark with 249 cases across four categories. We propose a Virtual Biopsy framework comprising: MRI-Processor for standardization; Tumor-Localizer employing vision-language models for coarse-to-fine localization via weak supervision; and Adaptive-Diagnoser with a Masked Channel Attention mechanism fusing local discriminative features with global contexts. Experiments demonstrate over 90% accuracy, outperforming baselines by more than 20%.", "AI": {"tldr": "They release a first-of-its-kind, biopsy-verified MRI dataset (ICT\u2011MRI, 249 cases, 4 classes) and propose a non-invasive \u201cVirtual Biopsy\u201d pipeline that standardizes MRI, weakly localizes tumors with a vision\u2013language model, and diagnoses via a masked channel attention network, achieving >90% accuracy and >20% over baselines.", "motivation": "Stereotactic biopsy for deep, eloquent-brain tumors is risky (hemorrhage, deficits) and suffers sampling bias due to spatial heterogeneity. MRI-based pathology prediction could reduce invasiveness, but progress is hindered by scarce, expert-verified data and small lesions without segmentation masks where signal is swamped by background.", "method": "1) Build ICT\u2011MRI: a public, biopsy-verified benchmark with 249 cases across four tumor categories. 2) Virtual Biopsy framework: (a) MRI-Processor for input standardization; (b) Tumor-Localizer using vision\u2013language models and weak supervision for coarse-to-fine lesion localization; (c) Adaptive-Diagnoser with Masked Channel Attention to fuse localized discriminative features with global context for classification.", "result": "On ICT\u2011MRI, the framework attains over 90% accuracy and surpasses baselines by >20% absolute, indicating strong gains from the localization and attention mechanisms.", "conclusion": "Biopsy-verified data plus a localization-aware diagnostic model can mitigate sampling bias and small-lesion challenges, offering a promising non-invasive alternative for tumor pathology assessment and a public benchmark to catalyze further research."}}
{"id": "2602.21627", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21627", "abs": "https://arxiv.org/abs/2602.21627", "authors": ["Abhineet Singh", "Justin Rozeboom", "Nilanjan Ray"], "title": "Tokenizing Semantic Segmentation with RLE", "comment": null, "summary": "This paper presents a new unified approach to semantic segmentation in both images and videos by using language modeling to output the masks as sequences of discrete tokens. We use run length encoding (RLE) to discretize the segmentation masks and then train a modified version of Pix2Seq \\cite{p2s} to output these RLE tokens through autoregression. We propose novel tokenization strategies to compress the length of the token sequence to make it practicable to extend this approach to videos. We also show how instance information can be incorporated into the tokenization process to perform panoptic segmentation. We evaluate our proposed models on two datasets to show that they are competitive with the state of the art in spite of being bottlenecked by our limited computational resources.", "AI": {"tldr": "They frame semantic (and panoptic) segmentation as language modeling by emitting run-length\u2013encoded mask tokens with a modified Pix2Seq, introduce tokenization schemes to compress sequences for video, and report competitive results on two datasets despite limited compute.", "motivation": "Unify image and video segmentation (including panoptic) under a single sequence-generation paradigm, reduce reliance on dense pixel-wise decoding, and make sequence-based mask generation practical for videos by shortening token sequences.", "method": "Discretize masks via run-length encoding (RLE). Train a modified Pix2Seq model to autoregressively output RLE tokens. Propose new tokenization/compression strategies to shorten sequences (critical for videos). Embed instance information into the token stream to support panoptic segmentation. Evaluate on two datasets.", "result": "The approach achieves competitive performance with state-of-the-art baselines on two datasets. The compressed tokenization makes video segmentation feasible. Performance is constrained by limited computational resources rather than method design.", "conclusion": "Treating segmentation as sequence prediction with RLE token outputs is a viable, unified solution for images, videos, and panoptic tasks. Efficient tokenization is key, and with more compute the method could further close or surpass SOTA."}}
{"id": "2602.21631", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21631", "abs": "https://arxiv.org/abs/2602.21631", "authors": ["Zhihao Sun", "Tong Wu", "Ruirui Tu", "Daoguo Dong", "Zuxuan Wu"], "title": "UniHand: A Unified Model for Diverse Controlled 4D Hand Motion Modeling", "comment": null, "summary": "Hand motion plays a central role in human interaction, yet modeling realistic 4D hand motion (i.e., 3D hand pose sequences over time) remains challenging. Research in this area is typically divided into two tasks: (1) Estimation approaches reconstruct precise motion from visual observations, but often fail under hand occlusion or absence; (2) Generation approaches focus on synthesizing hand poses by exploiting generative priors under multi-modal structured inputs and infilling motion from incomplete sequences. However, this separation not only limits the effective use of heterogeneous condition signals that frequently arise in practice, but also prevents knowledge transfer between the two tasks. We present UniHand, a unified diffusion-based framework that formulates both estimation and generation as conditional motion synthesis. UniHand integrates heterogeneous inputs by embedding structured signals into a shared latent space through a joint variational autoencoder, which aligns conditions such as MANO parameters and 2D skeletons. Visual observations are encoded with a frozen vision backbone, while a dedicated hand perceptron extracts hand-specific cues directly from image features, removing the need for complex detection and cropping pipelines. A latent diffusion model then synthesizes consistent motion sequences from these diverse conditions. Extensive experiments across multiple benchmarks demonstrate that UniHand delivers robust and accurate hand motion modeling, maintaining performance under severe occlusions and temporally incomplete inputs.", "AI": {"tldr": "UniHand is a unified diffusion-based framework that treats both hand-motion estimation and generation as conditional synthesis, integrating heterogeneous inputs (images, MANO, 2D keypoints) via a joint VAE and producing robust 4D hand motion even under occlusion or missing data.", "motivation": "Modeling realistic 4D hand motion is difficult; existing methods split into estimation (fails with occlusions/absences) and generation (uses priors but lacks coupling with estimation). This separation wastes heterogeneous conditioning signals and prevents knowledge transfer across tasks.", "method": "Formulate both tasks as conditional motion synthesis. Map diverse structured inputs (e.g., MANO parameters, 2D skeletons) into a shared latent space using a joint variational autoencoder; encode images with a frozen vision backbone; extract hand-specific cues via a hand perceptron directly from image features; then use a latent diffusion model to synthesize temporally consistent 3D hand pose sequences from these conditions.", "result": "Across multiple benchmarks, UniHand achieves robust, accurate hand motion modeling and maintains strong performance under severe occlusions and temporally incomplete inputs.", "conclusion": "A single diffusion-based framework can unify estimation and generation of hand motion, effectively leveraging heterogeneous cues and simplifying pipelines (no complex detection/cropping), leading to improved robustness to occlusion and missing data."}}
{"id": "2602.21647", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21647", "abs": "https://arxiv.org/abs/2602.21647", "authors": ["Tangsang Chongbang", "Pranesh Pyara Shrestha", "Amrit Sarki", "Anku Jaiswal"], "title": "Mitigating Structural Noise in Low-Resource S2TT: An Optimized Cascaded Nepali-English Pipeline with Punctuation Restoration", "comment": "13 pages, 4 figures, 12 tables", "summary": "This paper presents and evaluates an optimized cascaded Nepali speech-to-English text translation (S2TT) system, focusing on mitigating structural noise introduced by Automatic Speech Recognition (ASR). We first establish highly proficient ASR and NMT components: a Wav2Vec2-XLS-R-300m model achieved a state-of-the-art 2.72% CER on OpenSLR-54, and a multi-stage fine-tuned MarianMT model reached a 28.32 BLEU score on the FLORES-200 benchmark. We empirically investigate the influence of punctuation loss, demonstrating that unpunctuated ASR output significantly degrades translation quality, causing a massive 20.7% relative BLEU drop on the FLORES benchmark. To overcome this, we propose and evaluate an intermediate Punctuation Restoration Module (PRM). The final S2TT pipeline was tested across three configurations on a custom dataset. The optimal configuration, which applied the PRM directly to ASR output, achieved a 4.90 BLEU point gain over the direct ASR-to-NMT baseline (BLEU 36.38 vs. 31.48). This improvement was validated by human assessment, which confirmed the optimized pipeline's superior Adequacy (3.673) and Fluency (3.804). This work validates that targeted punctuation restoration is the most effective intervention for mitigating structural noise in the Nepali S2TT pipeline. It establishes an optimized baseline and demonstrates a critical architectural insight for developing cascaded speech translation systems for similar low-resource languages.", "AI": {"tldr": "Adding a punctuation restoration module between ASR and NMT in a Nepali speech-to-English cascaded system measurably boosts translation quality: strong ASR (2.72% CER) and NMT (28.32 BLEU) are built, punctuation loss is shown to hurt BLEU heavily, and restoring punctuation yields +4.90 BLEU and better human adequacy/fluency\u2014an insight likely transferable to low-resource S2TT.", "motivation": "Cascaded S2TT systems for low-resource languages suffer from ASR-induced structural noise\u2014especially missing punctuation\u2014that degrades downstream NMT. The authors aim to quantify this effect and identify a practical intervention to improve translation quality, while establishing strong Nepali baselines.", "method": "1) Train/evaluate a high-quality ASR (Wav2Vec2-XLS-R-300m) on OpenSLR-54; 2) Multi-stage fine-tune MarianMT and validate on FLORES-200; 3) Empirically analyze the impact of unpunctuated ASR text on translation (BLEU drop); 4) Insert an intermediate Punctuation Restoration Module (PRM) in the cascade; 5) Compare three S2TT configurations on a custom dataset using BLEU and human Adequacy/Fluency ratings.", "result": "- ASR: 2.72% CER (state-of-the-art on OpenSLR-54). - NMT: 28.32 BLEU on FLORES-200. - Punctuation removal causes a 20.7% relative BLEU drop on FLORES. - Best S2TT config (PRM applied directly to ASR output) improves BLEU by +4.90 over baseline (36.38 vs. 31.48) and achieves higher human Adequacy (3.673) and Fluency (3.804).", "conclusion": "Targeted punctuation restoration is the most effective way to mitigate ASR structural noise in Nepali cascaded S2TT, yielding sizable automatic and human-evaluated gains. The work establishes optimized ASR/NMT baselines and offers an architectural guideline likely applicable to other low-resource languages."}}
{"id": "2602.21652", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21652", "abs": "https://arxiv.org/abs/2602.21652", "authors": ["Minhao Jiang", "Zhikai Li", "Xuewen Liu", "Jing Zhang", "Mengjuan Chen", "Qingyi Gu"], "title": "Sparsity Induction for Accurate Post-Training Pruning of Large Language Models", "comment": "5 pages, 1 figure, 4 tables", "summary": "Large language models have demonstrated capabilities in text generation, while their increasing parameter scales present challenges in computational and memory efficiency. Post-training sparsity (PTS), which reduces model cost by removing weights from dense networks, is an effective approach. However, native dense matrices lack high sparsity, making existing approaches that directly remove weights disrupt model states, resulting in unsatisfactory performance recovery even with post-tuning. We propose Sparsity Induction, which promotes models toward higher sparsity at both distribution and feature levels before pruning, to push the limits of PTS. At the distribution level, we enhance distributional sparsity through mathematically equivalent scaling transformations, which are fully absorbable and incur no extra parameters or inference-time overhead. At the feature level, we introduce Spectral Norm Loss to promote feature sparsity from a low-rank perspective. Experiments across diverse model architectures and tasks demonstrate that our method further enhances sparsity-friendliness, achieving superior pruning performance over existing approaches.", "AI": {"tldr": "Induce sparsity before pruning: apply absorbable scaling to increase distributional sparsity and add a spectral-norm loss to encourage low-rank, sparse features, enabling higher-quality post-training pruning of LLMs without inference overhead.", "motivation": "LLMs are costly in compute and memory. Standard post-training sparsity that directly removes weights from dense models often damages model states and recovers poorly, as dense matrices are not naturally highly sparse. The authors aim to make models intrinsically more sparsity-friendly before pruning to improve efficiency without large accuracy loss.", "method": "Sparsity Induction with two complementary components prior to pruning: (1) Distribution-level induction via mathematically equivalent (fully absorbable) scaling transformations that increase distributional sparsity without adding parameters or runtime cost; (2) Feature-level induction using a Spectral Norm Loss to encourage low-rank structure and feature sparsity. After induction, standard pruning is applied.", "result": "Across multiple model architectures and tasks, the approach increases sparsity-friendliness and outperforms existing post-training sparsity methods in pruning quality (e.g., better accuracy/quality at comparable or higher sparsity).", "conclusion": "Preparing models for sparsity\u2014via absorbable scaling and spectral-norm regularization\u2014pushes the limits of post-training pruning, enabling more aggressive sparsification with minimal overhead and improved performance retention."}}
{"id": "2602.21728", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21728", "abs": "https://arxiv.org/abs/2602.21728", "authors": ["Shiqi Yan", "Yubo Chen", "Ruiqi Zhou", "Zhengxi Yao", "Shuai Chen", "Tianyi Zhang", "Shijie Zhang", "Wei Qiang Zhang", "Yongfeng Huang", "Haixin Duan", "Yunqi Zhang"], "title": "Explore-on-Graph: Incentivizing Autonomous Exploration of Large Language Models on Knowledge Graphs with Path-refined Reward Modeling", "comment": "Published as a conference paper at ICLR 2026", "summary": "The reasoning process of Large Language Models (LLMs) is often plagued by hallucinations and missing facts in question-answering tasks. A promising solution is to ground LLMs' answers in verifiable knowledge sources, such as Knowledge Graphs (KGs). Prevailing KG-enhanced methods typically constrained LLM reasoning either by enforcing rules during generation or by imitating paths from a fixed set of demonstrations. However, they naturally confined the reasoning patterns of LLMs within the scope of prior experience or fine-tuning data, limiting their generalizability to out-of-distribution graph reasoning problems. To tackle this problem, in this paper, we propose Explore-on-Graph (EoG), a novel framework that encourages LLMs to autonomously explore a more diverse reasoning space on KGs. To incentivize exploration and discovery of novel reasoning paths, we propose to introduce reinforcement learning during training, whose reward is the correctness of the reasoning paths' final answers. To enhance the efficiency and meaningfulness of the exploration, we propose to incorporate path information as additional reward signals to refine the exploration process and reduce futile efforts. Extensive experiments on five KGQA benchmark datasets demonstrate that, to the best of our knowledge, our method achieves state-of-the-art performance, outperforming not only open-source but also even closed-source LLMs.", "AI": {"tldr": "EoG trains LLMs to explore knowledge graphs with reinforcement learning, using answer-correctness and path-aware rewards to discover diverse, verifiable reasoning paths, yielding state-of-the-art KGQA results across five benchmarks.", "motivation": "Existing KG-augmented LLM methods restrict reasoning to predefined rules or demonstration paths, which limits generalization\u2014especially to out-of-distribution graph queries. The authors aim to let LLMs autonomously discover novel, verifiable reasoning patterns on KGs to reduce hallucinations and missing facts.", "method": "Explore-on-Graph (EoG): an RL-based framework where the LLM reasons over a KG by exploring paths. The primary reward is the correctness of the final answer; auxiliary rewards use path information to guide exploration and reduce unproductive search. This moves beyond imitation or rule-constrained decoding by incentivizing discovery of diverse reasoning trajectories grounded in KGs.", "result": "Across five KGQA benchmarks, EoG achieves state-of-the-art performance, outperforming both open-source and closed-source LLM systems per the authors\u2019 experiments.", "conclusion": "RL-driven, path-aware exploration over KGs broadens LLM reasoning beyond prior demonstrations and improves factuality and generalization in KGQA. Incorporating path signals makes exploration more efficient, contributing to SOTA results."}}
{"id": "2602.21741", "categories": ["cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.21741", "abs": "https://arxiv.org/abs/2602.21741", "authors": ["MD. Sagor Chowdhury", "Adiba Fairooz Chowdhury"], "title": "Robust Long-Form Bangla Speech Processing: Automatic Speech Recognition and Speaker Diarization", "comment": "6 pages, 5 figures, 3 tables; system paper submitted to DL Sprint 4.0 (Kaggle)", "summary": "We describe our end-to-end system for Bengali long-form speech recognition (ASR) and speaker diarization submitted to the DL Sprint 4.0 competition on Kaggle. Bengali presents substantial challenges for both tasks: a large phoneme inventory, significant dialectal variation, frequent code-mixing with English, and a relative scarcity of large-scale labelled corpora. For ASR we achieve a best private Word Error Rate (WER) of 0.37738 and public WER of 0.36137, combining a BengaliAI fine-tuned Whisper medium model with Demucs source separation for vocal isolation, silence-boundary chunking, and carefully tuned generation hyperparameters. For speaker diarization we reach a best private Diarization Error Rate (DER) of 0.27671 and public DER of 0.20936 by replacing the default segmentation model inside the pyannote.audio pipeline with a Bengali-fine-tuned variant, pairing it with wespeaker-voxceleb-resnet34-LM embeddings and centroid-based agglomerative clustering. Our experiments demonstrate that domain-specific fine-tuning of the segmentation component, vocal source separation, and natural silence-aware chunking are the three most impactful design choices for low-resource Bengali speech processing.", "AI": {"tldr": "End-to-end Bengali long-form ASR and diarization system that pairs a Bengali-fine-tuned Whisper medium model with vocal-source separation and silence-aware chunking for transcription, and a Bengali-fine-tuned pyannote segmentation plus wespeaker embeddings with centroid agglomerative clustering for diarization\u2014achieving ~0.36\u20130.38 WER and ~0.21\u20130.28 DER on Kaggle DL Sprint 4.0.", "motivation": "Bengali speech processing is hard due to a large phoneme inventory, dialectal diversity, English code-mixing, and limited labeled data. The work targets robust long-form ASR and speaker diarization for a low-resource language in a competitive benchmark setting.", "method": "ASR: Apply Demucs for vocal isolation, chunk audio at natural silence boundaries, and transcribe with a BengaliAI-fine-tuned Whisper medium model using carefully tuned decoding parameters. Diarization: Replace the default pyannote.audio segmentation model with a Bengali-fine-tuned variant, use wespeaker-voxceleb-resnet34-LM embeddings, and perform centroid-based agglomerative clustering.", "result": "ASR: private WER 0.37738, public WER 0.36137. Diarization: private DER 0.27671, public DER 0.20936, representing the team\u2019s best submissions in the competition.", "conclusion": "Three choices had the largest impact for low-resource Bengali speech: domain-specific fine-tuning of the segmentation model, vocal source separation, and silence-aware chunking. These design decisions enable competitive long-form ASR and diarization performance despite data scarcity and linguistic variability."}}
{"id": "2602.21657", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21657", "abs": "https://arxiv.org/abs/2602.21657", "authors": ["Shaoxuan Wu", "Jingkun Chen", "Chong Ma", "Cong Shen", "Xiao Zhang", "Jun Feng"], "title": "Following the Diagnostic Trace: Visual Cognition-guided Cooperative Network for Chest X-Ray Diagnosis", "comment": null, "summary": "Computer-aided diagnosis (CAD) has significantly advanced automated chest X-ray diagnosis but remains isolated from clinical workflows and lacks reliable decision support and interpretability. Human-AI collaboration seeks to enhance the reliability of diagnostic models by integrating the behaviors of controllable radiologists. However, the absence of interactive tools seamlessly embedded within diagnostic routines impedes collaboration, while the semantic gap between radiologists' decision-making patterns and model representations further limits clinical adoption. To overcome these limitations, we propose a visual cognition-guided collaborative network (VCC-Net) to achieve the cooperative diagnostic paradigm. VCC-Net centers on visual cognition (VC) and employs clinically compatible interfaces, such as eye-tracking or the mouse, to capture radiologists' visual search traces and attention patterns during diagnosis. VCC-Net employs VC as a spatial cognition guide, learning hierarchical visual search strategies to localize diagnostically key regions. A cognition-graph co-editing module subsequently integrates radiologist VC with model inference to construct a disease-aware graph. The module captures dependencies among anatomical regions and aligns model representations with VC-driven features, mitigating radiologist bias and facilitating complementary, transparent decision-making. Experiments on the public datasets SIIM-ACR, EGD-CXR, and self-constructed TB-Mouse dataset achieved classification accuracies of 88.40%, 85.05%, and 92.41%, respectively. The attention maps produced by VCC-Net exhibit strong concordance with radiologists' gaze distributions, demonstrating a mutual reinforcement of radiologist and model inference. The code is available at https://github.com/IPMI-NWU/VCC-Net.", "AI": {"tldr": "VCC-Net is a human\u2013AI collaborative chest X-ray framework that ingests radiologists\u2019 visual cognition (gaze/mouse traces) to guide hierarchical attention and co-edits a disease-aware cognition graph with the model, yielding higher classification accuracy and attention maps aligned with human gaze.", "motivation": "Conventional CAD systems are isolated from clinical workflows and provide limited, nontransparent decision support. The authors aim to close the semantic and interaction gaps between radiologists\u2019 decision-making and model representations, enabling reliable, interpretable collaboration within routine diagnosis.", "method": "Radiologists\u2019 visual search traces (via eye-tracking or mouse) are captured as visual cognition (VC). VC serves as a spatial guide to learn hierarchical visual search strategies that localize diagnostically key regions. A cognition-graph co-editing module fuses VC with model inference to construct a disease-aware graph over anatomical regions, aligning model features with VC-driven representations and modeling inter-regional dependencies while mitigating radiologist bias.", "result": "On SIIM-ACR, EGD-CXR, and a self-constructed TB-Mouse dataset, VCC-Net attains classification accuracies of 88.40%, 85.05%, and 92.41%, respectively. Attention maps show strong concordance with radiologists\u2019 gaze distributions. Code is released at the provided repository.", "conclusion": "Leveraging radiologists\u2019 visual cognition to steer spatial reasoning and a co-edited disease-aware graph, VCC-Net supports a cooperative diagnostic paradigm that improves performance and produces explanations aligned with expert attention, promising more transparent and clinically compatible CAD."}}
{"id": "2602.21763", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21763", "abs": "https://arxiv.org/abs/2602.21763", "authors": ["Heng Wang", "Changxing Wu"], "title": "Improving Implicit Discourse Relation Recognition with Natural Language Explanations from LLMs", "comment": "AAAI26'0ral", "summary": "Implicit Discourse Relation Recognition (IDRR) remains a challenging task due to the requirement for deep semantic understanding in the absence of explicit discourse markers. A further limitation is that existing methods only predict relations without providing any supporting explanations. Recent advances in large language models (LLMs) have shown strong reasoning capabilities in both deep language understanding and natural language explanation generation. In this work, we propose a simple yet effective approach to distill the reasoning capabilities of LLMs into lightweight IDRR models to improve both performance and interpretability. Specifically, we first prompt an LLM to generate explanations for each training instance conditioned on its gold label. Then, we introduce a novel classification-generation framework that jointly performs relation prediction and explanation generation, and train it with the additional supervision of LLM-generated explanations. Our framework is plug-and-play, enabling easy integration with most existing IDRR models. Experimental results on PDTB demonstrate that our approach significantly improves IDRR performance, while human evaluation further confirms that the generated explanations enhance model interpretability. Furthermore, we validate the generality of our approach on sentiment classification and natural language inference", "AI": {"tldr": "Distill LLM-generated explanations into lightweight IDRR models via a joint classification\u2013generation framework, yielding better accuracy on PDTB and more interpretable predictions; approach also transfers to sentiment and NLI.", "motivation": "Implicit discourse relations lack explicit markers and most models output labels without explanations. LLMs can reason and explain but are heavy/costly. The goal is to transfer (distill) LLM reasoning power and explanatory ability into smaller IDRR models to improve both performance and interpretability.", "method": "1) Prompt an LLM to produce instance-level explanations conditioned on gold labels. 2) Train a plug-and-play joint model that predicts relations and generates explanations, using the LLM explanations as additional supervision; can be attached to existing IDRR architectures.", "result": "On PDTB, the approach significantly improves IDRR performance; human evaluation finds the generated explanations helpful for interpretability. The method also shows benefits on sentiment classification and NLI.", "conclusion": "LLM explanation distillation into compact models is effective for IDRR, improving accuracy and human-perceived interpretability, and generalizes beyond discourse relations."}}
{"id": "2602.21667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21667", "abs": "https://arxiv.org/abs/2602.21667", "authors": ["Sheng Xu", "Enshu Wang", "Hongfei Xue", "Jian Teng", "Bingyi Liu", "Yi Zhu", "Pu Wang", "Libing Wu", "Chunming Qiao"], "title": "Send Less, Perceive More: Masked Quantized Point Cloud Communication for Loss-Tolerant Collaborative Perception", "comment": null, "summary": "Collaborative perception allows connected vehicles to overcome occlusions and limited viewpoints by sharing sensory information. However, existing approaches struggle to achieve high accuracy under strict bandwidth constraints and remain highly vulnerable to random transmission packet loss. We introduce QPoint2Comm, a quantized point-cloud communication framework that dramatically reduces bandwidth while preserving high-fidelity 3D information. Instead of transmitting intermediate features, QPoint2Comm directly communicates quantized point-cloud indices using a shared codebook, enabling efficient reconstruction with lower bandwidth than feature-based methods. To ensure robustness to possible communication packet loss, we employ a masked training strategy that simulates random packet loss, allowing the model to maintain strong performance even under severe transmission failures. In addition, a cascade attention fusion module is proposed to enhance multi-vehicle information integration. Extensive experiments on both simulated and real-world datasets demonstrate that QPoint2Comm sets a new state of the art in accuracy, communication efficiency, and resilience to packet loss.", "AI": {"tldr": "QPoint2Comm is a collaborative perception framework that transmits quantized point\u2011cloud indices (via a shared codebook) instead of features, sharply cutting bandwidth while preserving 3D fidelity; it adds masked training to withstand packet loss and a cascade attention fusion to integrate multi-vehicle data, achieving SOTA accuracy, efficiency, and robustness on sim and real datasets.", "motivation": "Connected vehicles need to share 3D information for occlusion handling, but prior methods send intermediate features that are bandwidth-heavy and fragile to packet loss. The goal is to keep high perception accuracy under tight bandwidth and unreliable links.", "method": "Learn a shared codebook to vector-quantize point clouds; transmit compact indices rather than dense features; reconstruct 3D at the receiver from the shared codebook. Train with packet-loss masking to mimic random dropouts, improving robustness. Use a cascade attention fusion module to aggregate information from multiple vehicles. Evaluate on simulated and real-world datasets.", "result": "Compared to feature-based communication baselines, QPoint2Comm achieves state-of-the-art perception accuracy with significantly lower bandwidth usage and maintains strong performance under severe (random) packet loss on both simulated and real datasets.", "conclusion": "Transmitting quantized point-cloud tokens with a shared codebook, combined with loss-aware training and attentive fusion, is an effective and practical strategy for collaborative perception under strict bandwidth and lossy channels, advancing SOTA in accuracy, efficiency, and resilience."}}
{"id": "2602.21786", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21786", "abs": "https://arxiv.org/abs/2602.21786", "authors": ["Shunsuke Ubukata"], "title": "D-COT: Disciplined Chain-of-Thought Learning for Efficient Reasoning in Small Language Models", "comment": "9 pages, 3 figures. Code: https://github.com/gitpullpull/DisciplinedChainOfThought | Benchmarks: https://huggingface.co/datasets/gitpullpull/D-CoT-Benchmarks | Dataset: https://huggingface.co/datasets/gitpullpull/D-CoT-datasets", "summary": "Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) often induces \"overthinking\" in Small Language Models (SLMs), leading to performance degradation and excessive token consumption. In this study, we propose Disciplined Chain-of-Thought (D-CoT), a novel framework that enforces a structured reasoning process using control tags -- such as <TEMP_LOW> for fact-checking and <TEMP_HIGH> for multi-perspective exploration -- as auxiliary scaffolding during training. By optimizing the CoT trajectory, D-CoT suppresses reasoning drift and simultaneously achieves token reduction and performance improvement. We demonstrate the efficacy of our approach on Qwen3-8B: with only 5,000 training samples, D-CoT significantly boosts accuracy on GPQA-diamond by 9.9% and MMLU-Pro (0-shot) by 9.1%, while drastically reducing computational costs. Furthermore, we confirm that the model internalizes this disciplined thought structure, maintaining high performance even without explicit control tags during inference.", "AI": {"tldr": "D-CoT adds lightweight control tags to structure CoT during distillation, curbing overthinking in small LMs, cutting tokens and improving accuracy\u2014even when tags are removed at inference.", "motivation": "Standard CoT distillation can cause small models to mimic verbose, meandering reasoning from LLMs, leading to reasoning drift, higher token costs, and lower accuracy. The authors aim to constrain CoT so SLMs reason efficiently and reliably.", "method": "Introduce Disciplined CoT (D-CoT): augment training traces with control tags (e.g., <TEMP_LOW> for cautious fact-checking, <TEMP_HIGH> for exploration) that scaffold a staged reasoning process. Train the student to follow this structured trajectory, optimizing for concise yet accurate steps. Evaluate both with and without tags at inference to test internalization.", "result": "On Qwen3-8B using only 5k training samples, D-CoT improves GPQA-diamond accuracy by +9.9% and MMLU-Pro (0-shot) by +9.1% while substantially reducing token usage and compute. The model sustains strong performance even without tags at inference, indicating the structure is learned rather than merely prompted.", "conclusion": "Structuring CoT with simple control tags during distillation suppresses drift and overthinking, yielding higher accuracy and lower token costs. The disciplined reasoning pattern is internalized, enabling tag-free inference and suggesting a practical path to efficient, reliable SLM reasoning with minimal data."}}
{"id": "2602.21668", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.21668", "abs": "https://arxiv.org/abs/2602.21668", "authors": ["Junmyeong Lee", "Hoseung Choi", "Minsu Cho"], "title": "Space-Time Forecasting of Dynamic Scenes with Motion-aware Gaussian Grouping", "comment": "20 pages, 13 figures", "summary": "Forecasting dynamic scenes remains a fundamental challenge in computer vision, as limited observations make it difficult to capture coherent object-level motion and long-term temporal evolution. We present Motion Group-aware Gaussian Forecasting (MoGaF), a framework for long-term scene extrapolation built upon the 4D Gaussian Splatting representation. MoGaF introduces motion-aware Gaussian grouping and group-wise optimization to enforce physically consistent motion across both rigid and non-rigid regions, yielding spatially coherent dynamic representations. Leveraging this structured space-time representation, a lightweight forecasting module predicts future motion, enabling realistic and temporally stable scene evolution. Experiments on synthetic and real-world datasets demonstrate that MoGaF consistently outperforms existing baselines in rendering quality, motion plausibility, and long-term forecasting stability. Our project page is available at https://slime0519.github.io/mogaf", "AI": {"tldr": "MoGaF is a 4D Gaussian-splatting\u2013based framework that groups Gaussians by motion and forecasts their future dynamics, yielding coherent, physically plausible, and stable long-term scene renderings that outperform baselines.", "motivation": "Long-term forecasting of dynamic scenes is hard with limited observations because models struggle to capture coherent object-level motion and maintain temporal consistency across rigid and non-rigid regions.", "method": "Build on 4D Gaussian Splatting; introduce motion-aware Gaussian grouping and group-wise optimization to enforce physically consistent motion across rigid and non-rigid parts; leverage the resulting structured space\u2013time representation with a lightweight forecasting module to predict future motion and render future frames.", "result": "On synthetic and real-world datasets, MoGaF surpasses prior methods in rendering quality, motion plausibility, and stability over long forecasting horizons.", "conclusion": "Structuring dynamic scenes via motion-aware Gaussian groups and optimizing them jointly enables realistic, temporally stable long-term forecasts; combining this with a light motion predictor yields superior, coherent extrapolations across diverse scenes."}}
{"id": "2602.21854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21854", "abs": "https://arxiv.org/abs/2602.21854", "authors": ["Mustafa Dogan", "Ilker Kesen", "Iacer Calixto", "Aykut Erdem", "Erkut Erdem"], "title": "FewMMBench: A Benchmark for Multimodal Few-Shot Learning", "comment": "Preprint. 49 pages, 38 Figures, 5 Tables", "summary": "As multimodal large language models (MLLMs) advance in handling interleaved image-text data, assessing their few-shot learning capabilities remains an open challenge. In this paper, we introduce FewMMBench, a comprehensive benchmark designed to evaluate MLLMs under few-shot conditions, with a focus on In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting. Covering a diverse suite of multimodal understanding tasks, from attribute recognition to temporal reasoning, FewMMBench enables systematic analysis across task types, model families, and prompting strategies. We evaluate 26 open-weight MLLMs from six model families across zero-shot, few-shot, and CoT-augmented few-shot settings. Our findings reveal that instruction-tuned models exhibit strong zero-shot performance but benefit minimally, or even regress, with additional demonstrations or CoT reasoning. Retrieval-based demonstrations and increased context size also yield limited gains. These results highlight FewMMBench as a rigorous testbed for diagnosing and advancing few-shot capabilities in multimodal LLMs. The data is available at: https://huggingface.co/datasets/mustafaa/FewMMBench", "AI": {"tldr": "FewMMBench is a benchmark to evaluate multimodal LLMs\u2019 few-shot, in-context, and chain-of-thought abilities. Testing 26 open-weight models shows instruction-tuned MLLMs are strong zero-shot but often fail to benefit\u2014and can regress\u2014from extra demonstrations, CoT, retrieval-based examples, or larger context windows.", "motivation": "Despite rapid progress in MLLMs for interleaved image-text, their true few-shot (ICL/CoT) capabilities are unclear and inconsistently measured. A standardized, diverse benchmark is needed to diagnose when and how prompting strategies help or hinder multimodal reasoning.", "method": "Construct FewMMBench spanning diverse multimodal understanding tasks (e.g., attribute recognition, temporal reasoning). Systematically evaluate 26 open-weight MLLMs from six families under zero-shot, few-shot, and CoT-augmented few-shot settings, including retrieval-based demonstrations and varied context sizes.", "result": "Instruction-tuned models exhibit strong zero-shot performance but gain little\u2014or even degrade\u2014when given demonstrations or CoT prompts. Retrieval-based exemplars and longer context also yield limited improvements across tasks and model families.", "conclusion": "FewMMBench serves as a rigorous testbed, revealing current limitations of ICL/CoT for MLLMs. Advancing few-shot multimodal capabilities likely requires model/training changes beyond prompting; the benchmark can guide systematic diagnosis and progress."}}
{"id": "2602.21720", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21720", "abs": "https://arxiv.org/abs/2602.21720", "authors": ["Andrea Silvi", "Ponrawee Prasertsom", "Jennifer Culbertson", "Devdatt Dubhashi", "Moa Johansson", "Kenny Smith"], "title": "Evaluating the relationship between regularity and learnability in recursive numeral systems using Reinforcement Learning", "comment": null, "summary": "Human recursive numeral systems (i.e., counting systems such as English base-10 numerals), like many other grammatical systems, are highly regular. Following prior work that relates cross-linguistic tendencies to biases in learning, we ask whether regular systems are common because regularity facilitates learning. Adopting methods from the Reinforcement Learning literature, we confirm that highly regular human(-like) systems are easier to learn than unattested but possible irregular systems. This asymmetry emerges under the natural assumption that recursive numeral systems are designed for generalisation from limited data to represent all integers exactly. We also find that the influence of regularity on learnability is absent for unnatural, highly irregular systems, whose learnability is influenced instead by signal length, suggesting that different pressures may influence learnability differently in different parts of the space of possible numeral systems. Our results contribute to the body of work linking learnability to cross-linguistic prevalence.", "AI": {"tldr": "Using reinforcement learning, the paper shows that human-like, recursive (regular) numeral systems are learned more easily than possible but irregular systems, largely because they support generalization from limited data; in highly irregular regions, signal length, not regularity, drives learnability.", "motivation": "Human numeral systems are strikingly regular across languages. The authors ask whether this cross-linguistic regularity arises because regular systems are easier to learn, linking typological prevalence to learnability biases.", "method": "They frame learning numeral systems as a reinforcement learning problem where agents must generalize from limited training data to represent all integers exactly. They compare learning difficulty across human-like regular recursive systems and constructed irregular systems, analyzing how regularity and signal length affect learnability.", "result": "Regular, human-like recursive numeral systems are learned more reliably and quickly than unattested but logically possible irregular systems when the task requires exact generalization to all integers. In extremely irregular systems, the benefit of regularity disappears; learnability is instead predicted by signal length.", "conclusion": "Learnability pressures likely contribute to the global prevalence of regular recursive numeral systems, but different regions of the design space are shaped by different pressures: regularity aids learning where structure supports generalization, whereas in highly irregular systems, compression (shorter signals) matters more."}}
{"id": "2602.21862", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21862", "abs": "https://arxiv.org/abs/2602.21862", "authors": ["Chia Cheng Chang", "An-Zi Yen", "Hen-Hsen Huang", "Hsin-Hsi Chen"], "title": "Personalized Graph-Empowered Large Language Model for Proactive Information Access", "comment": null, "summary": "Since individuals may struggle to recall all life details and often confuse events, establishing a system to assist users in recalling forgotten experiences is essential. While numerous studies have proposed memory recall systems, these primarily rely on deep learning techniques that require extensive training and often face data scarcity due to the limited availability of personal lifelogs. As lifelogs grow over time, systems must also adapt quickly to newly accumulated data. Recently, large language models (LLMs) have demonstrated remarkable capabilities across various tasks, making them promising for personalized applications. In this work, we present a framework that leverages LLMs for proactive information access, integrating personal knowledge graphs to enhance the detection of access needs through a refined decision-making process. Our framework offers high flexibility, enabling the replacement of base models and the modification of fact retrieval methods for continuous improvement. Experimental results demonstrate that our approach effectively identifies forgotten events, supporting users in recalling past experiences more efficiently.", "AI": {"tldr": "Proposes a flexible, LLM-driven framework augmented with personal knowledge graphs to proactively surface forgotten life events; experiments indicate it effectively detects access needs and improves recall efficiency.", "motivation": "People often forget or confuse personal events. Prior memory-recall systems rely on data-hungry deep learning models that struggle with scarce, private lifelog data and slow adaptation as lifelogs grow. LLMs\u2019 generalization and few-shot abilities make them promising for personalized recall without extensive training.", "method": "A modular framework that (1) uses large language models for proactive information access and decision-making about when to surface information, (2) integrates personal knowledge graphs to structure and retrieve user facts, and (3) allows swapping base models and adjusting retrieval strategies for continual improvement and rapid adaptation to new lifelog data.", "result": "Experimental evaluations suggest the system accurately detects forgotten or relevant past events and supports users in recalling experiences more efficiently; specific metrics are not reported in the abstract.", "conclusion": "LLM + personal knowledge graph integration provides an adaptable, effective approach to personalized memory support, enabling proactive recall with better flexibility and continual improvement as lifelogs expand."}}
{"id": "2602.21779", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21779", "abs": "https://arxiv.org/abs/2602.21779", "authors": ["Zheyuan Gu", "Qingsong Zhao", "Yusong Wang", "Zhaohong Huang", "Xinqi Li", "Cheng Yuan", "Jiaowei Shao", "Chi Zhang", "Xuelong Li"], "title": "Beyond Static Artifacts: A Forensic Benchmark for Video Deepfake Reasoning in Vision Language Models", "comment": "16 pages, 9 figures. Submitted to CVPR 2026", "summary": "Current Vision-Language Models (VLMs) for deepfake detection excel at identifying spatial artifacts but overlook a critical dimension: temporal inconsistencies in video forgeries. Adapting VLMs to reason about these dynamic cues remains a distinct challenge. To bridge this gap, we propose Forensic Answer-Questioning (FAQ), a large-scale benchmark that formulates temporal deepfake analysis as a multiple-choice task. FAQ introduces a three-level hierarchy to progressively evaluate and equip VLMs with forensic capabilities: (1) Facial Perception, testing the ability to identify static visual artifacts; (2) Temporal Deepfake Grounding, requiring the localization of dynamic forgery artifacts across frames; and (3) Forensic Reasoning, challenging models to synthesize evidence for final authenticity verdicts. We evaluate a range of VLMs on FAQ and generate a corresponding instruction-tuning set, FAQ-IT. Extensive experiments show that models fine-tuned on FAQ-IT achieve advanced performance on both in-domain and cross-dataset detection benchmarks. Ablation studies further validate the impact of our key design choices, confirming that FAQ is the driving force behind the temporal reasoning capabilities of these VLMs.", "AI": {"tldr": "They introduce FAQ, a hierarchical multiple-choice benchmark and instruction-tuning set that teaches/evaluates VLMs on temporal cues for deepfake detection, yielding strong in-domain and cross-dataset gains and attributing improvements to the benchmark\u2019s design.", "motivation": "Current VLMs focus on spatial artifacts and largely ignore temporal inconsistencies crucial for video forgery detection; enabling temporal reasoning in VLMs is challenging and underexplored.", "method": "Create FAQ, a large-scale benchmark casting temporal deepfake analysis as multiple choice with three levels: (1) Facial Perception for static artifacts; (2) Temporal Deepfake Grounding to localize dynamic artifacts across frames; (3) Forensic Reasoning to synthesize evidence for authenticity decisions. Evaluate diverse VLMs, build an instruction-tuning dataset (FAQ-IT), fine-tune models, and conduct ablations on design choices.", "result": "VLMs fine-tuned on FAQ-IT achieve advanced performance on both in-domain and cross-dataset deepfake detection tasks; ablation studies indicate that the benchmark\u2019s hierarchical structure and temporal grounding tasks are key drivers of the gains.", "conclusion": "The FAQ benchmark and its instruction-tuning data effectively endow VLMs with temporal forensic reasoning, improving generalization beyond static artifact detection and offering a scalable path to better deepfake video analysis."}}
{"id": "2602.21887", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21887", "abs": "https://arxiv.org/abs/2602.21887", "authors": ["Changjiang Gao", "Zixian Huang", "Kaichen Yang", "Jiajun Chen", "Jixing Li", "Shujian Huang"], "title": "ExpLang: Improved Exploration and Exploitation in LLM Reasoning with On-Policy Thinking Language Selection", "comment": null, "summary": "Current large reasoning models (LRMs) have shown strong ability on challenging tasks after reinforcement learning (RL) based post-training. However, previous work mainly focuses on English reasoning in expectation of the strongest performance, despite the demonstrated potential advantage of multilingual thinking, as well as the requirement for native thinking traces by global users. In this paper, we propose ExpLang, a novel LLM post-training pipeline that enables on-policy thinking language selection to improve exploration and exploitation during RL with the use of multiple languages. The results show that our method steadily outperforms English-only training with the same training budget, while showing high thinking language compliance for both seen and unseen languages. Analysis shows that, by enabling on-policy thinking language selection as an action during RL, ExpLang effectively extends the RL exploration space with diversified language preference and improves the RL exploitation outcome with leveraged non-English advantage. The method is orthogonal to most RL algorithms and opens up a new perspective on using multilinguality to improve LRMs.", "AI": {"tldr": "ExpLang makes the \u201cthinking language\u201d a learnable, on\u2011policy action during RL post\u2011training, letting the model explore and exploit multiple languages for reasoning; this yields consistent gains over English\u2011only training under the same budget and maintains high language\u2011compliance even for unseen languages.", "motivation": "Most RL post\u2011training for large reasoning models optimizes English chain\u2011of\u2011thought, ignoring evidence that multilingual reasoning can help and that users want native\u2011language traces. The authors aim to leverage multilinguality during RL itself, not just at inference, to improve both exploration and final performance.", "method": "Introduce ExpLang, a post\u2011training pipeline where the model selects the reasoning (chain\u2011of\u2011thought) language as an explicit action in on\u2011policy RL. The chosen language conditions generation; rewards are task\u2011centric, enabling the policy to discover advantageous language preferences. This expands the exploration space (diverse language choices) and improves exploitation (using non\u2011English advantages). The approach is designed to be orthogonal to standard RL algorithms and enforces high language\u2011compliance.", "result": "Across evaluations, ExpLang consistently outperforms English\u2011only RL training with the same compute/data budget and shows high adherence to the selected thinking language for both seen and unseen languages.", "conclusion": "Treating thinking language as an on\u2011policy action unlocks multilingual benefits for LRMs, enlarging exploration and strengthening exploitation during RL. The approach is broadly compatible with existing RL methods and suggests multilinguality as a new lever for improving reasoning models."}}
{"id": "2602.21703", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21703", "abs": "https://arxiv.org/abs/2602.21703", "authors": ["T. Schaffer", "A. Brawanski", "S. Wein", "A. M. Tom\u00e9", "E. W. Lang"], "title": "Brain Tumor Segmentation with Special Emphasis on the Non-Enhancing Brain Tumor Compartment", "comment": null, "summary": "A U-Net based deep learning architecture is designed to segment brain tumors as they appear on various MRI modalities. Special emphasis is lent to the non-enhancing tumor compartment. The latter has not been considered anymore in recent brain tumor segmentation challenges like the MICCAI challenges. However, it is considered to be indicative of the survival time of the patient as well as of areas of further tumor growth. Hence it deems essential to have means to automatically delineate its extension within the tumor.", "AI": {"tldr": "U\u2011Net model for multimodal MRI brain\u2011tumor segmentation with a dedicated focus on the non\u2011enhancing compartment, aiming to restore automatic delineation of this prognostically important region.", "motivation": "The non\u2011enhancing tumor (NET) region, though indicative of patient survival and a likely site of future growth, has been omitted from recent challenges (e.g., MICCAI). There is a clinical need for reliable automatic delineation of NET on MRI.", "method": "Design a U\u2011Net\u2013based deep learning architecture for brain\u2011tumor segmentation across MRI modalities, with special emphasis (e.g., targeted outputs/loss) on accurately segmenting the non\u2011enhancing compartment.", "result": "The abstract does not report quantitative metrics; it states the development of a U\u2011Net architecture to segment tumors with focus on the NET region.", "conclusion": "Automated segmentation of the NET compartment is essential for prognosis and planning; the proposed U\u2011Net approach addresses this unmet need, re\u2011introducing NET delineation into automated pipelines."}}
{"id": "2602.21933", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21933", "abs": "https://arxiv.org/abs/2602.21933", "authors": ["Bitan Majumder", "Anirban Sen"], "title": "Small Wins Big: Comparing Large Language Models and Domain Fine-Tuned Models for Sarcasm Detection in Code-Mixed Hinglish Text", "comment": null, "summary": "Sarcasm detection in multilingual and code-mixed environments remains a challenging task for natural language processing models due to structural variations, informal expressions, and low-resource linguistic availability. This study compares four large language models, Llama 3.1, Mistral, Gemma 3, and Phi-4, with a fine-tuned DistilBERT model for sarcasm detection in code-mixed Hinglish text. The results indicate that the smaller, sequentially fine-tuned DistilBERT model achieved the highest overall accuracy of 84%, outperforming all of the LLMs in zero and few-shot set ups, using minimal LLM generated code-mixed data used for fine-tuning. These findings indicate that domain-adaptive fine-tuning of smaller transformer based models may significantly improve sarcasm detection over general LLM inference, in low-resource and data scarce settings.", "AI": {"tldr": "Fine-tuned DistilBERT on minimal LLM-generated Hinglish data reaches 84% accuracy and outperforms Llama 3.1, Mistral, Gemma 3, and Phi-4 in zero/few-shot sarcasm detection, suggesting small domain-adapted transformers can beat general LLM inference in low-resource code-mixed settings.", "motivation": "Sarcasm detection in multilingual, code-mixed text (e.g., Hinglish) is difficult due to structural variation, informality, and scarce labeled resources. The study asks whether targeted fine-tuning of a smaller model can outperform prompting large general LLMs under data scarcity.", "method": "Head-to-head comparison: zero- and few-shot prompting of four LLMs versus a sequentially domain-adaptively fine-tuned DistilBERT trained using a small amount of LLM-generated code-mixed Hinglish data; evaluation via accuracy on a sarcasm detection task.", "result": "DistilBERT achieved the best overall accuracy (84%), surpassing all four LLMs in zero/few-shot settings despite using minimal synthetic data for fine-tuning.", "conclusion": "In low-resource, code-mixed scenarios, domain-adaptive fine-tuning of compact transformer models can deliver superior sarcasm detection to general-purpose LLM inference, offering a cost-effective, data-efficient strategy."}}
{"id": "2602.21864", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.21864", "abs": "https://arxiv.org/abs/2602.21864", "authors": ["Yanbin Wei", "Jiangyue Yan", "Chun Kang", "Yang Chen", "Hua Liu", "James Kwok", "Yu Zhang"], "title": "DynamicGTR: Leveraging Graph Topology Representation Preferences to Boost VLM Capabilities on Graph QAs", "comment": "CVPR 2026", "summary": "Vision-Language Models (VLMs) have emerged as versatile solutions for zero-shot question answering (QA) across various domains. However, enabling VLMs to effectively comprehend structured graphs and perform accurate, efficient QA remains challenging. Existing approaches typically rely on one single graph topology representation (GTR), such as fixed-style visual images or unified text descriptions. This ``one-size-fits-all'' strategy often neglects model-specific and task-specific preferences, resulting in inaccurate or over-lengthy responses to graph-related queries. To address this, we propose the $\\mbox{DynamicGTR}$ framework, which dynamically selects the optimal GTR for each query during inference, thereby enhancing the zero-shot graph QA capabilities of VLMs with a customizable accuracy and brevity trade-off. Extensive experiments show that DynamicGTR not only improves VLM-based graph algorithm QA performance but also successfully transfers the experience trained from synthetic graph algorithm tasks to real-world applications like link prediction and node classification, without any additional training. Additionally, DynamicGTR demonstrates strong transferability across tasks, domains, and models, suggesting its potential as a flexible solution for broad graph scenarios.", "AI": {"tldr": "DynamicGTR dynamically picks the best graph topology representation per query to let vision\u2013language models answer graph questions zero-shot with better accuracy and shorter responses, and it transfers from synthetic graph tasks to real-world ones without extra training.", "motivation": "VLMs struggle with structured graph understanding. Using a single, fixed graph representation (image or text) ignores model- and task-specific preferences, often causing inaccurate or overly long answers in graph QA.", "method": "Introduce a framework that, at inference time, selects among multiple graph topology representations (e.g., visual renderings, textual encodings) tailored to the query and model. The selection enables a controllable accuracy\u2013brevity trade-off and requires no additional training when moving from synthetic to real tasks.", "result": "Across benchmarks, DynamicGTR improves zero-shot graph algorithm QA and transfers the learned experience to link prediction and node classification without further training. It also generalizes across tasks, domains, and different VLMs.", "conclusion": "Choosing the representation dynamically is an effective, flexible way to enable VLMs to perform accurate and concise graph QA, with strong cross-task, cross-domain, and cross-model transfer, making it a promising general solution for graph-related applications."}}
{"id": "2602.22144", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22144", "abs": "https://arxiv.org/abs/2602.22144", "authors": ["Lingfeng Ren", "Weihao Yu", "Runpeng Yu", "Xinchao Wang"], "title": "NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors", "comment": "Code: https://github.com/lingfengren/NoLan", "summary": "Object hallucination is a critical issue in Large Vision-Language Models (LVLMs), where outputs include objects that do not appear in the input image. A natural question arises from this phenomenon: Which component of the LVLM pipeline primarily contributes to object hallucinations? The vision encoder to perceive visual information, or the language decoder to generate text responses? In this work, we strive to answer this question through designing a systematic experiment to analyze the roles of the vision encoder and the language decoder in hallucination generation. Our observations reveal that object hallucinations are predominantly associated with the strong priors from the language decoder. Based on this finding, we propose a simple and training-free framework, No-Language-Hallucination Decoding, NoLan, which refines the output distribution by dynamically suppressing language priors, modulated based on the output distribution difference between multimodal and text-only inputs. Experimental results demonstrate that NoLan effectively reduces object hallucinations across various LVLMs on different tasks. For instance, NoLan achieves substantial improvements on POPE, enhancing the accuracy of LLaVA-1.5 7B and Qwen-VL 7B by up to 6.45 and 7.21, respectively. The code is publicly available at: https://github.com/lingfengren/NoLan.", "AI": {"tldr": "Object hallucinations in LVLMs mainly arise from strong language-decoder priors. The authors introduce NoLan, a training-free decoding scheme that suppresses these priors by contrasting multimodal vs text-only output distributions, reducing hallucinations and boosting POPE accuracy (e.g., +6.45 for LLaVA-1.5 7B, +7.21 for Qwen-VL 7B).", "motivation": "Identify which LVLM component (vision encoder vs language decoder) primarily causes object hallucinations and find a simple, retraining-free way to mitigate them.", "method": "(1) Conduct attribution experiments to disentangle the roles of the vision encoder and the language decoder in hallucination generation. (2) Propose No-Language-Hallucination Decoding (NoLan): compute token distributions with image+prompt and with prompt-only; dynamically down-weight tokens overly supported by the language-only prior, refining the multimodal distribution at inference time without training or model modification.", "result": "NoLan consistently lowers object hallucinations across several LVLMs and tasks. On the POPE benchmark, it yields accuracy gains up to +6.45 (LLaVA-1.5 7B) and +7.21 (Qwen-VL 7B).", "conclusion": "Language priors in the decoder are the dominant source of object hallucinations. Suppressing them via a simple, training-free decoding adjustment (NoLan) is an effective, plug-and-play method for reducing hallucinations across models."}}
{"id": "2602.22207", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22207", "abs": "https://arxiv.org/abs/2602.22207", "authors": ["Hanna Yukhymenko", "Anton Alexandrov", "Martin Vechev"], "title": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets", "comment": null, "summary": "The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies, specifically Universal Self-Improvement (USI) and our proposed multi-round ranking method, T-RANK, allows for significantly higher quality outputs compared to traditional pipelines. Our framework ensures that benchmarks preserve their original task structure and linguistic nuances during localization. We apply this approach to translate popular benchmarks and datasets into eight Eastern and Southern European languages (Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, Greek). Evaluations using both reference-based metrics and LLM-as-a-judge show that our translations surpass existing resources, resulting in more accurate downstream model assessment. We release both the framework and the improved benchmarks to facilitate robust and reproducible multilingual AI development.", "AI": {"tldr": "They introduce an automated framework that uses test-time compute scaling (USI and a new multi-round ranking method, T-RANK) to generate higher-quality translations of evaluation datasets, preserving task structure and nuance across eight European languages, yielding more reliable multilingual LLM benchmarking and releasing both code and benchmarks.", "motivation": "Multilingual LLM evaluation is unreliable because many translated benchmarks suffer semantic drift and context loss, giving misleading performance metrics. There is a need for scalable, consistent, high-fidelity localization of evaluation datasets that preserves the original intent and difficulty.", "method": "A fully automated translation/localization pipeline that adapts test-time compute scaling via Universal Self-Improvement (USI) and introduces T-RANK, a multi-round ranking procedure to iteratively select better translations. The framework enforces preservation of task structure and linguistic nuances. It is applied to several popular benchmarks, translating them into Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, and Greek. Quality is assessed using reference-based metrics and LLM-as-a-judge.", "result": "The proposed approach produces translations that outperform existing multilingual benchmark resources on both automatic metrics and LLM-judge evaluations, and leads to more accurate downstream model assessments than prior translated datasets/pipelines.", "conclusion": "Test-time compute scaling (USI + T-RANK) is effective for high-quality, scalable benchmark translation that maintains semantic fidelity and structure, enabling more trustworthy multilingual evaluation. The authors release the framework and improved benchmarks to support robust, reproducible research."}}
{"id": "2602.21778", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21778", "abs": "https://arxiv.org/abs/2602.21778", "authors": ["Liangbing Zhao", "Le Zhuo", "Sayak Paul", "Hongsheng Li", "Mohamed Elhoseiny"], "title": "From Statics to Dynamics: Physics-Aware Image Editing with Latent Transition Priors", "comment": "All code, checkpoints, and datasets are available at https://liangbingzhao.github.io/statics2dynamics/", "summary": "Instruction-based image editing has achieved remarkable success in semantic alignment, yet state-of-the-art models frequently fail to render physically plausible results when editing involves complex causal dynamics, such as refraction or material deformation. We attribute this limitation to the dominant paradigm that treats editing as a discrete mapping between image pairs, which provides only boundary conditions and leaves transition dynamics underspecified. To address this, we reformulate physics-aware editing as predictive physical state transitions and introduce PhysicTran38K, a large-scale video-based dataset comprising 38K transition trajectories across five physical domains, constructed via a two-stage filtering and constraint-aware annotation pipeline. Building on this supervision, we propose PhysicEdit, an end-to-end framework equipped with a textual-visual dual-thinking mechanism. It combines a frozen Qwen2.5-VL for physically grounded reasoning with learnable transition queries that provide timestep-adaptive visual guidance to a diffusion backbone. Experiments show that PhysicEdit improves over Qwen-Image-Edit by 5.9% in physical realism and 10.1% in knowledge-grounded editing, setting a new state-of-the-art for open-source methods, while remaining competitive with leading proprietary models.", "AI": {"tldr": "They reframe instruction-based image editing as predicting physical state transitions rather than mapping between two static images, release a 38K-trajectory video dataset (PhysicTran38K), and introduce PhysicEdit, a dual-thinking diffusion framework that fuses frozen multimodal reasoning (Qwen2.5-VL) with learnable transition queries. It yields state-of-the-art open-source performance, notably +5.9% physical realism and +10.1% knowledge-grounded editing over Qwen-Image-Edit.", "motivation": "Current instruction-based image editors often fail on phenomena requiring causal, time-evolving physics (e.g., refraction, deformation) because they treat editing as a discrete image-to-image mapping, which only sets boundary conditions and under-specifies the transition dynamics. There is also a lack of supervision capturing physical evolutions to guide models toward physically plausible edits.", "method": "1) Dataset: PhysicTran38K\u201438K video trajectories across five physics domains, built via two-stage filtering and constraint-aware annotation to ensure physically consistent transitions. 2) Model: PhysicEdit\u2014an end-to-end diffusion-based editor with a textual-visual dual-thinking mechanism: a frozen Qwen2.5-VL provides physics-grounded reasoning, while learnable transition queries deliver timestep-adaptive visual guidance to the diffusion backbone.", "result": "PhysicEdit surpasses Qwen-Image-Edit by 5.9% in physical realism and 10.1% in knowledge-grounded editing, achieves a new state-of-the-art among open-source methods, and is competitive with leading proprietary systems.", "conclusion": "Modeling edits as predictive physical state transitions, supported by a large transition-trajectory dataset and a dual-thinking diffusion framework, substantially improves physical plausibility and knowledge grounding in image editing. The approach sets a new open-source benchmark and narrows the gap with proprietary models."}}
{"id": "2602.22182", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.22182", "abs": "https://arxiv.org/abs/2602.22182", "authors": ["Sourav Saha", "Dwaipayan Roy", "Mandar Mitra"], "title": "LiCQA : A Lightweight Complex Question Answering System", "comment": null, "summary": "Over the last twenty years, significant progress has been made in designing and implementing Question Answering (QA) systems. However, addressing complex questions, the answers to which are spread across multiple documents, remains a challenging problem. Recent QA systems that are designed to handle complex questions work either on the basis of knowledge graphs, or utilise contem- porary neural models that are expensive to train, in terms of both computational resources and the volume of training data required. In this paper, we present LiCQA, an unsupervised question answer- ing model that works primarily on the basis of corpus evidence. We empirically compare the effectiveness and efficiency of LiCQA with two recently presented QA systems, which are based on different underlying principles. The results of our experiments show that LiCQA significantly outperforms these two state-of-the-art systems on benchmark data with noteworthy reduction in latency.", "AI": {"tldr": "LiCQA is an unsupervised, corpus-evidence QA system for complex, multi-document questions that outperforms two recent state-of-the-art baselines on benchmarks while reducing latency, avoiding knowledge graphs and heavy neural training.", "motivation": "Complex questions often require aggregating evidence across multiple documents. Existing approaches either depend on knowledge graphs or data- and compute-intensive neural models, creating scalability and resource barriers. A lighter-weight alternative is needed.", "method": "Introduce LiCQA, an unsupervised question answering model that relies primarily on corpus evidence (without KG dependence or supervised neural training). The paper conducts an empirical comparison against two recent QA systems built on different principles, evaluating both effectiveness and efficiency.", "result": "Across benchmark datasets, LiCQA significantly outperforms the two state-of-the-art comparison systems and achieves notably lower latency.", "conclusion": "Unsupervised, corpus-based QA can be both effective and efficient for complex, multi-document questions. LiCQA provides a strong alternative to KG-based and heavy neural methods, setting improved performance and latency among evaluated systems."}}
{"id": "2602.21818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21818", "abs": "https://arxiv.org/abs/2602.21818", "authors": ["Guibin Chen", "Dixuan Lin", "Jiangping Yang", "Youqiang Zhang", "Zhengcong Fei", "Debang Li", "Sheng Chen", "Chaofeng Ao", "Nuo Pang", "Yiming Wang", "Yikun Dou", "Zheng Chen", "Mingyuan Fan", "Tuanhui Li", "Mingshan Chang", "Hao Zhang", "Xiaopeng Sun", "Jingtao Xu", "Yuqiang Xie", "Jiahua Wang", "Zhiheng Xu", "Weiming Xiong", "Yuzhe Jin", "Baoxuan Gu", "Binjie Mao", "Yunjie Yu", "Jujie He", "Yuhao Feng", "Shiwen Tu", "Chaojie Wang", "Rui Yan", "Wei Shen", "Jingchen Wu", "Peng Zhao", "Xuanyue Zhong", "Zhuangzhuang Liu", "Kaifei Wang", "Fuxiang Zhang", "Weikai Xu", "Wenyan Liu", "Binglu Zhang", "Yu Shen", "Tianhui Xiong", "Bin Peng", "Liang Zeng", "Xuchen Song", "Haoxiang Guo", "Peiyu Wang", "Max W. Y. Lam", "Chien-Hung Liu", "Yahui Zhou"], "title": "SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model", "comment": null, "summary": "SkyReels V4 is a unified multi modal video foundation model for joint video audio generation, inpainting, and editing. The model adopts a dual stream Multimodal Diffusion Transformer (MMDiT) architecture, where one branch synthesizes video and the other generates temporally aligned audio, while sharing a powerful text encoder based on the Multimodal Large Language Models (MMLM). SkyReels V4 accepts rich multi modal instructions, including text, images, video clips, masks, and audio references. By combining the MMLMs multi modal instruction following capability with in context learning in the video branch MMDiT, the model can inject fine grained visual guidance under complex conditioning, while the audio branch MMDiT simultaneously leverages audio references to guide sound generation. On the video side, we adopt a channel concatenation formulation that unifies a wide range of inpainting style tasks, such as image to video, video extension, and video editing under a single interface, and naturally extends to vision referenced inpainting and editing via multi modal prompts. SkyReels V4 supports up to 1080p resolution, 32 FPS, and 15 second duration, enabling high fidelity, multi shot, cinema level video generation with synchronized audio. To make such high resolution, long-duration generation computationally feasible, we introduce an efficiency strategy: Joint generation of low resolution full sequences and high-resolution keyframes, followed by dedicated super-resolution and frame interpolation models. To our knowledge, SkyReels V4 is the first video foundation model that simultaneously supports multi-modal input, joint video audio generation, and a unified treatment of generation, inpainting, and editing, while maintaining strong efficiency and quality at cinematic resolutions and durations.", "AI": {"tldr": "SkyReels V4 is a dual\u2011stream diffusion transformer that jointly generates and edits video and temporally aligned audio from rich multimodal prompts, unifying generation, inpainting, and editing at cinematic resolution and duration with an efficiency\u2011oriented pipeline.", "motivation": "Fragmented tools separately handle video generation, audio synthesis, and editing/inpainting, often lacking multimodal instruction following, temporal A/V alignment, or efficiency at high resolutions and durations. A unified, instruction\u2011following foundation model is needed to simplify workflows and scale quality/efficiency.", "method": "A dual\u2011branch Multimodal Diffusion Transformer (MMDiT) where one branch synthesizes video and the other generates synchronized audio. Both share a powerful text encoder built on Multimodal LLMs (MMLM) to follow complex multimodal instructions (text, images, video clips, masks, audio references). The video branch uses a channel\u2011concatenation formulation to unify inpainting\u2011style tasks (image\u2011to\u2011video, extension, editing) and supports vision\u2011referenced editing. Efficiency comes from jointly generating low\u2011res full sequences plus high\u2011res keyframes, then applying dedicated super\u2011resolution and frame\u2011interpolation models.", "result": "Supports up to 1080p, 32 FPS, and 15 s videos with synchronized audio, enabling high\u2011fidelity, multi\u2011shot, cinema\u2011level generation and editing under complex multimodal conditioning, while remaining computationally feasible.", "conclusion": "SkyReels V4 is positioned as the first video foundation model to simultaneously offer multimodal input, joint video\u2011audio generation, and a unified interface for generation/inpainting/editing, achieving strong quality and efficiency at cinematic settings."}}
{"id": "2602.21835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21835", "abs": "https://arxiv.org/abs/2602.21835", "authors": ["Jianhui Wei", "Xiaotian Zhang", "Yichen Li", "Yuan Wang", "Yan Zhang", "Ziyi Chen", "Zhihang Tang", "Wei Xu", "Zuozhu Liu"], "title": "UniVBench: Towards Unified Evaluation for Video Foundation Models", "comment": null, "summary": "Video foundation models aim to integrate video understanding, generation, editing, and instruction following within a single framework, making them a central direction for next-generation multimodal systems. However, existing evaluation benchmarks remain fragmented and limited in scope, as they each target a single task, rely on task-specific metrics, and typically use short or simple video clips. As a result, they do not capture the unified capabilities that these models are designed to deliver. To address this gap, we introduce UniVBench, a benchmark purpose-built for evaluating video foundation models across four core abilities: video understanding, video generation, video editing, and a newly proposed task, video reconstruction, which assesses how faithfully a model can reproduce video content it has encountered. Our benchmark substantially expands the complexity of evaluation by incorporating 200 high-quality, diverse and multi-shot videos, each paired with detailed captions, multi-format editing instructions, and reference images. All videos are human-created and carefully validated, offering richer cinematic information than prior benchmarks. In addition, we develop a unified agentic evaluation system (UniV-Eval) that standardizes prompting, instruction parsing, and scoring across all tasks, enabling fair, scalable, and reproducible comparisons of unified video models. By grounding evaluation in instruction-based multi-shot video tasks, UniVBench provides the first framework for measuring the integrated capabilities that video foundation models aim to achieve. Extensive human annotations ensure our evaluation aligns with human judgment, enabling rigorous assessment and accelerating progress toward robust video intelligence.", "AI": {"tldr": "UniVBench introduces a unified, instruction-driven benchmark and evaluation system (UniV-Eval) to assess video foundation models across understanding, generation, editing, and a new reconstruction task, using 200 diverse multi-shot human-created videos with standardized prompting, parsing, and scoring aligned to human judgment.", "motivation": "Existing video benchmarks are fragmented\u2014each targets a single task, uses task-specific metrics, and relies on short/simple clips\u2014so they fail to assess the unified capabilities modern video foundation models are meant to deliver.", "method": "Curate 200 high-quality, diverse, human-created multi-shot videos paired with detailed captions, multi-format editing instructions, and reference images. Define four evaluation tracks: video understanding, generation, editing, and a new reconstruction task. Build a unified agentic evaluation pipeline (UniV-Eval) to standardize prompting, instruction parsing, and scoring across all tasks, and validate alignment with extensive human annotations.", "result": "Provides the first integrated benchmark and evaluation framework for unified video models, increasing evaluation complexity and cinematic richness while enabling fair, scalable, and reproducible comparisons. Human studies indicate the evaluation aligns with human judgment.", "conclusion": "By grounding evaluation in instruction-based multi-shot scenarios and standardizing assessment via UniV-Eval, UniVBench fills a key gap in measuring integrated capabilities of video foundation models, supporting rigorous, human-aligned progress toward robust video intelligence."}}
{"id": "2602.22208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22208", "abs": "https://arxiv.org/abs/2602.22208", "authors": ["Georgy Savva", "Oscar Michel", "Daohan Lu", "Suppakit Waiwitlikhit", "Timothy Meehan", "Dhairya Mishra", "Srivats Poddar", "Jack Lu", "Saining Xie"], "title": "Solaris: Building a Multiplayer Video World Model in Minecraft", "comment": "Project website: https://solaris-wm.github.io/", "summary": "Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models.", "AI": {"tldr": "Solaris is a multi-agent, multi-view video world model trained on synchronized multiplayer game data (e.g., Minecraft) with a staged training pipeline and a memory\u2011efficient Self Forcing variant, achieving state\u2011of\u2011the\u2011art results over baselines.", "motivation": "Current action\u2011conditioned video generators are single\u2011agent and fail to capture coordinated multi\u2011agent interactions and view consistency seen in real environments. There is also a lack of scalable, automated, synchronized data collection for multiplayer settings and standardized evaluation for multi-agent capabilities.", "method": "1) Build a multiplayer data system that supports coordinated multi\u2011agent interaction and synchronized video+action logging; 2) collect 12.64M multiplayer frames; 3) propose an evaluation framework targeting multiplayer movement, memory, grounding, building, and view consistency; 4) train Solaris via a staged curriculum transitioning from single\u2011player to multiplayer using a mix of bidirectional, causal, and Self Forcing training; 5) introduce Checkpointed Self Forcing to extend teacher horizon with lower memory cost.", "result": "Solaris trained on the new dataset outperforms existing baselines across the proposed multiplayer evaluation dimensions. The Checkpointed Self Forcing and staged training improve long\u2011horizon consistency and multi-view coherence.", "conclusion": "A scalable data pipeline plus a staged, memory\u2011efficient training strategy enables consistent multi-view, multi-agent video world modeling. Open-sourcing is intended to catalyze research on multi-agent world models and standardized evaluation."}}
