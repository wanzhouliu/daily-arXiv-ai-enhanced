{"id": "2510.06229", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06229", "abs": "https://arxiv.org/abs/2510.06229", "authors": ["Josh Hunter", "John McDermid", "Simon Burton", "Poppy Fynes", "Mia Dempster"], "title": "Milestone Determination for Autonomous Railway Operation", "comment": "Paper submitted and partially accepted to ICART 2025, paper is 8\n  pages and has 1 figure, 2 tables", "summary": "In the field of railway automation, one of the key challenges has been the\ndevelopment of effective computer vision systems due to the limited\navailability of high-quality, sequential data. Traditional datasets are\nrestricted in scope, lacking the spatio temporal context necessary for\nreal-time decision-making, while alternative solutions introduce issues related\nto realism and applicability. By focusing on route-specific, contextually\nrelevant cues, we can generate rich, sequential datasets that align more\nclosely with real-world operational logic. The concept of milestone\ndetermination allows for the development of targeted, rule-based models that\nsimplify the learning process by eliminating the need for generalized\nrecognition of dynamic components, focusing instead on the critical decision\npoints along a route. We argue that this approach provides a practical\nframework for training vision agents in controlled, predictable environments,\nfacilitating safer and more efficient machine learning systems for railway\nautomation.", "AI": {"tldr": "Proposes a route-specific, milestone-based framework to build sequential, context-rich datasets and targeted rule-based vision models for railway automation, simplifying perception at critical decision points to improve safety and efficiency.", "motivation": "Railway vision systems lack high-quality, spatiotemporal, sequential data. Existing datasets miss real-time context, and synthetic/alternative sources suffer from realism and applicability gaps, hindering reliable automation.", "method": "Focus on route-specific, contextually relevant cues to generate rich sequential datasets aligned with operational logic. Introduce milestone determination to identify critical decision points and train targeted, rule-based models that avoid broad, generalized recognition of dynamic components.", "result": "Conceptual demonstration/argument that milestone-centric, route-aware datasets and rule-based models reduce learning complexity, better reflect operational realities, and are poised to enable safer, more efficient decision-making compared to generic approaches (no quantitative results reported).", "conclusion": "A practical framework for training vision agents in controlled, predictable railway environments: prioritize milestone determination along routes to reduce perception scope, enhance realism, and facilitate safer, more efficient machine learning\u2014pending empirical validation."}}
{"id": "2510.06231", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06231", "abs": "https://arxiv.org/abs/2510.06231", "authors": ["Mingzhe Zheng", "Dingjie Song", "Guanyu Zhou", "Jun You", "Jiahao Zhan", "Xuran Ma", "Xinyuan Song", "Ser-Nam Lim", "Qifeng Chen", "Harry Yang"], "title": "CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation", "comment": "24 pages, 9 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating highly structured texts. However, while exhibiting a high degree of\nstructural organization, movie scripts demand an additional layer of nuanced\nstorytelling and emotional depth-the 'soul' of compelling cinema-that LLMs\noften fail to capture. To investigate this deficiency, we first curated\nCML-Dataset, a dataset comprising (summary, content) pairs for Cinematic Markup\nLanguage (CML), where 'content' consists of segments from esteemed,\nhigh-quality movie scripts and 'summary' is a concise description of the\ncontent. Through an in-depth analysis of the intrinsic multi-shot continuity\nand narrative structures within these authentic scripts, we identified three\npivotal dimensions for quality assessment: Dialogue Coherence (DC), Character\nConsistency (CC), and Plot Reasonableness (PR). Informed by these findings, we\npropose the CML-Bench, featuring quantitative metrics across these dimensions.\nCML-Bench effectively assigns high scores to well-crafted, human-written\nscripts while concurrently pinpointing the weaknesses in screenplays generated\nby LLMs. To further validate our benchmark, we introduce CML-Instruction, a\nprompting strategy with detailed instructions on character dialogue and event\nlogic, to guide LLMs to generate more structured and cinematically sound\nscripts. Extensive experiments validate the effectiveness of our benchmark and\ndemonstrate that LLMs guided by CML-Instruction generate higher-quality\nscreenplays, with results aligned with human preferences.", "AI": {"tldr": "They build a domain-specific dataset and benchmark (CML-Dataset/CML-Bench) to evaluate LLM-written movie scripts on dialogue coherence, character consistency, and plot reasonableness, and propose an instruction strategy (CML-Instruction) that measurably improves script quality and aligns with human preferences.", "motivation": "LLMs can produce structurally formatted scripts but lack nuanced storytelling\u2014the continuity, consistent characterization, and logical plot progression needed for compelling cinema. The field lacks targeted metrics and guidance to both assess and improve these aspects.", "method": "1) Curate CML-Dataset of (summary, content) pairs in Cinematic Markup Language from high-quality scripts. 2) Analyze multi-shot continuity and narrative structure to define three evaluation dimensions: Dialogue Coherence (DC), Character Consistency (CC), Plot Reasonableness (PR). 3) Propose CML-Bench with quantitative metrics for these dimensions. 4) Introduce CML-Instruction, a detailed prompting strategy focusing on character dialogue and event logic to guide LLM generation. 5) Conduct experiments comparing human scripts, baseline LLM outputs, and LLMs guided by CML-Instruction, with human preference checks.", "result": "CML-Bench assigns high scores to human-written scripts and highlights weaknesses in LLM-generated ones. Using CML-Instruction, LLMs produce more structured, cinematically sound scripts, with improvements reflected in benchmark metrics and aligning with human judgments.", "conclusion": "A focused benchmark (CML-Bench) effectively quantifies critical cinematic storytelling qualities, and targeted prompting (CML-Instruction) materially improves LLM screenplay generation. This provides a foundation for evaluating and steering LLMs toward higher-quality narrative outputs."}}
{"id": "2510.06233", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06233", "abs": "https://arxiv.org/abs/2510.06233", "authors": ["Haoyang Zhang", "Zhou Yang", "Yucai Pang"], "title": "User to Video: A Model for Spammer Detection Inspired by Video Classification Technology", "comment": "Accepted by International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "This article is inspired by video classification technology. If the user\nbehavior subspace is viewed as a frame image, consecutive frame images are\nviewed as a video. Following this novel idea, a model for spammer detection\nbased on user videoization, called UVSD, is proposed. Firstly, a user2piexl\nalgorithm for user pixelization is proposed. Considering the adversarial\nbehavior of user stances, the user is viewed as a pixel, and the stance is\nquantified as the pixel's RGB. Secondly, a behavior2image algorithm is proposed\nfor transforming user behavior subspace into frame images. Low-rank dense\nvectorization of subspace user relations is performed using representation\nlearning, while cutting and diffusion algorithms are introduced to complete the\nframe imageization. Finally, user behavior videos are constructed based on\ntemporal features. Subsequently, a video classification algorithm is combined\nto identify the spammers. Experiments using publicly available datasets, i.e.,\nWEIBO and TWITTER, show an advantage of the UVSD model over state-of-the-art\nmethods.", "AI": {"tldr": "UVSD reframes user behavior as a sequence of images (a \u201cvideo\u201d)\u2014users become pixels, stances map to RGB, behavior subspaces become frames\u2014and applies video classification to detect spammers, outperforming prior methods on WEIBO and TWITTER.", "motivation": "Conventional spammer detectors struggle to capture evolving, adversarial user behaviors and rich temporal dependencies. By leveraging video models\u2019 strength in spatiotemporal pattern learning, the authors aim to create a representation that preserves temporal and relational dynamics for robust spam detection.", "method": "Pipeline: (1) user2pixel maps each user to a pixel, encoding stance as RGB to reflect adversarial behavior. (2) behavior2image turns the user behavior subspace into frame images via representation learning for low\u2011rank dense embeddings of user relations, followed by cutting and diffusion to complete imageization. (3) Temporal stacking builds per\u2011user behavior videos. (4) A video classification model is trained to label spammers vs. non\u2011spammers.", "result": "On public WEIBO and TWITTER datasets, UVSD outperforms state\u2011of\u2011the\u2011art baselines (exact metrics not provided in the abstract).", "conclusion": "Transforming behavioral data into a video domain and leveraging video classifiers provides an effective approach for spammer detection, suggesting that spatiotemporal representations can improve robustness to adversarial behavior and capture dynamics better than prior methods."}}
{"id": "2510.06238", "categories": ["cs.CV", "cs.AI", "cs.LG", "stat.OT"], "pdf": "https://arxiv.org/pdf/2510.06238", "abs": "https://arxiv.org/abs/2510.06238", "authors": ["Sagar Lekhak", "Emmett J. Ientilucci", "Dimah Dera", "Susmita Ghosh"], "title": "Uncertainty Quantification In Surface Landmines and UXO Classification Using MC Dropout", "comment": "This work has been accepted and presented at IGARSS 2025 and will\n  appear in the IEEE IGARSS 2025 proceedings", "summary": "Detecting surface landmines and unexploded ordnances (UXOs) using deep\nlearning has shown promise in humanitarian demining. However, deterministic\nneural networks can be vulnerable to noisy conditions and adversarial attacks,\nleading to missed detection or misclassification. This study introduces the\nidea of uncertainty quantification through Monte Carlo (MC) Dropout, integrated\ninto a fine-tuned ResNet-50 architecture for surface landmine and UXO\nclassification, which was tested on a simulated dataset. Integrating the MC\nDropout approach helps quantify epistemic uncertainty, providing an additional\nmetric for prediction reliability, which could be helpful to make more informed\ndecisions in demining operations. Experimental results on clean, adversarially\nperturbed, and noisy test images demonstrate the model's ability to flag\nunreliable predictions under challenging conditions. This proof-of-concept\nstudy highlights the need for uncertainty quantification in demining, raises\nawareness about the vulnerability of existing neural networks in demining to\nadversarial threats, and emphasizes the importance of developing more robust\nand reliable models for practical applications.", "AI": {"tldr": "Adds Monte Carlo Dropout to a fine-tuned ResNet-50 to quantify epistemic uncertainty in classifying simulated images of surface landmines/UXOs, enabling the system to flag unreliable predictions, especially under noise and adversarial perturbations.", "motivation": "Deterministic neural networks used for demining can be brittle in the presence of noise and adversarial attacks, risking missed detections or misclassifications. Reliable, uncertainty-aware predictions are needed to support safe decision-making in the field.", "method": "Integrate MC Dropout into a fine-tuned ResNet-50 and perform multiple stochastic forward passes at inference to estimate epistemic uncertainty. Evaluate on simulated datasets with clean, noisy, and adversarially perturbed images; use uncertainty as an auxiliary reliability metric to identify questionable predictions.", "result": "The MC Dropout-enhanced model successfully flags higher uncertainty on challenging (noisy/adversarial) inputs while maintaining reliable confidence on clean data, demonstrating practical usefulness of uncertainty estimates. Specific performance metrics are not reported in the abstract.", "conclusion": "Uncertainty quantification is valuable for demining models, both to expose vulnerabilities of current deterministic approaches and to guide more robust, safety-critical deployment. The study is a proof of concept and calls for developing stronger, uncertainty-aware models for real-world use."}}
{"id": "2510.06261", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06261", "abs": "https://arxiv.org/abs/2510.06261", "authors": ["Zhanke Zhou", "Chentao Cao", "Xiao Feng", "Xuan Li", "Zongze Li", "Xiangyu Lu", "Jiangchao Yao", "Weikai Huang", "Linrui Xu", "Tian Cheng", "Guanyu Jiang", "Yiming Zheng", "Brando Miranda", "Tongliang Liu", "Sanmi Koyejo", "Masashi Sugiyama", "Bo Han"], "title": "AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning", "comment": "Ongoing project", "summary": "We present AlphaApollo, a self-evolving agentic reasoning system that aims to\naddress two bottlenecks in foundation model (FM) reasoning-limited\nmodel-intrinsic capacity and unreliable test-time iteration. AlphaApollo\norchestrates multiple models with professional tools to enable deliberate,\nverifiable reasoning. It couples (i) a computation tool (Python with numerical\nand symbolic libraries) and (ii) a retrieval tool (task-relevant external\ninformation) to execute exact calculations and ground decisions. The system\nfurther supports multi-round, multi-model solution evolution via a shared state\nmap that records candidates, executable checks, and feedback for iterative\nrefinement. In evaluations on AIME 2024/2025 across multiple models,\nAlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32\nfor Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for\nLlama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool\ncalls are successfully executed, with consistent outperformance of non-tool\nbaselines, thereby lifting the capability ceiling of FMs. More empirical\nresults and implementation details will be updated at\nhttps://github.com/tmlr-group/AlphaApollo.", "AI": {"tldr": "AlphaApollo is a multi-model, tool-augmented, self-evolving reasoning agent that integrates Python computation and external retrieval with an iterative, verifiable workflow, yielding sizable accuracy gains on AIME 2024/2025 versus non-tool baselines.", "motivation": "Foundation models struggle with complex reasoning due to limited intrinsic capacity and unstable test-time iteration. The paper aims to make reasoning more deliberate, grounded, and checkable by augmenting models with tools and structured iteration.", "method": "Orchestrate multiple LLMs with two core tools: (i) a computation tool (Python with numerical and symbolic libraries) for exact calculations; (ii) a retrieval tool for task-relevant external information. Maintain a shared state map that records candidate solutions, executable checks, and feedback, enabling multi-round, multi-model refinement and verification.", "result": "On AIME 2024/2025: Qwen2.5-14B-Instruct improves by +5.15% Average@32 and +23.34% Pass@32; Llama-3.3-70B-Instruct by +8.91% Average@32 and +26.67% Pass@32. Over 80% of tool calls execute successfully, and tool-augmented runs consistently outperform non-tool baselines.", "conclusion": "Coupling LLM orchestration with computation and retrieval tools plus an iterative, verifiable state-sharing scheme improves reliability and lifts the reasoning ceiling of FMs. The approach demonstrates consistent gains on math-style benchmarks and suggests broader benefits for grounded, checkable reasoning; code/details to be released on GitHub."}}
{"id": "2510.06239", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06239", "abs": "https://arxiv.org/abs/2510.06239", "authors": ["Pranav Gupta"], "title": "OpenStaxQA: A multilingual dataset based on open-source college textbooks", "comment": null, "summary": "We present OpenStaxQA, an evaluation benchmark specific to college-level\neducational applications based on 43 open-source college textbooks in English,\nSpanish, and Polish, available under a permissive Creative Commons license. We\nfinetune and evaluate large language models (LLMs) with approximately 7 billion\nparameters on this dataset using quantized low rank adapters (QLoRa).\nAdditionally we also perform a zero-shot evaluation on the AI2 reasoning\nchallenge dev dataset in order to check if OpenStaxQA can lead to an improved\nperformance on other tasks. We also discuss broader impacts relevant to\ndatasets such as OpenStaxQA.", "AI": {"tldr": "OpenStaxQA introduces a multilingual, college-level QA benchmark built from 43 open-licensed textbooks; the authors fine-tune ~7B-parameter LLMs with QLoRA, evaluate on this benchmark, and probe zero-shot transfer on ARC-Challenge, while discussing broader impacts.", "motivation": "Educational LLMs lack domain-specific, permissively licensed, multilingual benchmarks at the college level. The authors seek to enable rigorous evaluation and fine-tuning for classroom-relevant tasks and to test whether such training data yields transferable reasoning gains on external benchmarks.", "method": "Construct a QA benchmark (OpenStaxQA) from 43 OpenStax textbooks in English, Spanish, and Polish. Fine-tune approximately 7B-parameter LLMs using quantized LoRA (QLoRA) and evaluate on OpenStaxQA. Assess zero-shot transfer by evaluating on the AI2 ARC-Challenge dev set. Include a discussion of broader societal and educational impacts.", "result": "They release the OpenStaxQA dataset and report fine-tuning/evaluation of ~7B LLMs with QLoRA on it, plus zero-shot results on ARC-Challenge dev to gauge transfer. The abstract does not provide quantitative results; it suggests empirical evaluation but omits specific metrics.", "conclusion": "OpenStaxQA offers a permissively licensed, multilingual benchmark tailored to college-level educational applications and suitable for training/evaluating compact LLMs. The work indicates potential for transfer to broader reasoning tasks and underscores ethical and societal considerations for such datasets."}}
{"id": "2510.06241", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2510.06241", "abs": "https://arxiv.org/abs/2510.06241", "authors": ["Anselm W. Stark", "Marc Ilic", "Ali Mokhtari", "Pooya Mohammadi Kazaj", "Christoph Graeni", "Isaac Shiri"], "title": "multimodars: A Rust-powered toolkit for multi-modality cardiac image fusion and registration", "comment": null, "summary": "Combining complementary imaging modalities is critical to build reliable 3D\ncoronary models: intravascular imaging gives sub-millimetre resolution but\nlimited whole-vessel context, while CCTA supplies 3D geometry but suffers from\nlimited spatial resolution and artefacts (e.g., blooming). Prior work\ndemonstrated intravascular/CCTA fusion, yet no open, flexible toolkit is\ntailored for multi-state analysis (rest/stress, pre-/post-stenting) while\noffering deterministic behaviour, high performance, and easy pipeline\nintegration. multimodars addresses this gap with deterministic alignment\nalgorithms, a compact NumPy-centred data model, and an optimised Rust backend\nsuitable for scalable, reproducible experiments. The package accepts CSV/NumPy\ninputs including data formats produced by the AIVUS-CAA software", "AI": {"tldr": "multimodars is an open, deterministic, high\u2011performance toolkit to fuse intravascular imaging with CCTA for multi\u2011state coronary 3D modeling, built on a NumPy\u2011centric data model with an optimized Rust backend and support for CSV/NumPy (incl. AIVUS\u2011CAA) inputs.", "motivation": "High\u2011fidelity coronary models require combining complementary modalities: intravascular imaging offers sub\u2011millimeter detail but poor global context, while CCTA provides full\u2011vessel geometry with lower resolution and artefacts. Existing fusion efforts lack an open, flexible, deterministic, and easily integrable toolkit that supports multi\u2011state (rest/stress, pre/post\u2011stent) analysis at scale.", "method": "Provide deterministic alignment algorithms within a compact NumPy\u2011centered data model, backed by an optimized Rust core for speed and reproducibility. Accept standard CSV/NumPy inputs, including formats from AIVUS\u2011CAA, to ease pipeline integration and enable scalable experiments.", "result": "Introduces a software package that operationalizes intravascular/CCTA fusion with deterministic behavior and high performance, suitable for multi\u2011state analyses and reproducible large\u2011scale experiments. No quantitative benchmarks are reported in the abstract.", "conclusion": "The toolkit fills a practical gap by enabling reproducible, scalable, and integrable multimodal coronary reconstruction workflows across multiple physiological or interventional states, potentially standardizing and accelerating research and pipeline deployment."}}
{"id": "2510.06274", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06274", "abs": "https://arxiv.org/abs/2510.06274", "authors": ["Mohammad Mahdi Samiei Paqaleh", "Arash Marioriyad", "Arman Tahmasebi-Zadeh", "Mohamadreza Fereydooni", "Mahdi Ghaznavai", "Mahdieh Soleymani Baghshah"], "title": "Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization", "comment": null, "summary": "Recent progress has pushed AI frontiers from pattern recognition tasks toward\nproblems that require step by step, System2 style reasoning, especially with\nlarge language models. Yet, unlike learning, where generalization and out of\ndistribution (OoD) evaluation concepts are well formalized, there is no clear,\nconsistent definition or metric for reasoning ability. We propose Complexity\nOut of Distribution (Complexity OoD) generalization as a framework and problem\nsetting to define and measure reasoning. A model exhibits Complexity OoD\ngeneralization when it maintains performance on test instances whose minimal\nrequired solution complexity, either representational (richer solution\nstructure) or computational (more reasoning steps/program length), exceeds that\nof all training examples. We formalize complexity via solution description\nKolmogorov complexity and operational proxies (e.g., object/relation counts;\nreasoning step counts), clarifying how Complexity OoD differs from length and\ncompositional OoD. This lens unifies learning and reasoning: many cases\nsolvable with System1 like processing at low complexity become System2 like\nunder complexity pressure, while System2 can be viewed as generalization over\nsolution structures. We translate this perspective into practice with\nrecommendations for operationalizing Complexity OoD across the stack:\nincorporating complexity into benchmark and evaluation metric design,\nrethinking supervision to target solution traces, seeking and designing\ninductive biases for Complexity OoD generalization, addressing learning to\nreason spillovers such as spurious shortcuts, semantic robustness, catastrophic\nforgetting, and step wise calibration. Because Complexity OoD cannot be solved\nby scaling data alone, progress toward robust reasoning will require\narchitectures and training regimes that explicitly model and allocate\ncomputation with respect to complexity.", "AI": {"tldr": "Defines Complexity Out-of-Distribution (Complexity OoD) generalization as a concrete way to measure \u201creasoning\u201d: a model should maintain performance when test items require higher minimal solution complexity than any training example. Formalizes complexity and offers practical guidance for benchmarks, supervision, inductive biases, and training; argues that scaling data alone won\u2019t yield robust reasoning\u2014models must allocate computation with respect to complexity.", "motivation": "Reasoning with modern LLMs lacks a clear, consistent definition or metric, unlike generalization in learning. Existing OoD notions (length, compositionality) miss cases where difficulty stems from higher minimal solution complexity. The authors seek a unifying lens that connects System 1 and System 2 behavior and makes reasoning measurable and actionable.", "method": "Conceptual/theoretical framework: define reasoning as Complexity OoD generalization. Formalize solution complexity via description (Kolmogorov) complexity and operational proxies (e.g., counts of objects/relations; number of reasoning steps/program length). Differentiate Complexity OoD from length/compositional OoD. Translate to practice with recommendations: design benchmarks/metrics that control and report complexity; supervise solution traces; build inductive biases for Complexity OoD; mitigate spillovers like spurious shortcuts, semantic robustness issues, catastrophic forgetting, and step-wise calibration problems.", "result": "Provides a precise definition and measurement paradigm for reasoning grounded in solution complexity, unifying low-complexity (System 1) and high-complexity (System 2) regimes. Clarifies distinctions from other OoD notions and enumerates actionable evaluation/training practices. No new empirical results are reported; contributions are conceptual and prescriptive.", "conclusion": "Robust reasoning equates to maintaining performance under higher solution complexity than seen in training. Achieving it requires architectures and training regimes that explicitly model and allocate computation according to complexity; simply scaling data is insufficient. The community should adopt complexity-aware benchmarks, supervision, and inductive biases to drive progress."}}
{"id": "2510.06240", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.06240", "abs": "https://arxiv.org/abs/2510.06240", "authors": ["Jiqun Pan", "Zhenke Duan", "Jiani Tu", "Anzhi Cheng", "Yanqing Wang"], "title": "Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets", "comment": "41 pages, 12 figures, 6 tables", "summary": "Industrial question-answering (QA) systems require higher safety and\nreliability than general-purpose dialogue models, as errors in high-risk\nscenarios such as equipment fault diagnosis can have severe consequences.\nAlthough multi-agent large language models enhance reasoning depth, they suffer\nfrom uncontrolled iterations and unverifiable outputs, and conventional\ndistillation methods struggle to transfer collaborative reasoning capabilities\nto lightweight, deployable student models. To address these challenges, we\npropose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our\napproach formulates distillation as a Markov Decision Process and incorporates\na knowledge graph as a verifiable structured prior to enrich state\nrepresentation and ensure convergence. By integrating collaborative reasoning\nwith knowledge grounding, KG-MASD generates high-confidence instruction-tuning\ndata and jointly distills reasoning depth and verifiability into compact\nstudent models suitable for edge deployment. Experiments on an industrial QA\ndataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent\nover baselines and significantly enhances reliability, enabling trustworthy AI\ndeployment in safety-critical industrial scenarios. Code and data are available\nat https://github.com/erwinmsmith/KG-MAD/.", "AI": {"tldr": "KG-MASD distills multi-agent LLM reasoning into compact, edge-deployable QA models by grounding collaboration in a knowledge graph and casting distillation as an MDP, yielding higher accuracy (2.4\u201320.1% over baselines) and more verifiable outputs for industrial use.", "motivation": "Industrial QA in safety-critical settings needs reliable, auditable reasoning. Multi-agent LLMs can reason deeply but are iterative, costly, and produce unverifiable outputs; standard distillation fails to transfer their collaborative reasoning to small models. A structured, verifiable prior is needed to make distilled reasoning dependable.", "method": "Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD): formulate the distillation process as a Markov Decision Process; inject a knowledge graph as a structured, verifiable prior to enrich state representations and stabilize/ensure convergence; integrate collaborative multi-agent reasoning with knowledge grounding to generate high-confidence instruction-tuning data; jointly distill both reasoning depth and verifiability into a compact student model suitable for edge deployment.", "result": "On an industrial QA dataset, KG-MASD improves accuracy by 2.4%\u201320.1% over baselines and shows markedly better reliability/verifiability; produces high-confidence instruction data. Code/data are released (GitHub).", "conclusion": "Grounding multi-agent collaboration with a knowledge graph and an MDP framing enables distillation that preserves depth and verifiability, producing trustworthy, deployable industrial QA models. The approach appears effective and practical for safety-critical scenarios."}}
{"id": "2510.06251", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06251", "abs": "https://arxiv.org/abs/2510.06251", "authors": ["Ieva Bagdonaviciute", "Vibhav Vineet"], "title": "Does Physics Knowledge Emerge in Frontier Models?", "comment": "8 pages, 7 figures. Preprint", "summary": "Leading Vision-Language Models (VLMs) show strong results in visual\nperception and general reasoning, but their ability to understand and predict\nphysical dynamics remains unclear. We benchmark six frontier VLMs on three\nphysical simulation datasets - CLEVRER, Physion, and Physion++ - where the\nevaluation tasks test whether a model can predict outcomes or hypothesize about\nalternative situations. To probe deeper, we design diagnostic subtests that\nisolate perception (objects, colors, occluders) from physics reasoning (motion\nprediction, spatial relations). Intuitively, stronger diagnostic performance\nshould support higher evaluation accuracy. Yet our analysis reveals weak\ncorrelations: models that excel at perception or physics reasoning do not\nconsistently perform better on predictive or counterfactual evaluation. This\ncounterintuitive gap exposes a central limitation of current VLMs: perceptual\nand physics skills remain fragmented and fail to combine into causal\nunderstanding, underscoring the need for architectures that bind perception and\nreasoning more tightly.", "AI": {"tldr": "They benchmark six top vision-language models on physical reasoning tasks (CLEVRER, Physion, Physion++) and find that good perception or physics subskills don\u2019t translate into accurate predictions/counterfactuals, revealing a gap in causal understanding.", "motivation": "Assess whether current VLMs truly understand and can predict physical dynamics, not just recognize visual elements or perform generic reasoning; identify why strong perceptual or isolated physics skills may fail on causal prediction tasks.", "method": "Evaluate six frontier VLMs on predictive and counterfactual tasks across three physical simulation datasets. Create diagnostic subtests that separately measure perception (objects, colors, occlusion) and physics reasoning (motion prediction, spatial relations). Analyze correlations between diagnostic scores and end-task performance.", "result": "Weak correlations between diagnostic subskills and evaluation accuracy: models that excel at perception or isolated physics reasoning do not consistently perform better on predictive/counterfactual tasks.", "conclusion": "Current VLMs exhibit fragmented perceptual and physics abilities that do not integrate into causal understanding. Progress likely requires architectures that tightly bind perception with physics reasoning/causal modeling."}}
{"id": "2510.06288", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06288", "abs": "https://arxiv.org/abs/2510.06288", "authors": ["Raj Ghugare", "Catherine Ji", "Kathryn Wantlin", "Jin Schofield", "Benjamin Eysenbach"], "title": "BuilderBench -- A benchmark for generalist agents", "comment": "Project page: https://rajghugare19.github.io/builderbench and Code:\n  https://github.com/rajghugare19/builderbench", "summary": "Today's AI models learn primarily through mimicry and sharpening, so it is\nnot surprising that they struggle to solve problems beyond the limits set by\nexisting data. To solve novel problems, agents should acquire skills for\nexploring and learning through experience. Finding a scalable learning\nmechanism for developing agents that learn through interaction remains a major\nopen problem. In this work, we introduce BuilderBench, a benchmark to\naccelerate research into agent pre-training that centers open-ended\nexploration. BuilderBench requires agents to learn how to build any structure\nusing blocks. BuilderBench is equipped with $(1)$ a hardware accelerated\nsimulator of a robotic agent interacting with various physical blocks, and\n$(2)$ a task-suite with over 42 diverse target structures that are carefully\ncurated to test an understanding of physics, mathematics, and long-horizon\nplanning. During training, agents have to explore and learn general principles\nabout the environment without any external supervision. During evaluation,\nagents have to build the unseen target structures from the task suite. Solving\nthese tasks requires a sort of \\emph{embodied reasoning} that is not reflected\nin words but rather in actions, experimenting with different strategies and\npiecing them together. Our experiments show that many of these tasks challenge\nthe current iteration of algorithms. Hence, we also provide a ``training\nwheels'' protocol, in which agents are trained and evaluated to build a single\ntarget structure from the task suite. Finally, we provide single-file\nimplementations of six different algorithms as a reference point for\nresearchers.", "AI": {"tldr": "BuilderBench is a benchmark for pre-training interactive agents via open-ended exploration to build diverse block structures, featuring a fast physics simulator, 42+ target structures, a simplified \u201ctraining-wheels\u201d setting, and six baseline implementations; current algorithms struggle, underscoring the need for scalable embodied-learning methods.", "motivation": "Prevailing AI models learn mainly by imitation and post-hoc sharpening, limiting generalization to novel problems. The authors aim to catalyze scalable pre-training for agents that learn through interaction\u2014developing exploration skills and embodied reasoning not captured by language supervision.", "method": "Introduce BuilderBench: (1) a hardware-accelerated simulator of a robotic agent manipulating physical blocks; (2) a curated suite of 42+ target structures stressing physics understanding, mathematics, and long-horizon planning. Agents train via unsupervised exploration to acquire general principles, then are evaluated on building unseen targets. A \u201ctraining wheels\u201d protocol trains/evaluates on a single target. Single-file implementations of six algorithms are provided as baselines.", "result": "Empirically, many tasks remain unsolved or difficult for current algorithms, indicating substantial gaps in exploration, planning, and embodied reasoning. The training-wheels variant eases the problem by focusing on a single target. Baselines are provided for reference rather than state-of-the-art performance.", "conclusion": "BuilderBench offers a testbed to study and scale agent pre-training through open-ended interaction, highlighting the limitations of current methods and providing resources and protocols to spur progress toward embodied reasoning and long-horizon competence."}}
{"id": "2510.06242", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06242", "abs": "https://arxiv.org/abs/2510.06242", "authors": ["Subin An", "Yugyeong Ji", "Junyoung Kim", "Heejin Kook", "Yang Lu", "Josh Seltzer"], "title": "Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses", "comment": "EMNLP Industry Track", "summary": "Open-ended survey responses provide valuable insights in marketing research,\nbut low-quality responses not only burden researchers with manual filtering but\nalso risk leading to misleading conclusions, underscoring the need for\neffective evaluation. Existing automatic evaluation methods target\nLLM-generated text and inadequately assess human-written responses with their\ndistinct characteristics. To address such characteristics, we propose a\ntwo-stage evaluation framework specifically designed for human survey\nresponses. First, gibberish filtering removes nonsensical responses. Then,\nthree dimensions-effort, relevance, and completeness-are evaluated using LLM\ncapabilities, grounded in empirical analysis of real-world survey data.\nValidation on English and Korean datasets shows that our framework not only\noutperforms existing metrics but also demonstrates high practical applicability\nfor real-world applications such as response quality prediction and response\nrejection, showing strong correlations with expert assessment.", "AI": {"tldr": "Introduces a two-stage, LLM-assisted framework to assess the quality of human-written open-ended survey responses: first filter gibberish, then score effort, relevance, and completeness; validated on English and Korean datasets, it outperforms existing metrics and correlates strongly with expert judgments.", "motivation": "Low-quality open-ended survey responses create manual filtering burdens and can bias conclusions. Existing automatic evaluators focus on LLM-generated text and don\u2019t capture the distinct characteristics of human responses, motivating a human-oriented evaluation approach.", "method": "A two-stage framework: (1) gibberish filtering to remove nonsensical responses; (2) LLM-based evaluation along three empirically derived dimensions\u2014effort, relevance, and completeness\u2014tailored to human survey data.", "result": "On English and Korean datasets, the framework outperforms prior metrics, shows strong correlation with expert assessments, and proves useful for tasks like predicting response quality and rejecting low-quality responses.", "conclusion": "The proposed framework is an effective, practical solution for evaluating human-written survey responses, enabling higher-quality datasets and reducing manual workload, with demonstrated multilingual applicability and better alignment with expert judgments than existing methods."}}
{"id": "2510.06254", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06254", "abs": "https://arxiv.org/abs/2510.06254", "authors": ["Xiaochen Zhao", "Chengting Yu", "Kairong Yu", "Lei Liu", "Aili Wang"], "title": "Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training", "comment": null, "summary": "Spiking Neural Networks (SNNs) exhibit exceptional energy efficiency on\nneuromorphic hardware due to their sparse activation patterns. However,\nconventional training methods based on surrogate gradients and Backpropagation\nThrough Time (BPTT) not only lag behind Artificial Neural Networks (ANNs) in\nperformance, but also incur significant computational and memory overheads that\ngrow linearly with the temporal dimension. To enable high-performance SNN\ntraining under limited computational resources, we propose an enhanced\nself-distillation framework, jointly optimized with rate-based backpropagation.\nSpecifically, the firing rates of intermediate SNN layers are projected onto\nlightweight ANN branches, and high-quality knowledge generated by the model\nitself is used to optimize substructures through the ANN pathways. Unlike\ntraditional self-distillation paradigms, we observe that low-quality\nself-generated knowledge may hinder convergence. To address this, we decouple\nthe teacher signal into reliable and unreliable components, ensuring that only\nreliable knowledge is used to guide the optimization of the model. Extensive\nexperiments on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate that\nour method reduces training complexity while achieving high-performance SNN\ntraining. Our code is available at\nhttps://github.com/Intelli-Chip-Lab/enhanced-self-distillation-framework-for-snn.", "AI": {"tldr": "They train spiking neural networks efficiently by adding lightweight ANN side-branches and performing reliable self-distillation on firing-rate signals, cutting BPTT-style cost while maintaining strong accuracy across CIFAR, CIFAR-DVS, and ImageNet.", "motivation": "SNNs are energy-efficient but standard surrogate/BPTT training is memory- and compute-heavy and still lags ANNs in accuracy. There is a need for a training approach that scales better with time and resources without sacrificing performance.", "method": "Project intermediate SNN firing rates to lightweight ANN branches and jointly optimize with a rate-based backpropagation objective. Use self-distillation where the model\u2019s own signals supervise submodules, but split the teacher signal into reliable vs. unreliable components and train only with the reliable part to avoid harmful guidance.", "result": "Across CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet, the approach reduces training complexity and achieves high-performance SNN models (abstract implies competitive or improved accuracy; specific numbers not provided).", "conclusion": "Decoupled, reliability-aware self-distillation via ANN side-branches enables compute- and memory-efficient SNN training that maintains high accuracy, offering a practical alternative to BPTT-based methods."}}
{"id": "2510.06302", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06302", "abs": "https://arxiv.org/abs/2510.06302", "authors": ["Ksenija Lace", "Marite Kirikova"], "title": "Requirements for Game-Based Learning Design Framework for Information System Integration in the Context of Post-Merger Integration", "comment": null, "summary": "Post-merger integration states unique challenges for professionals\nresponsible for information system integration aimed on alignment and\ncombination diverse system architectures of merging organizations. Although the\ntheoretical and practical guidance exists for post-merger integration on the\nbusiness level, there is a significant gap in training for information system\nintegration in this context. In prior research specific methods AMILI (Support\nmethod for informed decision identification) and AMILP (Support method for\ninformed decision-making) were introduced for the support of information system\nintegration decisions in the post-merger integration. But during the practical\napplication was reported high learning curve and low learner motivation. This\npaper explores how game-based learning design can address these limitations by\ntransforming static method training into engaging learning experience. The\nstudy analyzes foundational learning theories, cognitive load and motivation\nmodels, and serious game design frameworks to identify the essential\nrequirements for a game-based learning design framework tailored to information\nsystem integration in post-merger integration. Requirements are structured in\ntwo components: the transformation process and resulting learning experience.\nThe paper concludes with a plan for developing and evaluating the proposed\nframework through iterative design and real-world validation.", "AI": {"tldr": "Proposes a game\u2011based learning design framework to train post\u2011merger information systems integration, addressing the high learning curve and low motivation observed with AMILI/AMILP. Derives requirements from learning theories, cognitive load and motivation models, and serious-game design, and outlines an iterative development/validation plan.", "motivation": "Post\u2011merger IS integration is complex and undertrained. Although decision-support methods (AMILI/AMILP) exist, practitioners report steep learning curves and low engagement, creating a need for more effective training approaches.", "method": "Conceptual synthesis and requirements engineering: review of foundational learning theories, cognitive load and motivation models, and serious game design frameworks to extract requirements for a game\u2011based learning design tailored to IS integration in PMI. Requirements are organized into two parts: (1) the transformation process from static method training to game-based training, and (2) the target learning experience.", "result": "A requirements set for a game\u2011based learning design framework specific to IS integration in post\u2011merger contexts, structured around the transformation process and the intended learning experience. No empirical results yet.", "conclusion": "Presents a plan to build and iteratively evaluate the framework through real\u2011world pilots, aiming to improve training effectiveness and learner motivation in PMI IS integration."}}
{"id": "2510.06243", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06243", "abs": "https://arxiv.org/abs/2510.06243", "authors": ["Qihua Dong", "Luis Figueroa", "Handong Zhao", "Kushal Kafle", "Jason Kuen", "Zhihong Ding", "Scott Cohen", "Yun Fu"], "title": "CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning", "comment": "MLLM, Referring Expression Segmentation", "summary": "Referring Expression Comprehension and Segmentation are critical tasks for\nassessing the integration of language understanding and image comprehension,\nserving as benchmarks for Multimodal Large Language Models (MLLMs)\ncapabilities. To address these challenges, we propose a new strategy, CoT\nReferring, which enhances model reasoning across modalities through a\nstructured, chain-of-thought training data structure. Our approach\nsystematically parses textual structures to a sequential referring step, where\nin each step it identifies relationships and ensures consistent reference\nalignment, thereby improving accuracy in complex query scenarios. We\nrestructure the training data to enforce a new output form, providing new\nannotations for existing datasets and compiling an evaluation benchmark from\nexisting resources. This benchmark is designed explicitly for complex referring\ncases. We also integrate detection and segmentation capabilities into a unified\nMLLM framework, training it with a novel adaptive weighted loss to optimize\nperformance. Experimental results on our curated benchmark and RefCOCO/+/g\ndemonstrate the effectiveness of our approach, with a notable increase of 2.5%+\nover baseline models.", "AI": {"tldr": "Proposes \u201cCoT Referring,\u201d a chain-of-thought style supervision and output format for referring expression comprehension/segmentation, plus a unified detection+segmentation MLLM with adaptive weighted loss and a new complex-case benchmark, yielding ~2.5%+ gains on RefCOCO/+/g and a curated benchmark.", "motivation": "Referring tasks require multi-step cross-modal reasoning over objects and relations; existing MLLMs struggle with complex, compositional queries and lack targeted benchmarks evaluating such complexity.", "method": "(1) Parse referring expressions into sequential reasoning steps with explicit relationship identification and reference alignment; (2) re-annotate/reshape datasets to enforce a structured output format; (3) build a benchmark emphasizing complex referring cases; (4) integrate detection and segmentation into a unified MLLM; (5) train with a novel adaptive weighted loss for joint optimization.", "result": "Consistent improvements on their curated complex benchmark and on RefCOCO/+/g, achieving \u22652.5% absolute gains over baseline models, particularly on complex queries.", "conclusion": "Structured, stepwise supervision (CoT Referring) enhances multimodal reasoning and alignment for referring tasks. A unified detection+segmentation framework with adaptive loss further improves performance, and the new benchmark highlights gains on complex cases, indicating a promising direction for MLLMs."}}
{"id": "2510.06260", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06260", "abs": "https://arxiv.org/abs/2510.06260", "authors": ["Sher Khan", "Raz Muhammad", "Adil Hussain", "Muhammad Sajjad", "Muhammad Rashid"], "title": "Ensemble Deep Learning and LLM-Assisted Reporting for Automated Skin Lesion Diagnosis", "comment": null, "summary": "Cutaneous malignancies demand early detection for favorable outcomes, yet\ncurrent diagnostics suffer from inter-observer variability and access\ndisparities. While AI shows promise, existing dermatological systems are\nlimited by homogeneous architectures, dataset biases across skin tones, and\nfragmented approaches that treat natural language processing as separate\npost-hoc explanations rather than integral to clinical decision-making. We\nintroduce a unified framework that fundamentally reimagines AI integration for\ndermatological diagnostics through two synergistic innovations. First, a\npurposefully heterogeneous ensemble of architecturally diverse convolutional\nneural networks provides complementary diagnostic perspectives, with an\nintrinsic uncertainty mechanism flagging discordant cases for specialist review\n-- mimicking clinical best practices. Second, we embed large language model\ncapabilities directly into the diagnostic workflow, transforming classification\noutputs into clinically meaningful assessments that simultaneously fulfill\nmedical documentation requirements and deliver patient-centered education. This\nseamless integration generates structured reports featuring precise lesion\ncharacterization, accessible diagnostic reasoning, and actionable monitoring\nguidance -- empowering patients to recognize early warning signs between\nvisits. By addressing both diagnostic reliability and communication barriers\nwithin a single cohesive system, our approach bridges the critical\ntranslational gap that has prevented previous AI implementations from achieving\nclinical impact. The framework represents a significant advancement toward\ndeployable dermatological AI that enhances diagnostic precision while actively\nsupporting the continuum of care from initial detection through patient\neducation, ultimately improving early intervention rates for skin lesions.", "AI": {"tldr": "A unified dermatology AI that fuses a diverse CNN ensemble with built\u2011in uncertainty triage and an embedded LLM to produce structured clinical reports and patient education, aiming to boost diagnostic reliability and close communication gaps.", "motivation": "Early melanoma detection is critical, but current practice suffers from inter\u2011observer variability and unequal access. Existing AI tools underperform due to homogeneous model architectures, dataset bias across skin tones, and treating language explanations as an afterthought rather than part of the clinical workflow.", "method": "Two-part framework: (1) a deliberately heterogeneous ensemble of CNNs provides complementary visual diagnoses and an intrinsic uncertainty mechanism that flags discordant cases for specialist review; (2) an LLM is integrated into the diagnostic pipeline to convert model outputs into structured, clinically meaningful documentation with lesion characterization, reasoning, and patient\u2011centered education and monitoring guidance.", "result": "The system purportedly increases diagnostic reliability via ensemble agreement/triage and improves clinical usability by generating ready-to-use reports and patient guidance. However, no quantitative performance, fairness metrics, or prospective validation results are reported in the abstract.", "conclusion": "The approach is positioned as a step toward deployable dermatology AI that unifies perception and communication, potentially improving early intervention rates. Real-world impact depends on rigorous, bias-sensitive evaluation across diverse skin tones and care settings, and on clinical workflow integration."}}
{"id": "2510.06307", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06307", "abs": "https://arxiv.org/abs/2510.06307", "authors": ["Wentao Deng", "Jiahuan Pei", "Zhiwei Xu", "Zhaochun Ren", "Zhumin Chen", "Pengjie Ren"], "title": "Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks", "comment": "This paper has been accepted by NeurIPS 2025", "summary": "A multi-agent system (MAS) enhances its capacity to solve complex natural\nlanguage processing (NLP) tasks through collaboration among multiple agents,\nwhere consensus-seeking serves as a fundamental mechanism. However, existing\nconsensus-seeking approaches typically rely on voting mechanisms to judge\nconsensus, overlooking contradictions in system-internal beliefs that\ndestabilize the consensus. Moreover, these methods often involve agents\nupdating their results through indiscriminate collaboration with every other\nagent. Such uniform interaction fails to identify the optimal collaborators for\neach agent, hindering the emergence of a stable consensus. To address these\nchallenges, we provide a theoretical framework for selecting optimal\ncollaborators that maximize consensus stability. Based on the theorems, we\npropose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate\nstable consensus via selecting optimal collaborators and calibrating the\nconsensus judgment by system-internal beliefs. Experimental results on the MATH\nand MMLU benchmark datasets demonstrate that the proposed BCCS framework\noutperforms the best existing results by 2.23% and 3.95% of accuracy on\nchallenging tasks, respectively. Our code and data are available at\nhttps://github.com/dengwentao99/BCCS.", "AI": {"tldr": "They introduce a belief-aware, collaborator-selective consensus framework (BCCS) for multi\u2011agent NLP systems that replaces naive voting with stability\u2011maximizing collaborator selection and belief-calibrated consensus, yielding state\u2011of\u2011the\u2011art gains on MATH and MMLU.", "motivation": "Current MAS consensus methods depend on simple voting and indiscriminate all-to-all collaboration, which ignores contradictions in agents\u2019 internal beliefs and leads to unstable or suboptimal consensus. There is a need to systematically choose collaborators and account for belief consistency to stabilize consensus and improve task performance.", "method": "Provide a theoretical framework to select optimal collaborators that maximize consensus stability, then instantiate it as the Belief\u2011Calibrated Consensus Seeking (BCCS) framework. BCCS (1) selects collaborators per agent based on the theory and (2) calibrates consensus judgments using system-internal beliefs rather than raw votes.", "result": "On challenging benchmarks, BCCS surpasses prior best methods: +2.23% accuracy on MATH and +3.95% on MMLU. Code and data are released at the provided repository.", "conclusion": "Selecting optimal collaborators and calibrating consensus by agents\u2019 internal beliefs stabilizes consensus formation in MAS and improves accuracy on difficult NLP tasks. Theoretical guidance plus belief calibration is a practical path to more reliable multi-agent consensus."}}
{"id": "2510.06244", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06244", "abs": "https://arxiv.org/abs/2510.06244", "authors": ["Nouman Ahmed", "Ronin Wu", "Victor Botev"], "title": "Evaluating Embedding Frameworks for Scientific Domain", "comment": null, "summary": "Finding an optimal word representation algorithm is particularly important in\nterms of domain specific data, as the same word can have different meanings and\nhence, different representations depending on the domain and context. While\nGenerative AI and transformer architecture does a great job at generating\ncontextualized embeddings for any given work, they are quite time and compute\nextensive, especially if we were to pre-train such a model from scratch. In\nthis work, we focus on the scientific domain and finding the optimal word\nrepresentation algorithm along with the tokenization method that could be used\nto represent words in the scientific domain. The goal of this research is two\nfold: 1) finding the optimal word representation and tokenization methods that\ncan be used in downstream scientific domain NLP tasks, and 2) building a\ncomprehensive evaluation suite that could be used to evaluate various word\nrepresentation and tokenization algorithms (even as new ones are introduced) in\nthe scientific domain. To this end, we build an evaluation suite consisting of\nseveral downstream tasks and relevant datasets for each task. Furthermore, we\nuse the constructed evaluation suite to test various word representation and\ntokenization algorithms.", "AI": {"tldr": "Proposes and applies an evaluation suite to compare word representation and tokenization methods for scientific-domain NLP, aiming to identify optimal choices; concrete comparative results are not reported in the abstract.", "motivation": "In scientific text, words can be highly polysemous and domain-dependent, so general-purpose embeddings/tokenizers may be suboptimal. Training large transformer models from scratch is expensive, motivating a search for efficient, domain-suitable representations and tokenization strategies, plus a standardized way to evaluate them.", "method": "Construct a comprehensive evaluation suite comprising multiple downstream scientific NLP tasks and datasets, then use it to benchmark a range of word representation algorithms and tokenization methods for the scientific domain.", "result": "The abstract states that the authors built the suite and used it to test several approaches, but it does not provide quantitative findings or name which methods performed best.", "conclusion": "An evaluation framework for scientific-domain word representations/tokenizers is presented and used; it is intended to guide selection of methods for downstream tasks and to remain useful as new algorithms emerge, though the abstract leaves conclusions about specific winners unspecified."}}
{"id": "2510.06273", "categories": ["cs.CV", "astro-ph.IM", "cs.LG", "gr-qc"], "pdf": "https://arxiv.org/pdf/2510.06273", "abs": "https://arxiv.org/abs/2510.06273", "authors": ["Divyansh Srivastava", "Andrzej Niedzielski"], "title": "Vision Transformer for Transient Noise Classification", "comment": "9 pages, 4 figures", "summary": "Transient noise (glitches) in LIGO data hinders the detection of\ngravitational waves (GW). The Gravity Spy project has categorized these noise\nevents into various classes. With the O3 run, there is the inclusion of two\nadditional noise classes and thus a need to train new models for effective\nclassification. We aim to classify glitches in LIGO data into 22 existing\nclasses from the first run plus 2 additional noise classes from O3a using the\nVision Transformer (ViT) model. We train a pre-trained Vision Transformer\n(ViT-B/32) model on a combined dataset consisting of the Gravity Spy dataset\nwith the additional two classes from the LIGO O3a run. We achieve a\nclassification efficiency of 92.26%, demonstrating the potential of Vision\nTransformer to improve the accuracy of gravitational wave detection by\neffectively distinguishing transient noise.\n  Key words: gravitational waves --vision transformer --machine learning", "AI": {"tldr": "Fine-tunes a pre-trained Vision Transformer (ViT-B/32) to classify 24 LIGO glitch classes (22 Gravity Spy + 2 new O3a classes), reaching 92.26% accuracy, suggesting improved transient-noise discrimination for gravitational-wave searches.", "motivation": "Glitch transients in LIGO data impede gravitational-wave detection. The O3 run introduced two new noise classes, creating a gap in existing classifiers trained on earlier runs and motivating an updated, more capable model to maintain/boost data quality and detection reliability.", "method": "Transfer learning with a pre-trained ViT-B/32, fine-tuned on a combined dataset comprising the Gravity Spy glitch classes plus two additional O3a classes. The model outputs multi-class predictions across 24 noise categories.", "result": "Reported classification efficiency of 92.26% on the combined dataset, indicating strong performance of ViT for multi-class glitch categorization.", "conclusion": "Vision Transformers are effective for LIGO glitch classification and can help improve gravitational-wave detection by better separating transient noise from true signals; expanding/updating class coverage enables adaptation to newer observing runs."}}
{"id": "2510.06410", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06410", "abs": "https://arxiv.org/abs/2510.06410", "authors": ["Aochong Oliver Li", "Tanya Goyal"], "title": "Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?", "comment": null, "summary": "Reasoning LLMs are trained to verbalize their reasoning process, yielding\nstrong gains on complex tasks. This transparency also opens a promising\ndirection: multiple reasoners can directly collaborate on each other's thinking\nwithin a shared trajectory, yielding better inference efficiency and\nexploration. A key prerequisite, however, is the ability to assess the\nusefulness and build on another model's partial thinking -- we call this\noff-trajectory reasoning. Our paper investigates a critical question: can\nstandard solo-reasoning training pipelines deliver desired off-trajectory\nbehaviors? We propose twin tests that capture the two extremes of the\noff-trajectory spectrum, namely Recoverability, which tests whether LLMs can\nbacktrack from \"distractions\" induced by misleading reasoning traces, and\nGuidability, which tests their ability to build upon correct reasoning from\nstronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and\nreveals a counterintuitive finding -- \"stronger\" LLMs on benchmarks are often\nmore fragile under distraction. Moreover, all models tested fail to effectively\nleverage guiding steps from collaborators on problems beyond their inherent\ncapabilities with solve rates remaining under 9.2%. Finally, we conduct control\nstudies to isolate the effects of three factors in post-training on these\nbehaviors: the choice of distillation teacher, the use of RL, and data\nselection strategy. Our results provide actionable insights for training\nnatively strong reasoning collaborators; e.g., we find that suboptimal\nrecoverability behaviors of teacher models are transferred to distilled\nstudents even if the distillation trajectories are correct. Taken together,\nthis work lays the groundwork for evaluating multi-model collaborations in\nshared reasoning trajectories and highlights the limitations of off-the-shelf\nreasoning LLMs.", "AI": {"tldr": "They introduce off-trajectory reasoning and two tests\u2014Recoverability (resisting/undoing misleading traces) and Guidability (building on correct collaborator steps)\u2014to evaluate collaborative reasoning in LLMs. Across 15 open-weight models (1.5B\u201332B), stronger benchmark models are paradoxically more distractible, and none effectively leverage guidance beyond their native ability (<9.2% solve). Teacher weaknesses in recoverability transfer through distillation. The work exposes limits of current solo-reasoning training and sets evaluation ground rules for multi-model collaboration.", "motivation": "Reasoning LLMs now expose their chain-of-thought, enabling potential collaboration where models read and augment each other\u2019s partial reasoning. Realizing this requires models that can ignore misleading traces and use correct partial solutions\u2014capabilities not guaranteed by standard training. The paper asks whether current solo-reasoning pipelines already yield such off-trajectory behaviors.", "method": "Define off-trajectory reasoning and propose twin evaluations: (1) Recoverability\u2014can a model backtrack from deliberate distractions in a shared trajectory? (2) Guidability\u2014can a model build upon correct intermediate steps from a stronger collaborator. Evaluate 15 open-weight LLMs spanning 1.5B\u201332B params. Run control studies to isolate post-training factors: distillation teacher choice, use of RL, and data selection strategy.", "result": "- Stronger models on standard benchmarks often show worse recoverability; they are more fragile to misleading traces.\n- All tested models fail to capitalize on guiding steps from stronger collaborators for tasks beyond their inherent capability; solve rates stay under 9.2%.\n- Distillation transfers teacher recoverability flaws to students even when the distilled trajectories are correct. Additional controls examine RL and data selection effects.", "conclusion": "Standard solo-reasoning training does not reliably produce models that are robust to distractors or that can exploit collaborators\u2019 correct reasoning. Benchmark strength overestimates collaborative reliability. Training pipelines must explicitly target recoverability/guidability and ensure teacher robustness. The paper establishes evaluation protocols and highlights current limitations of off-the-shelf reasoning LLMs for shared-trajectory collaboration."}}
{"id": "2510.06249", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06249", "abs": "https://arxiv.org/abs/2510.06249", "authors": ["Toshiki Nakai", "Ravi Kiran Chikkala", "Lena Sophie Oberkircher", "Nicholas Jennings", "Natalia Skachkova", "Tatiana Anikina", "Jesujoba Oluwadara Alabi"], "title": "TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B", "comment": "It is work in progress", "summary": "The 2025 Multimodal Models for Low-Resource Contexts and Social Impact\n(MMLoSo) Language Challenge addresses one of India's most pressing linguistic\ngaps: the lack of resources for its diverse low-resource languages (LRLs). In\nthis study, we investigate whether enforcing cross-lingual similarity in\nspecific internal layers of a decoder-only multilingual large language model\n(LLM) can improve translation quality from LRL to high-resource language (HRL).\nSpecifically, we combine Centered Kernel Alignment (CKA), a similarity metric\nthat encourages representations of different languages to align, with REPINA, a\nregularization method that constrains parameter updates to remain close to the\npretrained model, into a joint method we call TRepLiNa. In this research\nproject, we experiment with zero-shot, few-shot, and fine-tuning settings using\nAya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari,\nSantali, Bhili) with Hindi/English pivots. Our results show that aligning\nmid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach\nto improving LRL translation, especially in data-scarce settings.", "AI": {"tldr": "Proposes TRepLiNa, a low-cost method that aligns cross-lingual representations at mid-level layers of a decoder-only multilingual LLM by combining CKA-based similarity with REPINA regularization, improving LRL\u2192HRL translation (Mundari, Santali, Bhili) on Aya-23 8B with QLoRA, especially in zero/few-shot settings.", "motivation": "India\u2019s low-resource languages lack parallel data and tooling, hindering translation quality and access. The work targets social impact by boosting LRL translation with minimal data and compute, addressing the MMLoSo challenge\u2019s gap.", "method": "Combine Centered Kernel Alignment (CKA) to encourage representation similarity across languages with REPINA to constrain parameter drift from the pretrained model. Apply this joint objective (TRepLiNa) to specific internal (mid-level) layers of a decoder-only multilingual LLM (Aya-23 8B) trained with QLoRA under zero-shot, few-shot, and fine-tuning regimes on MMLoSo pairs (Mundari, Santali, Bhili) using Hindi/English as pivots.", "result": "Aligning mid-level layers with TRepLiNa improves LRL\u2192HRL translation quality in data-scarce scenarios; benefits are reported across zero/few-shot and fine-tuning settings. The approach is characterized as low-cost and practical. (No numeric metrics provided in the abstract.)", "conclusion": "Mid-layer cross-lingual alignment via a CKA+REPINA joint objective is an effective, resource-efficient strategy for improving LRL translation in multilingual LLMs, offering a practical path for social-impact applications where data and compute are limited."}}
{"id": "2510.06277", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06277", "abs": "https://arxiv.org/abs/2510.06277", "authors": ["Fahim Shahriar", "Cheryl Wang", "Alireza Azimi", "Gautham Vasan", "Hany Hamed Elanwar", "A. Rupam Mahmood", "Colin Bellinger"], "title": "General and Efficient Visual Goal-Conditioned Reinforcement Learning using Object-Agnostic Masks", "comment": null, "summary": "Goal-conditioned reinforcement learning (GCRL) allows agents to learn diverse\nobjectives using a unified policy. The success of GCRL, however, is contingent\non the choice of goal representation. In this work, we propose a mask-based\ngoal representation system that provides object-agnostic visual cues to the\nagent, enabling efficient learning and superior generalization. In contrast,\nexisting goal representation methods, such as target state images, 3D\ncoordinates, and one-hot vectors, face issues of poor generalization to unseen\nobjects, slow convergence, and the need for special cameras. Masks can be\nprocessed to generate dense rewards without requiring error-prone distance\ncalculations. Learning with ground truth masks in simulation, we achieved 99.9%\nreaching accuracy on training and unseen test objects. Our proposed method can\nbe utilized to perform pick-up tasks with high accuracy, without using any\npositional information of the target. Moreover, we demonstrate learning from\nscratch and sim-to-real transfer applications using two different physical\nrobots, utilizing pretrained open vocabulary object detection models for mask\ngeneration.", "AI": {"tldr": "Proposes a mask-based goal representation for goal-conditioned RL that supplies object-agnostic visual goals and dense rewards, yielding fast learning, strong generalization to unseen objects, and successful sim-to-real transfer.", "motivation": "Existing goal encodings (goal images, 3D coordinates, one-hot IDs) often generalize poorly to unseen objects, converge slowly, or require special sensing. The authors seek a representation that is simple, object-agnostic, supports dense rewards without fragile distance metrics, and works in both simulation and the real world.", "method": "Represent the goal as a segmentation mask highlighting the target object/region. Use the mask both as an input cue to the policy and to compute dense reward signals by comparing the robot\u2019s effect relative to the masked region\u2014avoiding explicit geometric distance calculations. Train in simulation with ground-truth masks; for real robots, obtain masks from pretrained open-vocabulary object detectors. Evaluate on reaching and pick-up tasks.", "result": "In simulation with ground-truth masks, the method achieves 99.9% reaching accuracy on both training and unseen objects. It performs high-accuracy pick-up without using any target positional coordinates. It learns policies from scratch and transfers from simulation to two different physical robots using detector-generated masks.", "conclusion": "Mask-based goal representations provide efficient learning and superior generalization for GCRL, eliminating dependence on precise position measurements or special cameras, and enabling practical sim-to-real applications across diverse objects."}}
{"id": "2510.06433", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06433", "abs": "https://arxiv.org/abs/2510.06433", "authors": ["Aryan Singh Dalal", "Yinglun Zhang", "Duru Do\u011fan", "Atalay Mert \u0130leri", "Hande K\u00fc\u00e7\u00fck McGinty"], "title": "Flavonoid Fusion: Creating a Knowledge Graph to Unveil the Interplay Between Food and Health", "comment": null, "summary": "The focus on \"food as medicine\" is gaining traction in the field of health\nand several studies conducted in the past few years discussed this aspect of\nfood in the literature. However, very little research has been done on\nrepresenting the relationship between food and health in a standardized,\nmachine-readable format using a semantic web that can help us leverage this\nknowledge effectively. To address this gap, this study aims to create a\nknowledge graph to link food and health through the knowledge graph's ability\nto combine information from various platforms focusing on flavonoid contents of\nfood found in the USDA databases and cancer connections found in the\nliterature. We looked closely at these relationships using KNARM methodology\nand represented them in machine-operable format. The proposed knowledge graph\nserves as an example for researchers, enabling them to explore the complex\ninterplay between dietary choices and disease management. Future work for this\nstudy involves expanding the scope of the knowledge graph by capturing nuances,\nadding more related data, and performing inferences on the acquired knowledge\nto uncover hidden relationships.", "AI": {"tldr": "Proof-of-concept knowledge graph that semantically links food (flavonoid content from USDA) to health (cancer-related literature) using KNARM, enabling machine-readable exploration of diet\u2013disease relationships.", "motivation": "Although \u201cfood as medicine\u201d is a growing focus, there is no standardized, machine-readable way to represent and integrate evidence about food\u2013health relationships across data sources.", "method": "Integrate USDA flavonoid databases with cancer associations extracted from literature; model entities/relations using KNARM methodology; publish as a semantic web/knowledge graph for machine operation.", "result": "A working knowledge graph that exemplifies how to connect dietary components with disease-related evidence, allowing researchers to query and explore complex diet\u2013disease links.", "conclusion": "The study demonstrates feasibility and utility of a semantic, interoperable representation of food\u2013health knowledge; future work includes expanding coverage, adding nuance, and running inference to discover hidden relationships."}}
{"id": "2510.06250", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06250", "abs": "https://arxiv.org/abs/2510.06250", "authors": ["Bharti Meena", "Joanna Skubisz", "Harshit Rajgarhia", "Nand Dave", "Kiran Ganesh", "Shivali Dalmia", "Abhishek Mukherji", "Vasudevan Sundarababu", "Olga Pospelova"], "title": "Scalable multilingual PII annotation for responsible AI in LLMs", "comment": null, "summary": "As Large Language Models (LLMs) gain wider adoption, ensuring their reliable\nhandling of Personally Identifiable Information (PII) across diverse regulatory\ncontexts has become essential. This work introduces a scalable multilingual\ndata curation framework designed for high-quality PII annotation across 13\nunderrepresented locales, covering approximately 336 locale-specific PII types.\nOur phased, human-in-the-loop annotation methodology combines linguistic\nexpertise with rigorous quality assurance, leading to substantial improvements\nin recall and false positive rates from pilot, training, and production phases.\nBy leveraging inter-annotator agreement metrics and root-cause analysis, the\nframework systematically uncovers and resolves annotation inconsistencies,\nresulting in high-fidelity datasets suitable for supervised LLM fine-tuning.\nBeyond reporting empirical gains, we highlight common annotator challenges in\nmultilingual PII labeling and demonstrate how iterative, analytics-driven\npipelines can enhance both annotation quality and downstream model reliability.", "AI": {"tldr": "They present a scalable, multilingual, human-in-the-loop framework to curate high-quality PII annotations across 13 underrepresented locales (\u2248336 locale-specific PII types), using phased QA and analytics to boost recall and reduce false positives, yielding datasets suitable for LLM fine-tuning.", "motivation": "LLMs must reliably detect and handle PII under diverse, locale-specific regulations, yet high-quality labeled data\u2014especially for underrepresented languages and PII categories\u2014is scarce and inconsistently annotated. A robust, scalable curation process is needed to ensure compliance and model reliability.", "method": "A phased pipeline (pilot \u2192 training \u2192 production) with human-in-the-loop annotation by linguistic experts, rigorous QA, inter-annotator agreement measurements, and root-cause analysis. The framework iteratively refines guidelines and resolves inconsistencies, covering 13 locales and ~336 locale-specific PII types to create high-fidelity datasets for supervised LLM fine-tuning.", "result": "Across phases, they report substantial gains in recall and reductions in false positives, improved inter-annotator agreement, and systematic resolution of annotation inconsistencies. The outcome is high-fidelity, multilingual PII datasets ready for training and evaluating LLMs.", "conclusion": "Iterative, analytics-driven human-in-the-loop curation can scale multilingual PII labeling and significantly improve annotation quality, which in turn enhances downstream LLM reliability for PII handling. The framework also surfaces common annotator challenges and offers a replicable path for multilingual compliance-sensitive data curation."}}
{"id": "2510.06281", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06281", "abs": "https://arxiv.org/abs/2510.06281", "authors": ["Chenyang Li", "Qin Li", "Haimin Wang", "Bo Shen"], "title": "Improving the Spatial Resolution of GONG Solar Images to GST Quality Using Deep Learning", "comment": "5 pages; accepted as a workshop paper in ICDM 2025", "summary": "High-resolution (HR) solar imaging is crucial for capturing fine-scale\ndynamic features such as filaments and fibrils. However, the spatial resolution\nof the full-disk H$\\alpha$ images is limited and insufficient to resolve these\nsmall-scale structures. To address this, we propose a GAN-based superresolution\napproach to enhance low-resolution (LR) full-disk H$\\alpha$ images from the\nGlobal Oscillation Network Group (GONG) to a quality comparable with HR\nobservations from the Big Bear Solar Observatory/Goode Solar Telescope\n(BBSO/GST). We employ Real-ESRGAN with Residual-in-Residual Dense Blocks and a\nrelativistic discriminator. We carefully aligned GONG-GST pairs. The model\neffectively recovers fine details within sunspot penumbrae and resolves fine\ndetails in filaments and fibrils, achieving an average mean squared error (MSE)\nof 467.15, root mean squared error (RMSE) of 21.59, and cross-correlation (CC)\nof 0.7794. Slight misalignments between image pairs limit quantitative\nperformance, which we plan to address in future work alongside dataset\nexpansion to further improve reconstruction quality.", "AI": {"tldr": "Uses a GAN-based super-resolution (Real-ESRGAN with RRDB and a relativistic discriminator) to enhance low-resolution full-disk GONG H\u03b1 images toward high-resolution GST-like quality, recovering fine filament/fibril details; performance limited by residual pair misalignments.", "motivation": "Full-disk H\u03b1 imagery lacks the spatial resolution needed to study fine solar features (filaments, fibrils, sunspot penumbrae), while HR instruments like GST provide detail but not full-disk coverage. A super-resolution method could bridge this gap by upgrading ubiquitous LR observations to HR-like quality.", "method": "Curated and carefully aligned paired GONG (LR) and GST (HR) H\u03b1 images; trained a Real-ESRGAN model using Residual-in-Residual Dense Blocks and a relativistic discriminator to learn LR\u2192HR mapping; evaluated with pixel-wise and similarity metrics.", "result": "Qualitatively, the model restores fine penumbral textures and resolves filament/fibril structures. Quantitatively, it attains MSE 467.15, RMSE 21.59, and cross-correlation 0.7794 on test pairs. Slight residual misalignments between training/validation pairs reduce numerical scores.", "conclusion": "GAN-based super-resolution can substantially enhance GONG full-disk H\u03b1 images toward GST-like detail, enabling better study of small-scale solar structures at global scales. Improving image co-registration and enlarging/diversifying the dataset are planned to further boost reconstruction fidelity and metrics."}}
{"id": "2510.06475", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06475", "abs": "https://arxiv.org/abs/2510.06475", "authors": ["Yitao Long", "Yuru Jiang", "Hongjun Liu", "Yilun Zhao", "Jingchen Sun", "Yiqiu Shen", "Chen Zhao", "Arman Cohan", "Dennis Shasha"], "title": "PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles", "comment": null, "summary": "This work investigates the reasoning and planning capabilities of foundation\nmodels and their scalability in complex, dynamic environments. We introduce\nPuzzlePlex, a benchmark designed to assess these capabilities through a diverse\nset of puzzles. PuzzlePlex consists of 15 types of puzzles, including\ndeterministic and stochastic games of varying difficulty, as well as\nsingle-player and two-player scenarios. The PuzzlePlex framework provides a\ncomprehensive environment for each game, and supports extensibility to generate\nmore challenging instances as foundation models evolve. Additionally, we\nimplement customized game-playing strategies for comparison. Building on this\nbenchmark, we develop fine-grained metrics to measure performance and conduct\nan in-depth analysis of frontier foundation models across two settings:\ninstruction-based and code-based. Furthermore, we systematically investigate\ntheir scaling limits. Our findings show that reasoning models outperform others\nin instruction-based settings, while code-based execution presents greater\nchallenges but offers a scalable and efficient alternative. PuzzlePlex enables\ntargeted evaluation and guides future improvements in reasoning, planning, and\ngeneralization for foundation models.", "AI": {"tldr": "PuzzlePlex is a scalable, extensible multi-game benchmark to evaluate and stress-test foundation models\u2019 reasoning and planning, showing reasoning-focused models lead in instruction use while code execution is harder but a promising, efficient path.", "motivation": "Current benchmarks inadequately probe reasoning/planning in complex, dynamic settings and lack systematic analysis of scaling and the gap between natural-language (instruction) and programmatic (code) interactions. A unified, extensible suite with fine-grained metrics is needed to guide model improvements.", "method": "Introduce PuzzlePlex with 15 puzzle/game types (deterministic/stochastic; single- and two-player) and a full environment for each, plus extensibility to harder instances. Provide custom game-playing baselines, design fine-grained performance metrics, and evaluate frontier foundation models under instruction-based vs. code-based settings while examining scaling behavior.", "result": "Reasoning-specialized models outperform others in instruction-based play. Code-based execution poses greater difficulty but scales better and is more efficient to run. The study characterizes scaling limits across puzzles and settings.", "conclusion": "PuzzlePlex enables targeted, extensible evaluation of reasoning, planning, and generalization, highlighting that while instruction-following favors reasoning-tuned models, code-driven interaction\u2014though challenging\u2014offers a scalable, efficient alternative to push capabilities forward."}}
{"id": "2510.06262", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06262", "abs": "https://arxiv.org/abs/2510.06262", "authors": ["Aryan Kumar Singh", "Janvi Singh"], "title": "Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments", "comment": "4 pages, 4 figures", "summary": "This dataset provides responses to a standardized, bilingual (English-Hindi)\nPrakriti Assessment Questionnaire designed to evaluate the physical,\nphysiological, and psychological characteristics of individuals according to\nclassical Ayurvedic principles. The questionnaire consists of 24\nmultiple-choice items covering body features, appetite, sleep patterns, energy\nlevels, and temperament. It was developed following AYUSH/CCRAS guidelines to\nensure comprehensive and accurate data collection. All questions are mandatory\nand neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha)\nare hidden from participants. Data were collected via a Google Forms\ndeployment, enabling automated scoring of responses to map individual traits to\ndosha-specific scores. The resulting dataset provides a structured platform for\nresearch in computational intelligence, Ayurvedic studies, and personalized\nhealth analytics, supporting analysis of trait distributions, correlations, and\npredictive modeling. It can also serve as a reference for future Prakriti-based\nstudies and the development of intelligent health applications.", "AI": {"tldr": "A bilingual (English-Hindi), standardized 24-item questionnaire\u2014aligned with AYUSH/CCRAS guidelines\u2014collects mandatory, neutrally phrased responses on physical, physiological, and psychological traits, maps them via automated scoring to Ayurveda dosha (Vata/Pitta/Kapha) scores, and yields a structured dataset for analytics and modeling.", "motivation": "Provide a rigorously designed, reproducible, and bias-minimized Prakriti assessment dataset to bridge classical Ayurvedic profiling with modern computational research and personalized health applications; address the scarcity of standardized, machine-usable data in this domain.", "method": "Design a 24-item multiple-choice Prakriti questionnaire per AYUSH/CCRAS guidance; make it bilingual (English-Hindi), mandatory, and neutrally worded; hide dosha labels from participants; deploy via Google Forms to capture responses and automatically score items into dosha-specific (Vata, Pitta, Kapha) scores.", "result": "A structured dataset of questionnaire responses paired with automated dosha scores, suitable for analyzing trait distributions and correlations and for developing predictive models in computational intelligence and Ayurvedic studies.", "conclusion": "The dataset establishes a standardized reference resource that can underpin future Prakriti-based research and the creation of intelligent, personalized health applications grounded in Ayurvedic principles."}}
{"id": "2510.06292", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06292", "abs": "https://arxiv.org/abs/2510.06292", "authors": ["Yike Wu", "Yiwei Wang", "Yujun Cai"], "title": "ChainMPQ: Interleaved Text-Image Reasoning Chains for Mitigating Relation Hallucinations", "comment": null, "summary": "While Large Vision-Language Models (LVLMs) achieve strong performance in\nmultimodal tasks, hallucinations continue to hinder their reliability. Among\nthe three categories of hallucinations, which include object, attribute, and\nrelation, relation hallucinations account for the largest proportion but have\nreceived the least attention. To address this issue, we propose ChainMPQ\n(Multi-Perspective Questions guided Interleaved Chain of Image and Text), a\ntraining-free method that improves relational inference in LVLMs by utilizing\naccumulated textual and visual memories. ChainMPQ first extracts subject and\nobject keywords from the question to enhance the corresponding image regions.\nIt then constructs multi-perspective questions that focus on the three core\ncomponents of a relationship: the subject, the object, and the relation that\nlinks them. These questions are sequentially input to the model, with textual\nand visual memories from earlier steps providing supporting context for\nsubsequent ones, thereby forming an interleaved chain of images and text that\nguides progressive relational reasoning. Experiments on multiple LVLMs and\nbenchmarks show that ChainMPQ substantially reduces relation hallucinations,\nwhile ablation studies further validate the effectiveness of its three core\nmodules.", "AI": {"tldr": "ChainMPQ is a training-free prompting framework that reduces relation hallucinations in LVLMs by interleaving multi-perspective questions with accumulated visual and textual memories, yielding substantial improvements across models and benchmarks.", "motivation": "Among object, attribute, and relation hallucinations in LVLMs, relation errors are the most prevalent yet least studied. Improving reliability in relational reasoning is crucial, ideally without model retraining.", "method": "ChainMPQ (Multi-Perspective Questions guided Interleaved Chain of Image and Text) operates in three steps: (1) extract subject/object keywords from the question to enhance corresponding image regions (visual focus/memory); (2) build multi-perspective questions targeting subject, object, and the linking relation; (3) sequentially query the LVLM so that textual and visual outputs from earlier steps are reused as context for later steps, forming an interleaved chain that supports progressive relational inference. The approach is training-free and model-agnostic.", "result": "Across multiple LVLMs and benchmarks, ChainMPQ substantially lowers relation hallucination rates. Ablation studies show that each core module\u2014region enhancement, multi-perspective questioning, and interleaved memory\u2014contributes meaningfully to the gains.", "conclusion": "Interleaving visual and textual memories with multi-perspective questioning improves relational inference in LVLMs without retraining, making ChainMPQ a practical, general method for mitigating relation hallucinations."}}
{"id": "2510.06534", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06534", "abs": "https://arxiv.org/abs/2510.06534", "authors": ["Jiahe Jin", "Abhijay Paladugu", "Chenyan Xiong"], "title": "Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them", "comment": null, "summary": "Agentic search leverages large language models (LLMs) to interpret complex\nuser information needs and execute a multi-step process of planning, searching,\nand synthesizing information to provide answers. This paradigm introduces\nunique challenges for LLMs' reasoning and agentic capabilities when interacting\nwith retrieval systems and the broader web. In this paper, we propose a\nreasoning-driven LLM-based pipeline to study effective reasoning behavior\npatterns in agentic search. Using this pipeline, we analyze successful agentic\nsearch trajectories and identify four beneficial reasoning behaviors:\nInformation Verification, Authority Evaluation, Adaptive Search, and Error\nRecovery. Based on these findings, we propose a technique called Behavior\nPriming to train more effective agentic search models. It synthesizes agentic\nsearch trajectories that exhibit these four behaviors and integrates them into\nthe agentic search model through supervised fine-tuning (SFT), followed by\nstandard reinforcement learning (RL). Experiments on three benchmarks (GAIA,\nWebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in\nLlama3.2-3B and Qwen3-1.7B compared to directly training agentic search models\nwith RL. Crucially, we demonstrate that the desired reasoning behaviors in the\nSFT data, rather than the correctness of the final answer, is the critical\nfactor for achieving strong final performance after RL: fine-tuning on\ntrajectories with desirable reasoning behaviors but incorrect answers leads to\nbetter performance than fine-tuning on trajectories with correct answers. Our\nanalysis further reveals the underlying mechanism: the introduced reasoning\nbehaviors endow models with more effective exploration (higher pass@k and\nentropy) and test-time scaling (longer trajectories) capabilities, providing a\nstrong foundation for RL. Our code will be released as open source.", "AI": {"tldr": "They identify four reasoning behaviors that make agentic search work (verification, authority checks, adaptive search, error recovery) and propose Behavior Priming: synthesize trajectories that exhibit these behaviors, fine-tune (SFT) small LLMs on them, then apply RL. This outperforms training with RL alone (>35% gains on GAIA, WebWalker, HLE) and shows that behavior-rich SFT\u2014even with incorrect final answers\u2014yields better downstream RL performance than SFT on correct answers without those behaviors.", "motivation": "Agentic search requires LLMs to plan, query, browse, and synthesize across the open web, where brittle reasoning leads to failure. Existing RL-based training often underperforms without strong priors about good reasoning. The authors aim to discover which reasoning patterns actually drive success and to bootstrap models to exhibit them before RL.", "method": "1) Build a reasoning-driven pipeline to mine/inspect successful agentic search trajectories. 2) Distill four beneficial behaviors: Information Verification, Authority Evaluation, Adaptive Search, and Error Recovery. 3) Synthesize trajectories that explicitly demonstrate these behaviors. 4) Supervised fine-tune models on these behavior-primed trajectories, then apply standard RL. 5) Evaluate on GAIA, WebWalker, and HLE; analyze exploration metrics (pass@k, entropy) and test-time scaling (trajectory length).", "result": "Behavior Priming yields >35% improvements for Llama3.2-3B and Qwen3-1.7B versus direct RL training. SFT data quality in terms of explicit reasoning behaviors, not correctness of final answers, is the key factor for post-RL performance; behavior-rich but answer-wrong trajectories outperform answer-correct but behavior-poor data. Mechanistically, the primed models explore more (higher pass@k, entropy) and scale better at inference (longer, more effective trajectories).", "conclusion": "Encoding desirable reasoning behaviors via SFT provides a stronger initialization for RL in agentic search, enabling better exploration and performance. Training data that models process quality (behaviors) can be more valuable than final-answer correctness. The approach generalizes across benchmarks and small models; code to be released."}}
{"id": "2510.06263", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06263", "abs": "https://arxiv.org/abs/2510.06263", "authors": ["Jiajun Wu", "Swaleh Zaidi", "Braden Teitge", "Henry Leung", "Jiayu Zhou", "Jessalyn Holodinsky", "Steve Drew"], "title": "Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians", "comment": "Accepted at the IEEE Annual Congress on Artificial Intelligence of\n  Things (IEEE AIoT) 2025", "summary": "Electronic health records (EHRs) contain extensive unstructured clinical data\nthat can overwhelm emergency physicians trying to identify critical\ninformation. We present a two-stage summarization system that runs entirely on\nembedded devices, enabling offline clinical summarization while preserving\npatient privacy. In our approach, a dual-device architecture first retrieves\nrelevant patient record sections using the Jetson Nano-R (Retrieve), then\ngenerates a structured summary on another Jetson Nano-S (Summarize),\ncommunicating via a lightweight socket link. The summarization output is\ntwo-fold: (1) a fixed-format list of critical findings, and (2) a\ncontext-specific narrative focused on the clinician's query. The retrieval\nstage uses locally stored EHRs, splits long notes into semantically coherent\nsections, and searches for the most relevant sections per query. The generation\nstage uses a locally hosted small language model (SLM) to produce the summary\nfrom the retrieved text, operating within the constraints of two NVIDIA Jetson\ndevices. We first benchmarked six open-source SLMs under 7B parameters to\nidentify viable models. We incorporated an LLM-as-Judge evaluation mechanism to\nassess summary quality in terms of factual accuracy, completeness, and clarity.\nPreliminary results on MIMIC-IV and de-identified real EHRs demonstrate that\nour fully offline system can effectively produce useful summaries in under 30\nseconds.", "AI": {"tldr": "A fully offline, privacy-preserving, two-stage RAG system runs on two NVIDIA Jetson devices to summarize EHRs: one retrieves relevant sections; the other uses a \u22647B small language model to generate a structured list of critical findings plus a query-focused narrative, producing useful summaries in <30 seconds on MIMIC-IV and de-identified EHRs.", "motivation": "Emergency physicians face cognitive overload from long, unstructured EHR notes and need rapid, privacy-preserving summaries that work without cloud connectivity or PHI leakage, within the tight compute and latency constraints of embedded hardware.", "method": "A dual-device architecture: Jetson Nano-R performs local retrieval by splitting notes into semantically coherent sections and searching for query-relevant segments; Jetson Nano-S hosts a small language model to generate two outputs (fixed-format critical findings and a query-focused narrative). Devices communicate via a lightweight socket. Six open-source \u22647B models are benchmarked and an LLM-as-Judge evaluates factuality, completeness, and clarity.", "result": "Preliminary experiments on MIMIC-IV and de-identified EHRs show the system can produce useful summaries in under 30 seconds; benchmarking identifies viable SLMs for the hardware. Quality is assessed via LLM-as-Judge, though detailed quantitative results are not provided in the abstract.", "conclusion": "On-device two-stage EHR summarization is feasible on embedded hardware while preserving privacy and meeting near-real-time needs. Stronger evidence\u2014human clinician evaluation, rigorous baselines, detailed metrics, and safety guards\u2014is needed to validate clinical utility and reliability."}}
{"id": "2510.06295", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06295", "abs": "https://arxiv.org/abs/2510.06295", "authors": ["Young D. Kwon", "Abhinav Mehrotra", "Malcolm Chadwick", "Alberto Gil Ramos", "Sourav Bhattacharya"], "title": "Efficient High-Resolution Image Editing with Hallucination-Aware Loss and Adaptive Tiling", "comment": "Preprint. Under review", "summary": "High-resolution (4K) image-to-image synthesis has become increasingly\nimportant for mobile applications. Existing diffusion models for image editing\nface significant challenges, in terms of memory and image quality, when\ndeployed on resource-constrained devices. In this paper, we present\nMobilePicasso, a novel system that enables efficient image editing at high\nresolutions, while minimising computational cost and memory usage.\nMobilePicasso comprises three stages: (i) performing image editing at a\nstandard resolution with hallucination-aware loss, (ii) applying latent\nprojection to overcome going to the pixel space, and (iii) upscaling the edited\nimage latent to a higher resolution with adaptive context-preserving tiling.\nOur user study with 46 participants reveals that MobilePicasso not only\nimproves image quality by 18-48% but reduces hallucinations by 14-51% over\nexisting methods. MobilePicasso demonstrates significantly lower latency, e.g.,\nup to 55.8$\\times$ speed-up, yet with a small increase in runtime memory, e.g.,\na mere 9% increase over prior work. Surprisingly, the on-device runtime of\nMobilePicasso is observed to be faster than a server-based high-resolution\nimage editing model running on an A100 GPU.", "AI": {"tldr": "MobilePicasso is a three-stage diffusion-based pipeline that enables 4K on-device image editing by editing at standard resolution with hallucination-aware loss, staying in latent space via projection, and upscaling with context-preserving tiling\u2014yielding large latency gains (up to 55.8\u00d7), better quality (18\u201348%), fewer hallucinations (14\u201351%), and only ~9% more memory than prior work, even outperforming a server A100 setup.", "motivation": "High-resolution (4K) image-to-image editing is increasingly needed on mobile, but existing diffusion models are too memory-hungry and prone to quality degradation/hallucinations on resource-constrained devices.", "method": "A three-stage system: (i) perform image editing at a standard resolution with a hallucination-aware loss to penalize spurious content; (ii) apply latent projection so the pipeline avoids expensive pixel-space operations; (iii) upscale the edited latent to the target resolution using adaptive, context-preserving tiling to maintain coherence across tiles.", "result": "User study with 46 participants shows 18\u201348% image-quality improvement and 14\u201351% hallucination reduction vs existing methods. Runtime latency is much lower\u2014up to 55.8\u00d7 speed-up\u2014with only a ~9% runtime memory increase over prior work. On-device runtime is reported faster than a server-based high-res model on an A100 GPU.", "conclusion": "MobilePicasso makes high-res on-device image editing practical, improving quality and robustness while significantly reducing latency with modest memory overhead, and can outperform powerful server-side baselines."}}
{"id": "2510.06538", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06538", "abs": "https://arxiv.org/abs/2510.06538", "authors": ["Jiajie Li", "Huayi Zhang", "Peng Lin", "Jinjun Xiong", "Wei Xu"], "title": "Auto-Prompt Ensemble for LLM Judge", "comment": null, "summary": "We present a novel framework that improves the reliability of LLM judges by\nselectively augmenting LLM with auxiliary evaluation dimensions. Existing LLM\njudges often miss crucial evaluation dimensions because they fail to recognize\nthe implicit standards underlying human assessments. To address this challenge,\nwe propose the Auto-Prompt Ensemble (APE), an adaptive framework that\nautomatically learns evaluation dimensions from its failure cases. APE\nincorporates a confidence-based ensemble mechanism to decide when to adopt the\njudgments from additional evaluation dimensions through a novel confidence\nestimation approach called Collective Confidence. Extensive experiments\ndemonstrate that APE improves the reliability of LLM Judge across diverse\nstandard benchmarks. For instance, APE enhances GPT-4o agreement rate on Reward\nBench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a\nprincipled approach for LLM Judge to leverage test-time computation, and bridge\nthe evaluation gap between human and LLM judges.", "AI": {"tldr": "APE is an adaptive Auto-Prompt Ensemble that learns missing evaluation dimensions from failure cases and uses a collective-confidence ensemble to selectively incorporate them, improving LLM-judge reliability (e.g., GPT-4o RewardBench agreement 87.2%\u219290.5% in zero-shot).", "motivation": "LLM judges often overlook implicit human standards and key evaluation dimensions, leading to unreliable or incomplete assessments. The goal is to capture these missing dimensions and better align LLM judges with human judgments.", "method": "Auto-Prompt Ensemble (APE) automatically mines new evaluation dimensions from prior failure cases, then applies a confidence-based ensemble\u2014using a novel Collective Confidence estimator\u2014to decide when additional dimension-specific judgments should override or augment the base judgment at test time.", "result": "Across multiple standard benchmarks, APE consistently increases agreement with human judges; notably, on RewardBench in a zero-shot setting it raises GPT-4o agreement from 87.2% to 90.5%, indicating more reliable evaluations.", "conclusion": "APE provides a principled, test-time computation framework that narrows the gap between human and LLM judges by adaptively adding learned evaluation dimensions and adopting them when confidence warrants, thereby improving reliability."}}
{"id": "2510.06265", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06265", "abs": "https://arxiv.org/abs/2510.06265", "authors": ["Aisha Alansari", "Hamzah Luqman"], "title": "A Comprehensive Survey of Hallucination in Large Language Models: Causes, Detection, and Mitigation", "comment": null, "summary": "Large language models (LLMs) have transformed natural language processing,\nachieving remarkable performance across diverse tasks. However, their\nimpressive fluency often comes at the cost of producing false or fabricated\ninformation, a phenomenon known as hallucination. Hallucination refers to the\ngeneration of content by an LLM that is fluent and syntactically correct but\nfactually inaccurate or unsupported by external evidence. Hallucinations\nundermine the reliability and trustworthiness of LLMs, especially in domains\nrequiring factual accuracy. This survey provides a comprehensive review of\nresearch on hallucination in LLMs, with a focus on causes, detection, and\nmitigation. We first present a taxonomy of hallucination types and analyze\ntheir root causes across the entire LLM development lifecycle, from data\ncollection and architecture design to inference. We further examine how\nhallucinations emerge in key natural language generation tasks. Building on\nthis foundation, we introduce a structured taxonomy of detection approaches and\nanother taxonomy of mitigation strategies. We also analyze the strengths and\nlimitations of current detection and mitigation approaches and review existing\nevaluation benchmarks and metrics used to quantify LLMs hallucinations.\nFinally, we outline key open challenges and promising directions for future\nresearch, providing a foundation for the development of more truthful and\ntrustworthy LLMs.", "AI": {"tldr": "Survey of LLM hallucinations: defines types and lifecycle causes, analyzes task-specific manifestations, organizes detection and mitigation methods, reviews benchmarks/metrics, and outlines open challenges to build more truthful LLMs.", "motivation": "LLMs are fluent but can generate factually incorrect content, eroding reliability in high-stakes settings. A comprehensive, structured synthesis is needed to understand why hallucinations occur and how to detect, mitigate, and evaluate them.", "method": "Systematic literature review. Proposes taxonomies for (1) hallucination types and lifecycle causes (data collection, model/architecture/training, inference), (2) detection approaches, and (3) mitigation strategies. Examines how hallucinations arise across NLG tasks, compares methods, and surveys benchmarks and metrics.", "result": "Delivers structured taxonomies, a mapping from causes to manifestations and interventions, a comparative analysis of detection/mitigation techniques with their strengths and weaknesses, and a catalog of evaluation datasets and metrics. Identifies gaps in current practices.", "conclusion": "Establishes a foundation for more truthful, trustworthy LLMs. Emphasizes needs for standardized evaluation, robust and task-aware detection, principled mitigation tied to root causes, better datasets, and integrated pipelines; outlines open research directions."}}
{"id": "2510.06298", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06298", "abs": "https://arxiv.org/abs/2510.06298", "authors": ["Tobias J. Bauer"], "title": "RGBD Gaze Tracking Using Transformer for Feature Fusion", "comment": "Master Thesis with 125 pages, 59 figures, 17 tables", "summary": "Subject of this thesis is the implementation of an AI-based Gaze Tracking\nsystem using RGBD images that contain both color (RGB) and depth (D)\ninformation. To fuse the features extracted from the images, a module based on\nthe Transformer architecture is used. The combination of RGBD input images and\nTransformers was chosen because it has not yet been investigated. Furthermore,\na new dataset is created for training the AI models as existing datasets either\ndo not contain depth information or only contain labels for Gaze Point\nEstimation that are not suitable for the task of Gaze Angle Estimation. Various\nmodel configurations are trained, validated and evaluated on a total of three\ndifferent datasets. The trained models are then to be used in a real-time\npipeline to estimate the gaze direction and thus the gaze point of a person in\nfront of a computer screen. The AI model architecture used in this thesis is\nbased on an earlier work by Lian et al. It uses a Generative Adversarial\nNetwork (GAN) to simultaneously remove depth map artifacts and extract head\npose features. Lian et al. achieve a mean Euclidean error of 38.7mm on their\nown dataset ShanghaiTechGaze+. In this thesis, a model architecture with a\nTransformer module for feature fusion achieves a mean Euclidean error of 55.3mm\non the same dataset, but we show that using no pre-trained GAN module leads to\na mean Euclidean error of 30.1mm. Replacing the Transformer module with a\nMultilayer Perceptron (MLP) improves the error to 26.9mm. These results are\ncoherent with the ones on the other two datasets. On the ETH-XGaze dataset, the\nmodel with Transformer module achieves a mean angular error of 3.59{\\deg} and\nwithout Transformer module 3.26{\\deg}, whereas the fundamentally different\nmodel architecture used by the dataset authors Zhang et al. achieves a mean\nangular error of 2.04{\\deg}. On the OTH-Gaze-Estimation dataset created for...", "AI": {"tldr": "RGBD-based gaze tracking thesis evaluates Transformer vs simpler fusion on multiple datasets; finds that removing a pre-trained GAN and using an MLP for feature fusion outperforms a Transformer, though results still trail state-of-the-art on ETH-XGaze; also introduces a new RGBD dataset and a real-time pipeline.", "motivation": "Explore an unstudied combination\u2014RGBD inputs with Transformer-based feature fusion\u2014for gaze angle/point estimation. Address dataset gaps: existing sets often lack depth or only provide gaze point labels, not angles, motivating the creation of a new RGBD dataset for training and evaluation.", "method": "Build on Lian et al.\u2019s RGBD gaze architecture that uses a GAN to denoise depth maps and extract head-pose features. Propose a Transformer module for RGB and depth feature fusion; train multiple configurations with/without the pre-trained GAN and with Transformer vs MLP fusion. Evaluate across three datasets (ShanghaiTechGaze+, ETH-XGaze, and a newly created OTH-Gaze-Estimation) and integrate the best model into a real-time gaze estimation pipeline.", "result": "On ShanghaiTechGaze+: Transformer fusion yields 55.3 mm mean Euclidean error; removing the pre-trained GAN gives 30.1 mm; replacing Transformer with an MLP achieves 26.9 mm, beating Lian et al.\u2019s 38.7 mm. On ETH-XGaze: Transformer 3.59\u00b0, without Transformer 3.26\u00b0, while Zhang et al.\u2019s distinct architecture reports 2.04\u00b0. Results are consistent on the new OTH dataset (details truncated). Overall, the Transformer fusion and pre-trained GAN underperform relative to simpler alternatives.", "conclusion": "For RGBD gaze angle estimation, simpler fusion (MLP) and avoiding a misaligned pre-trained GAN can outperform a Transformer-based fusion in this setting, though the approach still lags state-of-the-art on ETH-XGaze. Contributions include a new RGBD dataset and a real-time pipeline; future gains may come from better-suited fusion strategies and pretraining tailored to RGBD gaze."}}
{"id": "2510.06587", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06587", "abs": "https://arxiv.org/abs/2510.06587", "authors": ["Jingbo Yang", "Bairu Hou", "Wei Wei", "Shiyu Chang", "Yujia Bao"], "title": "WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks", "comment": null, "summary": "Large language model (LLM) agents are becoming competent at straightforward\nweb tasks, such as opening an item page or submitting a form, but still\nstruggle with objectives that require long horizon navigation, large scale\ninformation extraction, and reasoning under constraints. We present WebDART, a\ngeneral framework that enables a single LLM to handle such complex chores.\nWebDART (i) dynamically decomposes each objective into three focused subtasks:\nnavigation, information extraction, and execution, so the model concentrates on\none skill at a time, and (ii) continuously replans the decomposition as new\nwebpages are revealed, taking advantage of newly discovered filters or\nshortcuts and avoiding redundant exploration. Evaluated on WebChoreArena,\nWebDART lifts success rates by up to 13.7 percentage points over previous SOTA\nagents, while matching their performance on the easier WebArena suite and\ncompleting tasks with up to 14.7 fewer navigation steps.", "AI": {"tldr": "WebDART lets a single LLM solve complex web chores by dynamically decomposing goals into navigation, information extraction, and execution, and by continually replanning as new pages/shortcuts appear\u2014yielding up to +13.7pp success on WebChoreArena, parity on WebArena, and up to 14.7 fewer navigation steps.", "motivation": "Current LLM web agents handle simple actions but fail on long-horizon navigation, large-scale information extraction, and constraint-based reasoning. A general, scalable approach is needed to manage task complexity and adapt to dynamically revealed website structure.", "method": "Use one LLM that (1) dynamically decomposes each objective into three focused subtasks\u2014navigation, information extraction, execution\u2014so the model concentrates on one competency at a time; and (2) continually replans the subtask sequence as new webpages expose filters/shortcuts, reducing redundant exploration.", "result": "On WebChoreArena, WebDART improves success rates by up to 13.7 percentage points over prior SOTA agents; on the easier WebArena suite it matches their performance and completes tasks with up to 14.7 fewer navigation steps.", "conclusion": "Dynamic subtask decomposition plus continual replanning enables a single LLM agent to tackle complex, long-horizon web tasks more effectively and efficiently without sacrificing performance on simpler tasks."}}
{"id": "2510.06266", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06266", "abs": "https://arxiv.org/abs/2510.06266", "authors": ["Rohitash Chandra", "Yathin Suresh", "Divyansh Raj Sinha", "Sanchit Jindal"], "title": "Language models for longitudinal analysis of abusive content in Billboard Music Charts", "comment": null, "summary": "There is no doubt that there has been a drastic increase in abusive and\nsexually explicit content in music, particularly in Billboard Music Charts.\nHowever, there is a lack of studies that validate the trend for effective\npolicy development, as such content has harmful behavioural changes in children\nand youths. In this study, we utilise deep learning methods to analyse songs\n(lyrics) from Billboard Charts of the United States in the last seven decades.\nWe provide a longitudinal study using deep learning and language models and\nreview the evolution of content using sentiment analysis and abuse detection,\nincluding sexually explicit content. Our results show a significant rise in\nexplicit content in popular music from 1990 onwards. Furthermore, we find an\nincreasing prevalence of songs with lyrics containing profane, sexually\nexplicit, and otherwise inappropriate language. The longitudinal analysis of\nthe ability of language models to capture nuanced patterns in lyrical content,\nreflecting shifts in societal norms and language use over time.", "AI": {"tldr": "Longitudinal deep\u2011learning analysis of Billboard (U.S.) lyrics over seven decades shows a marked post\u20111990 rise in profane and sexually explicit content; language models capture evolving lyrical norms.", "motivation": "Perceived surge in abusive/sexual content in popular music may affect youth behavior, yet rigorous, longitudinal validation is scarce; evidence is needed to inform policy and content moderation debates.", "method": "Collected lyrics from U.S. Billboard charts spanning ~70 years; applied deep learning and language models for sentiment, abuse/profanity, and sexual\u2011explicitness detection; performed trend analysis over time and examined models\u2019 ability to capture nuanced shifts in language and norms.", "result": "Significant increase in explicit, profane, and sexually explicit language in popular songs beginning around 1990; models reveal evolving linguistic patterns aligned with societal/norm shifts.", "conclusion": "Popular music has become more explicit over recent decades; deep learning provides a viable framework to monitor content trends and cultural change over time."}}
{"id": "2510.06299", "categories": ["cs.CV", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.06299", "abs": "https://arxiv.org/abs/2510.06299", "authors": ["Tiago de Conto", "John Armston", "Ralph Dubayah"], "title": "Scalable deep fusion of spaceborne lidar and synthetic aperture radar for global forest structural complexity mapping", "comment": null, "summary": "Forest structural complexity metrics integrate multiple canopy attributes\ninto a single value that reflects habitat quality and ecosystem function.\nSpaceborne lidar from the Global Ecosystem Dynamics Investigation (GEDI) has\nenabled mapping of structural complexity in temperate and tropical forests, but\nits sparse sampling limits continuous high-resolution mapping. We present a\nscalable, deep learning framework fusing GEDI observations with multimodal\nSynthetic Aperture Radar (SAR) datasets to produce global, high-resolution (25\nm) wall-to-wall maps of forest structural complexity. Our adapted\nEfficientNetV2 architecture, trained on over 130 million GEDI footprints,\nachieves high performance (global R2 = 0.82) with fewer than 400,000\nparameters, making it an accessible tool that enables researchers to process\ndatasets at any scale without requiring specialized computing infrastructure.\nThe model produces accurate predictions with calibrated uncertainty estimates\nacross biomes and time periods, preserving fine-scale spatial patterns. It has\nbeen used to generate a global, multi-temporal dataset of forest structural\ncomplexity from 2015 to 2022. Through transfer learning, this framework can be\nextended to predict additional forest structural variables with minimal\ncomputational cost. This approach supports continuous, multi-temporal\nmonitoring of global forest structural dynamics and provides tools for\nbiodiversity conservation and ecosystem management efforts in a changing\nclimate.", "AI": {"tldr": "They fuse sparse GEDI spaceborne lidar with multimodal SAR using a compact EfficientNetV2-based model to generate global, 25 m, multi-temporal maps (2015\u20132022) of forest structural complexity with high accuracy and calibrated uncertainty, enabling scalable monitoring and conservation applications.", "motivation": "Forest structural complexity underpins habitat quality and ecosystem function, but GEDI\u2019s sparse sampling prevents continuous, high-resolution mapping needed for monitoring, biodiversity, and management decisions.", "method": "A scalable deep learning framework that integrates GEDI lidar observations with multimodal SAR inputs. An adapted, lightweight EfficientNetV2 (\u2264400k parameters) is trained on >130 million GEDI footprints to predict structural complexity at 25 m resolution, providing calibrated uncertainty and supporting transfer learning to other forest structure variables.", "result": "Global predictive performance R\u00b2 = 0.82; accurate, uncertainty-calibrated predictions across biomes and time, preserving fine spatial patterns. Produced a global, wall-to-wall, multi-temporal dataset of structural complexity from 2015\u20132022; efficient enough to run without specialized computing.", "conclusion": "The approach enables continuous, high-resolution, multi-temporal monitoring of forest structural dynamics globally, is scalable and extensible via transfer learning, and provides practical tools for biodiversity conservation and ecosystem management under climate change."}}
{"id": "2510.06600", "categories": ["cs.AI", "H.3.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.06600", "abs": "https://arxiv.org/abs/2510.06600", "authors": ["Zhaochun Ren", "Zhou Yang", "Chenglong Ye", "Haizhou Sun", "Chao Chen", "Xiaofei Zhu", "Xiangwen Liao"], "title": "Fine-Grained Emotion Recognition via In-Context Learning", "comment": "9 pages, 10 figures, 4 tables", "summary": "Fine-grained emotion recognition aims to identify the emotional type in\nqueries through reasoning and decision-making processes, playing a crucial role\nin various systems. Recent methods use In-Context Learning (ICL), enhancing the\nrepresentation of queries in the reasoning process through semantically similar\nexamples, while further improving emotion recognition by explaining the\nreasoning mechanisms. However, these methods enhance the reasoning process but\noverlook the decision-making process. This paper investigates decision-making\nin fine-grained emotion recognition through prototype theory. We show that ICL\nrelies on similarity matching between query representations and emotional\nprototypes within the model, where emotion-accurate representations are\ncritical. However, semantically similar examples often introduce emotional\ndiscrepancies, hindering accurate representations and causing errors. To\naddress this, we propose Emotion In-Context Learning (EICL), which introduces\nemotionally similar examples and uses a dynamic soft-label strategy to improve\nquery representations in the emotion reasoning process. A two-stage exclusion\nstrategy is then employed to assess similarity from multiple angles, further\noptimizing the decision-making process. Extensive experiments show that EICL\nsignificantly outperforms ICL on multiple datasets.", "AI": {"tldr": "They argue ICL for fine-grained emotion recognition fails when semantically similar examples carry different emotions. Viewing ICL as prototype matching, they introduce Emotion ICL (EICL) that selects emotionally similar examples, applies dynamic soft labels, and uses a two-stage exclusion strategy to improve both reasoning representations and the final decision, yielding significant gains over standard ICL across datasets.", "motivation": "Current ICL methods strengthen reasoning via semantically similar exemplars and explanations but largely ignore the decision stage. Because ICL effectively matches a query to internal emotion prototypes, accurate emotion-centric representations are crucial. Semantically similar but emotionally mismatched examples distort representations and cause wrong decisions; hence, a method that prioritizes emotional similarity is needed.", "method": "Adopt a prototype-theory perspective of ICL: decisions arise from similarity between query representations and emotion prototypes. Propose EICL that (1) retrieves emotionally similar in-context examples rather than merely semantically similar ones, (2) applies a dynamic soft-label strategy to refine the query\u2019s emotion representation during reasoning, and (3) employs a two-stage exclusion mechanism to filter and assess examples from multiple similarity dimensions, thereby optimizing decision-making.", "result": "Across multiple fine-grained emotion datasets, EICL consistently and significantly outperforms conventional ICL baselines. Improvements stem from better emotion-aligned representations and reduced errors from emotionally mismatched exemplars.", "conclusion": "Decision-making is a critical, under-explored component of emotion recognition with ICL. Modeling ICL as prototype matching and aligning examples by emotion (with soft labels and multi-angle exclusion) improves both reasoning quality and final predictions; EICL is an effective, general strategy for fine-grained emotion tasks."}}
{"id": "2510.06275", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06275", "abs": "https://arxiv.org/abs/2510.06275", "authors": ["Ranjan Mishra", "Julian I. Bibo", "Quinten van Engelen", "Henk Schaapman"], "title": "Reproducibility Study of \"XRec: Large Language Models for Explainable Recommendation\"", "comment": null, "summary": "In this study, we reproduced the work done in the paper \"XRec: Large Language\nModels for Explainable Recommendation\" by Ma et al. (2024). The original\nauthors introduced XRec, a model-agnostic collaborative instruction-tuning\nframework that enables large language models (LLMs) to provide users with\ncomprehensive explanations of generated recommendations. Our objective was to\nreplicate the results of the original paper, albeit using Llama 3 as the LLM\nfor evaluation instead of GPT-3.5-turbo. We built on the source code provided\nby Ma et al. (2024) to achieve our goal. Our work extends the original paper by\nmodifying the input embeddings or deleting the output embeddings of XRec's\nMixture of Experts module. Based on our results, XRec effectively generates\npersonalized explanations and its stability is improved by incorporating\ncollaborative information. However, XRec did not consistently outperform all\nbaseline models in every metric. Our extended analysis further highlights the\nimportance of the Mixture of Experts embeddings in shaping the explanation\nstructures, showcasing how collaborative signals interact with language\nmodeling. Through our work, we provide an open-source evaluation implementation\nthat enhances accessibility for researchers and practitioners alike. Our\ncomplete code repository can be found at\nhttps://github.com/julianbibo/xrec-reproducibility.", "AI": {"tldr": "Reproduces XRec with Llama 3, probes Mixture-of-Experts (MoE) embeddings, and releases an open-source evaluation; finds XRec yields personalized, more stable explanations via collaborative signals but doesn\u2019t dominate all baselines.", "motivation": "To validate and extend XRec\u2019s claims about explainable recommendations by testing with a different LLM (Llama 3 vs. GPT\u20113.5\u2011turbo), assess the role of collaborative information and MoE embeddings, and provide a reproducible, accessible evaluation toolkit.", "method": "Built on Ma et al.\u2019s codebase, swapped the evaluation LLM to Llama 3, and conducted ablations by modifying input embeddings and removing output embeddings in XRec\u2019s MoE module; evaluated recommendation explanation quality, stability, and performance versus baselines.", "result": "XRec generates personalized explanations; collaborative signals improve explanation stability. However, XRec does not consistently outperform all baselines across metrics. Analyses show MoE embeddings strongly influence explanation structures, indicating interactions between collaborative and language modeling signals. An open-source evaluation implementation is provided.", "conclusion": "XRec is effective for personalized, explainable recommendations and benefits from collaborative information, but gains are metric- and setting-dependent. MoE embeddings are pivotal in shaping explanations. The released codebase facilitates further research and practical evaluation."}}
{"id": "2510.06308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06308", "abs": "https://arxiv.org/abs/2510.06308", "authors": ["Yi Xin", "Qi Qin", "Siqi Luo", "Kaiwen Zhu", "Juncheng Yan", "Yan Tai", "Jiayi Lei", "Yuewen Cao", "Keqi Wang", "Yibin Wang", "Jinbin Bai", "Qian Yu", "Dengyang Jiang", "Yuandong Pu", "Haoxing Chen", "Le Zhuo", "Junjun He", "Gen Luo", "Tianbin Li", "Ming Hu", "Jin Ye", "Shenglong Ye", "Bo Zhang", "Chang Xu", "Wenhai Wang", "Hongsheng Li", "Guangtao Zhai", "Tianfan Xue", "Bin Fu", "Xiaohong Liu", "Yu Qiao", "Yihao Liu"], "title": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "comment": "33 pages, 13 figures, 10 tables", "summary": "We introduce Lumina-DiMOO, an open-source foundational model for seamless\nmulti-modal generation and understanding. Lumina-DiMOO sets itself apart from\nprior unified models by utilizing a fully discrete diffusion modeling to handle\ninputs and outputs across various modalities. This innovative approach allows\nLumina-DiMOO to achieve higher sampling efficiency compared to previous\nautoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a\nbroad spectrum of multi-modal tasks, including text-to-image generation,\nimage-to-image generation (e.g., image editing, subject-driven generation, and\nimage inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves\nstate-of-the-art performance on multiple benchmarks, surpassing existing\nopen-source unified multi-modal models. To foster further advancements in\nmulti-modal and discrete diffusion model research, we release our code and\ncheckpoints to the community. Project Page:\nhttps://synbol.github.io/Lumina-DiMOO.", "AI": {"tldr": "Lumina-DiMOO is an open-source unified multi-modal model that uses fully discrete diffusion to handle both inputs and outputs across modalities, delivering faster sampling and state-of-the-art results for text-to-image, image-to-image, and image understanding tasks.", "motivation": "Create a single foundation model that can both generate and understand across modalities while overcoming sampling inefficiency and modeling limitations of autoregressive and hybrid AR-diffusion approaches.", "method": "Adopt a fully discrete diffusion framework that represents multi-modal inputs/outputs as discrete variables, enabling a unified architecture for text-to-image and image-to-image generation (editing, subject-driven, inpainting) as well as image understanding, with an emphasis on improved sampling efficiency; release code and checkpoints for reproducibility.", "result": "Reports state-of-the-art performance on multiple benchmarks and higher sampling efficiency compared to existing open-source unified multi-modal models.", "conclusion": "Fully discrete diffusion is an effective, efficient backbone for unified multi-modal generation and understanding; Lumina-DiMOO advances the state of the art and is released to support further research."}}
{"id": "2510.06674", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06674", "abs": "https://arxiv.org/abs/2510.06674", "authors": ["Cen", "Zhao", "Tiantian Zhang", "Hanchen Su", "Yufeng", "Zhang", "Shaowei Su", "Mingzhi Xu", "Yu", "Liu", "Wei Han", "Jeremy Werner", "Claire Na Cheng", "Yashar Mehdad"], "title": "Agent-in-the-Loop: A Data Flywheel for Continuous Improvement in LLM-based Customer Support", "comment": "EMNLP 2025 Industry Track submission (Paper #305). Preprint. Main\n  text within the 7-page industry limit (references/appendices excluded).\n  Contains multiple figures and tables", "summary": "We introduce an Agent-in-the-Loop (AITL) framework that implements a\ncontinuous data flywheel for iteratively improving an LLM-based customer\nsupport system. Unlike standard offline approaches that rely on batch\nannotations, AITL integrates four key types of annotations directly into live\ncustomer operations: (1) pairwise response preferences, (2) agent adoption and\nrationales, (3) knowledge relevance checks, and (4) identification of missing\nknowledge. These feedback signals seamlessly feed back into models' updates,\nreducing retraining cycles from months to weeks. Our production pilot involving\nUS-based customer support agents demonstrated significant improvements in\nretrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation quality\n(+8.4% helpfulness) and agent adoption rates (+4.5%). These results underscore\nthe effectiveness of embedding human feedback loops directly into operational\nworkflows to continuously refine LLM-based customer support system.", "AI": {"tldr": "Agent-in-the-Loop (AITL) creates a live feedback flywheel that continuously upgrades an LLM-based customer support system using four in-flow annotation types, shortening retraining cycles and boosting retrieval, generation quality, and agent adoption.", "motivation": "Offline, batch-annotated improvement loops are slow, infrequent, and disconnected from real operations, leading to stale models and low trust/adoption. The paper aims to create a faster, operationally embedded feedback mechanism to continuously refine LLM support systems.", "method": "Deploy an AITL framework in production that captures four live feedback signals: pairwise response preferences, agent adoption plus rationales, knowledge relevance checks, and identification of missing knowledge. These signals are fed into ongoing model and knowledge updates, forming a continuous data flywheel that replaces long offline retraining cycles.", "result": "In a production pilot with US-based support agents, AITL improved retrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation helpfulness (+8.4%), and agent adoption (+4.5%), while cutting retraining cycles from months to weeks.", "conclusion": "Embedding human feedback directly into operational workflows is effective for continuously refining LLM-based customer support, yielding faster iteration and measurable gains in retrieval, response quality, and system adoption."}}
{"id": "2510.06304", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06304", "abs": "https://arxiv.org/abs/2510.06304", "authors": ["Robin Kokot", "Wessel Poelman"], "title": "Type and Complexity Signals in Multilingual Question Representations", "comment": "Workshop on Multilingual Representation Learning at EMNLP 2025", "summary": "This work investigates how a multilingual transformer model represents\nmorphosyntactic properties of questions. We introduce the Question Type and\nComplexity (QTC) dataset with sentences across seven languages, annotated with\ntype information and complexity metrics including dependency length, tree\ndepth, and lexical density. Our evaluation extends probing methods to\nregression labels with selectivity controls to quantify gains in\ngeneralizability. We compare layer-wise probes on frozen Glot500-m (Imani et\nal., 2023) representations against subword TF-IDF baselines, and a fine-tuned\nmodel. Results show that statistical features classify questions effectively in\nlanguages with explicit marking, while neural probes capture fine-grained\nstructural complexity patterns better. We use these results to evaluate when\ncontextual representations outperform statistical baselines and whether\nparameter updates reduce the availability of pre-trained linguistic\ninformation.", "AI": {"tldr": "They introduce a multilingual dataset (QTC) and probing framework to study how a multilingual transformer encodes question types and structural complexity, finding that statistical cues suffice for overtly marked languages while contextual neural probes better capture fine-grained complexity; they also examine how fine-tuning affects the availability of pre-trained linguistic information.", "motivation": "To understand what multilingual transformers actually encode about questions\u2019 morphosyntax and structural complexity across languages, and to determine when contextual representations add value over simple statistical features, especially under fine-tuning.", "method": "Create the QTC dataset spanning seven languages with annotations for question type and complexity metrics (dependency length, tree depth, lexical density). Extend probing to handle regression targets with selectivity controls. Train layer-wise probes on frozen Glot500-m representations and compare against subword TF-IDF baselines and a fine-tuned model.", "result": "Statistical features classify question types well in languages with explicit morphological/lexical marking of questions. Neural probes over contextual representations better capture nuanced structural complexity patterns. Layer-wise analysis identifies regimes where contextual embeddings outperform statistical baselines; fine-tuning is analyzed for its impact on pre-trained linguistic information.", "conclusion": "Contextual models do not uniformly dominate: their advantage emerges for fine-grained structural complexity, while simple statistics can suffice for overtly marked question types. The QTC dataset and selective regression probing provide tools to assess representation quality and to gauge potential trade-offs introduced by fine-tuning."}}
{"id": "2510.06353", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06353", "abs": "https://arxiv.org/abs/2510.06353", "authors": ["Allen Tu", "Kartik Narayan", "Joshua Gleason", "Jennifer Xu", "Matthew Meyn", "Tom Goldstein", "Vishal M. Patel"], "title": "TransFIRA: Transfer Learning for Face Image Recognizability Assessment", "comment": "Project Page: https://transfira.github.io/", "summary": "Face recognition in unconstrained environments such as surveillance, video,\nand web imagery must contend with extreme variation in pose, blur,\nillumination, and occlusion, where conventional visual quality metrics fail to\npredict whether inputs are truly recognizable to the deployed encoder. Existing\nFIQA methods typically rely on visual heuristics, curated annotations, or\ncomputationally intensive generative pipelines, leaving their predictions\ndetached from the encoder's decision geometry. We introduce TransFIRA (Transfer\nLearning for Face Image Recognizability Assessment), a lightweight and\nannotation-free framework that grounds recognizability directly in embedding\nspace. TransFIRA delivers three advances: (i) a definition of recognizability\nvia class-center similarity (CCS) and class-center angular separation (CCAS),\nyielding the first natural, decision-boundary--aligned criterion for filtering\nand weighting; (ii) a recognizability-informed aggregation strategy that\nachieves state-of-the-art verification accuracy on BRIAR and IJB-C while nearly\ndoubling correlation with true recognizability, all without external labels,\nheuristics, or backbone-specific training; and (iii) new extensions beyond\nfaces, including encoder-grounded explainability that reveals how degradations\nand subject-specific factors affect recognizability, and the first\nrecognizability-aware body recognition assessment. Experiments confirm\nstate-of-the-art results on faces, strong performance on body recognition, and\nrobustness under cross-dataset shifts. Together, these contributions establish\nTransFIRA as a unified, geometry-driven framework for recognizability\nassessment -- encoder-specific, accurate, interpretable, and extensible across\nmodalities -- significantly advancing FIQA in accuracy, explainability, and\nscope.", "AI": {"tldr": "TransFIRA is an annotation-free, encoder-grounded FIQA framework that defines recognizability in embedding space (via CCS/CCAS), enabling effective filtering/weighting and recognizability-aware aggregation to achieve SOTA verification on BRIAR and IJB-C, higher correlation with true recognizability, interpretability, cross-dataset robustness, and extensions to body recognition.", "motivation": "Face recognition in the wild breaks conventional image-quality heuristics: pose, blur, illumination, and occlusion can make faces unrecognizable to a specific encoder even when visuals look \u201cgood.\u201d Existing FIQA relies on heuristics, curated labels, or heavy generative models and is detached from the recognition model\u2019s decision geometry. The goal is to align quality assessment with the deployed encoder\u2019s actual decision boundaries, without extra labels or costly pipelines.", "method": "Ground recognizability directly in the embedding space using two metrics: (1) class-center similarity (CCS) and (2) class-center angular separation (CCAS). These provide a decision-boundary\u2013aligned criterion for filtering and weighting samples. Build a recognizability-informed aggregation strategy for verification without external labels or backbone-specific training. Provide encoder-grounded explainability to localize degradations/subject factors affecting recognizability and extend the approach to body recognition assessment.", "result": "The approach delivers state-of-the-art verification accuracy on BRIAR and IJB-C, nearly doubles correlation with true recognizability, shows strong performance for body recognition, and remains robust under cross-dataset shifts\u2014all achieved without extra annotations, heuristics, or backbone-specific tuning.", "conclusion": "TransFIRA offers a unified, geometry-driven, annotation-free FIQA that is encoder-specific, accurate, interpretable, and extensible across modalities, substantially advancing the accuracy, explainability, and scope of recognizability assessment."}}
{"id": "2510.06711", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06711", "abs": "https://arxiv.org/abs/2510.06711", "authors": ["Batu El", "Mert Yuksekgonul", "James Zou"], "title": "Inefficiencies of Meta Agents for Agent Design", "comment": null, "summary": "Recent works began to automate the design of agentic systems using\nmeta-agents that propose and iteratively refine new agent architectures. In\nthis paper, we examine three key challenges in a common class of meta-agents.\nFirst, we investigate how a meta-agent learns across iterations and find that\nsimply expanding the context with all previous agents, as proposed by previous\nworks, performs worse than ignoring prior designs entirely. We show that the\nperformance improves with an evolutionary approach. Second, although the\nmeta-agent designs multiple agents during training, it typically commits to a\nsingle agent at test time. We find that the designed agents have low behavioral\ndiversity, limiting the potential for their complementary use. Third, we assess\nwhen automated design is economically viable. We find that only in a few\ncases--specifically, two datasets--the overall cost of designing and deploying\nthe agents is lower than that of human-designed agents when deployed on over\n15,000 examples. In contrast, the performance gains for other datasets do not\njustify the design cost, regardless of scale.", "AI": {"tldr": "Automated meta-agents for designing agent architectures face three core issues: naive iteration memory hurts performance, evolutionary search helps; designed agents lack behavioral diversity, limiting ensemble gains; and cost-effectiveness appears only in rare, large-scale deployments on select datasets.", "motivation": "Assess whether meta-agent-driven automated agent design actually improves performance, diversity, and economic viability over human-designed systems, and understand how to make iterative learning more effective.", "method": "Empirical analysis of a common meta-agent pipeline. Compare three iteration strategies: (1) expanding the prompt with all prior designs, (2) ignoring history, and (3) an evolutionary approach. Measure behavioral diversity among designed agents and evaluate total cost of design plus deployment versus human-designed baselines across multiple datasets and scales.", "result": "Including all prior designs in context underperforms even ignoring them; an evolutionary approach yields better performance. The designed agents show low behavioral diversity, limiting complementary use at test time. Economically, automated design beats human-designed agents only on two datasets and only when deployed at very large scale (>15,000 examples); elsewhere, gains do not offset design costs.", "conclusion": "For meta-agent design, simple context accumulation is counterproductive; evolutionary search is preferable. The low diversity of produced agents curtails ensemble benefits, and the return on investment is narrow\u2014automated design is only worthwhile in specific, large-scale settings. Improving diversity and reducing design costs are key to broader viability."}}
{"id": "2510.06354", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06354", "abs": "https://arxiv.org/abs/2510.06354", "authors": ["Ingroj Shrestha", "Padmini Srinivasan"], "title": "LLM Bias Detection and Mitigation through the Lens of Desired Distributions", "comment": "Accepted to EMNLP 2025", "summary": "Although prior work on bias mitigation has focused on promoting social\nequality and demographic parity, less attention has been given to aligning\nLLM's outputs to desired distributions. For example, we might want to align a\nmodel with real-world distributions to support factual grounding. Thus, we\ndefine bias as deviation from a desired distribution, which may be an equal or\nreal-world distribution, depending on application goals. We propose a weighted\nadaptive loss based fine-tuning method that aligns LLM's gender-profession\noutput distribution with the desired distribution, while preserving language\nmodeling capability. Using 3 profession sets -- male-dominated,\nfemale-dominated, and gender-balanced -- derived from U.S. labor statistics\n(2024), we assess both our adaptive method for reflecting reality and a\nnon-adaptive variant for equality. Across three masked language models, bias is\nobserved under both distributions. We achieve near-complete mitigation under\nequality and 30-75% reduction under real-world settings. Autoregressive LLMs\nshow no bias under equality but notable bias under real-world settings, with\nthe Llama Instruct models (3.2-3B, 3.1-8B) achieving a 50-62% reduction.", "AI": {"tldr": "Reframes bias as deviation from a target distribution (equality or real-world) and introduces a weighted adaptive loss for fine-tuning LLMs to match desired gender\u2013profession distributions, achieving near-complete parity alignment and substantial\u2014but not total\u2014alignment to real-world statistics.", "motivation": "Equality-centric debiasing may conflict with applications that require outputs reflecting real-world frequencies for factual grounding. The authors seek a principled way to align LLM outputs to either equal or real-world distributions while preserving general language modeling performance.", "method": "Define bias as divergence from a specified target distribution. Propose a weighted adaptive loss for fine-tuning that reweights model outputs to match that target. Evaluate on gender\u2013profession associations using three profession sets (male-dominated, female-dominated, balanced) derived from 2024 U.S. labor statistics. Compare adaptive (real-world alignment) vs. non-adaptive (equality) variants across masked LMs and assess autoregressive LLMs as well.", "result": "Across three masked LMs, bias exists under both equality and real-world targets. Fine-tuning yields near-complete mitigation under equality and 30\u201375% bias reduction under real-world targets. Autoregressive LLMs show no bias under equality but show notable bias under real-world targets; Llama Instruct (3.2-3B, 3.1-8B) achieves 50\u201362% reduction.", "conclusion": "Bias can be operationalized as deviation from a user-specified distribution, enabling controlled alignment. The proposed weighted adaptive loss effectively aligns to equality and partially to real-world distributions while purportedly preserving language modeling ability. Model family matters: masked LMs and autoregressive LLMs behave differently, and full real-world alignment remains challenging."}}
{"id": "2510.06440", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06440", "abs": "https://arxiv.org/abs/2510.06440", "authors": ["Carly Sutter", "Kara J. Sulia", "Nick P. Bassill", "Christopher D. Wirz", "Christopher D. Thorncroft", "Jay C. Rothenberger", "Vanessa Przybylo", "Mariana G. Cains", "Jacob Radford", "David Aaron Evans"], "title": "Road Surface Condition Detection with Machine Learning using New York State Department of Transportation Camera Images and Weather Forecast Data", "comment": null, "summary": "The New York State Department of Transportation (NYSDOT) has a network of\nroadside traffic cameras that are used by both the NYSDOT and the public to\nobserve road conditions. The NYSDOT evaluates road conditions by driving on\nroads and observing live cameras, tasks which are labor-intensive but necessary\nfor making critical operational decisions during winter weather events.\nHowever, machine learning models can provide additional support for the NYSDOT\nby automatically classifying current road conditions across the state. In this\nstudy, convolutional neural networks and random forests are trained on camera\nimages and weather data to predict road surface conditions. Models are trained\non a hand-labeled dataset of ~22,000 camera images, each classified by human\nlabelers into one of six road surface conditions: severe snow, snow, wet, dry,\npoor visibility, or obstructed. Model generalizability is prioritized to meet\nthe operational needs of the NYSDOT decision makers, and the weather-related\nroad surface condition model in this study achieves an accuracy of 81.5% on\ncompletely unseen cameras.", "AI": {"tldr": "Trains CNNs (images) and random forests (weather features) on ~22k labeled NYSDOT traffic-camera images to auto-classify six road surface conditions; emphasizes generalization and attains 81.5% accuracy on completely unseen cameras.", "motivation": "Manual road condition assessment via patrols and live camera monitoring is labor-intensive and time-critical during winter weather. An automated, scalable classifier could provide consistent, near-real-time statewide situational awareness to support NYSDOT operational decisions.", "method": "Assemble a hand-labeled dataset (~22,000 images) from NYSDOT roadside cameras with six classes (severe snow, snow, wet, dry, poor visibility, obstructed). Train convolutional neural networks on images and random forest models using weather data; combine modalities to predict current road surface condition. Evaluate with a camera-level holdout to test generalization to unseen cameras, prioritizing operational robustness.", "result": "The weather-related road surface condition model achieves 81.5% accuracy when evaluated on entirely unseen cameras, indicating cross-camera generalization. The system distinguishes six classes relevant to winter operations.", "conclusion": "Automated classification of road surface conditions from public cameras, augmented by weather data, can reliably support NYSDOT decision-making during winter events at scale. The achieved accuracy on unseen cameras suggests deployability; further gains may come from more data, handling edge cases (visibility/obstruction), temporal modeling, and uncertainty calibration."}}
{"id": "2510.06742", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06742", "abs": "https://arxiv.org/abs/2510.06742", "authors": ["Ali Sarabadani", "Kheirolah Rahsepar Fard"], "title": "MultiCNKG: Integrating Cognitive Neuroscience, Gene, and Disease Knowledge Graphs Using Large Language Models", "comment": null, "summary": "The advent of large language models (LLMs) has revolutionized the integration\nof knowledge graphs (KGs) in biomedical and cognitive sciences, overcoming\nlimitations in traditional machine learning methods for capturing intricate\nsemantic links among genes, diseases, and cognitive processes. We introduce\nMultiCNKG, an innovative framework that merges three key knowledge sources: the\nCognitive Neuroscience Knowledge Graph (CNKG) with 2.9K nodes and 4.3K edges\nacross 9 node types and 20 edge types; Gene Ontology (GO) featuring 43K nodes\nand 75K edges in 3 node types and 4 edge types; and Disease Ontology (DO)\ncomprising 11.2K nodes and 8.8K edges with 1 node type and 2 edge types.\nLeveraging LLMs like GPT-4, we conduct entity alignment, semantic similarity\ncomputation, and graph augmentation to create a cohesive KG that interconnects\ngenetic mechanisms, neurological disorders, and cognitive functions. The\nresulting MultiCNKG encompasses 6.9K nodes across 5 types (e.g., Genes,\nDiseases, Cognitive Processes) and 11.3K edges spanning 7 types (e.g., Causes,\nAssociated with, Regulates), facilitating a multi-layered view from molecular\nto behavioral domains. Assessments using metrics such as precision (85.20%),\nrecall (87.30%), coverage (92.18%), graph consistency (82.50%), novelty\ndetection (40.28%), and expert validation (89.50%) affirm its robustness and\ncoherence. Link prediction evaluations with models like TransE (MR: 391, MRR:\n0.411) and RotatE (MR: 263, MRR: 0.395) show competitive performance against\nbenchmarks like FB15k-237 and WN18RR. This KG advances applications in\npersonalized medicine, cognitive disorder diagnostics, and hypothesis\nformulation in cognitive neuroscience.", "AI": {"tldr": "MultiCNKG integrates cognitive neuroscience, gene, and disease ontologies into a single KG using LLM-assisted alignment and augmentation, yielding a 6.9K-node, 11.3K-edge graph with strong intrinsic quality metrics and competitive link-prediction performance for biomedical\u2013cognitive applications.", "motivation": "Existing KGs in neuroscience and biomedicine are siloed and traditional ML struggles to capture cross-domain, high-order semantics among genes, diseases, and cognitive processes. A unified, multi-layered KG could enable reasoning from molecular mechanisms to cognition, and recent LLMs can help align entities, infer semantic links, and fill gaps across heterogeneous sources.", "method": "Combine three sources\u2014CNKG (2.9K nodes/4.3K edges; 9 node and 20 edge types), Gene Ontology (43K/75K; 3 node and 4 edge types), and Disease Ontology (11.2K/8.8K; 1 node and 2 edge types). Use GPT-4 to perform entity alignment across ontologies, compute semantic similarity, and augment the graph with inferred relations. Construct MultiCNKG with 5 node types (e.g., Genes, Diseases, Cognitive Processes) and 7 relation types (e.g., Causes, Associated with, Regulates). Evaluate with precision, recall, coverage, graph consistency, novelty detection, and expert validation, and test link prediction using TransE and RotatE against standard benchmarks (FB15k-237, WN18RR).", "result": "MultiCNKG totals 6.9K nodes and 11.3K edges, offering a layered representation from molecular to behavioral levels. Reported metrics: precision 85.20%, recall 87.30%, coverage 92.18%, graph consistency 82.50%, novelty detection 40.28%, and expert validation 89.50%. Link prediction achieves MR 391/MRR 0.411 with TransE and MR 263/MRR 0.395 with RotatE, claimed as competitive relative to FB15k-237 and WN18RR baselines.", "conclusion": "LLM-guided integration yields a coherent, cross-domain KG that connects genes, diseases, and cognitive processes, supporting applications in personalized medicine, cognitive disorder diagnostics, and hypothesis generation. Results suggest robustness and utility, though broader validation and scalability to larger or continuously updated biomedical corpora remain important next steps."}}
{"id": "2510.06370", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06370", "abs": "https://arxiv.org/abs/2510.06370", "authors": ["Kshitish Ghate", "Andy Liu", "Devansh Jain", "Taylor Sorensen", "Atoosa Kasirzadeh", "Aylin Caliskan", "Mona T. Diab", "Maarten Sap"], "title": "EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preference", "comment": "Preprint under review", "summary": "As large language models (LLMs) are deployed globally, creating pluralistic\nsystems that can accommodate the diverse preferences and values of users\nworldwide becomes essential. We introduce EVALUESTEER, a benchmark to measure\nLLMs' and reward models' (RMs) steerability towards users' value and stylistic\npreference profiles grounded in psychology and human-LLM interaction\nliterature. To address the gap in existing datasets that do not support\ncontrolled evaluations of RM steering, we synthetically generated 165,888\npreference pairs -- systematically varying pairs along 4 value dimensions\n(traditional, secular-rational, survival, and self-expression) and 4 style\ndimensions (verbosity, readability, confidence, and warmth). We use EVALUESTEER\nto evaluate whether, given a user profile and a pair of candidate value-laden\nand style-laden responses, LLMs and RMs are able to select the output that\naligns with the user's preferences. We evaluate six open-source and proprietary\nLLMs and RMs under sixteen systematic prompting conditions and six preference\ncomparison scenarios. Notably, our results show that, when given the user's\nfull profile of values and stylistic preferences, the best models achieve <75%\naccuracy at choosing the correct response, in contrast to >99% accuracy when\nonly relevant style and value preferences are provided. EVALUESTEER thus\nhighlights the limitations of current RMs at identifying and adapting to\nrelevant user profile information, and provides a challenging testbed for\ndeveloping RMs that can be steered towards diverse human values and\npreferences.", "AI": {"tldr": "EVALUESTEER is a benchmark that tests how well LLMs and reward models can be steered to match users\u2019 value and stylistic preferences; models struggle (<75% accuracy) when given full profiles but excel (>99%) when only relevant preferences are provided.", "motivation": "LLMs are deployed globally and must serve users with diverse values and stylistic preferences. Existing datasets don\u2019t allow controlled, granular evaluation of how reward models/LLMs prioritize and adapt to such preferences, especially distinguishing relevant from irrelevant profile information.", "method": "Construct a synthetic, controlled benchmark of 165,888 preference pairs varying along 4 value dimensions (traditional, secular-rational, survival, self-expression) and 4 style dimensions (verbosity, readability, confidence, warmth). Given a user profile and two candidate responses (value- and style-laden), test whether LLMs/RMs choose the response aligned with the profile. Evaluate six open-source and proprietary models under 16 prompting conditions and 6 comparison scenarios.", "result": "When models receive full user profiles covering many values and styles, the best achieve under 75% accuracy in selecting the aligned response. If provided only the preferences relevant to the pairwise choice, accuracy exceeds 99%. This indicates difficulty in filtering and weighting relevant attributes within complex profiles.", "conclusion": "Current reward models and LLMs are limited at identifying and prioritizing relevant user preferences within rich profiles, hindering pluralistic steering. EVALUESTEER offers a challenging, controlled testbed to drive development of models that more robustly adapt to diverse human values and styles."}}
{"id": "2510.06460", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06460", "abs": "https://arxiv.org/abs/2510.06460", "authors": ["Piyush Dashpute", "Niki Nezakati", "Wolfgang Heidrich", "Vishwanath Saragadam"], "title": "TDiff: Thermal Plug-And-Play Prior with Patch-Based Diffusion", "comment": null, "summary": "Thermal images from low-cost cameras often suffer from low resolution, fixed\npattern noise, and other localized degradations. Available datasets for thermal\nimaging are also limited in both size and diversity. To address these\nchallenges, we propose a patch-based diffusion framework (TDiff) that leverages\nthe local nature of these distortions by training on small thermal patches. In\nthis approach, full-resolution images are restored by denoising overlapping\npatches and blending them using smooth spatial windowing. To our knowledge,\nthis is the first patch-based diffusion framework that models a learned prior\nfor thermal image restoration across multiple tasks. Experiments on denoising,\nsuper-resolution, and deblurring demonstrate strong results on both simulated\nand real thermal data, establishing our method as a unified restoration\npipeline.", "AI": {"tldr": "TDiff is a patch-based diffusion framework that restores thermal images by denoising overlapping patches and smoothly blending them, achieving strong performance across denoising, super-resolution, and deblurring on simulated and real data.", "motivation": "Low-cost thermal cameras produce low-resolution images with fixed pattern noise and localized degradations, and available thermal datasets are small and limited. A method that leverages the locality of distortions and can be trained with limited data is needed to provide a unified restoration solution.", "method": "Train a diffusion model on small thermal patches to learn a prior tailored to local thermal distortions. At inference, process full-resolution images by denoising overlapping patches and blending them via smooth spatial windowing. This patch-based diffusion prior is applied uniformly to multiple tasks: denoising, super-resolution, and deblurring.", "result": "Across denoising, super-resolution, and deblurring tasks, the approach delivers strong results on both simulated and real thermal datasets, indicating robustness and generality. The abstract implies state-of-the-art or highly competitive performance, though no metrics are provided.", "conclusion": "A unified, patch-based diffusion prior effectively restores thermal images across multiple tasks by exploiting the local nature of thermal degradations, offering a practical pipeline especially suited to limited-data regimes."}}
{"id": "2510.06756", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06756", "abs": "https://arxiv.org/abs/2510.06756", "authors": ["Dennis Gross", "Helge Spieker", "Arnaud Gotlieb"], "title": "Verifying Memoryless Sequential Decision-making of Large Language Models", "comment": null, "summary": "We introduce a tool for rigorous and automated verification of large language\nmodel (LLM)- based policies in memoryless sequential decision-making tasks.\nGiven a Markov decision process (MDP) representing the sequential\ndecision-making task, an LLM policy, and a safety requirement expressed as a\nPCTL formula, our approach incrementally constructs only the reachable portion\nof the MDP guided by the LLM's chosen actions. Each state is encoded as a\nnatural language prompt, the LLM's response is parsed into an action, and\nreachable successor states by the policy are expanded. The resulting formal\nmodel is checked with Storm to determine whether the policy satisfies the\nspecified safety property. In experiments on standard grid world benchmarks, we\nshow that open source LLMs accessed via Ollama can be verified when\ndeterministically seeded, but generally underperform deep reinforcement\nlearning baselines. Our tool natively integrates with Ollama and supports\nPRISM-specified tasks, enabling continuous benchmarking in user-specified\nsequential decision-making tasks and laying a practical foundation for formally\nverifying increasingly capable LLMs.", "AI": {"tldr": "They present a practical tool that formally verifies safety properties of LLM-driven policies on MDP tasks by building only the policy-reachable sub-MDP from natural-language state prompts and then model-checking PCTL properties with Storm; it integrates with Ollama/PRISM and shows verifiability under deterministic seeding, though LLM policies underperform DRL baselines.", "motivation": "LLMs are increasingly used as decision-making controllers but lack rigorous guarantees. Traditional model checking requires explicit models and faces state explosion. The authors seek a way to automatically and soundly verify safety properties of LLM policies in sequential decision-making without constructing the full MDP and while bridging natural language interfaces with formal verification.", "method": "For a given MDP, safety property (PCTL), and an LLM policy, the tool iteratively constructs only the portion of the MDP reachable under the policy: encode each state as a natural-language prompt, query the LLM to obtain an action, parse it, and expand successors per the environment dynamics. The resulting policy-induced MDP fragment is checked using the Storm model checker. The system integrates with PRISM specifications and accesses open-source LLMs via Ollama; deterministic seeding is used to stabilize the policy behavior.", "result": "On grid-world benchmarks, the tool can verify open-source LLM policies when they are deterministically seeded. However, these LLM policies generally perform worse than deep RL baselines. The implementation natively supports Ollama and PRISM tasks, enabling continuous benchmarking of LLM controllers on user-defined environments.", "conclusion": "It is feasible to rigorously verify safety properties of memoryless LLM policies by constructing and checking only the reachable sub-MDP. The approach provides a practical foundation for formal verification of LLM controllers, but current limitations include policy nondeterminism, performance gaps versus DRL, and reliance on memoryless settings; addressing these will be key for broader applicability."}}
{"id": "2510.06371", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.06371", "abs": "https://arxiv.org/abs/2510.06371", "authors": ["Firoj Alam", "Ali Ezzat Shahroor", "Md. Arid Hasan", "Zien Sheikh Ali", "Hunzalah Hassan Bhatti", "Mohamed Bayan Kmainasi", "Shammur Absar Chowdhury", "Basel Mousi", "Fahim Dalvi", "Nadir Durrani", "Natasa Milic-Frayling"], "title": "EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA", "comment": "Multimodal Foundation Models, Large Language Models, Native,\n  Multilingual, Language Diversity, Contextual Understanding, Culturally\n  Informed", "summary": "Large-scale multimodal models achieve strong results on tasks like Visual\nQuestion Answering (VQA), but they often fail when queries require culturally\ngrounded, everyday knowledge, particularly in low-resource and underrepresented\nlanguages. To bridge this gap, we introduce Everyday Multimodal and\nMultilingual QA (EverydayMMQA), a framework for creating large-scale,\nculturally-grounded datasets for spoken and visual question answering (SVQA).\nUsing this framework, we developed OASIS, a multimodal dataset integrating\nspeech, images, and text. With over ~0.92M images and 14.8M QA pairs, OASIS\ncontains 3.7M spoken questions, enabling four unique input combinations:\nspeech-only, text-only, speech+image, and text+image. Focused on English and\nArabic varieties, 18 countries, the dataset content is curated to reflect\ndiverse, real-world situations. OASIS tests models on tasks beyond object\nrecognition that involve pragmatic, commonsense, and culturally aware\nreasoning. We benchmarked four closed-source models, three open-source models,\nand one fine-tuned model. EverydayMMQA and OASIS together provide a benchmark\nand training dataset for building multimodal LLMs for a comprehensive set of\neveryday tasks within cultural contexts. The framework and dataset will be made\npublicly available to the community.", "AI": {"tldr": "They present EverydayMMQA, a framework, and OASIS, a large culturally grounded multimodal QA dataset (speech, images, text) focused on English and Arabic across 18 countries, enabling speech/text with and without images. With ~0.92M images, 14.8M QA pairs, and 3.7M spoken questions, they benchmark multiple models to set baselines for pragmatic, commonsense, and culture-aware reasoning, and will release the resources publicly.", "motivation": "Current large multimodal models often fail on culturally grounded, everyday reasoning\u2014especially for low-resource and underrepresented languages\u2014due to a lack of appropriate multimodal datasets and benchmarks that include speech and visual context in diverse cultural settings.", "method": "Introduce the EverydayMMQA framework to systematically create culturally grounded spoken and visual QA data. Using it, they build OASIS: a dataset integrating speech, images, and text with four input modes (speech-only, text-only, speech+image, text+image), curated from 18 countries and focused on English and Arabic varieties. The tasks go beyond object recognition to require pragmatic, commonsense, and culturally aware reasoning. They benchmark four closed-source, three open-source, and one fine-tuned model.", "result": "Constructed OASIS with ~0.92M images, 14.8M QA pairs, and 3.7M spoken questions covering diverse real-world situations and modalities. Established baseline performance by evaluating multiple open- and closed-source models, demonstrating the dataset\u2019s breadth and difficulty for culture-aware, pragmatic reasoning (no specific metrics given in the abstract).", "conclusion": "EverydayMMQA and OASIS provide a public benchmark and training resource for building and evaluating multimodal LLMs on everyday, culturally grounded tasks\u2014especially benefiting underrepresented languages\u2014thereby addressing key gaps in current multimodal reasoning capabilities."}}
{"id": "2510.06469", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06469", "abs": "https://arxiv.org/abs/2510.06469", "authors": ["Oindrila Saha", "Vojtech Krs", "Radomir Mech", "Subhransu Maji", "Kevin Blackburn-Matzen", "Matheus Gadelha"], "title": "SIGMA-GEN: Structure and Identity Guided Multi-subject Assembly for Image Generation", "comment": "Webpage: https://oindrilasaha.github.io/SIGMA-Gen/", "summary": "We present SIGMA-GEN, a unified framework for multi-identity preserving image\ngeneration. Unlike prior approaches, SIGMA-GEN is the first to enable\nsingle-pass multi-subject identity-preserved generation guided by both\nstructural and spatial constraints. A key strength of our method is its ability\nto support user guidance at various levels of precision -- from coarse 2D or 3D\nboxes to pixel-level segmentations and depth -- with a single model. To enable\nthis, we introduce SIGMA-SET27K, a novel synthetic dataset that provides\nidentity, structure, and spatial information for over 100k unique subjects\nacross 27k images. Through extensive evaluation we demonstrate that SIGMA-GEN\nachieves state-of-the-art performance in identity preservation, image\ngeneration quality, and speed. Code and visualizations at\nhttps://oindrilasaha.github.io/SIGMA-Gen/", "AI": {"tldr": "SIGMA-GEN is a single-pass, multi-subject image generator that preserves individual identities while following flexible structural/spatial guidance, trained with a new synthetic dataset (SIGMA-SET27K). It reports state-of-the-art identity fidelity, quality, and speed.", "motivation": "Existing image generators struggle to compose multiple distinct subjects in one pass while reliably preserving each identity, especially when users provide diverse forms of spatial or structural guidance. A unified approach that handles coarse-to-fine constraints (boxes, depth, segments) without separate models is needed.", "method": "Introduce SIGMA-GEN, a unified conditioning framework that accepts varied guidance signals\u2014from 2D/3D boxes to pixel-level segmentation and depth\u2014alongside identity cues to generate multi-subject images in a single pass. They also create SIGMA-SET27K, a synthetic dataset with identity, structure, and spatial annotations for 100k+ subjects across 27k images to train and evaluate the model.", "result": "Across extensive evaluations, SIGMA-GEN attains state-of-the-art performance in identity preservation, overall image quality, and inference speed for multi-subject generation tasks. The new SIGMA-SET27K dataset supports these results and enables rigorous benchmarking.", "conclusion": "A single model can robustly compose multiple identities under diverse structural/spatial constraints, advancing controllable multi-subject image generation. The introduced dataset and framework set a new bar for identity fidelity, quality, and efficiency."}}
{"id": "2510.06761", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06761", "abs": "https://arxiv.org/abs/2510.06761", "authors": ["Zhi Zhang", "Yan Liu", "Zhejing Hu", "Gong Chen", "Sheng-hua Zhong", "Jiannong Cao"], "title": "Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration", "comment": null, "summary": "Automating the end-to-end scientific research process poses a fundamental\nchallenge: it requires both evolving high-level plans that are novel and sound,\nand executing these plans correctly amidst dynamic and uncertain conditions. To\naddress this bilevel challenge, we propose a novel Double-Loop Multi-Agent\n(DLMA) framework to solve the given research problem automatically. The leader\nloop, composed of professor agents, is responsible for evolving research plans.\nIt employs an evolutionary algorithm through involvement, improvement, and\nintegration meetings to iteratively generate and refine a pool of research\nproposals, exploring the solution space effectively. The follower loop,\ncomposed of doctoral student agents, is responsible for executing the\nbest-evolved plan. It dynamically adjusts the plan during implementation via\npre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is\nwell-supported by contextual and external observations. Extensive experiments\non benchmarks like ACLAward and Laboratory show that DLMA generates research\npapers that achieve state-of-the-art scores in automated evaluation,\nsignificantly outperforming strong baselines. Ablation studies confirm the\ncritical roles of both loops, with evolution driving novelty and execution\nensuring soundness.", "AI": {"tldr": "DLMA is a two-loop multi-agent system that evolves high-level research plans (leader/professor loop) and executes them adaptively (follower/doctoral loop), achieving state-of-the-art automated paper-generation results on benchmarks.", "motivation": "End-to-end automation of scientific research demands both (1) generating novel, sound high-level plans and (2) reliably executing those plans under uncertainty\u2014two tightly coupled yet distinct challenges that current systems struggle to address together.", "method": "A Double-Loop Multi-Agent framework: (1) Leader loop with professor agents uses an evolutionary algorithm via involvement, improvement, and integration meetings to iteratively generate and refine a pool of research proposals; (2) Follower loop with doctoral student agents executes the best plan and dynamically adjusts it through pre-hoc and post-hoc meetings, grounding steps (drafting, coding) in contextual and external observations.", "result": "On ACLAward and Laboratory benchmarks, DLMA generates research papers that obtain state-of-the-art scores under automated evaluation and significantly outperform strong baselines. Ablation studies show the leader loop drives novelty and the follower loop ensures soundness.", "conclusion": "Coupling evolutionary plan generation with adaptive execution enables robust, end-to-end automated research. The two loops are complementary\u2014evolution fosters novelty while execution secures correctness\u2014yielding superior performance on standard benchmarks."}}
{"id": "2510.06378", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06378", "abs": "https://arxiv.org/abs/2510.06378", "authors": ["Angie Boggust", "Donghao Ren", "Yannick Assogba", "Dominik Moritz", "Arvind Satyanarayan", "Fred Hohman"], "title": "Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language", "comment": null, "summary": "Automated interpretability aims to translate large language model (LLM)\nfeatures into human understandable descriptions. However, these natural\nlanguage feature descriptions are often vague, inconsistent, and require manual\nrelabeling. In response, we introduce semantic regexes, structured language\ndescriptions of LLM features. By combining primitives that capture linguistic\nand semantic feature patterns with modifiers for contextualization,\ncomposition, and quantification, semantic regexes produce precise and\nexpressive feature descriptions. Across quantitative benchmarks and qualitative\nanalyses, we find that semantic regexes match the accuracy of natural language\nwhile yielding more concise and consistent feature descriptions. Moreover,\ntheir inherent structure affords new types of analyses, including quantifying\nfeature complexity across layers, scaling automated interpretability from\ninsights into individual features to model-wide patterns. Finally, in user\nstudies, we find that semantic regex descriptions help people build accurate\nmental models of LLM feature activations.", "AI": {"tldr": "Proposes \u201csemantic regexes,\u201d a structured, composable language for describing LLM features that matches NL description accuracy while being more precise, concise, and consistent, enabling scalable, model-wide interpretability analyses and better human understanding.", "motivation": "Natural-language feature labels in automated interpretability are often vague, inconsistent, and labor-intensive to relabel, limiting precision, comparability, and scalability of analyses.", "method": "Define semantic regexes: a set of primitives for linguistic/semantic patterns with modifiers for context, composition, and quantification to describe LLM features. Evaluate against NL descriptions via quantitative benchmarks and qualitative studies; use the structured form to analyze feature complexity across layers and aggregate from individual features to model-wide patterns; run user studies on mental model formation.", "result": "Semantic regexes achieve accuracy on par with natural-language descriptions but are more concise and consistent. Their structure enables new analyses (e.g., quantifying feature complexity across layers, moving from single-feature insights to model-wide trends). User studies indicate they help people form accurate mental models of feature activations.", "conclusion": "Structured, composable semantic regexes improve the fidelity and consistency of feature descriptions and unlock scalable automated interpretability analyses, offering practical benefits over free-form natural language for both researchers and end users."}}
{"id": "2510.06487", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06487", "abs": "https://arxiv.org/abs/2510.06487", "authors": ["Jack Roberts", "Jeova Farias Sales Rocha Neto"], "title": "Superpixel Integrated Grids for Fast Image Segmentation", "comment": null, "summary": "Superpixels have long been used in image simplification to enable more\nefficient data processing and storage. However, despite their computational\npotential, their irregular spatial distribution has often forced deep learning\napproaches to rely on specialized training algorithms and architectures,\nundermining the original motivation for superpixelations. In this work, we\nintroduce a new superpixel-based data structure, SIGRID (Superpixel-Integrated\nGrid), as an alternative to full-resolution images in segmentation tasks. By\nleveraging classical shape descriptors, SIGRID encodes both color and shape\ninformation of superpixels while substantially reducing input dimensionality.\nWe evaluate SIGRIDs on four benchmark datasets using two popular convolutional\nsegmentation architectures. Our results show that, despite compressing the\noriginal data, SIGRIDs not only match but in some cases surpass the performance\nof pixel-level representations, all while significantly accelerating model\ntraining. This demonstrates that SIGRIDs achieve a favorable balance between\naccuracy and computational efficiency.", "AI": {"tldr": "They propose SIGRID, a superpixel-integrated grid representation that encodes color and shape descriptors into a regular, lower-dimensional input for CNN segmentation, matching or surpassing pixel-level accuracy while training substantially faster on four benchmarks and two standard architectures.", "motivation": "Superpixels can reduce data size, but their irregular layout forces custom deep-learning pipelines, negating simplicity and efficiency benefits. The goal is to keep superpixel efficiency while enabling use of off-the-shelf convolutional architectures without specialized training.", "method": "Construct SIGRID, a regular grid that aggregates superpixel information. Each cell encodes both color and classical shape descriptors of the corresponding superpixel, yielding a compact representation compatible with standard convolutional segmentation models. Evaluate on four benchmark datasets with two popular CNN segmentation architectures.", "result": "Despite compressed inputs, SIGRID achieves segmentation performance that matches or even surpasses pixel-level baselines and significantly accelerates model training across four datasets and two architectures.", "conclusion": "SIGRID offers a practical trade-off between accuracy and computational efficiency, enabling standard CNNs to leverage superpixel-based compression without bespoke architectures or training procedures."}}
{"id": "2510.06857", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06857", "abs": "https://arxiv.org/abs/2510.06857", "authors": ["Qi Guo", "Jianing Wang", "Jianfei Zhang", "Deyang Kong", "Xiangzhou Huang", "Xiangyu Xi", "Wei Wang", "Jingang Wang", "Xunliang Cai", "Shikun Zhang", "Wei Ye"], "title": "Autoformalizer with Tool Feedback", "comment": null, "summary": "Autoformalization addresses the scarcity of data for Automated Theorem\nProving (ATP) by translating mathematical problems from natural language into\nformal statements. Efforts in recent work shift from directly prompting large\nlanguage models to training an end-to-end formalizer model from scratch,\nachieving remarkable advancements. However, existing formalizer still struggles\nto consistently generate valid statements that meet syntactic validity and\nsemantic consistency. To address this issue, we propose the Autoformalizer with\nTool Feedback (ATF), a novel approach that incorporates syntactic and\nconsistency information as tools into the formalization process. By integrating\nLean 4 compilers for syntax corrections and employing a multi-LLMs-as-judge\napproach for consistency validation, the model is able to adaptively refine\ngenerated statements according to the tool feedback, enhancing both syntactic\nvalidity and semantic consistency. The training of ATF involves a cold-start\nphase on synthetic tool-calling data, an expert iteration phase to improve\nformalization capabilities, and Direct Preference Optimization to alleviate\nineffective revisions. Experimental results show that ATF markedly outperforms\na range of baseline formalizer models, with its superior performance further\nvalidated by human evaluations. Subsequent analysis reveals that ATF\ndemonstrates excellent inference scaling properties. Moreover, we open-source\nNumina-ATF, a dataset containing 750K synthetic formal statements to facilitate\nadvancements in autoformalization and ATP research.", "AI": {"tldr": "ATF is an autoformalization system that uses tool feedback\u2014Lean 4 compiler checks for syntax and multi-LLM judges for semantic consistency\u2014combined with staged training (synthetic tool-calling pretrain, expert iteration, and DPO) to iteratively refine formal statements, outperforming prior formalizers and releasing a 750K-statement dataset.", "motivation": "Autoformalization suffers from scarce training data and current formalizers frequently produce syntactically invalid or semantically inconsistent statements. The goal is to raise both validity and consistency while scaling performance.", "method": "Introduce Autoformalizer with Tool Feedback (ATF): 1) integrate Lean 4 compiler as a syntax-correction tool; 2) use multiple LLMs as judges for semantic consistency; 3) adaptively revise outputs based on tool feedback. Training pipeline: (a) cold-start on synthetic tool-calling data, (b) expert iteration to improve formalization, (c) Direct Preference Optimization to discourage ineffective revisions.", "result": "ATF significantly outperforms baseline formalizers on benchmarks, with improvements corroborated by human evaluation; exhibits strong inference scaling behavior; and provides an open-source dataset, Numina-ATF, with 750K synthetic formal statements.", "conclusion": "Tool-feedback-driven autoformalization yields more syntactically valid and semantically consistent formal statements and advances state-of-the-art performance. The released dataset supports further progress in autoformalization and ATP research."}}
{"id": "2510.06383", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06383", "abs": "https://arxiv.org/abs/2510.06383", "authors": ["Pierre Lison", "Mark Anderson"], "title": "Protecting De-identified Documents from Search-based Linkage Attacks", "comment": null, "summary": "While de-identification models can help conceal the identity of the\nindividual(s) mentioned in a document, they fail to address linkage risks,\ndefined as the potential to map the de-identified text back to its source. One\nstraightforward way to perform such linkages is to extract phrases from the\nde-identified document and then check their presence in the original dataset.\nThis paper presents a method to counter search-based linkage attacks while\npreserving the semantic integrity of the text. The method proceeds in two\nsteps. We first construct an inverted index of the N-grams occurring in the\ndocument collection, making it possible to efficiently determine which N-grams\nappear in less than $k$ documents (either alone or in combination with other\nN-grams). An LLM-based rewriter is then iteratively queried to reformulate\nthose spans until linkage is no longer possible. Experimental results on a\ncollection of court cases show that the method is able to effectively prevent\nsearch-based linkages while remaining faithful to the original content.", "AI": {"tldr": "Identify rare N-grams that enable search-based linkage and paraphrase them with an LLM until they achieve k-anonymity, preserving meaning.", "motivation": "Standard de-identification removes explicit identifiers but leaves distinctive phrases that can re-link a document to its source via search. The paper aims to mitigate this linkage risk without degrading the document\u2019s semantic content.", "method": "Two-step approach: (1) Build an inverted index over the corpus to detect N-grams (alone or in combination) that occur in fewer than k documents. (2) Iteratively query an LLM to rewrite those rare spans until they are no longer uniquely linkable, striving to maintain semantic integrity.", "result": "On a corpus of court cases, the method effectively prevented search-based linkages while remaining faithful to the original text.", "conclusion": "An index-guided, LLM-based rewriting pipeline can reduce linkage risk beyond conventional de-identification while preserving utility, offering a practical defense for sharing sensitive text."}}
{"id": "2510.06504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06504", "abs": "https://arxiv.org/abs/2510.06504", "authors": ["Qingxuan Wu", "Zhiyang Dou", "Chuan Guo", "Yiming Huang", "Qiao Feng", "Bing Zhou", "Jian Wang", "Lingjie Liu"], "title": "Text2Interact: High-Fidelity and Diverse Text-to-Two-Person Interaction Generation", "comment": null, "summary": "Modeling human-human interactions from text remains challenging because it\nrequires not only realistic individual dynamics but also precise,\ntext-consistent spatiotemporal coupling between agents. Currently, progress is\nhindered by 1) limited two-person training data, inadequate to capture the\ndiverse intricacies of two-person interactions; and 2) insufficiently\nfine-grained text-to-interaction modeling, where language conditioning\ncollapses rich, structured prompts into a single sentence embedding. To address\nthese limitations, we propose our Text2Interact framework, designed to generate\nrealistic, text-aligned human-human interactions through a scalable\nhigh-fidelity interaction data synthesizer and an effective spatiotemporal\ncoordination pipeline. First, we present InterCompose, a scalable\nsynthesis-by-composition pipeline that aligns LLM-generated interaction\ndescriptions with strong single-person motion priors. Given a prompt and a\nmotion for an agent, InterCompose retrieves candidate single-person motions,\ntrains a conditional reaction generator for another agent, and uses a neural\nmotion evaluator to filter weak or misaligned samples-expanding interaction\ncoverage without extra capture. Second, we propose InterActor, a\ntext-to-interaction model with word-level conditioning that preserves\ntoken-level cues (initiation, response, contact ordering) and an adaptive\ninteraction loss that emphasizes contextually relevant inter-person joint\npairs, improving coupling and physical plausibility for fine-grained\ninteraction modeling. Extensive experiments show consistent gains in motion\ndiversity, fidelity, and generalization, including out-of-distribution\nscenarios and user studies. We will release code and models to facilitate\nreproducibility.", "AI": {"tldr": "Text2Interact is a two-person, text-conditioned motion framework that pairs a scalable synthetic data pipeline (InterCompose) with a fine-grained text-to-interaction model (InterActor), yielding more diverse, realistic, and text-aligned human-human interactions than prior work.", "motivation": "Two-person interaction generation needs precise spatiotemporal coupling and adherence to rich textual cues, but current progress is limited by scarce two-person datasets and coarse sentence-level language conditioning that discards structured interaction details.", "method": "1) InterCompose: a synthesis-by-composition pipeline that, given a prompt and a motion for one agent, retrieves single-person motion candidates, trains a conditional reaction generator for the second agent, and filters outputs with a neural motion evaluator to scale high-fidelity interaction data without new motion capture. 2) InterActor: a text-to-interaction model with word-level conditioning to preserve token-level signals (e.g., initiation, response, contact ordering) and an adaptive interaction loss that emphasizes context-relevant inter-person joint pairs for better coupling and physical plausibility.", "result": "Experiments report consistent improvements in motion diversity, fidelity, and generalization\u2014including out-of-distribution settings\u2014and favorable user study outcomes, indicating stronger spatiotemporal coupling and text consistency than baselines.", "conclusion": "Combining scalable interaction synthesis with token-aware conditioning and adaptive inter-person losses enables realistic, text-aligned two-person motion generation. The approach promises better fine-grained coupling and broader coverage, with planned code/model release to support reproducibility."}}
{"id": "2510.06878", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06878", "abs": "https://arxiv.org/abs/2510.06878", "authors": ["Daria Ozerova", "Ekaterina Trofimova"], "title": "TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs", "comment": null, "summary": "Iterative refinement has been a promising paradigm to enable large language\nmodels (LLMs) to resolve difficult reasoning and problem-solving tasks. One of\nthe key challenges, however, is how to effectively search through the enormous\nsearch space of possible refinements. Existing methods typically fall back on\npredefined heuristics, which are troubled by the exploration-exploitation\ndilemma and cannot adapt based on past refinement outcomes. We introduce\nTree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with\na Thompson-Sampling-based tree search. TGPR explores both failed and successful\nrefinement paths actively, with denser training trajectories and more adaptive\npolicies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to\n+4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to\n+12.51 percentage points absolute improvement in pass@10 (on APPS) compared to\na competitive GRPO baseline. Apart from debugging code, TGPR focuses on a\nprincipled approach to combining learned policies with structured search\nmethods, offering a general framework for enhancing iterative refinement and\nstateful reasoning in LLMs.", "AI": {"tldr": "TGPR merges a learned policy (GRPO) with a Thompson-sampling tree search to guide iterative refinement, yielding sizable gains on code-generation benchmarks and offering a general template for stateful LLM reasoning.", "motivation": "Iterative refinement for LLMs faces a vast search space and the exploration\u2013exploitation dilemma. Heuristic strategies are brittle and cannot adapt to feedback from prior refinements, limiting effectiveness on complex reasoning and debugging tasks.", "method": "Tree-Guided Policy Refinement (TGPR): integrate GRPO with a Thompson-sampling-based tree search that actively explores both successful and failed refinement branches. This produces denser training trajectories and adaptively updates the policy based on observed outcomes, balancing exploration and exploitation in a principled way.", "result": "On HumanEval, MBPP, and APPS, TGPR improves over a competitive GRPO baseline: up to +4.2 percentage points in pass@1 (MBPP) and up to +12.51 percentage points in pass@10 (APPS).", "conclusion": "Coupling structured search with learned policy refinement better navigates the refinement space and improves stateful reasoning. Beyond code debugging, TGPR offers a general and scalable framework for iterative refinement in LLMs."}}
{"id": "2510.06386", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06386", "abs": "https://arxiv.org/abs/2510.06386", "authors": ["Fan Zhou", "Chang Tian", "Tim Van de Cruys"], "title": "Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion", "comment": "Preprint under review", "summary": "Generating stylistic text with specific attributes is a key problem in\ncontrollable text generation. Recently, diffusion models have emerged as a\npowerful paradigm for both visual and textual generation. Existing approaches\ncan be broadly categorized into classifier-free guidance (CFG) and classifier\nguidance (CG) methods. While CFG effectively preserves semantic content, it\noften fails to provide effective attribute control. In contrast, CG modifies\nthe denoising trajectory using classifier gradients, enabling better attribute\nalignment but incurring high computational costs during sampling and suffering\nfrom classifier generalization issues. In this work, we propose RegDiff, a\nregularized diffusion framework that leverages attribute features without\nrequiring a pretrained classifier during sampling, thereby achieving\ncontrollable generation with reduced computational costs. Specifically, RegDiff\nemploys a VAE-based encoder--decoder architecture to ensure reconstruction\nfidelity and a latent diffusion model trained with attribute supervision to\nenable controllable text generation. Attribute information is injected only\nduring training. Experiments on five datasets spanning multiple stylistic\nattributes demonstrate that RegDiff outperforms strong baselines in generating\nstylistic texts. These results validate the effectiveness of RegDiff as an\nefficient solution for attribute-controllable text diffusion. Our code,\ndatasets, and resources will be released upon publication at\nhttps://github.com/xxxx.", "AI": {"tldr": "RegDiff is a latent text diffusion framework that uses attribute supervision only during training\u2014without a classifier at sampling\u2014to generate stylistically controlled text more efficiently than classifier-guided approaches while preserving content better than CFG.", "motivation": "Controllable text generation needs reliable attribute control (style, sentiment, etc.) without sacrificing semantics or incurring high sampling costs. Existing diffusion methods trade off: CFG preserves content but weakly enforces attributes; CG enforces attributes but is expensive and can suffer from classifier mismatch/generalization. The goal is to achieve strong attribute control with low inference cost and robustness.", "method": "Use a VAE encoder\u2013decoder to ensure reconstruction fidelity and operate diffusion in the latent space. Train the latent diffusion model with attribute supervision (a regularization signal) so that at inference time no external classifier is needed. Attribute information is injected only during training, enabling controllability through the learned latent dynamics at sampling.", "result": "Across five datasets with multiple stylistic attributes, RegDiff outperforms strong baselines in stylistic text generation, achieving better attribute alignment with reduced sampling cost compared to classifier guidance and stronger control than classifier-free guidance.", "conclusion": "RegDiff provides an efficient, classifier-free-at-inference approach for attribute-controllable text diffusion that improves style alignment while lowering computational overhead. Code and resources will be released upon publication."}}
{"id": "2510.06509", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06509", "abs": "https://arxiv.org/abs/2510.06509", "authors": ["Shih-Yao Lin", "Sibendu Paul", "Caren Chen"], "title": "From Captions to Keyframes: Efficient Video Summarization via Caption- and Context-Aware Frame Scoring", "comment": "10 pages, 4 figures", "summary": "Efficient video-language understanding requires selecting a small set of\nframes that retain semantic and contextual information from long videos. We\npropose KeyScore, a multimodal frame scoring framework that jointly leverages\ncaptions and visual context to estimate frame-level importance. By combining\nsemantic similarity, temporal diversity, and contextual drop impact, KeyScore\nidentifies the most informative frames for downstream tasks such as retrieval,\ncaptioning, and video-language reasoning. To complement KeyScore, we introduce\nSTACFP (Spatio-Temporal Adaptive Clustering for Frame Proposals), which\ngenerates compact and diverse frame candidates for long-form videos. Together,\nthese modules achieve up to 99\\% frame reduction compared to full-frame\ninference and substantially outperform standard 8-frame encoders on MSRVTT,\nMSVD, and DiDeMo. Our results demonstrate that emphasizing multimodal alignment\nbetween visual and textual signals enables scalable, efficient, and\ncaption-grounded video understanding -- without explicit video summarization.", "AI": {"tldr": "KeyScore + STACFP select a tiny, diverse, and text-aligned set of video frames, yielding up to 99% frame reduction while improving or surpassing 8-frame baselines on standard video-language benchmarks.", "motivation": "Long videos make video-language understanding expensive; fixed, sparse sampling can miss semantically crucial moments and lacks grounding to the textual query/caption. The goal is to pick few frames that maximally retain multimodal (vision\u2013language) semantics for downstream tasks.", "method": "KeyScore scores frame importance by jointly using captions and visual context, combining (1) semantic similarity to text, (2) temporal diversity, and (3) contextual drop impact (how removal affects understanding). STACFP (Spatio-Temporal Adaptive Clustering for Frame Proposals) proposes compact, diverse frame candidates in long videos; KeyScore then selects the best frames for tasks like retrieval, captioning, and reasoning.", "result": "Using KeyScore+STACFP achieves up to 99% fewer frames than full-frame inference and substantially outperforms standard 8-frame encoders on MSRVTT, MSVD, and DiDeMo across retrieval, captioning, and reasoning tasks.", "conclusion": "Emphasizing multimodal alignment and adaptive, diverse frame selection enables scalable, efficient, caption-grounded video understanding without explicit video summarization, improving accuracy while drastically reducing computation."}}
{"id": "2510.06911", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06911", "abs": "https://arxiv.org/abs/2510.06911", "authors": ["Hacane Hechehouche", "Andre Antakli", "Matthias Klusch"], "title": "LLM-Assisted Modeling of Semantic Web-Enabled Multi-Agents Systems with AJAN", "comment": null, "summary": "There are many established semantic Web standards for implementing\nmulti-agent driven applications. The AJAN framework allows to engineer\nmulti-agent systems based on these standards. In particular, agent knowledge is\nrepresented in RDF/RDFS and OWL, while agent behavior models are defined with\nBehavior Trees and SPARQL to access and manipulate this knowledge. However, the\nappropriate definition of RDF/RDFS and SPARQL-based agent behaviors still\nremains a major hurdle not only for agent modelers in practice. For example,\ndealing with URIs is very error-prone regarding typos and dealing with complex\nSPARQL queries in large-scale environments requires a high learning curve. In\nthis paper, we present an integrated development environment to overcome such\nhurdles of modeling AJAN agents and at the same time to extend the user\ncommunity for AJAN by the possibility to leverage Large Language Models for\nagent engineering.", "AI": {"tldr": "Presents an IDE for the AJAN multi\u2011agent framework that simplifies RDF/OWL knowledge modeling and SPARQL/Behavior Tree authoring, leveraging LLMs to reduce errors and lower the learning curve.", "motivation": "Engineering AJAN agents today is hampered by error\u2011prone URI handling and the steep learning curve of crafting complex SPARQL queries and RDF/RDFS/OWL models, which limits adoption and productivity.", "method": "Design and integration of an IDE tailored to AJAN: supports Behavior Trees tied to SPARQL for knowledge access/manipulation, provides tooling to mitigate URI and query errors, and incorporates Large Language Models to assist agent engineering.", "result": "Delivers an integrated development environment that addresses common modeling hurdles and enables users to employ LLMs during AJAN agent development. (No empirical evaluation details are provided in the abstract.)", "conclusion": "An IDE with LLM assistance can lower barriers to modeling AJAN agents, reduce mistakes, and broaden the framework\u2019s user base; further validation would depend on empirical studies not described here."}}
{"id": "2510.06391", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06391", "abs": "https://arxiv.org/abs/2510.06391", "authors": ["Elle"], "title": "Reward Model Perspectives: Whose Opinions Do Reward Models Reward?", "comment": "Published at EMNLP 2025 under the full author name \"Elle\"", "summary": "Reward models (RMs) are central to the alignment of language models (LMs). An\nRM often serves as a proxy for human preferences to guide downstream LM\nbehavior. However, our understanding of RM behavior is limited. Our work (i)\nformalizes a framework for measuring the alignment of opinions captured by RMs,\n(ii) investigates the extent to which RMs demonstrate sociodemographic biases,\nand (iii) explores the effects of prompting to steer rewards towards the\npreferences of a target group. We study the subjective and diverse perspectives\non controversial topics, which allows us to quantify RM perspectives in terms\nof their opinions, attitudes, and values. We show that RMs are poorly aligned\nwith several demographic groups and can systematically reward harmful\nstereotypes, and steering alone is not enough to overcome these limitations.\nOur findings underscore the need for more careful consideration of RM behavior\nin model alignment during preference learning to prevent the propagation of\nunwanted social biases in the language technologies that we use.", "AI": {"tldr": "They propose a framework to quantify how reward models (RMs) reflect opinions on controversial topics, audit sociodemographic biases, and test prompt-based steering toward target-group preferences. They find RMs misaligned with several groups, sometimes rewarding harmful stereotypes, and that prompting alone cannot fix these issues.", "motivation": "RMs guide language model behavior as proxies for human preferences, yet their value alignment and potential social biases are poorly understood. Without clearer measurement and mitigation, RMs can propagate unwanted biases into downstream LMs.", "method": "(i) Formalize a measurement framework capturing RM-held perspectives\u2014opinions, attitudes, and values\u2014on controversial topics; (ii) evaluate sociodemographic alignment to detect biases across groups; (iii) experiment with prompt-based steering to shift rewards toward target-group preferences.", "result": "RMs exhibit poor alignment with multiple demographic groups and can systematically reward harmful stereotypes. Prompt-based steering improves alignment only partially and is insufficient to overcome core limitations.", "conclusion": "RM behavior requires more rigorous scrutiny and improved preference learning practices. Reliance on steering alone is inadequate; alignment pipelines must explicitly address demographic alignment and bias to avoid propagating social harms in language technologies."}}
{"id": "2510.06512", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06512", "abs": "https://arxiv.org/abs/2510.06512", "authors": ["Avishree Khare", "Hideki Okamoto", "Bardh Hoxha", "Georgios Fainekos", "Rajeev Alur"], "title": "LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval", "comment": null, "summary": "Neural models such as YOLO and HuBERT can be used to detect local properties\nsuch as objects (\"car\") and emotions (\"angry\") in individual frames of videos\nand audio clips respectively. The likelihood of these detections is indicated\nby scores in [0, 1]. Lifting these scores to temporal properties over sequences\ncan be useful for several downstream applications such as query matching (e.g.,\n\"does the speaker eventually sound happy in this audio clip?\"), and ranked\nretrieval (e.g., \"retrieve top 5 videos with a 10 second scene where a car is\ndetected until a pedestrian is detected\"). In this work, we formalize this\nproblem of assigning Scores for TempOral Properties (STOPs) over sequences,\ngiven potentially noisy score predictors for local properties. We then propose\na scoring function called LogSTOP that can efficiently compute these scores for\ntemporal properties represented in Linear Temporal Logic. Empirically, LogSTOP,\nwith YOLO and HuBERT, outperforms Large Vision / Audio Language Models and\nother Temporal Logic-based baselines by at least 16% on query matching with\ntemporal properties over objects-in-videos and emotions-in-speech respectively.\nSimilarly, on ranked retrieval with temporal properties over objects and\nactions in videos, LogSTOP with Grounding DINO and SlowR50 reports at least a\n19% and 16% increase in mean average precision and recall over zero-shot\ntext-to-video retrieval baselines respectively.", "AI": {"tldr": "They introduce LogSTOP, an efficient scoring function that lifts frame-level detector scores (e.g., YOLO, HuBERT) into scores for Linear Temporal Logic (LTL) properties over sequences, enabling temporal query matching and ranked retrieval; it yields sizable gains over LVLMs and prior temporal-logic baselines.", "motivation": "Frame- or segment-level neural detectors provide probabilistic scores for local properties (objects, emotions), but many applications require reasoning about how these properties evolve over time (e.g., eventually, until). There is a need to robustly aggregate noisy local scores into reliable scores for temporal properties to support tasks like query matching and retrieval.", "method": "Formalize Scores for TempOral Properties (STOPs) over sequences and propose LogSTOP, a computationally efficient scoring function for evaluating LTL-encoded temporal queries using noisy detector outputs. They instantiate it with detectors such as YOLO, HuBERT, Grounding DINO, and SlowR50 to compute temporal-property scores over videos and audio.", "result": "On temporal query matching, LogSTOP combined with YOLO/HuBERT surpasses Large Vision/Audio Language Models and other LTL baselines by at least 16%. For ranked retrieval of temporal patterns in videos, LogSTOP with Grounding DINO and SlowR50 improves mean average precision by at least 19% and recall by at least 16% over zero-shot text-to-video baselines.", "conclusion": "LogSTOP effectively turns local detection scores into reliable scores for LTL temporal properties, enabling better temporal reasoning for multimedia search and analysis. It achieves substantial empirical gains across audio and video tasks, suggesting it is a strong, efficient alternative to LVLMs for temporally-structured queries."}}
{"id": "2510.06953", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06953", "abs": "https://arxiv.org/abs/2510.06953", "authors": ["Minju Gwak", "Guijin Son", "Jaehyung Kim"], "title": "Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces", "comment": null, "summary": "The Uniform Information Density (UID) hypothesis suggests that effective\ncommunication maintains a stable flow of information. In this work, we revisit\nthis principle in the context of large language model (LLM) reasoning traces,\nasking whether step-level uniformity reflects reasoning quality. To this end,\nwe propose an entropy-based stepwise information density metric and introduce\ntwo complementary measures of uniformity, local and global uniformity scores.\nAcross the experiments on six different reasoning benchmarks, we find that\nstep-level uniformity not only provides a strong theoretical lens but also\nyields practical performance benefits; for example, selecting reasoning traces\nwith more uniform information density at the step-level improves accuracy by\n10-32\\% relative gains over baselines at AIME2025. Our analysis further reveals\nthat correct reasoning traces tend to avoid sharp information density spikes,\nwhile incorrect traces exhibit irregular information bursts. These results\ndemonstrate that UID-inspired information density measures outperform\nalternative internal signals as predictors of reasoning quality. Results\nhighlight the uniformity of the information density as a robust diagnostic and\nselection criterion for building more reliable and accurate reasoning systems.", "AI": {"tldr": "Operationalizes UID for LLM reasoning by measuring stepwise information density via entropy and defining local and global uniformity; more uniform traces correlate with correctness and enable trace selection that improves accuracy (10\u201332% relative gains on AIME2025), outperforming other internal signals.", "motivation": "Test whether the Uniform Information Density hypothesis applies to chain-of-thought: does maintaining a steady information rate across reasoning steps signal higher reasoning quality, and can this property be exploited to select better traces?", "method": "Define an entropy-based stepwise information density for each reasoning step; compute complementary local and global uniformity scores; evaluate across six reasoning benchmarks; rank/select reasoning traces by uniformity; analyze patterns (spikes vs. bursts) and compare with alternative internal signals as predictors of correctness.", "result": "Correct reasoning traces display smooth, uniform information density; incorrect traces show sharp spikes/irregular bursts. Using uniformity to select traces yields 10\u201332% relative accuracy gains on AIME2025 and surpasses competing internal confidence signals.", "conclusion": "UID-inspired uniformity metrics are effective diagnostics and selection criteria for LLM reasoning, providing a theoretically grounded signal that improves reliability and accuracy over alternative internal measures."}}
{"id": "2510.06411", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06411", "abs": "https://arxiv.org/abs/2510.06411", "authors": ["R. Alexander Knipper", "Indrani Dey", "Souvika Sarkar", "Hari Narayanan", "Sadhana Puntambekar", "Santu Karmaker"], "title": "Instructional Goal-Aligned Question Generation for Student Evaluation in Virtual Lab Settings: How Closely Do LLMs Actually Align?", "comment": null, "summary": "Virtual Labs offer valuable opportunities for hands-on, inquiry-based science\nlearning, yet teachers often struggle to adapt them to fit their instructional\ngoals. Third-party materials may not align with classroom needs, and developing\ncustom resources can be time-consuming and difficult to scale. Recent advances\nin Large Language Models (LLMs) offer a promising avenue for addressing these\nlimitations. In this paper, we introduce a novel alignment framework for\ninstructional goal-aligned question generation, enabling teachers to leverage\nLLMs to produce simulation-aligned, pedagogically meaningful questions through\nnatural language interaction. The framework integrates four components:\ninstructional goal understanding via teacher-LLM dialogue, lab understanding\nvia knowledge unit and relationship analysis, a question taxonomy for\nstructuring cognitive and pedagogical intent, and the TELeR taxonomy for\ncontrolling prompt detail. Early design choices were informed by a small\nteacher-assisted case study, while our final evaluation analyzed over 1,100\nquestions from 19 open-source LLMs. With goal and lab understanding grounding\nquestions in teacher intent and simulation context, the question taxonomy\nelevates cognitive demand (open-ended formats and relational types raise\nquality by 0.29-0.39 points), and optimized TELeR prompts enhance format\nadherence (80% parsability, >90% adherence). Larger models yield the strongest\ngains: parsability +37.1%, adherence +25.7%, and average quality +0.8 Likert\npoints.", "AI": {"tldr": "A four-part LLM-based framework generates virtual-lab questions aligned to teachers\u2019 goals and simulation context, improving quality and format adherence across 19 open-source models; larger models perform best.", "motivation": "Teachers struggle to adapt virtual labs to specific instructional goals; third\u2011party materials often misalign with classroom needs, and crafting custom resources is time\u2011consuming and hard to scale.", "method": "Introduce an alignment framework combining: (1) instructional goal elicitation via teacher\u2013LLM dialogue, (2) lab understanding through knowledge units and their relationships, (3) a question taxonomy encoding cognitive/pedagogical intent, and (4) TELeR taxonomy to control prompt specificity. Early design was guided by a small teacher-assisted case study; final evaluation assessed >1,100 questions generated by 19 open-source LLMs, measuring parsability, format adherence, and quality.", "result": "Grounded goal and lab understanding plus the question taxonomy increased cognitive demand; open\u2011ended formats and relational types improved quality by 0.29\u20130.39 Likert points. Optimized TELeR prompts yielded ~80% parsability and >90% format adherence. Larger models showed the biggest gains: parsability +37.1%, adherence +25.7%, and average quality +0.8 Likert points.", "conclusion": "Structuring LLM prompts with explicit teacher goals, lab knowledge structures, and taxonomies produces simulation\u2011aligned, pedagogically meaningful questions at scale. Prompt design and model size materially affect outcomes, suggesting a practical path for teachers to generate higher\u2011quality virtual\u2011lab assessments with stronger adherence to desired formats."}}
{"id": "2510.06516", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06516", "abs": "https://arxiv.org/abs/2510.06516", "authors": ["Zhantao Deng", "M\u00e9riem Er-Rafik", "Anna Sushko", "C\u00e9cile H\u00e9bert", "Pascal Fua"], "title": "Limited-Angle Tomography Reconstruction via Projector Guided 3D Diffusion", "comment": "10 pages, 11 figures", "summary": "Limited-angle electron tomography aims to reconstruct 3D shapes from 2D\nprojections of Transmission Electron Microscopy (TEM) within a restricted range\nand number of tilting angles, but it suffers from the missing-wedge problem\nthat causes severe reconstruction artifacts. Deep learning approaches have\nshown promising results in alleviating these artifacts, yet they typically\nrequire large high-quality training datasets with known 3D ground truth which\nare difficult to obtain in electron microscopy. To address these challenges, we\npropose TEMDiff, a novel 3D diffusion-based iterative reconstruction framework.\nOur method is trained on readily available volumetric FIB-SEM data using a\nsimulator that maps them to TEM tilt series, enabling the model to learn\nrealistic structural priors without requiring clean TEM ground truth. By\noperating directly on 3D volumes, TEMDiff implicitly enforces consistency\nacross slices without the need for additional regularization. On simulated\nelectron tomography datasets with limited angular coverage, TEMDiff outperforms\nstate-of-the-art methods in reconstruction quality. We further demonstrate that\na trained TEMDiff model generalizes well to real-world TEM tilts obtained under\ndifferent conditions and can recover accurate structures from tilt ranges as\nnarrow as 8 degrees, with 2-degree increments, without any retraining or\nfine-tuning.", "AI": {"tldr": "TEMDiff is a 3D diffusion-based, data-consistent reconstruction framework for limited-angle TEM tomography trained on FIB-SEM volumes via a TEM simulator, reducing missing-wedge artifacts and outperforming prior methods on simulated data while generalizing to real tilts, even from extremely narrow ranges (\u22488\u00b0 with 2\u00b0 steps) without retraining.", "motivation": "Limited-angle TEM tomography suffers from severe missing-wedge artifacts and lacks abundant ground-truth 3D TEM datasets for supervised learning. The goal is to leverage accessible volumetric data to learn strong structural priors and enforce 3D consistency without requiring clean TEM ground truth.", "method": "Train a 3D diffusion model on volumetric FIB-SEM data that are mapped to simulated TEM tilt series. Use an iterative reconstruction scheme that alternates data-consistency with diffusion-based 3D prior refinement, operating directly on volumes to implicitly maintain inter-slice consistency and avoiding extra regularizers.", "result": "On simulated limited-angle ET datasets, TEMDiff surpasses state-of-the-art in reconstruction quality. A single trained model generalizes to real TEM tilts under different conditions, recovering accurate structures from very narrow tilt ranges (\u22488\u00b0 total, 2\u00b0 increments) without retraining or fine-tuning.", "conclusion": "Diffusion-based 3D priors learned from FIB-SEM with a TEM simulator can effectively mitigate missing-wedge artifacts in limited-angle TEM tomography, providing data-efficient, generalizable reconstructions and potentially reducing acquisition requirements while maintaining cross-slice consistency."}}
{"id": "2510.07038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07038", "abs": "https://arxiv.org/abs/2510.07038", "authors": ["Wenxun Wu", "Yuanyang Li", "Guhan Chen", "Linyue Wang", "Hongyang Chen"], "title": "Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning", "comment": null, "summary": "Recent advances in large language models (LLMs) have popularized test-time\nscaling, where models generate additional reasoning tokens before producing\nfinal answers. These approaches have demonstrated significant performance\nimprovements on benchmarks involving mathematical reasoning. However, language\nmodels relying solely on direct inference still struggle with tasks demanding\nup-to-date knowledge or computational tools such as calculators and code\ninterpreters for complex arithmetic operations. To overcome these limitations,\nwe propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement\nlearning framework that systematically integrates multi-hop reasoning with\nadaptive tool-calling capabilities. Our approach employs a modified version of\nDynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,\nwhich we adapt specifically for tool invocation scenarios, enabling models to\ndynamically interleave complex reasoning with on-demand tool usage (including\nsearch APIs and Python interpreters).\n  To support this research, we introduce two new datasets: TAPO-easy-60K and\nTAPO-hard-18K, specifically designed to train and evaluate both fact-based\nreasoning and mathematical calculation capabilities. Our experiments on\nQwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,\nwith both models achieving state-of-the-art performance on tasks requiring\nexternal knowledge and mathematical computation among methods with comparable\nparameters. Notably, TAPO achieves more efficient tool utilization than\nbaseline methods while preventing excessive calls caused by reward hacking.\nThese results highlight the significant potential of combining advanced\nreasoning with tool usage to enhance model performance in knowledge-intensive\nand computationally demanding tasks.", "AI": {"tldr": "TAPO is an RL framework that teaches LLMs to interleave multi-hop reasoning with adaptive tool calls (e.g., search, Python), delivering state-of-the-art accuracy and more efficient tool usage on knowledge- and math-intensive tasks, supported by two new datasets.", "motivation": "Test-time scaling helps reasoning but doesn\u2019t give LLMs up-to-date facts or reliable computation. Models need a principled way to decide when and how to call tools without overusing them or exploiting rewards, especially for fact retrieval and complex arithmetic.", "method": "Introduce Tool-Augmented Policy Optimization (TAPO), adapting Dynamic Sampling Policy Optimization (DAPO) to tool-invocation settings. The policy learns to dynamically alternate between reasoning steps and on-demand tools (search APIs, interpreters). Two datasets\u2014TAPO-easy-60K and TAPO-hard-18K\u2014are created to train/evaluate fact-based reasoning and mathematical calculations.", "result": "On Qwen2.5-3B and Qwen2.5-7B, TAPO achieves state-of-the-art performance on tasks requiring external knowledge and math compared to parameter-matched baselines. It uses tools more efficiently and mitigates excessive tool calls arising from reward hacking.", "conclusion": "Coupling advanced reasoning with adaptive tool use via RL improves accuracy and efficiency on knowledge- and computation-heavy tasks. TAPO and its datasets indicate a promising path for scalable, reliable tool-augmented LLMs."}}
{"id": "2510.06426", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06426", "abs": "https://arxiv.org/abs/2510.06426", "authors": ["Yitao Long", "Tiansheng Hu", "Yilun Zhao", "Arman Cohan", "Chen Zhao"], "title": "FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering", "comment": "EMNLP 2025 Findings", "summary": "Large Language Models (LLMs) frequently hallucinate to long-form questions,\nproducing plausible yet factually incorrect answers. A common mitigation\nstrategy is to provide attribution to LLM outputs. However, existing benchmarks\nprimarily focus on simple attribution that retrieves supporting textual\nevidence as references. We argue that in real-world scenarios such as financial\napplications, attribution goes beyond reference retrieval. We introduce\nFinLFQA, a benchmark designed to evaluate the ability of LLMs to generate\nlong-form answers to complex financial questions with reliable and nuanced\nattributions. FinLFQA evaluates three critical aspects of attribution through\nhuman annotations: (1) supporting evidence extracted from financial reports,\n(2) intermediate numerical reasoning steps, and (3) domain-specific financial\nknowledge that informs the reasoning process. We further provide an automatic\nevaluation framework covering both answer quality and attribution quality.\nThrough extensive experiments on eight LLMs across multiple\nattribution-generation paradigms, we find that fine-grained metrics are\nimportant to distinguish model capabilities, that end-to-end generation\nachieves comparable performance to post-hoc approaches, and that iterative\nrefinement only helps when guided by external feedback.", "AI": {"tldr": "FinLFQA is a benchmark and evaluation framework for long-form financial QA that tests not only textual evidence attribution but also numerical reasoning steps and domain knowledge, revealing nuanced differences among LLM attribution strategies.", "motivation": "LLMs often hallucinate in long-form answers, and existing attribution benchmarks mostly check for retrieved text snippets. In high-stakes domains like finance, attribution must include verified evidence, explicit numerical reasoning, and domain knowledge justifications\u2014capabilities current benchmarks do not adequately test.", "method": "Create FinLFQA with human-annotated attributions for three dimensions: (1) supporting evidence from financial reports, (2) intermediate numerical reasoning steps, and (3) domain-specific financial knowledge. Provide an automatic evaluation framework for both answer and attribution quality. Evaluate eight LLMs across different attribution-generation paradigms (e.g., end-to-end vs. post-hoc, iterative refinement).", "result": "Fine-grained metrics are necessary to differentiate model capabilities; end-to-end generation can match post-hoc attribution; iterative refinement helps only when guided by external feedback.", "conclusion": "FinLFQA fills a gap for evaluating reliable, nuanced attributions in financial long-form QA, showing that richer attribution dimensions and metrics are needed and that guidance matters more than naive iterative generation."}}
{"id": "2510.06529", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06529", "abs": "https://arxiv.org/abs/2510.06529", "authors": ["Xiangyi Chen", "Th\u00e9ophane Vallaeys", "Maha Elbayad", "John Nguyen", "Jakob Verbeek"], "title": "VUGEN: Visual Understanding priors for GENeration", "comment": null, "summary": "Recent advances in Vision-Language Models (VLMs) have enabled unified\nunderstanding across text and images, yet equipping these models with robust\nimage generation capabilities remains challenging. Existing approaches often\nrely on reconstruction-oriented autoencoders or complex bridging mechanisms,\nleading to misalignment between understanding and generation representations,\nor architectural complexity. In this work, we propose VUGEN, a novel framework\nthat explicitly leverages VLM's pretrained visual understanding priors for\nefficient and high-quality image generation. Our approach first transforms the\nhigh-dimensional latent space of the VLM's native vision encoder into a\nlower-dimensional, tractable distribution that maximally preserves visual\ninformation. The VLM is then trained to sample within this reduced latent\nspace, ensuring alignment with its visual understanding capabilities. Finally,\na dedicated pixel decoder maps these generated latents back to the image space.\nWe find that a VAE-free pixel diffusion decoder to be on par or better than\ncommonly used complex latent diffusion decoders that internally rely on VAE\nlatents. Extensive experiments demonstrate that VUGEN achieves superior image\ngeneration performance, improving DPG Bench from 71.17 to 74.32 and FID from\n11.86 to 9.06 on COCO, while fully preserving the VLM's original understanding\ncapabilities.", "AI": {"tldr": "VUGEN aligns a pretrained vision-language model\u2019s visual understanding with image generation by sampling in a compact, information-preserving latent derived from the VLM\u2019s own encoder and decoding to pixels with a VAE-free diffusion decoder, yielding better COCO FID and DPG Bench scores while retaining understanding ability.", "motivation": "Current VLMs unify text\u2013image understanding but struggle to generate images well. Prior methods depend on reconstruction-oriented autoencoders or complex bridges between understanding and generation spaces, causing misalignment or architectural overhead. The goal is to exploit the VLM\u2019s native visual prior for generation without added complexity or loss of understanding.", "method": "1) Map the VLM vision encoder\u2019s high-dimensional latent to a lower-dimensional, tractable distribution that preserves maximal visual information. 2) Train the VLM to sample within this reduced latent space, ensuring consistency with its understanding representation. 3) Use a dedicated pixel decoder to convert generated latents to images; specifically, a VAE-free pixel diffusion decoder instead of common latent diffusion decoders that rely on VAE latents.", "result": "On COCO, FID improves from 11.86 to 9.06; DPG Bench improves from 71.17 to 74.32. The method maintains the original understanding capabilities of the VLM. The pixel diffusion decoder performs on par with or better than latent diffusion decoders that depend on VAE latents.", "conclusion": "Leveraging the VLM\u2019s own visual latent and aligning generation directly to it, then decoding with a pixel diffusion model, reduces complexity and misalignment while improving image quality. This indicates VAE-free pixel diffusion can be a competitive, simpler alternative to latent diffusion for VLM-based generation, preserving understanding while enhancing generation."}}
{"id": "2510.07064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07064", "abs": "https://arxiv.org/abs/2510.07064", "authors": ["Manh Hung Nguyen", "Sebastian Tschiatschek", "Adish Singla"], "title": "Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations", "comment": null, "summary": "The difficulty and expense of obtaining large-scale human responses make\nLarge Language Models (LLMs) an attractive alternative and a promising proxy\nfor human behavior. However, prior work shows that LLMs often produce\nhomogeneous outputs that fail to capture the rich diversity of human\nperspectives and behaviors. Thus, rather than trying to capture this diversity\nwith a single LLM agent, we propose a novel framework to construct a set of\nagents that collectively capture the diversity of a given human population.\nEach agent is an LLM whose behavior is steered by conditioning on a small set\nof human demonstrations (task-response pairs) through in-context learning. The\ncentral challenge is therefore to select a representative set of LLM agents\nfrom the exponentially large space of possible agents. We tackle this selection\nproblem from the lens of submodular optimization. In particular, we develop\nmethods that offer different trade-offs regarding time complexity and\nperformance guarantees. Extensive experiments in crowdsourcing and educational\ndomains demonstrate that our approach constructs agents that more effectively\nrepresent human populations compared to baselines. Moreover, behavioral\nanalyses on new tasks show that these agents reproduce the behavior patterns\nand perspectives of the students and annotators they are designed to represent.", "AI": {"tldr": "They build a diverse set of LLM \u201cagents,\u201d each steered by small in-context human demonstrations, and select a representative subset via submodular optimization to better approximate the diversity of human behavior than a single LLM.", "motivation": "Single LLMs are attractive proxies for human responses but tend to produce homogeneous outputs, failing to reflect the diversity of real human populations; collecting large-scale human data is costly.", "method": "Generate candidate LLM agents by conditioning on small sets of task\u2013response human demos (ICL). Cast the choice of which agents to keep as a submodular set selection problem and propose algorithms with varying time\u2013quality trade-offs and performance guarantees to select a representative subset.", "result": "Across crowdsourcing and educational tasks, the selected agent sets match population diversity more effectively than baselines and reproduce behavior patterns and perspectives of target groups on new tasks.", "conclusion": "A multi-agent, submodularly selected ICL framework is an effective and scalable way to approximate population-level human diversity, outperforming single-agent or naive baselines and generalizing to new tasks."}}
{"id": "2510.06427", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06427", "abs": "https://arxiv.org/abs/2510.06427", "authors": ["Elena Chistova"], "title": "Bridging Discourse Treebanks with a Unified Rhetorical Structure Parser", "comment": "Accepted to CODI CRAC 2025", "summary": "We introduce UniRST, the first unified RST-style discourse parser capable of\nhandling 18 treebanks in 11 languages without modifying their relation\ninventories. To overcome inventory incompatibilities, we propose and evaluate\ntwo training strategies: Multi-Head, which assigns separate relation\nclassification layer per inventory, and Masked-Union, which enables shared\nparameter training through selective label masking. We first benchmark\nmonotreebank parsing with a simple yet effective augmentation technique for\nlow-resource settings. We then train a unified model and show that (1) the\nparameter efficient Masked-Union approach is also the strongest, and (2) UniRST\noutperforms 16 of 18 mono-treebank baselines, demonstrating the advantages of a\nsingle-model, multilingual end-to-end discourse parsing across diverse\nresources.", "AI": {"tldr": "UniRST is a unified, multilingual RST-style discourse parser that trains across 18 treebanks in 11 languages without altering their relation inventories, using a Masked-Union strategy that yields state-of-the-art results versus most mono-treebank baselines.", "motivation": "Discourse parsing resources are fragmented across languages and treebanks with incompatible relation inventories, making it hard to build a single model and to leverage low-resource settings without manual label mapping. A unified approach promises better transfer and efficiency.", "method": "Introduce two strategies to reconcile inventory incompatibilities during joint training: (1) Multi-Head, which gives each inventory its own relation classification head; (2) Masked-Union, which forms a shared label space and applies selective label masking so only relevant labels are considered per treebank. Benchmark mono-treebank parsing with a simple augmentation for low-resource settings, then train a single multilingual end-to-end model across 18 treebanks/11 languages without modifying inventories.", "result": "Masked-Union is both parameter-efficient and empirically strongest. The unified UniRST model outperforms 16 of 18 mono-treebank baselines, showing cross-treebank and cross-lingual gains; the augmentation helps in low-resource cases.", "conclusion": "Maintaining original relation inventories while enabling shared-parameter training is feasible and beneficial. A single multilingual model can surpass most specialized baselines, suggesting unified discourse parsing is an effective path for diverse, low-resource scenarios."}}
{"id": "2510.06541", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06541", "abs": "https://arxiv.org/abs/2510.06541", "authors": ["Nicholas M. Kroeger", "Vincent Bindschaedler"], "title": "Cluster Paths: Navigating Interpretability in Neural Networks", "comment": null, "summary": "While modern deep neural networks achieve impressive performance in vision\ntasks, they remain opaque in their decision processes, risking unwarranted\ntrust, undetected biases and unexpected failures. We propose cluster paths, a\npost-hoc interpretability method that clusters activations at selected layers\nand represents each input as its sequence of cluster IDs. To assess these\ncluster paths, we introduce four metrics: path complexity (cognitive load),\nweighted-path purity (class alignment), decision-alignment faithfulness\n(predictive fidelity), and path agreement (stability under perturbations). In a\nspurious-cue CIFAR-10 experiment, cluster paths identify color-based shortcuts\nand collapse when the cue is removed. On a five-class CelebA hair-color task,\nthey achieve 90% faithfulness and maintain 96% agreement under Gaussian noise\nwithout sacrificing accuracy. Scaling to a Vision Transformer pretrained on\nImageNet, we extend cluster paths to concept paths derived from prompting a\nlarge language model on minimal path divergences. Finally, we show that cluster\npaths can serve as an effective out-of-distribution (OOD) detector, reliably\nflagging anomalous samples before the model generates over-confident\npredictions. Cluster paths uncover visual concepts, such as color palettes,\ntextures, or object contexts, at multiple network depths, demonstrating that\ncluster paths scale to large vision models while generating concise and\nhuman-readable explanations.", "AI": {"tldr": "Introduces \u201ccluster paths,\u201d a post-hoc interpretability method that clusters intermediate activations into discrete paths, supplies four evaluation metrics, scales to large vision models (ViTs) and LLM-derived concept paths, exposes spurious cues, stays faithful/stable, and doubles as an OOD detector.", "motivation": "Deep vision models are accurate but opaque, which can lead to misplaced trust, hidden biases (e.g., reliance on spurious cues), and brittle failures. The work seeks interpretable, human-understandable explanations that are faithful to the model\u2019s decision process and robust across inputs and perturbations.", "method": "Cluster activations at selected layers and represent each input by the sequence of cluster IDs (a \u201ccluster path\u201d). Evaluate interpretability with four metrics: (1) path complexity (cognitive load), (2) weighted-path purity (class alignment), (3) decision-alignment faithfulness (predictive fidelity), and (4) path agreement (stability under perturbations). Extend to ViTs and derive \u201cconcept paths\u201d by prompting a large language model on minimal path divergences to map discrete paths to human concepts. Use paths for tasks like OOD detection.", "result": "On a spurious-cue CIFAR-10 setup, cluster paths reveal color-based shortcuts and collapse when the cue is removed. On a 5-class CelebA hair-color task, they reach ~90% faithfulness and ~96% agreement under Gaussian noise without accuracy loss. Scaled to an ImageNet-pretrained ViT, the approach yields concept paths via LLM prompting. Cluster paths serve as a strong OOD detector, flagging anomalies before overconfident predictions. They uncover visual concepts (color palettes, textures, contexts) across depths.", "conclusion": "Cluster paths provide concise, human-readable, and scalable explanations of model behavior, help diagnose spurious cues, maintain high faithfulness and stability, and enable practical utilities like OOD detection. The approach bridges activation-level structure with semantic concepts in large vision models."}}
{"id": "2510.07069", "categories": ["cs.AI", "I.2.4"], "pdf": "https://arxiv.org/pdf/2510.07069", "abs": "https://arxiv.org/abs/2510.07069", "authors": ["Hongbo Hu", "Yisong Wang", "Yi Huang", "Kewen Wang"], "title": "Inductive Learning for Possibilistic Logic Programs Under Stable Models", "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "summary": "Possibilistic logic programs (poss-programs) under stable models are a major\nvariant of answer set programming (ASP). While its semantics (possibilistic\nstable models) and properties have been well investigated, the problem of\ninductive reasoning has not been investigated yet. This paper presents an\napproach to extracting poss-programs from a background program and examples\n(parts of intended possibilistic stable models). To this end, the notion of\ninduction tasks is first formally defined, its properties are investigated and\ntwo algorithms ilpsm and ilpsmmin for computing induction solutions are\npresented. An implementation of ilpsmmin is also provided and experimental\nresults show that when inputs are ordinary logic programs, the prototype\noutperforms a major inductive learning system for normal logic programs from\nstable models on the datasets that are randomly generated.", "AI": {"tldr": "They introduce the first inductive learning framework for possibilistic logic programs under stable-model semantics, define the induction task, propose two algorithms (ilpsm, ilpsmmin), implement ilpsmmin, and show it outperforms a leading ASP ILP system on random datasets when inputs are ordinary logic programs.", "motivation": "Inductive reasoning for possibilistic logic programs (ASP with uncertainty via possibility degrees) had not been addressed. There is a need to learn/extract possibilistic programs from background knowledge and example fragments of intended possibilistic stable models.", "method": "(1) Formally define an induction task for possibilistic stable models and study its properties. (2) Propose two algorithms\u2014ilpsm and ilpsmmin\u2014to compute induction solutions. (3) Implement ilpsmmin and evaluate it experimentally, including the special case where inputs are ordinary (non-possibilistic) logic programs, comparing against a major ILP system for normal logic programs from stable models.", "result": "Both algorithms solve the defined induction tasks in theory; the ilpsmmin implementation empirically outperforms a leading ASP ILP system on randomly generated datasets when restricted to ordinary logic program inputs. Specific metrics aren\u2019t stated in the abstract, but \u201coutperforms\u201d implies better efficiency and/or accuracy.", "conclusion": "Inductive learning for possibilistic ASP is feasible and effective. The formalization and algorithms provide a basis for extracting poss-programs from examples, and the prototype demonstrates competitive performance, especially on ordinary logic program settings, opening a path for further research and optimization."}}
{"id": "2510.06430", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06430", "abs": "https://arxiv.org/abs/2510.06430", "authors": ["Neeraja Kirtane", "Yuvraj Khanna", "Peter Relan"], "title": "MathRobust-LV: Evaluation of Large Language Models' Robustness to Linguistic Variations in Mathematical Reasoning", "comment": null, "summary": "Large language models excel on math benchmarks, but their math reasoning\nrobustness to linguistic variation is underexplored. While recent work\nincreasingly treats high-difficulty competitions like the IMO as the gold\nstandard for evaluating reasoning, we believe in comprehensive benchmarking of\nhigh school-level math problems in real educational settings. We introduce\nMathRobust-LV, a test set and evaluation methodology that mirrors how\ninstructors rephrase problems across assessments while keeping difficulty\nconstant: we change surface details (names, contexts, variables) while\npreserving numerical structure and answers. In contrast to prior efforts that\nalter problem content or emphasize IMO-level tasks, we focus on\nhigh-school-level dataset problems at the difficulty level where models are\ncurrently deployed in educational settings: tutoring and assessment systems. In\nthese applications, instructors rephrase identical concepts in varied ways,\nmaking linguistic robustness essential for reliable deployment. Although MATH\ndata benchmarking is often regarded as saturated, our experiment on 34 models\nreveals that accuracy declines when moving from the baseline to the variants.\nThese drops are severe for smaller models (9-11%) while stronger models also\nshow measurable degradation. Frontier models like GPT-5, Gemini-2.5pro remain\ncomparatively stable. Our results highlight that robustness to linguistic\nvariation is a fundamental challenge, exposing reasoning vulnerabilities in\nmodels.", "AI": {"tldr": "They introduce MathRobust-LV, a benchmark that rephrases high\u2011school math problems without changing their math content, and show that many LLMs lose accuracy on these linguistically varied versions\u2014especially smaller models\u2014highlighting a gap between math skill and robustness needed for real educational use.", "motivation": "Math benchmarks often reward solving canonical wordings, but in classrooms instructors routinely rephrase the same concept. Current evaluations (often IMO-level or content-altering perturbations) don\u2019t reflect these realistic linguistic variations at the deployment level (tutoring/assessment). The authors want a comprehensive, education-focused test of whether models\u2019 math reasoning holds up under natural rewordings.", "method": "Build MathRobust-LV: for high-school-level problems, create multiple variants that change only surface details (names, contexts, variable symbols) while preserving numerical structure and answers. Evaluate 34 models by comparing accuracy on original problems versus linguistically varied counterparts, keeping difficulty constant and mirroring instructor-style rephrasing.", "result": "Across 34 models, accuracy drops from baseline to rephrased variants. Smaller models show large declines (~9\u201311%), stronger models also degrade, and frontier systems (e.g., GPT-5, Gemini-2.5 Pro) are comparatively stable. Despite perceptions that MATH benchmarking is saturated, these variants expose nontrivial robustness gaps.", "conclusion": "Linguistic variation is a fundamental stress test for math reasoning. Even strong models show measurable brittleness, which undermines reliability in educational settings. Robustness to rephrasing should be a core evaluation and development target, beyond headline accuracy on standard problem wordings."}}
{"id": "2510.06564", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06564", "abs": "https://arxiv.org/abs/2510.06564", "authors": ["Qiongyang Hu", "Wenyang Liu", "Wenbin Zou", "Yuejiao Su", "Lap-Pui Chau", "Yi Wang"], "title": "HSNet: Heterogeneous Subgraph Network for Single Image Super-resolution", "comment": null, "summary": "Existing deep learning approaches for image super-resolution, particularly\nthose based on CNNs and attention mechanisms, often suffer from structural\ninflexibility. Although graph-based methods offer greater representational\nadaptability, they are frequently impeded by excessive computational\ncomplexity. To overcome these limitations, this paper proposes the\nHeterogeneous Subgraph Network (HSNet), a novel framework that efficiently\nleverages graph modeling while maintaining computational feasibility. The core\nidea of HSNet is to decompose the global graph into manageable sub-components.\nFirst, we introduce the Constructive Subgraph Set Block (CSSB), which generates\na diverse set of complementary subgraphs. Rather than relying on a single\nmonolithic graph, CSSB captures heterogeneous characteristics of the image by\nmodeling different relational patterns and feature interactions, producing a\nrich ensemble of both local and global graph structures. Subsequently, the\nSubgraph Aggregation Block (SAB) integrates the representations embedded across\nthese subgraphs. Through adaptive weighting and fusion of multi-graph features,\nSAB constructs a comprehensive and discriminative representation that captures\nintricate interdependencies. Furthermore, a Node Sampling Strategy (NSS) is\ndesigned to selectively retain the most salient features, thereby enhancing\naccuracy while reducing computational overhead. Extensive experiments\ndemonstrate that HSNet achieves state-of-the-art performance, effectively\nbalancing reconstruction quality with computational efficiency. The code will\nbe made publicly available.", "AI": {"tldr": "HSNet is a graph-based image super-resolution framework that decomposes a global graph into heterogeneous subgraphs, aggregates them adaptively, and samples salient nodes to achieve state-of-the-art quality with lower computational cost.", "motivation": "CNN- and attention-based SR models can be structurally rigid, while graph-based models are more flexible but often too computationally expensive. The paper aims to retain the adaptability of graph modeling while making it efficient and scalable.", "method": "1) Constructive Subgraph Set Block (CSSB) generates a set of complementary subgraphs that capture diverse local/global relations and feature interactions; 2) Subgraph Aggregation Block (SAB) fuses multi-graph representations via adaptive weighting to form a comprehensive representation; 3) Node Sampling Strategy (NSS) retains salient nodes to reduce computation without sacrificing accuracy. Overall, the approach replaces a single monolithic graph with a heterogeneous ensemble plus adaptive fusion and sampling.", "result": "Across extensive experiments (datasets and metrics not specified in the abstract), HSNet achieves state-of-the-art reconstruction accuracy while reducing computational overhead compared to prior CNN/attention and graph-based SR methods.", "conclusion": "Decomposing global graphs into heterogeneous subgraphs and aggregating them with adaptive fusion, plus selective node sampling, provides an effective and efficient SR model. HSNet balances representational flexibility and computational feasibility, and code will be released."}}
{"id": "2510.07073", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07073", "abs": "https://arxiv.org/abs/2510.07073", "authors": ["Andr\u00e9 Hottung", "Federico Berto", "Chuanbo Hua", "Nayeli Gast Zepeda", "Daniel Wetzel", "Michael R\u00f6mer", "Haoran Ye", "Davide Zago", "Michael Poli", "Stefano Massaroli", "Jinkyoo Park", "Kevin Tierney"], "title": "VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle Routing Problems", "comment": null, "summary": "Designing high-performing heuristics for vehicle routing problems (VRPs) is a\ncomplex task that requires both intuition and deep domain knowledge. Large\nlanguage model (LLM)-based code generation has recently shown promise across\nmany domains, but it still falls short of producing heuristics that rival those\ncrafted by human experts. In this paper, we propose VRPAgent, a framework that\nintegrates LLM-generated components into a metaheuristic and refines them\nthrough a novel genetic search. By using the LLM to generate problem-specific\noperators, embedded within a generic metaheuristic framework, VRPAgent keeps\ntasks manageable, guarantees correctness, and still enables the discovery of\nnovel and powerful strategies. Across multiple problems, including the\ncapacitated VRP, the VRP with time windows, and the prize-collecting VRP, our\nmethod discovers heuristic operators that outperform handcrafted methods and\nrecent learning-based approaches while requiring only a single CPU core. To our\nknowledge, \\VRPAgent is the first LLM-based paradigm to advance the\nstate-of-the-art in VRPs, highlighting a promising future for automated\nheuristics discovery.", "AI": {"tldr": "VRPAgent combines LLM-generated problem-specific operators with a generic metaheuristic and a genetic search to refine them, achieving state-of-the-art heuristics for several VRP variants on a single CPU core.", "motivation": "Crafting high-quality VRP heuristics demands expert intuition and domain knowledge, and while LLM code generation is promising, it has not matched expert-designed methods. The authors seek a principled way to harness LLM creativity while ensuring correctness and strong performance.", "method": "1) Use an LLM to propose problem-specific heuristic operators; 2) embed these operators within a generic metaheuristic framework that enforces correctness; 3) apply a novel genetic search over LLM-generated components to refine, select, and combine operators; 4) evaluate across CVRP, VRPTW, and PCVRP. Emphasis on modularity keeps tasks manageable and correctness verifiable while enabling exploration of novel strategies.", "result": "The framework discovers operators that outperform handcrafted and recent learning-based methods across multiple VRP benchmarks, reportedly setting new state-of-the-art results while running on a single CPU core.", "conclusion": "LLM-assisted automated heuristic discovery, when coupled with a correctness-guarded metaheuristic and evolutionary refinement, can surpass expert and learned baselines for VRPs. This points to a promising direction for scalable, automated design of combinatorial optimization heuristics."}}
{"id": "2510.06445", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06445", "abs": "https://arxiv.org/abs/2510.06445", "authors": ["Asif Shahriar", "Md Nafiu Rahman", "Sadif Ahmed", "Farig Sadeque", "Md Rizwan Parvez"], "title": "A Survey on Agentic Security: Applications, Threats and Defenses", "comment": null, "summary": "The rapid shift from passive LLMs to autonomous LLM-agents marks a new\nparadigm in cybersecurity. While these agents can act as powerful tools for\nboth offensive and defensive operations, the very agentic context introduces a\nnew class of inherent security risks. In this work we present the first\nholistic survey of the agentic security landscape, structuring the field around\nthree interdependent pillars: Applications, Threats, and Defenses. We provide a\ncomprehensive taxonomy of over 150 papers, explaining how agents are used, the\nvulnerabilities they possess, and the countermeasures designed to protect them.\nA detailed cross-cutting analysis shows emerging trends in agent architecture\nwhile revealing critical research gaps in model and modality coverage.", "AI": {"tldr": "Survey of security for autonomous LLM agents: taxonomy of applications, threats, and defenses over 150+ papers; identifies trends and research gaps in architectures, models, and modalities.", "motivation": "Autonomous LLM agents are increasingly used for cyber offense/defense, but their agentic operation introduces novel, inherent security risks not covered by prior work on passive LLMs. The field lacks a holistic, structured view.", "method": "Conduct a comprehensive literature survey (>150 papers) and organize it into a three-pillar framework\u2014Applications, Threats, Defenses\u2014providing taxonomy and cross-cutting analysis across agent architectures, model types, and modalities.", "result": "A unified taxonomy mapping how agents are used, where they are vulnerable, and what countermeasures exist; identification of emerging architectural trends and pinpointed gaps in model and modality coverage.", "conclusion": "Agentic LLM security is a distinct, rapidly evolving area; the provided taxonomy and analysis structure the landscape, surface critical research needs, and guide the development of more robust defenses across architectures, models, and modalities."}}
{"id": "2510.06582", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06582", "abs": "https://arxiv.org/abs/2510.06582", "authors": ["Fei Zhang", "Rob Chancia", "Josie Clapp", "Amirhossein Hassanzadeh", "Dimah Dera", "Richard MacKenzie", "Jan van Aardt"], "title": "Through the Perspective of LiDAR: A Feature-Enriched and Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud Segmentation", "comment": null, "summary": "Accurate semantic segmentation of terrestrial laser scanning (TLS) point\nclouds is limited by costly manual annotation. We propose a semi-automated,\nuncertainty-aware pipeline that integrates spherical projection, feature\nenrichment, ensemble learning, and targeted annotation to reduce labeling\neffort, while sustaining high accuracy. Our approach projects 3D points to a 2D\nspherical grid, enriches pixels with multi-source features, and trains an\nensemble of segmentation networks to produce pseudo-labels and uncertainty\nmaps, the latter guiding annotation of ambiguous regions. The 2D outputs are\nback-projected to 3D, yielding densely annotated point clouds supported by a\nthree-tier visualization suite (2D feature maps, 3D colorized point clouds, and\ncompact virtual spheres) for rapid triage and reviewer guidance. Using this\npipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove\nforests. We further evaluate data efficiency and feature importance to address\ntwo key questions: (1) how much annotated data are needed and (2) which\nfeatures matter most. Results show that performance saturates after ~12\nannotated scans, geometric features contribute the most, and compact\nnine-channel stacks capture nearly all discriminative power, with the mean\nIntersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm\nthe generalization of our feature-enrichment strategy through cross-dataset\ntests on ForestSemantic and Semantic3D.\n  Our contributions include: (i) a robust, uncertainty-aware TLS annotation\npipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii)\nempirical guidance on data efficiency and feature importance, thus enabling\nscalable, high-quality segmentation of TLS point clouds for ecological\nmonitoring and beyond. The dataset and processing scripts are publicly\navailable at https://fz-rit.github.io/through-the-lidars-eye/.", "AI": {"tldr": "A semi-automated, uncertainty-aware pipeline projects TLS point clouds to 2D, enriches features, ensembles segmenters for pseudo-labels and uncertainty, guides targeted annotation, and back-projects to 3D\u2014yielding accurate labels with less manual effort. It produces the Mangrove3D dataset and shows strong data efficiency (\u224812 scans), with geometric features dominating and ~0.76 mIoU using compact 9-channel features; the approach generalizes across datasets.", "motivation": "High-quality semantic segmentation of TLS point clouds requires expensive manual annotation. The authors aim to reduce labeling cost while maintaining accuracy, provide a new mangrove-specific TLS dataset, and offer empirical guidance on how much data and which features matter most.", "method": "1) Project 3D TLS points onto a 2D spherical grid. 2) Enrich pixels with multi-source features (notably geometric). 3) Train an ensemble of segmentation networks to produce pseudo-labels and uncertainty maps. 4) Use uncertainty to prioritize human annotation of ambiguous regions. 5) Back-project 2D outputs to 3D. 6) Provide a three-tier visualization suite (2D feature maps, 3D colorized point clouds, virtual spheres) to accelerate review. 7) Build Mangrove3D and conduct data-efficiency and feature-importance studies; validate generalization on ForestSemantic and Semantic3D.", "result": "Performance saturates after about 12 annotated scans; geometric features contribute most; a compact 9-channel feature set captures nearly all discriminative power. Mean IoU plateaus around 0.76. Cross-dataset tests support generalization of the feature-enrichment strategy. The pipeline reduces labeling effort while sustaining high accuracy (qualitatively stated).", "conclusion": "Contributions are: (i) a robust, uncertainty-aware TLS annotation pipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii) practical guidance on data efficiency and feature importance. These enable scalable, high-quality segmentation for ecological monitoring and related domains; data and code are publicly available."}}
{"id": "2510.07091", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07091", "abs": "https://arxiv.org/abs/2510.07091", "authors": ["Baixuan Xu", "Tianshi Zheng", "Zhaowei Wang", "Hong Ting Tsang", "Weiqi Wang", "Tianqing Fang", "Yangqiu Song"], "title": "The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas", "comment": "22 pages", "summary": "Enabling LLMs to effectively operate long-horizon task which requires\nlong-term planning and multiple interactions is essential for open-world\nautonomy. Conventional methods adopt planning with actions where a executable\naction list would be provided as reference. However, this action representation\nchoice would be impractical when the environment action space is combinatorial\nexploded (e.g., open-ended real world). This naturally leads to a question: As\nenvironmental action space scales, what is the optimal action representation\nfor long-horizon agents? In this paper, we systematically study the\neffectiveness of two different action representations. The first one is\nconventional planning with actions (PwA) which is predominantly adopted for its\neffectiveness on existing benchmarks. The other one is planning with schemas\n(PwS) which instantiate an action schema into action lists (e.g., \"move [OBJ]\nto [OBJ]\" -> \"move apple to desk\") to ensure concise action space and reliable\nscalability. This alternative is motivated by its alignment with human\ncognition and its compliance with environment-imposed action format\nrestriction. We propose cognitive bandwidth perspective as a conceptual\nframework to qualitatively understand the differences between these two action\nrepresentations and empirically observe a representation-choice inflection\npoint between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve\nas evidence of the need for scalable representations. We further conduct\ncontrolled experiments to study how the location of this inflection point\ninteracts with different model capacities: stronger planning proficiency shifts\nthe inflection rightward, whereas better schema instantiation shifts it\nleftward. Finally, noting the suboptimal performance of PwS agents, we provide\nan actionable guide for building more capable PwS agents for better scalable\nautonomy.", "AI": {"tldr": "Compares two action representations for long-horizon LLM agents\u2014concrete actions (PwA) vs schema-based actions (PwS)\u2014introduces a \u201ccognitive bandwidth\u201d lens, identifies an inflection point where PwS overtakes PwA as action spaces grow, analyzes how model capabilities shift this point, and offers guidance to build stronger PwS agents.", "motivation": "Open-world autonomy requires agents that plan over long horizons, but enumerating executable actions becomes impractical as the environment\u2019s action space explodes. The authors ask which action representation scales best and notes schema-based planning aligns with human cognition and environment constraints.", "method": "Propose a cognitive bandwidth conceptual framework; compare PwA and PwS across benchmarks with different action-space sizes (ALFWorld \u224835 actions; SciWorld \u2248500 actions); run controlled experiments varying two capabilities\u2014planning proficiency and schema instantiation quality\u2014to see how the representation-choice inflection point shifts; synthesize practical guidelines for PwS agents.", "result": "Empirically observe a representation-choice inflection point between ALFWorld and SciWorld: PwA excels in small action spaces, while PwS scales better in larger ones. Increasing planning proficiency shifts the inflection rightward (PwA remains viable longer); improving schema instantiation shifts it leftward (PwS becomes preferable sooner). Current PwS agents underperform but can be improved with targeted design.", "conclusion": "Action representation should be chosen based on environment scale and model skills: use PwA for smaller, well-defined action spaces; prefer PwS for large, open-ended spaces. Investing in schema instantiation reduces cognitive load and improves scalability. The paper offers actionable steps to make PwS agents more capable, enabling scalable long-horizon autonomy."}}
{"id": "2510.06461", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06461", "abs": "https://arxiv.org/abs/2510.06461", "authors": ["Massimo Daul", "Alessio Tosolini", "Claire Bowern"], "title": "Linguistically Informed Tokenization Improves ASR for Underresourced Languages", "comment": null, "summary": "Automatic speech recognition (ASR) is a crucial tool for linguists aiming to\nperform a variety of language documentation tasks. However, modern ASR systems\nuse data-hungry transformer architectures, rendering them generally unusable\nfor underresourced languages. We fine-tune a wav2vec2 ASR model on Yan-nhangu,\na dormant Indigenous Australian language, comparing the effects of phonemic and\northographic tokenization strategies on performance. In parallel, we explore\nASR's viability as a tool in a language documentation pipeline. We find that a\nlinguistically informed phonemic tokenization system substantially improves WER\nand CER compared to a baseline orthographic tokenization scheme. Finally, we\nshow that hand-correcting the output of an ASR model is much faster than\nhand-transcribing audio from scratch, demonstrating that ASR can work for\nunderresourced languages.", "AI": {"tldr": "Fine-tuning wav2vec2 for the underresourced Yan-nhangu language, the authors show that phonemic tokenization markedly improves ASR accuracy and that correcting ASR output is faster than transcribing from scratch, supporting ASR\u2019s practicality for language documentation.", "motivation": "Modern ASR is data-hungry and often unusable for underresourced languages, yet linguists need efficient tools to document such languages, including dormant ones like Yan-nhangu.", "method": "Fine-tune a wav2vec2 ASR model on Yan-nhangu; compare orthographic vs linguistically informed phonemic tokenization; evaluate with WER/CER; time the effort of hand-correcting ASR output versus manual transcription; assess integration into a documentation pipeline.", "result": "Phonemic tokenization substantially reduces WER and CER relative to an orthographic baseline. Post-editing ASR output is significantly faster than manual transcription from scratch.", "conclusion": "Linguistically informed phonemic tokenization makes low-resource ASR more accurate, and ASR-assisted transcription can accelerate documentation workflows for underresourced languages."}}
{"id": "2510.06584", "categories": ["cs.CV", "q-bio.TO"], "pdf": "https://arxiv.org/pdf/2510.06584", "abs": "https://arxiv.org/abs/2510.06584", "authors": ["Justin Cheung", "Samuel Savine", "Calvin Nguyen", "Lin Lu", "Alhassan S. Yasin"], "title": "Improving Artifact Robustness for CT Deep Learning Models Without Labeled Artifact Images via Domain Adaptation", "comment": "8 pages, 12 figures, 1 table", "summary": "Deep learning models which perform well on images from their training\ndistribution can degrade substantially when applied to new distributions. If a\nCT scanner introduces a new artifact not present in the training labels, the\nmodel may misclassify the images. Although modern CT scanners include design\nfeatures which mitigate these artifacts, unanticipated or difficult-to-mitigate\nartifacts can still appear in practice. The direct solution of labeling images\nfrom this new distribution can be costly. As a more accessible alternative,\nthis study evaluates domain adaptation as an approach for training models that\nmaintain classification performance despite new artifacts, even without\ncorresponding labels. We simulate ring artifacts from detector gain error in\nsinogram space and evaluate domain adversarial neural networks (DANN) against\nbaseline and augmentation-based approaches on the OrganAMNIST abdominal CT\ndataset. Our results demonstrate that baseline models trained only on clean\nimages fail to generalize to images with ring artifacts, and traditional\naugmentation with other distortion types provides no improvement on unseen\nartifact domains. In contrast, the DANN approach successfully maintains high\nclassification accuracy on ring artifact images using only unlabeled artifact\ndata during training, demonstrating the viability of domain adaptation for\nartifact robustness. The domain-adapted model achieved classification\nperformance on ring artifact test data comparable to models explicitly trained\nwith labeled artifact images, while also showing unexpected generalization to\nuniform noise. These findings provide empirical evidence that domain adaptation\ncan effectively address distribution shift in medical imaging without requiring\nexpensive expert labeling of new artifact distributions, suggesting promise for\ndeployment in clinical settings where novel artifacts may emerge.", "AI": {"tldr": "Using domain adversarial neural networks (DANN) with only unlabeled, artifacted CT images, the authors maintain classification accuracy under ring-artifact distribution shifts, outperforming clean-trained and augmentation-based baselines and matching models trained with labeled artifacts, with some spillover robustness to uniform noise.", "motivation": "CT images can suffer from scanner-specific artifacts that are absent during training, causing deep models to fail. Obtaining expert labels for every new artifact domain is costly. The study explores whether unsupervised domain adaptation can deliver robustness to novel artifacts without requiring new labels.", "method": "Simulate ring artifacts via detector gain error in sinogram space on the OrganAMNIST abdominal CT dataset. Compare (i) a baseline trained on clean images, (ii) traditional augmentation with other distortions, and (iii) a Domain Adversarial Neural Network (DANN) trained with labeled clean source data and unlabeled artifacted target data.", "result": "Baseline models trained only on clean images perform poorly on ring-artifact images; augmentations with other distortions do not help on unseen ring-artifact domains. DANN maintains high accuracy on ring-artifact test data using only unlabeled artifact data and performs comparably to fully supervised models trained with labeled artifact images. It also shows unexpected generalization to uniform noise.", "conclusion": "Unsupervised domain adaptation, specifically DANN, can mitigate distribution shift from CT artifacts without additional labeling, making it a practical route toward artifact-robust medical imaging models suitable for clinical settings where new artifacts may arise."}}
{"id": "2510.07117", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07117", "abs": "https://arxiv.org/abs/2510.07117", "authors": ["Leonardo Christov-Moore", "Arthur Juliani", "Alex Kiefer", "Nicco Reggente", "B. Scott Rousse", "Adam Safron", "Nicol'as Hinrichs", "Daniel Polani", "Antonio Damasio"], "title": "The Contingencies of Physical Embodiment Allow for Open-Endedness and Care", "comment": "15 pages, 1 figure", "summary": "Physical vulnerability and mortality are often seen as obstacles to be\navoided in the development of artificial agents, which struggle to adapt to\nopen-ended environments and provide aligned care. Meanwhile, biological\norganisms survive, thrive, and care for each other in an open-ended physical\nworld with relative ease and efficiency. Understanding the role of the\nconditions of life in this disparity can aid in developing more robust,\nadaptive, and caring artificial agents. Here we define two minimal conditions\nfor physical embodiment inspired by the existentialist phenomenology of Martin\nHeidegger: being-in-the-world (the agent is a part of the environment) and\nbeing-towards-death (unless counteracted, the agent drifts toward terminal\nstates due to the second law of thermodynamics). We propose that from these\nconditions we can obtain both a homeostatic drive - aimed at maintaining\nintegrity and avoiding death by expending energy to learn and act - and an\nintrinsic drive to continue to do so in as many ways as possible. Drawing\ninspiration from Friedrich Nietzsche's existentialist concept of will-to-power,\nwe examine how intrinsic drives to maximize control over future states, e.g.,\nempowerment, allow agents to increase the probability that they will be able to\nmeet their future homeostatic needs, thereby enhancing their capacity to\nmaintain physical integrity. We formalize these concepts within a reinforcement\nlearning framework, which enables us to examine how intrinsically driven\nembodied agents learning in open-ended multi-agent environments may cultivate\nthe capacities for open-endedness and care.ov", "AI": {"tldr": "The paper argues that embedding artificial agents as vulnerable, energy-dissipating bodies (subject to death) yields homeostatic and intrinsic control-seeking drives that can be formalized in RL to produce more robust, adaptive, and caring behavior in open-ended, multi-agent worlds.", "motivation": "Artificial agents struggle to adapt and provide aligned care in open-ended physical environments, whereas biological organisms do so efficiently. The authors seek fundamental conditions of life that could explain this gap and inform the design of agents that are robust, adaptive, and capable of caring behavior.", "method": "They define two minimal embodiment conditions\u2014being-in-the-world (agent as part of the environment) and being-towards-death (tendency toward terminal states due to thermodynamics). From these, they derive: (1) a homeostatic drive to maintain integrity and avoid death, and (2) an intrinsic drive to maximize future control (inspired by Nietzsche\u2019s will-to-power, operationalized via empowerment). They formalize these drives within a reinforcement learning framework for agents in open-ended, multi-agent environments.", "result": "Primarily a conceptual and formal framework: maximizing empowerment increases the likelihood of meeting future homeostatic needs, thereby strengthening an agent\u2019s capacity to maintain integrity. The formalization shows how intrinsically driven, embodied agents could cultivate open-endedness and care; no concrete empirical benchmarks are reported in the abstract.", "conclusion": "Physical vulnerability and thermodynamic pressures, when modeled as homeostatic and empowerment-based intrinsic drives in RL, may yield agents that better maintain integrity and can exhibit caring behaviors in open-ended settings. Embodiment plus intrinsic control-seeking could be a principled route to robustness, adaptability, and care in artificial agents."}}
{"id": "2510.06471", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.06471", "abs": "https://arxiv.org/abs/2510.06471", "authors": ["Zihao Li", "Shaoxiong Ji", "J\u00f6rg Tiedemann"], "title": "Test-Time Scaling of Reasoning Models for Machine Translation", "comment": null, "summary": "Test-time scaling (TTS) has enhanced the performance of Reasoning Models\n(RMs) on various tasks such as math and coding, yet its efficacy in machine\ntranslation (MT) remains underexplored. This paper investigates whether\nincreased inference-time computation improves translation quality. We evaluate\n12 RMs across a diverse suite of MT benchmarks spanning multiple domains,\nexamining three scenarios: direct translation, forced-reasoning extrapolation,\nand post-editing. Our findings show that for general-purpose RMs, TTS provides\nlimited and inconsistent benefits for direct translation, with performance\nquickly plateauing. However, the effectiveness of TTS is unlocked by\ndomain-specific fine-tuning, which aligns a model's reasoning process with task\nrequirements, leading to consistent improvements up to an optimal,\nself-determined reasoning depth. We also find that forcing a model to reason\nbeyond its natural stopping point consistently degrades translation quality. In\ncontrast, TTS proves highly effective in a post-editing context, reliably\nturning self-correction into a beneficial process. These results indicate that\nthe value of inference-time computation in MT lies not in enhancing single-pass\ntranslation with general models, but in targeted applications like multi-step,\nself-correction workflows and in conjunction with task-specialized models.", "AI": {"tldr": "Test-time scaling helps machine translation mainly when models are domain-specialized or used for multi-step post-editing; general-purpose models see small, inconsistent gains and over-reasoning harms quality.", "motivation": "While test-time scaling improves reasoning tasks like math and coding, its impact on machine translation is unclear; understanding when extra inference-time computation pays off can guide practical MT system design.", "method": "Evaluate 12 reasoning models on diverse MT benchmarks across three setups: (1) direct translation with increasing inference-time compute, (2) forced reasoning beyond the model\u2019s natural stopping point, and (3) post-editing workflows; study the effect of domain-specific fine-tuning and identify optimal reasoning depth.", "result": "General-purpose models show limited and quickly plateauing gains for direct translation under TTS. Domain-specific fine-tuning aligns reasoning with MT demands, yielding consistent improvements up to a model-specific optimal depth; pushing beyond that degrades outputs. TTS is notably effective in post-editing, turning self-correction into reliable quality gains.", "conclusion": "Inference-time compute is not broadly effective for single-pass MT with general models. Its value emerges in targeted settings: domain-finetuned models with self-selected stopping and multi-step post-editing/self-correction workflows. Avoid forcing excessive reasoning depth."}}
{"id": "2510.06590", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06590", "abs": "https://arxiv.org/abs/2510.06590", "authors": ["Ziyuan Huang", "DanDan Zheng", "Cheng Zou", "Rui Liu", "Xiaolong Wang", "Kaixiang Ji", "Weilong Chai", "Jianxin Sun", "Libin Wang", "Yongjie Lv", "Taozhi Huang", "Jiajia Liu", "Qingpei Guo", "Ming Yang", "Jingdong Chen", "Jun Zhou"], "title": "Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer", "comment": "Code released at https://github.com/inclusionAI/Ming-UniVision", "summary": "Visual tokenization remains a core challenge in unifying visual understanding\nand generation within the autoregressive paradigm. Existing methods typically\nemploy tokenizers in discrete latent spaces to align with the tokens from large\nlanguage models, where the quantization errors can limit semantic\nexpressiveness and degrade the capability of vision-language understanding. To\naddress this, we introduce MingTok, a new family of visual tokenizers with a\ncontinuous latent space, for unified autoregressive generation and\nunderstanding. While understanding tasks favor discriminative high-dimensional\nfeatures, generation tasks prefer compact low-level codes. Thus, to reconcile\nthese competing demands, MingTok adopts a three-stage sequential architecture\ninvolving low-level encoding, semantic expansion, and visual reconstruction.\nBuilt on top of it, Ming-UniVision eliminates the need for task-specific visual\nrepresentations, and unifies diverse vision-language tasks under a single\nautoregrsssive prediction paradigm. By formulating both understanding and\ngeneration as next-token prediction in a shared continuous space, it seamlessly\nsupports multi-round, in-context tasks such as iterative understanding,\ngeneration and editing. Empirically, we find that using a unified continuous\nvisual representation reconciles the competing requirements on the tokenizers\nby the understanding and generation tasks, thereby leading to state-of-the-art\nlevel performance across both domains. We hope our findings will facilitate\nunified visual tokenization in the continuous domain. Inference code and model\nweights are released to benefit community.", "AI": {"tldr": "MingTok introduces a continuous visual tokenizer and a unified autoregressive framework (Ming-UniVision) that formulates both understanding and generation as next-token prediction in a shared continuous space, achieving state-of-the-art performance across both domains.", "motivation": "Discrete latent tokenizers aligned to LLM tokens suffer from quantization errors, reducing semantic expressiveness and harming vision-language understanding. Moreover, understanding tasks prefer high-dimensional discriminative features, while generation favors compact low-level codes\u2014creating a tension that current tokenizers cannot reconcile.", "method": "Propose MingTok, a family of continuous latent visual tokenizers with a three-stage pipeline: (1) low-level encoding, (2) semantic expansion, and (3) visual reconstruction. Build Ming-UniVision on top to eliminate task-specific visual representations and unify diverse vision-language tasks under a single autoregressive next-token prediction paradigm in the shared continuous space, supporting multi-round in-context understanding, generation, and editing.", "result": "Using unified continuous visual representations reconciles the competing requirements of understanding and generation, yielding state-of-the-art-level results across both domains. The approach supports iterative, multi-round tasks. Inference code and model weights are released.", "conclusion": "Continuous visual tokenization can unify visual understanding and generation within an autoregressive framework, mitigating quantization-induced limitations of discrete tokenizers and enabling seamless, iterative, in-context multimodal tasks. The work positions continuous token spaces as a promising direction for unified vision-language modeling."}}
{"id": "2510.07161", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07161", "abs": "https://arxiv.org/abs/2510.07161", "authors": ["Ali Norouzifar", "Humam Kourani", "Marcus Dees", "Wil van der Aalst"], "title": "Integrating Domain Knowledge into Process Discovery Using Large Language Models", "comment": "This paper is currently under review for publication in a journal", "summary": "Process discovery aims to derive process models from event logs, providing\ninsights into operational behavior and forming a foundation for conformance\nchecking and process improvement. However, models derived solely from event\ndata may not accurately reflect the real process, as event logs are often\nincomplete or affected by noise, and domain knowledge, an important\ncomplementary resource, is typically disregarded. As a result, the discovered\nmodels may lack reliability for downstream tasks. We propose an interactive\nframework that incorporates domain knowledge, expressed in natural language,\ninto the process discovery pipeline using Large Language Models (LLMs). Our\napproach leverages LLMs to extract declarative rules from textual descriptions\nprovided by domain experts. These rules are used to guide the IMr discovery\nalgorithm, which recursively constructs process models by combining insights\nfrom both the event log and the extracted rules, helping to avoid problematic\nprocess structures that contradict domain knowledge. The framework coordinates\ninteractions among the LLM, domain experts, and a set of backend services. We\npresent a fully implemented tool that supports this workflow and conduct an\nextensive evaluation of multiple LLMs and prompt engineering strategies. Our\nempirical study includes a case study based on a real-life event log with the\ninvolvement of domain experts, who assessed the usability and effectiveness of\nthe framework.", "AI": {"tldr": "Interactive framework that uses LLMs to turn natural-language domain knowledge into declarative rules that steer the IMr process discovery algorithm, aiming to produce process models more faithful to reality; implemented and empirically evaluated with a real-life case study and expert feedback.", "motivation": "Process models discovered solely from event logs can be inaccurate due to incomplete/noisy data and the neglect of valuable domain knowledge, undermining conformance checking and improvement tasks. The work seeks to integrate domain expertise into discovery without heavy formalization burdens on experts.", "method": "LLMs extract declarative rules from experts\u2019 textual descriptions; these rules guide the IMr recursive discovery algorithm to avoid structures that contradict domain knowledge. A tool coordinates interactions among the LLM, domain experts, and backend services. Multiple LLMs and prompt strategies are compared.", "result": "A complete tool is presented. Extensive experiments evaluate different LLMs and prompts; a real-life case study is conducted with domain experts who assess usability and effectiveness. The framework shows it can guide IMr and mitigate problematic structures, though the abstract does not report quantitative gains.", "conclusion": "Integrating LLM-derived domain rules with event-log data in an interactive loop is feasible and improves the trustworthiness and alignment of discovered process models for downstream use. The human-in-the-loop design supports practical adoption and suggests benefits over event-log-only discovery."}}
{"id": "2510.06499", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06499", "abs": "https://arxiv.org/abs/2510.06499", "authors": ["Zhepeng Cen", "Haolin Chen", "Shiyu Wang", "Zuxin Liu", "Zhiwei Liu", "Ding Zhao", "Silvio Savarese", "Caiming Xiong", "Huan Wang", "Weiran Yao"], "title": "Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success through\nimitation learning on vast text corpora, but this paradigm creates a\ntraining-generation gap and limits robust reasoning. Reinforcement learning\n(RL) offers a more data-efficient solution capable of bridging this gap, yet\nits application has been constrained by a critical data bottleneck: existing RL\ndatasets are orders of magnitude smaller and less diverse than web-scale\npre-training corpora. To address this, we introduce the Webscale-RL pipeline, a\nscalable data engine that systematically converts large-scale pre-training\ndocuments into millions of diverse, verifiable question-answer pairs for RL.\nUsing this pipeline, we construct the Webscale-RL dataset, containing 1.2\nmillion examples across more than 9 domains. Our experiments show that the\nmodel trained on this dataset significantly outperforms continual pretraining\nand strong data refinement baselines across a suite of benchmarks. Notably, RL\ntraining with our dataset proves substantially more efficient, achieving the\nperformance of continual pre-training with up to 100$\\times$ fewer tokens. Our\nwork presents a viable path toward scaling RL to pre-training levels, enabling\nmore capable and efficient language models.", "AI": {"tldr": "They build a scalable pipeline that converts web-scale pretraining text into millions of verifiable QA tasks for RL, yielding a 1.2M-example, 9+ domain dataset that lets RL-trained LMs outperform strong baselines and match continual pretraining with up to 100\u00d7 fewer tokens.", "motivation": "Imitation-learning on large corpora creates a train\u2013generate mismatch and weakens robust reasoning. RL could fix this more data\u2011efficiently, but lacks web\u2011scale, diverse training data; existing RL datasets are tiny compared to pretraining corpora.", "method": "Introduce Webscale-RL: a data engine that systematically transforms pretraining documents into diverse, verifiable question\u2013answer pairs suitable for RL. Use it to assemble a 1.2M-example, 9+ domain dataset and train LMs with RL on these tasks.", "result": "Across multiple benchmarks, models trained with this dataset via RL significantly beat continual pretraining and strong data-refinement baselines. Efficiency improves markedly: similar performance to continual pretraining is achieved with up to 100\u00d7 fewer tokens.", "conclusion": "Converting web-scale corpora into RL-ready, verifiable tasks enables scaling RL to pretraining levels, yielding more capable and token-efficient language models and offering a practical route to stronger reasoning."}}
{"id": "2510.06592", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06592", "abs": "https://arxiv.org/abs/2510.06592", "authors": ["Tianyue Xu", "Yanlin Wu", "Abhai K. Tripathi", "Matthew M. Ippolito", "Benjamin D. Haeffele"], "title": "Adaptive Stain Normalization for Cross-Domain Medical Histology", "comment": "Accepted to the 28th International Conference on Medical Image\n  Computing and Computer-Assisted Intervention (MICCAI 2025)", "summary": "Deep learning advances have revolutionized automated digital pathology\nanalysis. However, differences in staining protocols and imaging conditions can\nintroduce significant color variability. In deep learning, such color\ninconsistency often reduces performance when deploying models on data acquired\nunder different conditions from the training data, a challenge known as domain\nshift. Many existing methods attempt to address this problem via color\nnormalization but suffer from several notable drawbacks such as introducing\nartifacts or requiring careful choice of a template image for stain mapping. To\naddress these limitations, we propose a trainable color normalization model\nthat can be integrated with any backbone network for downstream tasks such as\nobject detection and classification. Based on the physics of the imaging\nprocess per the Beer-Lambert law, our model architecture is derived via\nalgorithmic unrolling of a nonnegative matrix factorization (NMF) model to\nextract stain-invariant structural information from the original pathology\nimages, which serves as input for further processing. Experimentally, we\nevaluate the method on publicly available pathology datasets and an internally\ncurated collection of malaria blood smears for cross-domain object detection\nand classification, where our method outperforms many state-of-the-art stain\nnormalization methods. Our code is available at\nhttps://github.com/xutianyue/BeerLaNet.", "AI": {"tldr": "BeerLaNet: a physics-guided, trainable color-normalization module\u2014derived from the Beer\u2013Lambert law via unrolled NMF\u2014plugged before standard backbones to produce stain-invariant representations, boosting cross-domain pathology detection/classification and outperforming SOTA normalization methods.", "motivation": "Color/stain and imaging-condition variability cause domain shift in digital pathology, degrading model performance when test data differ from training data. Existing color normalization often introduces artifacts and depends on carefully chosen templates.", "method": "Design a trainable color normalization model grounded in the Beer\u2013Lambert imaging physics. Use algorithmic unrolling of a nonnegative matrix factorization (NMF) to separate stain components and extract stain-invariant structural information. Insert this module ahead of arbitrary backbones for downstream tasks (detection/classification).", "result": "On public pathology datasets and an internal malaria blood-smear set for cross-domain detection and classification, the proposed method surpasses many state-of-the-art stain normalization approaches.", "conclusion": "A physics-informed, unrolled NMF color-normalization module effectively mitigates stain variability, improves robustness across domains, and is broadly compatible with existing deep learning pipelines. Code is publicly available (https://github.com/xutianyue/BeerLaNet)."}}
{"id": "2510.07172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07172", "abs": "https://arxiv.org/abs/2510.07172", "authors": ["Tianshi Zheng", "Kelvin Kiu-Wai Tam", "Newt Hue-Nam K. Nguyen", "Baixuan Xu", "Zhaowei Wang", "Jiayang Cheng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Tianqing Fang", "Yangqiu Song", "Ginny Y. Wong", "Simon See"], "title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents", "comment": "60 pages, 18 figures, 13 tables", "summary": "Large language models are emerging as powerful tools for scientific law\ndiscovery, a foundational challenge in AI-driven science. However, existing\nbenchmarks for this task suffer from a fundamental methodological trilemma,\nforcing a trade-off between scientific relevance, scalability, and resistance\nto memorization. Furthermore, they oversimplify discovery as static function\nfitting, failing to capture the authentic scientific process of uncovering\nembedded laws through the interactive exploration of complex model systems. To\naddress these critical gaps, we introduce NewtonBench, a benchmark comprising\n324 scientific law discovery tasks across 12 physics domains. Our design\nmitigates the evaluation trilemma by using metaphysical shifts - systematic\nalterations of canonical laws - to generate a vast suite of problems that are\nscalable, scientifically relevant, and memorization-resistant. Moreover, we\nelevate the evaluation from static function fitting to interactive model\ndiscovery, requiring agents to experimentally probe simulated complex systems\nto uncover hidden principles. Our extensive experiment reveals a clear but\nfragile capability for discovery in frontier LLMs: this ability degrades\nprecipitously with increasing system complexity and exhibits extreme\nsensitivity to observational noise. Notably, we uncover a paradoxical effect of\ntool assistance: providing a code interpreter can hinder more capable models by\ninducing a premature shift from exploration to exploitation, causing them to\nsatisfice on suboptimal solutions. These results demonstrate that robust,\ngeneralizable discovery in complex, interactive environments remains the core\nchallenge. By providing a scalable, robust, and scientifically authentic\ntestbed, NewtonBench offers a crucial tool for measuring true progress and\nguiding the development of next-generation AI agents capable of genuine\nscientific discovery.", "AI": {"tldr": "NewtonBench is a new benchmark for interactive scientific law discovery that uses systematic alterations of physical laws to create scalable, relevant, and memorization-resistant tasks; it shows that current LLMs\u2019 discovery abilities are fragile, degrade with complexity and noise, and can be harmed by code-interpreter tools that encourage premature exploitation.", "motivation": "Existing benchmarks trade off among scientific relevance, scalability, and memorization resistance, and reduce discovery to static function fitting. The authors seek a testbed that reflects authentic scientific practice\u2014interactive probing of systems to infer hidden laws\u2014while avoiding leakage and scaling limits.", "method": "They construct NewtonBench: 324 tasks across 12 physics domains. The key design lever is \u201cmetaphysical shifts,\u201d systematic deviations from canonical laws, generating diverse, non-memorized targets. Evaluation is interactive: agents must design experiments within simulated complex systems to uncover governing principles. They test frontier LLMs, with and without tool assistance (e.g., code interpreters).", "result": "Frontier LLMs exhibit some capacity for discovery, but it degrades sharply with increasing system complexity and is highly sensitive to observational noise. Tool assistance can backfire: providing a code interpreter nudges capable models into early exploitation, leading them to settle on suboptimal hypotheses (satisficing).", "conclusion": "Robust, generalizable discovery in complex, noisy, interactive environments remains unsolved. NewtonBench offers a scalable, realistic benchmark that can track genuine progress and guide development of AI agents better at experimental design, hypothesis generation, and noise-robust inference."}}
{"id": "2510.06548", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06548", "abs": "https://arxiv.org/abs/2510.06548", "authors": ["Seng Pei Liew", "Takuya Kato"], "title": "From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining", "comment": "22 pages, 11 figures, an abridged version to appear in NeurIPS 2025\n  LLM Evaluation Workshop", "summary": "Bootstrapped pretraining, i.e., the reuse of a pretrained base model for\nfurther pretraining, such as continual pretraining or model growth, is\npromising at reducing the cost of training language models from scratch.\nHowever, its effectiveness remains unclear, especially when applied to\novertrained base models. In this work, we empirically study the scaling\nbehavior of bootstrapped pretraining and find that its scaling efficiency\ndiminishes in a predictable manner: The scaling exponent with respect to\nsecond-stage pretraining tokens decreases logarithmically with the number of\ntokens used to pretrain the base model. The joint dependence on first- and\nsecond-stage tokens is accurately modeled by a simple scaling law. Such\nsaturation effect reveals a fundamental trade-off in multi-stage pretraining\nstrategies: the more extensively a model is pretrained, the less additional\nbenefit bootstrapping provides. Our findings provide practical insights for\nefficient language model training and raise important considerations for the\nreuse of overtrained models.", "AI": {"tldr": "Bootstrapping a pretrained language model for additional pretraining yields predictably diminishing returns: the scaling exponent for second\u2011stage tokens shrinks roughly logarithmically with how many tokens the base model already saw. A simple joint scaling law over first\u2011 and second\u2011stage tokens captures this saturation, implying limited benefit from reusing heavily overtrained bases.", "motivation": "Reduce cost versus training from scratch by reusing pretrained models (continual pretraining or model growth), and clarify when this is efficient\u2014especially given uncertainty about benefits when the base is already overtrained.", "method": "Empirical scaling study: vary the amount of first\u2011stage (base) pretraining tokens and measure performance gains from additional second\u2011stage tokens. Fit and validate a simple scaling law that jointly models dependence on both token budgets and extract the scaling exponent for second\u2011stage training.", "result": "Scaling efficiency of bootstrapped pretraining diminishes in a predictable way: the scaling exponent with respect to second\u2011stage tokens decreases logarithmically with base tokens. A compact joint scaling law accurately models performance as a function of both stages, clearly showing saturation.", "conclusion": "There is a fundamental trade\u2011off in multi\u2011stage pretraining: the more you pretrain the base, the less incremental benefit you get from further bootstrapping. For efficient training, avoid overtraining bases you plan to reuse, and be cautious about relying on bootstrapping when the base is already saturated."}}
{"id": "2510.06596", "categories": ["cs.CV", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.06596", "abs": "https://arxiv.org/abs/2510.06596", "authors": ["Ayush Zenith", "Arnold Zumbrun", "Neel Raut", "Jing Lin"], "title": "SDQM: Synthetic Data Quality Metric for Object Detection Dataset Evaluation", "comment": null, "summary": "The performance of machine learning models depends heavily on training data.\nThe scarcity of large-scale, well-annotated datasets poses significant\nchallenges in creating robust models. To address this, synthetic data generated\nthrough simulations and generative models has emerged as a promising solution,\nenhancing dataset diversity and improving the performance, reliability, and\nresilience of models. However, evaluating the quality of this generated data\nrequires an effective metric. This paper introduces the Synthetic Dataset\nQuality Metric (SDQM) to assess data quality for object detection tasks without\nrequiring model training to converge. This metric enables more efficient\ngeneration and selection of synthetic datasets, addressing a key challenge in\nresource-constrained object detection tasks. In our experiments, SDQM\ndemonstrated a strong correlation with the mean Average Precision (mAP) scores\nof YOLOv11, a leading object detection model, while previous metrics only\nexhibited moderate or weak correlations. Additionally, it provides actionable\ninsights for improving dataset quality, minimizing the need for costly\niterative training. This scalable and efficient metric sets a new standard for\nevaluating synthetic data. The code for SDQM is available at\nhttps://github.com/ayushzenith/SDQM", "AI": {"tldr": "Proposes SDQM, a fast, training-free (or early-training) metric to evaluate synthetic datasets for object detection; shows strong correlation with YOLOv11 mAP, outperforming prior metrics; aims to guide dataset generation/selection and cut costly iterative training; code released.", "motivation": "High dependence of ML models on data quality and scarcity of large, annotated datasets. Synthetic data helps, but there is no reliable, efficient way to assess its utility for object detection without expensive full training runs.", "method": "Introduce Synthetic Dataset Quality Metric (SDQM) to score synthetic datasets for object detection without requiring a model to train to convergence. Validate SDQM by correlating it with downstream performance (mAP) of YOLOv11 and compare against existing metrics.", "result": "SDQM correlates strongly with YOLOv11 mAP, whereas prior metrics show only moderate/weak correlation. The metric provides actionable signals for improving datasets and reduces the need for iterative, compute-heavy training cycles.", "conclusion": "SDQM offers a scalable, efficient standard for evaluating synthetic datasets in object detection, enabling better dataset selection and generation under resource constraints. Public code is available for adoption and verification."}}
{"id": "2510.07276", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.07276", "abs": "https://arxiv.org/abs/2510.07276", "authors": ["Pulkit Rustagi", "Kyle Hollins Wray", "Sandhya Saisubramanian"], "title": "Multi-Objective Multi-Agent Path Finding with Lexicographic Cost Preferences", "comment": "8 pages, 7 figures", "summary": "Many real-world scenarios require multiple agents to coordinate in shared\nenvironments, while balancing trade-offs between multiple, potentially\ncompeting objectives. Current multi-objective multi-agent path finding\n(MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto\nfrontiers. They do not explicitly optimize for user-defined preferences, even\nwhen the preferences are available, and scale poorly with the number of\nobjectives. We propose a lexicographic framework for modeling MO-MAPF, along\nwith an algorithm \\textit{Lexicographic Conflict-Based Search} (LCBS) that\ndirectly computes a single solution aligned with a lexicographic preference\nover objectives. LCBS integrates a priority-aware low-level $A^*$ search with\nconflict-based search, avoiding Pareto frontier construction and enabling\nefficient planning guided by preference over objectives. We provide insights\ninto optimality and scalability, and empirically demonstrate that LCBS computes\noptimal solutions while scaling to instances with up to ten objectives -- far\nbeyond the limits of existing MO-MAPF methods. Evaluations on standard and\nrandomized MAPF benchmarks show consistently higher success rates against\nstate-of-the-art baselines, especially with increasing number of objectives.", "AI": {"tldr": "Proposes LCBS, a lexicographic conflict-based search that finds a single preference-aligned, conflict-free multi-agent path without building Pareto fronts, yielding optimal solutions and scaling to many objectives with higher success rates than prior MO-MAPF methods.", "motivation": "Existing MO-MAPF approaches rely on Pareto frontiers, which (1) ignore explicit user preference orderings over objectives and (2) scale poorly as the number of objectives grows. There is a need for an approach that directly optimizes user-specified priorities while remaining computationally tractable.", "method": "Model MO-MAPF with a lexicographic ordering of objectives and introduce LCBS (Lexicographic Conflict-Based Search). LCBS integrates a priority-aware low-level A* with high-level Conflict-Based Search, using the lexicographic order to guide search and avoid constructing Pareto frontiers, thereby computing a single solution consistent with the specified preference order.", "result": "Provides theoretical insights on optimality and scalability. Experiments on standard and randomized MAPF benchmarks show that LCBS finds optimal solutions, scales to problems with up to 10 objectives, and achieves consistently higher success rates than state-of-the-art baselines, with larger gains as the number of objectives increases.", "conclusion": "Lexicographic modeling combined with conflict-based search offers an efficient, preference-aligned solution for MO-MAPF, outperforming Pareto-frontier methods especially as objectives increase, and enabling scalable optimal planning in multi-agent settings."}}
{"id": "2510.06552", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06552", "abs": "https://arxiv.org/abs/2510.06552", "authors": ["Tarek Naous", "Philippe Laban", "Wei Xu", "Jennifer Neville"], "title": "Flipping the Dialogue: Training and Evaluating User Language Models", "comment": null, "summary": "Conversations with LMs involve two participants: a human user leading the\nconversation, and an LM assistant responding to the user's request. To satisfy\nthis specific role, LMs are post-trained to be helpful assistants -- optimized\nto produce exhaustive and well-structured responses, free of ambiguity and\ngrammar errors. User utterances, on the other hand, are rarely perfected, with\neach user phrasing requests in unique ways, sometimes putting in partial effort\nat each turn and refining on the fly. To evaluate LM performance in realistic\nsettings, prior work simulated users in multi-turn conversations, often\nprompting an LLM originally trained to be a helpful assistant to act as a user.\nHowever, we show that assistant LMs make for poor user simulators, with the\nsurprising finding that better assistants yield worse simulators. Instead, we\nintroduce purpose-built User Language Models (User LMs) - models post-trained\nto simulate human users in multi-turn conversations. Through various\nevaluations, we show how User LMs align better with human behavior and achieve\nbetter simulation robustness than existing simulation methods. When leveraging\nUser LMs to simulate coding and math conversations, the performance of a strong\nassistant (GPT-4o) drops from 74.6% to 57.4%, confirming that more realistic\nsimulation environments lead to assistant struggles as they fail to cope with\nthe nuances of users in multi-turn setups.", "AI": {"tldr": "Assistant-trained LMs are poor user simulators; purpose-built \u201cUser LMs\u201d better mimic real human behavior in multi-turn dialogues, revealing significantly lower assistant performance in realistic settings.", "motivation": "Evaluations often simulate users with assistant-optimized LMs, which produce overly clean, cooperative prompts unlike real users. This likely inflates assistants\u2019 measured performance and masks weaknesses in handling messy, evolving user intent.", "method": "Post-train language models specifically as User LMs to emulate human users\u2019 imperfect, evolving, and idiosyncratic utterances in multi-turn conversations. Compare against assistant-as-user simulations on alignment to human behavior and robustness. Use these simulators to evaluate assistants (e.g., GPT-4o) on coding and math tasks.", "result": "Assistant LMs make poor user simulators\u2014and the stronger the assistant, the worse the simulation quality. User LMs align better with human conversational behavior and yield more robust simulations. Under User LM simulations, GPT-4o\u2019s performance drops from 74.6% to 57.4% in coding/math tasks, indicating prior setups overestimate capability.", "conclusion": "Purpose-built User LMs create more realistic evaluation environments, exposing assistants\u2019 struggles with nuanced, incremental user behavior. Future evaluations should adopt User LMs, and assistant training should focus on robustness to imperfect, multi-turn user inputs."}}
{"id": "2510.06601", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06601", "abs": "https://arxiv.org/abs/2510.06601", "authors": ["Feiran Li", "Jiacheng Li", "Marcos V. Conde", "Beril Besbinar", "Vlad Hosu", "Daisuke Iso", "Radu Timofte"], "title": "AIM 2025 Challenge on Real-World RAW Image Denoising", "comment": null, "summary": "We introduce the AIM 2025 Real-World RAW Image Denoising Challenge, aiming to\nadvance efficient and effective denoising techniques grounded in data\nsynthesis. The competition is built upon a newly established evaluation\nbenchmark featuring challenging low-light noisy images captured in the wild\nusing five different DSLR cameras. Participants are tasked with developing\nnovel noise synthesis pipelines, network architectures, and training\nmethodologies to achieve high performance across different camera models.\nWinners are determined based on a combination of performance metrics, including\nfull-reference measures (PSNR, SSIM, LPIPS), and non-reference ones (ARNIQA,\nTOPIQ). By pushing the boundaries of camera-agnostic low-light RAW image\ndenoising trained on synthetic data, the competition promotes the development\nof robust and practical models aligned with the rapid progress in digital\nphotography. We expect the competition outcomes to influence multiple domains,\nfrom image restoration to night-time autonomous driving.", "AI": {"tldr": "AIM 2025 launches a real\u2011world low\u2011light RAW image denoising challenge with a new 5\u2011camera benchmark; teams must design synthetic\u2011noise pipelines and denoising models, ranked by a mix of full\u2011reference (PSNR, SSIM, LPIPS) and no\u2011reference (ARNIQA, TOPIQ) metrics to foster robust, camera\u2011agnostic performance.", "motivation": "Real\u2011world low\u2011light RAW denoising is difficult and data\u2011hungry; collecting paired clean/noisy data across diverse cameras is costly and impractical. A standardized benchmark and competition leveraging synthetic data can catalyze progress toward practical, generalizable denoisers.", "method": "Establish a benchmark of challenging in\u2011the\u2011wild low\u2011light RAW images from five DSLR cameras. Task participants with building noise\u2011synthesis pipelines, network architectures, and training recipes. Evaluate submissions with combined full\u2011reference (PSNR, SSIM, LPIPS) and no\u2011reference (ARNIQA, TOPIQ) metrics, aggregating them for final rankings.", "result": "No empirical model results are reported; the paper defines the dataset, task, and evaluation protocol that will surface high\u2011performing, camera\u2011agnostic denoisers trained on synthetic data.", "conclusion": "Formalizing this challenge should drive efficient, robust low\u2011light RAW denoising methods trained on synthetic data, with expected impact from general image restoration to night\u2011time autonomous driving."}}
{"id": "2510.07297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07297", "abs": "https://arxiv.org/abs/2510.07297", "authors": ["Henry Wang", "Sirajus Salekin", "Jake Lee", "Ross Claytor", "Shinan Zhang", "Michael Chi"], "title": "Agentic generative AI for media content discovery at the national football league", "comment": "13 pages, 7 figures, International Sports Analytics Conference and\n  Exhibition", "summary": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines.", "AI": {"tldr": "An agentic generative-AI system lets NFL staff search historical plays via natural language, translating queries into database calls and using semantic caching to deliver >95% accuracy and cut retrieval time from ~10 minutes to ~30 seconds.", "motivation": "Traditional filter-and-click video search is slow and cumbersome for media researchers. The NFL needs a faster, more intuitive way to find relevant historical plays to support content creation and storytelling.", "method": "Design an agentic workflow that parses a user\u2019s natural-language query, decomposes it into structured elements, and translates them into the database query language. Add carefully engineered semantic caching to reuse prior computations and speed up repeated or similar queries, improving both latency and accuracy.", "result": "The system achieves over 95% accuracy on retrieving relevant plays and reduces average search time from about 10 minutes to around 30 seconds, markedly improving operational efficiency.", "conclusion": "Generative AI-driven natural-language interfaces, combined with semantic caching, can substantially streamline media content discovery workflows, freeing analysts to focus on creative storytelling and production."}}
{"id": "2510.06559", "categories": ["cs.CL", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.06559", "abs": "https://arxiv.org/abs/2510.06559", "authors": ["Cheonkam Jeong", "Sungdo Kim", "Jewoo Park"], "title": "The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law", "comment": null, "summary": "Contemporary language models are fluent yet routinely mis-handle the types of\nmeaning their outputs entail. We argue that hallucination, brittle moderation,\nand opaque compliance outcomes are symptoms of missing type-theoretic semantics\nrather than data or scale limitations. Building on Montague's view of language\nas typed, compositional algebra, we recast alignment as a parsing problem:\nnatural-language inputs must be compiled into structures that make explicit\ntheir descriptive, normative, and legal dimensions under context.\n  We present Savassan, a neuro-symbolic architecture that compiles utterances\ninto Montague-style logical forms and maps them to typed ontologies extended\nwith deontic operators and jurisdictional contexts. Neural components extract\ncandidate structures from unstructured inputs; symbolic components perform type\nchecking, constraint reasoning, and cross-jurisdiction mapping to produce\ncompliance-aware guidance rather than binary censorship. In cross-border\nscenarios, the system \"parses once\" (e.g., defect claim(product x, company y))\nand projects the result into multiple legal ontologies (e.g., defamation risk\nin KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into\na single, explainable decision.\n  This paper contributes: (i) a diagnosis of hallucination as a type error;\n(ii) a formal Montague-ontology bridge for business/legal reasoning; and (iii)\na production-oriented design that embeds typed interfaces across the pipeline.\nWe outline an evaluation plan using legal reasoning benchmarks and synthetic\nmulti-jurisdiction suites. Our position is that trustworthy autonomy requires\ncompositional typing of meaning, enabling systems to reason about what is\ndescribed, what is prescribed, and what incurs liability within a unified\nalgebra of meaning.", "AI": {"tldr": "Proposes Savassan, a neuro-symbolic system that compiles natural-language inputs into typed Montague-style logical forms enriched with deontic and jurisdictional context to deliver compliance-aware, explainable decisions across legal regimes\u2014framing hallucination and moderation failures as type errors.", "motivation": "Fluent LMs still hallucinate, mis-handle normative/legal meaning, and yield opaque moderation/compliance outcomes. Authors argue the root cause is the absence of type-theoretic, compositional semantics that distinguishes descriptive facts from prescriptions and liabilities.", "method": "Recast alignment as parsing. Neural components extract candidate semantic structures from text; symbolic components perform typing, constraint checking, and cross-jurisdiction mapping over typed ontologies extended with deontic operators. A \"parse once, project many\" approach maps a single logical form into multiple legal ontologies to synthesize an explainable, compliance-aware outcome.", "result": "Conceptual and architectural contribution rather than empirical. Deliverables: (i) diagnosis of hallucination as type error; (ii) a formal bridge between Montague semantics and legal/business ontologies; (iii) a production-oriented design with typed interfaces. Plans an evaluation on legal reasoning benchmarks and synthetic multi-jurisdiction suites.", "conclusion": "Trustworthy autonomy requires compositional typing of meaning. By making descriptive, prescriptive, and liability dimensions explicit, systems can provide nuanced guidance instead of binary censorship and reduce hallucinations via type checking and constraint reasoning."}}
{"id": "2510.06611", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06611", "abs": "https://arxiv.org/abs/2510.06611", "authors": ["Jingran Xu", "Yuanyuan Liu", "Yanjie Zhu"], "title": "Self-supervised Physics-guided Model with Implicit Representation Regularization for Fast MRI Reconstruction", "comment": null, "summary": "Magnetic Resonance Imaging (MRI) is a vital clinical diagnostic tool, yet its\nwidespread application is limited by prolonged scan times. Fast MRI\nreconstruction techniques effectively reduce acquisition duration by\nreconstructing high-fidelity MR images from undersampled k-space data. In\nrecent years, deep learning-based methods have demonstrated remarkable progress\nin this field, with self-supervised and unsupervised learning approaches\nproving particularly valuable in scenarios where fully sampled data are\ndifficult to obtain. This paper proposes a novel zero-shot self-supervised\nreconstruction framework named UnrollINR, which enables scan-specific MRI\nreconstruction without relying on external training data. The method adopts a\nphysics-guided unrolled iterative reconstruction architecture and introduces\nImplicit Neural Representation (INR) as a regularization prior to effectively\nconstrain the solution space. By combining a deep unrolled structure with the\npowerful implicit representation capability of INR, the model's\ninterpretability and reconstruction performance are enhanced. Experimental\nresults demonstrate that even at a high acceleration rate of 10, UnrollINR\nachieves superior reconstruction performance compared to the supervised\nlearning method, validating the superiority of the proposed method.", "AI": {"tldr": "UnrollINR is a zero-shot, scan-specific MRI reconstruction method that unrolls physics-guided optimization and uses an implicit neural representation prior, outperforming supervised baselines even at 10\u00d7 acceleration without external training data.", "motivation": "MRI\u2019s long acquisition time motivates reconstruction from undersampled k-space. However, fully sampled data for supervised training are scarce, pushing the need for self/unsupervised, scan-specific, and interpretable methods that leverage physics and strong priors.", "method": "A zero-shot self-supervised framework (UnrollINR) that performs physics-guided unrolled iterative reconstruction while using an Implicit Neural Representation (INR) as a regularization prior to constrain the solution space. The model is optimized per scan (no external training data), combining data-consistency steps from the MR forward model with the expressive INR to improve interpretability and reconstruction quality.", "result": "On experiments with high undersampling (e.g., 10\u00d7 acceleration), UnrollINR delivers superior reconstruction performance compared to a supervised learning baseline, yielding higher-fidelity images from undersampled k-space.", "conclusion": "Marrying unrolled, physics-consistent optimization with INR priors enables effective zero-shot, self-supervised, scan-specific MRI reconstruction, improving both performance and interpretability, especially when fully sampled training data are unavailable."}}
{"id": "2510.06579", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06579", "abs": "https://arxiv.org/abs/2510.06579", "authors": ["Haofei Yu", "Keyang Xuan", "Fenghai Li", "Kunlun Zhu", "Zijie Lei", "Jiaxun Zhang", "Ziheng Qi", "Kyle Richardson", "Jiaxuan You"], "title": "TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents", "comment": "7 pages, EMNLP 2025 Demo track", "summary": "Automatic research with Large Language Models (LLMs) is rapidly gaining\nimportance, driving the development of increasingly complex workflows involving\nmulti-agent systems, planning, tool usage, code execution, and human-agent\ninteraction to accelerate research processes. However, as more researchers and\ndevelopers begin to use and build upon these tools and platforms, the\ncomplexity and difficulty of extending and maintaining such agentic workflows\nhave become a significant challenge, particularly as algorithms and\narchitectures continue to advance. To address this growing complexity,\nTinyScientist identifies the essential components of the automatic research\nworkflow and proposes an interactive, extensible, and controllable framework\nthat easily adapts to new tools and supports iterative growth. We provide an\nopen-source codebase, an interactive web demonstration, and a PyPI Python\npackage to make state-of-the-art auto-research pipelines broadly accessible to\nevery researcher and developer.", "AI": {"tldr": "TinyScientist introduces an interactive, extensible, and controllable framework that streamlines building and maintaining LLM-driven \u201cauto-research\u201d workflows, released as open-source code, a web demo, and a PyPI package.", "motivation": "LLM-based research pipelines (multi-agent planning, tool use, code execution, human-in-the-loop) are becoming complex and hard to extend/maintain as algorithms and architectures advance; there is a need for a modular, adaptable foundation.", "method": "Abstracts the essential components of automatic research workflows and implements a modular, interactive framework that supports iterative growth and easy integration of new tools; distributes it via open-source repository, interactive web interface, and Python package.", "result": "Delivers a working, accessible system enabling state-of-the-art auto-research pipelines with easier extensibility and control. The abstract does not report quantitative benchmarks or empirical gains.", "conclusion": "A practical foundation for building and evolving agentic research workflows, aiming to reduce complexity and broaden adoption of LLM-driven research automation."}}
{"id": "2510.06612", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06612", "abs": "https://arxiv.org/abs/2510.06612", "authors": ["Zibo Su", "Kun Wei", "Jiahua Li", "Xu Yang", "Cheng Deng"], "title": "A Bridge from Audio to Video: Phoneme-Viseme Alignment Allows Every Face to Speak Multiple Languages", "comment": null, "summary": "Speech-driven talking face synthesis (TFS) focuses on generating lifelike\nfacial animations from audio input. Current TFS models perform well in English\nbut unsatisfactorily in non-English languages, producing wrong mouth shapes and\nrigid facial expressions. The terrible performance is caused by the\nEnglish-dominated training datasets and the lack of cross-language\ngeneralization abilities. Thus, we propose Multilingual Experts (MuEx), a novel\nframework featuring a Phoneme-Guided Mixture-of-Experts (PG-MoE) architecture\nthat employs phonemes and visemes as universal intermediaries to bridge audio\nand video modalities, achieving lifelike multilingual TFS. To alleviate the\ninfluence of linguistic differences and dataset bias, we extract audio and\nvideo features as phonemes and visemes respectively, which are the basic units\nof speech sounds and mouth movements. To address audiovisual synchronization\nissues, we introduce the Phoneme-Viseme Alignment Mechanism (PV-Align), which\nestablishes robust cross-modal correspondences between phonemes and visemes. In\naddition, we build a Multilingual Talking Face Benchmark (MTFB) comprising 12\ndiverse languages with 95.04 hours of high-quality videos for training and\nevaluating multilingual TFS performance. Extensive experiments demonstrate that\nMuEx achieves superior performance across all languages in MTFB and exhibits\neffective zero-shot generalization to unseen languages without additional\ntraining.", "AI": {"tldr": "MuEx is a multilingual talking-face synthesis framework that uses phoneme/viseme intermediates with a phoneme-guided Mixture-of-Experts and a phoneme\u2013viseme alignment mechanism, plus a new 12-language dataset, to achieve better lip-sync and expressions across languages and zero-shot generalization.", "motivation": "Existing speech-driven talking-face systems trained mostly on English fail on non-English inputs, yielding incorrect mouth shapes and stiff expressions due to dataset bias and weak cross-language generalization. A language-agnostic bridge and robust audio\u2013visual alignment are needed.", "method": "Introduce Multilingual Experts (MuEx) with a Phoneme-Guided Mixture-of-Experts (PG-MoE) that maps audio to phonemes and video to visemes as universal units, then aligns them via a Phoneme\u2013Viseme Alignment (PV-Align) mechanism. Construct a Multilingual Talking Face Benchmark (MTFB) covering 12 languages (95.04 hours) for training/evaluation.", "result": "Across all languages in MTFB, MuEx outperforms baselines and demonstrates effective zero-shot transfer to unseen languages without extra training, producing more lifelike, synchronized facial animations.", "conclusion": "Using phonemes/visemes as universal intermediaries within a guided MoE and enforcing cross-modal alignment reduces language bias and improves synchronization, enabling robust multilingual talking-face synthesis; the new benchmark supports standardized evaluation."}}
{"id": "2510.06594", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06594", "abs": "https://arxiv.org/abs/2510.06594", "authors": ["Sri Durga Sai Sowmya Kadali", "Evangelos E. Papalexakis"], "title": "Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?", "comment": null, "summary": "Jailbreaking large language models (LLMs) has emerged as a pressing concern\nwith the increasing prevalence and accessibility of conversational LLMs.\nAdversarial users often exploit these models through carefully engineered\nprompts to elicit restricted or sensitive outputs, a strategy widely referred\nto as jailbreaking. While numerous defense mechanisms have been proposed,\nattackers continuously develop novel prompting techniques, and no existing\nmodel can be considered fully resistant. In this study, we investigate the\njailbreak phenomenon by examining the internal representations of LLMs, with a\nfocus on how hidden layers respond to jailbreak versus benign prompts.\nSpecifically, we analyze the open-source LLM GPT-J and the state-space model\nMamba2, presenting preliminary findings that highlight distinct layer-wise\nbehaviors. Our results suggest promising directions for further research on\nleveraging internal model dynamics for robust jailbreak detection and defense.", "AI": {"tldr": "The paper probes hidden-layer activations of GPT-J and the state-space model Mamba2 under jailbreak vs benign prompts, finding distinct layer-wise signatures that could be leveraged for jailbreak detection and defense.", "motivation": "Jailbreaking remains a persistent threat to conversational LLMs; existing defenses are brittle and reactive. The authors seek internal, model-native signals that differentiate malicious from benign prompting to enable more robust and proactive defenses.", "method": "Empirical analysis of internal representations: compare layer-wise activations/behaviors of GPT-J (transformer) and Mamba2 (state-space) when processing jailbreak versus benign prompts, examining how responses evolve across layers to identify discriminative patterns.", "result": "Preliminary evidence of systematic differences in hidden-layer responses between jailbreak and benign inputs, with distinct layer-wise behaviors observed in both GPT-J and Mamba2, suggesting separable internal signatures of jailbreak attempts.", "conclusion": "Internal model dynamics offer a promising basis for jailbreak detection and potential defenses. Further work should formalize detectors/classifiers on hidden states, test robustness across models and attack types, and integrate such signals into safety pipelines."}}
{"id": "2510.06619", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06619", "abs": "https://arxiv.org/abs/2510.06619", "authors": ["Tao Feng", "Tingfa Xu", "Haolin Qin", "Tianhao Li", "Shuaihao Han", "Xuyang Zou", "Zhan Lv", "Jianan Li"], "title": "MSITrack: A Challenging Benchmark for Multispectral Single Object Tracking", "comment": null, "summary": "Visual object tracking in real-world scenarios presents numerous challenges\nincluding occlusion, interference from similar objects and complex\nbackgrounds-all of which limit the effectiveness of RGB-based trackers.\nMultispectral imagery, which captures pixel-level spectral reflectance,\nenhances target discriminability. However, the availability of multispectral\ntracking datasets remains limited. To bridge this gap, we introduce MSITrack,\nthe largest and most diverse multispectral single object tracking dataset to\ndate. MSITrack offers the following key features: (i) More Challenging\nAttributes-including interference from similar objects and similarity in color\nand texture between targets and backgrounds in natural scenarios, along with a\nwide range of real-world tracking challenges; (ii) Richer and More Natural\nScenes-spanning 55 object categories and 300 distinct natural scenes, MSITrack\nfar exceeds the scope of existing benchmarks. Many of these scenes and\ncategories are introduced to the multispectral tracking domain for the first\ntime; (iii) Larger Scale-300 videos comprising over 129k frames of\nmultispectral imagery. To ensure annotation precision, each frame has undergone\nmeticulous processing, manual labeling and multi-stage verification. Extensive\nevaluations using representative trackers demonstrate that the multispectral\ndata in MSITrack significantly improves performance over RGB-only baselines,\nhighlighting its potential to drive future advancements in the field. The\nMSITrack dataset is publicly available at:\nhttps://github.com/Fengtao191/MSITrack.", "AI": {"tldr": "MSITrack introduces the largest, most diverse multispectral single-object tracking dataset, showing that multispectral cues markedly outperform RGB-only tracking in challenging real-world conditions.", "motivation": "RGB-based tracking suffers in real-world scenarios with occlusion, look-alike distractors, and complex backgrounds. Although multispectral imagery can improve target discriminability by leveraging spectral reflectance, progress is hampered by the scarcity of large, diverse, well-annotated multispectral tracking datasets.", "method": "Construct a large-scale, diverse multispectral tracking dataset (MSITrack) with: 300 videos, >129k frames, 55 object categories, and 300 natural scenes. Curate challenging attributes (similar-object interference, target\u2013background similarity, common tracking difficulties). Ensure high-quality annotations via meticulous processing, manual labeling, and multi-stage verification. Evaluate representative trackers to compare multispectral versus RGB-only performance.", "result": "Across extensive evaluations, multispectral data from MSITrack yields significant performance improvements over RGB-only baselines, establishing stronger benchmarks and demonstrating the practical value of spectral information for tracking.", "conclusion": "MSITrack fills a critical data gap for multispectral tracking, provides a challenging and comprehensive benchmark, and is poised to drive future advances in robust object tracking. The dataset is publicly released, enabling broad community use."}}
{"id": "2510.06640", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06640", "abs": "https://arxiv.org/abs/2510.06640", "authors": ["Nhat M. Hoang", "Do Xuan Long", "Cong-Duy Nguyen", "Min-Yen Kan", "Luu Anh Tuan"], "title": "A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures", "comment": null, "summary": "State Space Models (SSMs) have recently emerged as efficient alternatives to\nTransformer-Based Models (TBMs) for long-sequence processing, offering linear\nscaling and lower memory use. Yet, how contextual information flows across\nlayers and tokens in these architectures remains understudied. We present the\nfirst unified, token- and layer-level analysis of representation propagation in\nSSMs and TBMs. Using centered kernel alignment, stability metrics, and probing,\nwe characterize how representations evolve within and across layers. We find a\nkey divergence: TBMs rapidly homogenize token representations, with diversity\nreemerging only in later layers, while SSMs preserve token uniqueness early but\nconverge to homogenization deeper. Theoretical analysis and parameter\nrandomization further reveal that oversmoothing in TBMs stems from\narchitectural design, whereas in SSMs it arises mainly from training dynamics.\nThese insights clarify the inductive biases of both architectures and inform\nfuture model and training designs for long-context reasoning.", "AI": {"tldr": "Unified, token- and layer-level study compares how representations propagate in SSMs vs. Transformers for long sequences. Using CKA, stability metrics, probing, theory, and parameter randomization, the paper finds Transformers rapidly homogenize token embeddings then re-diversify later, while SSMs keep tokens distinct early but homogenize deep. Oversmoothing in Transformers is architectural; in SSMs it mainly emerges from training. Insights guide architecture and training for long-context reasoning.", "motivation": "Despite SSMs\u2019 efficiency for long sequences, we lack a clear understanding of how context and token information flow across layers compared to Transformer-based models. Clarifying representation dynamics and oversmoothing sources can inform better model and training design for long-context tasks.", "method": "Layer- and token-level analysis via centered kernel alignment (CKA), stability metrics, and probing to track representational evolution; complemented by theoretical analysis and parameter randomization to attribute causes of oversmoothing to architecture vs. training.", "result": "Key divergence in representational dynamics: Transformers quickly homogenize token representations with later re-emergence of diversity, whereas SSMs preserve token uniqueness early but gradually converge to homogeneous states in deeper layers. Oversmoothing in Transformers is tied to architectural design; in SSMs it arises primarily from training dynamics.", "conclusion": "SSMs and Transformers have distinct inductive biases affecting long-context reasoning. Mitigating oversmoothing should target architecture changes for Transformers and training strategies for SSMs. These findings can guide future model and training designs for long-sequence processing."}}
{"id": "2510.06638", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06638", "abs": "https://arxiv.org/abs/2510.06638", "authors": ["Zhihao Wen", "Wenkang Wei", "Yuan Fang", "Xingtong Yu", "Hui Zhang", "Weicheng Zhu", "Xin Zhang"], "title": "StaR-KVQA: Structured Reasoning Traces for Implicit-Knowledge Visual Question Answering", "comment": null, "summary": "Knowledge-based Visual Question Answering (KVQA) requires models to ground\nentities in images and reason over factual knowledge. We study its\nimplicit-knowledge variant, IK-KVQA, where a multimodal large language model\n(MLLM) is the sole knowledge source, without external retrieval. Yet, MLLMs\nlack explicit reasoning supervision and produce inconsistent justifications,\nand generalize poorly after standard supervised fine-tuning (SFT). We present\nStaR-KVQA (Structured Reasoning Traces for IK-KVQA), which supervises\nstructured traces - dual symbolic relation paths plus path-grounded\nnatural-language explanations - so that reasoning becomes transparent and\nverifiable. With one open-source MLLM, StaR-KVQA constructs and selects\npath-grounded reasoning traces to form a trace-enriched dataset, then\nfine-tunes via structured self-distillation to align generation with\nsupervision; no external retrievers, verifiers, or curated knowledge bases\n(KBs) are used, traces are built offline, and inference is a single\nautoregressive pass. Across benchmarks, StaR-KVQA improves both accuracy and\ninterpretability, achieving up to +11.3% higher answer accuracy on OK-VQA over\nthe strongest baseline while exhibiting robust cross-domain generalization.", "AI": {"tldr": "StaR-KVQA supervises multimodal LLMs with structured, path-grounded reasoning traces (symbolic relation paths plus natural-language explanations) to perform knowledge-based VQA without external retrieval, improving both accuracy and interpretability.", "motivation": "Implicit-knowledge KVQA relies on the MLLM as the sole knowledge source, but standard SFT yields poor generalization and inconsistent, non-verifiable justifications due to lack of explicit reasoning supervision.", "method": "Construct offline, with one open-source MLLM, dual symbolic relation paths that connect image-grounded entities to answers and pair them with path-grounded natural-language explanations; select high-quality traces to build a trace-enriched dataset; fine-tune the same MLLM via structured self-distillation to align generation with these traces; perform single-pass autoregressive inference without external retrievers, verifiers, or curated KBs.", "result": "Across benchmarks, StaR-KVQA raises both answer accuracy and interpretability, achieving up to +11.3% accuracy gain on OK-VQA over the strongest baseline and showing robust cross-domain generalization.", "conclusion": "Supervising transparent, verifiable reasoning traces enables MLLMs to perform IK-KVQA more accurately and interpretably without external knowledge sources, indicating a scalable path to better grounded reasoning and transfer."}}
{"id": "2510.06652", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06652", "abs": "https://arxiv.org/abs/2510.06652", "authors": ["Shangjian Yin", "Zhepei Wei", "Xinyu Zhu", "Wei-Lin Chen", "Yu Meng"], "title": "Aligning Large Language Models via Fully Self-Synthetic Data", "comment": null, "summary": "Traditional reinforcement learning from human feedback (RLHF) for large\nlanguage models (LLMs) relies on expensive human-annotated datasets, while\nReinforcement Learning from AI Feedback (RLAIF) also incurs significant costs,\nrequiring the collection of diverse prompts and corresponding responses, often\nnecessitating external reward models or proprietary models like GPT-4 to\nannotate preference pairs. In this work, we introduce Self-Alignment\nOptimization (SAO), a fully self-synthetic framework for LLM alignment, where\nall training data, including prompts (i.e., user queries), responses, and\npreferences, are generated by the model itself. Specifically, SAO first\ninstructs the LLM to engage in persona role-play and generate diverse prompts\nand responses, which are then self-evaluated for preference optimization.\nExtensive experiments demonstrate that SAO effectively enhances the model's\nchat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining\nstrong performance on downstream objective tasks (e.g., question-answering,\nmath reasoning). Our work provides a practical solution for self-improvement in\naligning LLMs, and the code for reproducing our results is available at:\nhttps://github.com/SJY8460/SAO.", "AI": {"tldr": "SAO is a fully self-synthetic alignment framework where an LLM generates its own prompts, responses, and preference labels via persona role-play and self-evaluation, improving chat performance while preserving downstream task ability\u2014cutting costs vs RLHF/RLAIF.", "motivation": "Conventional RLHF is costly due to human annotations; RLAIF still incurs significant expense, requiring diverse prompt/response data and external reward or proprietary models (e.g., GPT\u20114) to label preferences. The goal is to remove external data/labeling costs and dependencies by enabling self-improvement.", "method": "In SAO, the LLM is prompted to role\u2011play personas to synthesize diverse user queries and candidate responses. It then self-evaluates these responses to produce preference signals used for preference optimization, aligning the model without external annotators or reward models.", "result": "Experiments show improved chat capability on AlpacaEval 2.0 while maintaining strong performance on objective tasks like QA and math reasoning. Open-source code is provided.", "conclusion": "SAO offers a practical, low-cost path to align LLMs via self-generated data and preferences, demonstrating competitive chat improvements without degrading core task performance."}}
{"id": "2510.06669", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06669", "abs": "https://arxiv.org/abs/2510.06669", "authors": ["Yuxi Liu", "Yunfeng Ma", "Yi Tang", "Min Liu", "Shuai Jiang", "Yaonan Wang"], "title": "Automated Neural Architecture Design for Industrial Defect Detection", "comment": null, "summary": "Industrial surface defect detection (SDD) is critical for ensuring product\nquality and manufacturing reliability. Due to the diverse shapes and sizes of\nsurface defects, SDD faces two main challenges: intraclass difference and\ninterclass similarity. Existing methods primarily utilize manually designed\nmodels, which require extensive trial and error and often struggle to address\nboth challenges effectively. To overcome this, we propose AutoNAD, an automated\nneural architecture design framework for SDD that jointly searches over\nconvolutions, transformers, and multi-layer perceptrons. This hybrid design\nenables the model to capture both fine-grained local variations and long-range\nsemantic context, addressing the two key challenges while reducing the cost of\nmanual network design. To support efficient training of such a diverse search\nspace, AutoNAD introduces a cross weight sharing strategy, which accelerates\nsupernet convergence and improves subnet performance. Additionally, a\nsearchable multi-level feature aggregation module (MFAM) is integrated to\nenhance multi-scale feature learning. Beyond detection accuracy, runtime\nefficiency is essential for industrial deployment. To this end, AutoNAD\nincorporates a latency-aware prior to guide the selection of efficient\narchitectures. The effectiveness of AutoNAD is validated on three industrial\ndefect datasets and further applied within a defect imaging and detection\nplatform. Code will be available at https://github.com/Yuxi104/AutoNAD.", "AI": {"tldr": "AutoNAD is an automated neural architecture design framework for surface defect detection that jointly searches convolution, transformer, and MLP components, adds cross weight sharing and a searchable multi-level feature aggregation module, and uses a latency-aware prior to balance accuracy and speed; it shows strong results on three industrial datasets and is deployed in a defect detection platform.", "motivation": "Industrial surface defect detection suffers from large intra-class variation and high inter-class similarity. Manually designed models require heavy trial-and-error and struggle to capture both fine local details and long-range context while remaining efficient. An automated, accuracy\u2013efficiency-aware architecture search is needed.", "method": "A NAS framework (AutoNAD) that searches a hybrid space spanning convolutions, transformers, and MLPs. It introduces: (1) cross weight sharing to accelerate supernet convergence and boost subnet quality; (2) a searchable multi-level feature aggregation module (MFAM) for multi-scale feature learning; and (3) a latency-aware prior to steer the search toward deployable architectures.", "result": "Validated on three industrial defect datasets with improved detection accuracy and runtime efficiency (exact metrics not provided in the abstract) and integrated into a practical defect imaging/detection platform. Code to be released at the provided repository.", "conclusion": "AutoNAD reduces manual design cost and effectively addresses intra-class variation and inter-class similarity by combining local and global modeling within a latency-aware NAS. Its hybrid, efficiency-guided architectures make it suitable for real-world industrial deployment."}}
{"id": "2510.06664", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06664", "abs": "https://arxiv.org/abs/2510.06664", "authors": ["Yunzhong Xiao", "Yangmin Li", "Hewei Wang", "Yunlong Tang", "Zora Zhiruo Wang"], "title": "ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory", "comment": null, "summary": "Agents utilizing tools powered by large language models (LLMs) or\nvision-language models (VLMs) have demonstrated remarkable progress in diverse\ntasks across text and visual modalities. Unlike traditional tools such as\ncalculators, which give deterministic outputs, neural tools perform uncertainly\nacross task scenarios. While different tools for a task may excel in varied\nscenarios, existing agents typically rely on fixed tools, thus limiting the\nflexibility in selecting the most suitable tool for specific tasks. In\ncontrast, humans snowball their understanding of the capabilities of different\ntools by interacting with them, and apply this knowledge to select the optimal\ntool when solving a future task. To build agents that similarly benefit from\nthis process, we propose ToolMem that enables agents to develop memories of\ntool capabilities from previous interactions, by summarizing their strengths\nand weaknesses and storing them in memory; at inference, the agent can retrieve\nrelevant entries from ToolMem, and select the best tool to solve individual\ntasks more accurately. We evaluate ToolMem on learning varied text generation\nand text-to-image generation neural tools. Compared to no-memory, generic\nagents, we find ToolMem-augmented agents predict tool performance 14.8% and\n28.7% more accurately across text and multimodal generation scenarios.\nMoreover, ToolMem facilitates optimal tool selection among multiple choices by\n21% and 24% absolute increases in respective scenarios.", "AI": {"tldr": "ToolMem is a memory mechanism for LLM/VLM-based agents that summarizes each neural tool\u2019s strengths and weaknesses from past interactions and retrieves them at inference to predict performance and select the best tool, improving accuracy for text and text-to-image tasks.", "motivation": "Neural tools (LLMs/VLMs) are non-deterministic and perform unevenly across scenarios; fixed tool choices waste potential. Humans accumulate experiential knowledge about tools and pick the right one per task\u2014this work aims to give agents an analogous capability.", "method": "During training/interaction, the agent records outcomes from using different tools and summarizes per-tool capabilities (strengths/weaknesses) into memory entries keyed by task/context features. At inference, it retrieves relevant memories, predicts each tool\u2019s likely performance for the current instance, and selects the best. Evaluated on text generation and text-to-image generation; baselined against agents without such memory; metrics include accuracy of predicting tool performance and rate of selecting the optimal tool.", "result": "Compared to no-memory agents, ToolMem improves prediction of tool performance by 14.8% (text) and 28.7% (multimodal) and increases optimal tool selection by 21% and 24% absolute, respectively.", "conclusion": "Encoding and retrieving experiential knowledge about tool capabilities enables adaptive, scenario-aware tool selection, yielding sizable gains across modalities; memory-augmented agents are a promising direction for robust tool use."}}
{"id": "2510.06673", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06673", "abs": "https://arxiv.org/abs/2510.06673", "authors": ["Yongxin Zhu", "Jiawei Chen", "Yuanzhe Chen", "Zhuo Chen", "Dongya Jia", "Jian Cong", "Xiaobin Zhuang", "Yuping Wang", "Yuxuan Wang"], "title": "Heptapod: Language Modeling on Visual Signals", "comment": null, "summary": "We introduce Heptapod, an image autoregressive model that adheres to the\nfoundational principles of language modeling. Heptapod employs \\textbf{causal\nattention}, \\textbf{eliminates reliance on CFG}, and \\textbf{eschews the trend\nof semantic tokenizers}. Our key innovation is \\textit{next 2D distribution\nprediction}: a causal Transformer with reconstruction-focused visual tokenizer,\nlearns to predict the distribution over the entire 2D spatial grid of images at\neach timestep. This learning objective unifies the sequential modeling of\nautoregressive framework with the holistic self-supervised learning of masked\nautoencoding, enabling the model to capture comprehensive image semantics via\ngenerative training. On the ImageNet generation benchmark, Heptapod achieves an\nFID of $2.70$, significantly outperforming previous causal autoregressive\napproaches. We hope our work inspires a principled rethinking of language\nmodeling on visual signals and beyond.", "AI": {"tldr": "Heptapod is a causal Transformer for image generation that predicts a full 2D token distribution at each step using a reconstruction-focused visual tokenizer, avoiding CFG and semantic tokenizers; it achieves FID 2.70 on ImageNet, surpassing prior causal autoregressive methods.", "motivation": "Bring the principled simplicity of language modeling to vision while overcoming autoregressive image models\u2019 difficulty with global semantics and their reliance on classifier-free guidance or semantic tokenizers. Unify sequential AR training with holistic self-supervised objectives to better capture image semantics.", "method": "Use causal attention in a Transformer with a reconstruction-focused (non-semantic) visual tokenizer. Train with a next-2D distribution prediction objective: at each timestep the model predicts distributions over the entire 2D spatial grid, marrying AR sequencing with MAE-like holistic learning, and dispensing with CFG and semantic tokenizers.", "result": "On ImageNet generation, Heptapod attains FID 2.70, substantially outperforming previous causal autoregressive approaches, indicating improved semantic fidelity and sample quality.", "conclusion": "Next-2D distribution prediction provides a principled bridge between AR language modeling and masked autoencoding for vision, achieving strong generative performance without CFG or semantic tokenizers and motivating a reevaluation of language-modeling paradigms for visual signals."}}
{"id": "2510.06670", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06670", "abs": "https://arxiv.org/abs/2510.06670", "authors": ["Shangjian Yin", "Shining Liang", "Wenbiao Ding", "Yuli Qian", "Zhouxing Shi", "Hongzhi Li", "Yutao Xie"], "title": "PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone\nfor aligning large language models (LLMs). However, its effectiveness depends\non high-quality instruction data. Most existing alignment datasets are either\nprivate or require costly human annotation, which limits reproducibility and\nscalability. Even with Reinforcement Learning from AI Feedback (RLAIF),\nconcerns about data quality remain. Moreover, it is unclear how much data is\nactually required to fine-tune a base model into a strong instruction-following\nmodel. Current approaches often rely on over 300k examples even at the\nsupervised fine-tuning (SFT) stage, yet they still underperform compared to\nproprietary models, creating barriers for academic and resource-limited\ncommunities. To address this gap, we introduce PiKa, a data-efficient family of\nexpert-level alignment datasets. In particular, the PiKa-SFT dataset uses only\n30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through\nevaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets,\nwe show that PiKa-SFT outperforms models trained on much larger data. On\nAlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses\nthe official Llama-3-8B-Instruct model trained on over 10 million proprietary\nexamples. We further extend our study by training the Qwen2.5 series (0.5B to\n7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that\nhigh-quality alignment can be achieved with significantly less data, offering a\nscalable path for open-source LLM alignment. Code and data:\nhttps://github.com/SJY8460/PiKa.", "AI": {"tldr": "PiKa introduces a compact, high-quality alignment dataset (notably PiKa-SFT with 30k SFT examples) that enables open LLMs to reach or exceed the performance of models trained on orders-of-magnitude more data, even surpassing Llama\u20113\u20118B\u2011Instruct on AlpacaEval 2.0 and Arena-Hard.", "motivation": "LLM alignment via RLHF/RLAIF depends on expensive, private, or noisy instruction data, limiting reproducibility and scalability. It remains unclear how much data is truly needed to obtain strong instruction-following behavior; many pipelines use >300k SFT examples yet still lag behind proprietary models.", "method": "Construct the PiKa family of expert-level alignment datasets, focusing on PiKa-SFT (30k examples). Fine-tune base models (Llama\u20113\u20118B\u2011Base and Qwen2.5 series, 0.5B\u20137B) on PiKa-SFT and compare against training on larger public datasets. Evaluate on AlpacaEval 2.0 and Arena-Hard benchmarks; report cross-model scaling behavior.", "result": "Models fine-tuned on PiKa-SFT outperform counterparts trained on much larger datasets. Llama\u20113\u20118B\u2011Base fine-tuned on PiKa-SFT surpasses the official Llama\u20113\u20118B\u2011Instruct (trained on >10M proprietary examples) on AlpacaEval 2.0 and Arena-Hard. Gains are consistent across Qwen2.5 models from 0.5B to 7B parameters.", "conclusion": "High-quality alignment can be achieved with significantly less data than commonly assumed. PiKa offers a scalable, open-source path for efficient alignment and reduces dependency on massive proprietary corpora."}}
{"id": "2510.06679", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06679", "abs": "https://arxiv.org/abs/2510.06679", "authors": ["Bin Xia", "Bohao Peng", "Yuechen Zhang", "Junjia Huang", "Jiyang Liu", "Jingyao Li", "Haoru Tan", "Sitong Wu", "Chengyao Wang", "Yitong Wang", "Xinglong Wu", "Bei Yu", "Jiaya Jia"], "title": "DreamOmni2: Multimodal Instruction-based Editing and Generation", "comment": null, "summary": "Recent advancements in instruction-based image editing and subject-driven\ngeneration have garnered significant attention, yet both tasks still face\nlimitations in meeting practical user needs. Instruction-based editing relies\nsolely on language instructions, which often fail to capture specific editing\ndetails, making reference images necessary. Meanwhile, subject-driven\ngeneration is limited to combining concrete objects or people, overlooking\nbroader, abstract concepts. To address these challenges, we propose two novel\ntasks: multimodal instruction-based editing and generation. These tasks support\nboth text and image instructions and extend the scope to include both concrete\nand abstract concepts, greatly enhancing their practical applications. We\nintroduce DreamOmni2, tackling two primary challenges: data creation and model\nframework design. Our data synthesis pipeline consists of three steps: (1)\nusing a feature mixing method to create extraction data for both abstract and\nconcrete concepts, (2) generating multimodal instruction-based editing training\ndata using the editing and extraction models, and (3) further applying the\nextraction model to create training data for multimodal instruction-based\nediting. For the framework, to handle multi-image input, we propose an index\nencoding and position encoding shift scheme, which helps the model distinguish\nimages and avoid pixel confusion. Additionally, we introduce joint training\nwith the VLM and our generation/editing model to better process complex\ninstructions. In addition, we have proposed comprehensive benchmarks for these\ntwo new tasks to drive their development. Experiments show that DreamOmni2 has\nachieved impressive results. Models and codes will be released.", "AI": {"tldr": "DreamOmni2 introduces multimodal (text+image) instruction-based editing and generation that handle both concrete and abstract concepts, providing a data pipeline, a multi-image encoding scheme, joint VLM training, and new benchmarks, achieving strong experimental results.", "motivation": "Current instruction-based image editing lacks precise control from text alone and often needs reference images; subject-driven generation focuses on concrete entities and ignores abstract concepts. There is a practical need for systems that accept multimodal instructions and reason over both concrete and abstract ideas.", "method": "They define two new tasks\u2014multimodal instruction-based editing and generation\u2014and build DreamOmni2. Data: (1) feature mixing to synthesize extraction data for abstract and concrete concepts; (2) generate multimodal editing training data using editing and extraction models; (3) further use the extraction model to create additional multimodal training data. Framework: introduce index encoding and position-encoding shifts to disambiguate multiple input images and avoid pixel confusion; perform joint training with a vision-language model (VLM) and the gen/edit model to better follow complex instructions. They also construct comprehensive benchmarks for these tasks.", "result": "On the proposed benchmarks, DreamOmni2 achieves impressive performance (reported qualitatively as strong/improved results). Models and code will be released.", "conclusion": "By unifying multimodal instructions and supporting abstract and concrete concept manipulation, DreamOmni2 expands the scope and practicality of image editing and generation. Its data synthesis pipeline, multi-image encoding strategy, joint VLM training, and new benchmarks collectively deliver strong performance and lay groundwork for future research."}}
{"id": "2510.06677", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06677", "abs": "https://arxiv.org/abs/2510.06677", "authors": ["Yisha Wu", "Cen", "Zhao", "Yuanpei Cao", "Xiaoqing Su", "Yashar Mehdad", "Mindy Ji", "Claire Na Cheng"], "title": "Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback", "comment": "Accepted at EMNLP 2025 Industry Track", "summary": "We introduce an incremental summarization system for customer support agents\nthat intelligently determines when to generate concise bullet notes during\nconversations, reducing agents' context-switching effort and redundant review.\nOur approach combines a fine-tuned Mixtral-8x7B model for continuous note\ngeneration with a DeBERTa-based classifier to filter trivial content. Agent\nedits refine the online notes generation and regularly inform offline model\nretraining, closing the agent edits feedback loop. Deployed in production, our\nsystem achieved a 3% reduction in case handling time compared to bulk\nsummarization (with reductions of up to 9% in highly complex cases), alongside\nhigh agent satisfaction ratings from surveys. These results demonstrate that\nincremental summarization with continuous feedback effectively enhances summary\nquality and agent productivity at scale.", "AI": {"tldr": "Production system for incremental, in-conversation note-taking reduces customer support case handling time (\u22123% overall, up to \u22129% on complex cases) by combining a fine-tuned Mixtral-8x7B for continuous notes, a DeBERTa classifier to suppress trivial content, and a closed feedback loop from agent edits for online refinement and offline retraining.", "motivation": "Bulk, end-of-conversation summaries force context switching and re-reading, slowing agents and lowering utility. The authors aim to provide timely, concise bullet notes during the conversation to lighten cognitive load and improve productivity and summary quality.", "method": "- Fine-tuned Mixtral-8x7B generates incremental bullet notes throughout conversations.\n- A DeBERTa-based classifier decides when and what to note by filtering trivial/low-value content.\n- Agent edits are captured to refine online generation and are used for periodic offline retraining (closed feedback loop).\n- Deployed in production; compared against a bulk summarization baseline.", "result": "- 3% reduction in average case handling time versus bulk summarization.\n- Up to 9% reduction on highly complex cases.\n- High agent satisfaction from surveys.", "conclusion": "Incremental, feedback-driven summarization outperforms bulk summarization for customer support, improving both efficiency and user satisfaction; continuous human-in-the-loop signals are key to quality and scalability."}}
{"id": "2510.06687", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06687", "abs": "https://arxiv.org/abs/2510.06687", "authors": ["Jie Luo", "Yuxuan Jiang", "Xin Jin", "Mingyu Liu", "Yihui Fan"], "title": "Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion", "comment": null, "summary": "Semantic segmentation serves as a cornerstone of scene understanding in\nautonomous driving but continues to face significant challenges under complex\nconditions such as occlusion. Light field and LiDAR modalities provide\ncomplementary visual and spatial cues that are beneficial for robust\nperception; how- ever, their effective integration is hindered by limited\nviewpoint diversity and inherent modality discrepancies. To address these\nchallenges, the first multimodal semantic segmentation dataset integrating\nlight field data and point cloud data is proposed. Based on this dataset, we\nproposed a multi-modal light field point-cloud fusion segmentation\nnetwork(Mlpfseg), incorporating feature completion and depth perception to\nsegment both camera images and LiDAR point clouds simultaneously. The feature\ncompletion module addresses the density mismatch between point clouds and image\npixels by performing differential re- construction of point-cloud feature maps,\nenhancing the fusion of these modalities. The depth perception module improves\nthe segmentation of occluded objects by reinforcing attention scores for better\nocclusion awareness. Our method outperforms image- only segmentation by 1.71\nMean Intersection over Union(mIoU) and point cloud-only segmentation by 2.38\nmIoU, demonstrating its effectiveness.", "AI": {"tldr": "They introduce the first light field (LF) + LiDAR multimodal semantic segmentation dataset and a fusion network (Mlpfseg) with feature completion and depth-aware attention, yielding modest but consistent mIoU gains over image-only (+1.71) and LiDAR-only (+2.38), especially under occlusion.", "motivation": "Semantic segmentation in autonomous driving struggles with occlusion. LF images and LiDAR provide complementary angular/depth cues, yet are hard to fuse due to limited LF viewpoint diversity and modality gaps (e.g., density mismatch between sparse points and dense pixels).", "method": "1) Build a new dataset pairing LF imagery with LiDAR point clouds. 2) Propose Mlpfseg, a joint segmentation model for both images and point clouds. It includes: (a) a feature completion module that differentially reconstructs point-cloud feature maps to mitigate density mismatch and improve cross-modal alignment; (b) a depth perception module that reinforces attention to enhance occlusion awareness and depth reasoning; 3) Fuse features to segment both modalities simultaneously.", "result": "On their dataset, Mlpfseg surpasses image-only segmentation by +1.71 mIoU and point cloud-only by +2.38 mIoU, indicating better robustness, particularly for occluded objects.", "conclusion": "Combining LF and LiDAR via feature completion and depth-aware attention improves multimodal fusion and occlusion handling. The new dataset and model provide a foundation for more robust autonomous-driving segmentation under challenging visibility."}}
{"id": "2510.06695", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.06695", "abs": "https://arxiv.org/abs/2510.06695", "authors": ["Qinhao Zhou", "Xiang Xiang", "Kun He", "John E. Hopcroft"], "title": "Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks", "comment": null, "summary": "In recent years, the growing interest in Large Language Models (LLMs) has\nsignificantly advanced prompt engineering, transitioning from manual design to\nmodel-based optimization. Prompts for LLMs generally comprise two components:\nthe \\textit{instruction}, which defines the task or objective, and the\n\\textit{input}, which is tailored to the instruction type. In natural language\ngeneration (NLG) tasks such as machine translation, the \\textit{input}\ncomponent is particularly critical, while the \\textit{instruction} component\ntends to be concise. Existing prompt engineering methods primarily focus on\noptimizing the \\textit{instruction} component for general tasks, often\nrequiring large-parameter LLMs as auxiliary tools. However, these approaches\nexhibit limited applicability for tasks like machine translation, where the\n\\textit{input} component plays a more pivotal role. To address this limitation,\nthis paper introduces a novel prompt optimization method specifically designed\nfor machine translation tasks. The proposed approach employs a small-parameter\nmodel trained using a back-translation-based strategy, significantly reducing\ntraining overhead for single-task optimization while delivering highly\neffective performance. With certain adaptations, this method can also be\nextended to other downstream tasks.", "AI": {"tldr": "They propose a lightweight, back-translation\u2013driven prompt optimization method that targets the input component for machine translation, cutting training cost while achieving strong performance and offering extensibility to other tasks.", "motivation": "Most prompt engineering optimizes instructions and often relies on large LLMs, which is ill-suited for machine translation where the input content dominates and instructions are minimal. A low-cost, task-specific approach is needed to optimize inputs rather than instructions.", "method": "Introduce a prompt optimization framework tailored to machine translation that uses a small-parameter model trained with a back-translation-based strategy to refine/optimize the input component. The design focuses on single-task optimization with low overhead and can be adapted to other downstream tasks.", "result": "Claims include significantly reduced training overhead compared to large-LLM-based prompt optimization and highly effective translation performance when optimizing the input component. Empirical details are not provided in the abstract.", "conclusion": "Optimizing the input side of prompts via a small model and back-translation is an efficient and effective strategy for machine translation, lessening dependence on large LLMs and potentially generalizing\u2014with adaptations\u2014to other NLG tasks."}}
{"id": "2510.06694", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06694", "abs": "https://arxiv.org/abs/2510.06694", "authors": ["Jipeng Lyu", "Jiahua Dong", "Yu-Xiong Wang"], "title": "SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis", "comment": "Published in Transactions on Machine Learning Research (06/2025)", "summary": "Persistent dynamic scene modeling for tracking and novel-view synthesis\nremains challenging due to the difficulty of capturing accurate deformations\nwhile maintaining computational efficiency. We propose SCas4D, a cascaded\noptimization framework that leverages structural patterns in 3D Gaussian\nSplatting for dynamic scenes. The key idea is that real-world deformations\noften exhibit hierarchical patterns, where groups of Gaussians share similar\ntransformations. By progressively refining deformations from coarse part-level\nto fine point-level, SCas4D achieves convergence within 100 iterations per time\nframe and produces results comparable to existing methods with only\none-twentieth of the training iterations. The approach also demonstrates\neffectiveness in self-supervised articulated object segmentation, novel view\nsynthesis, and dense point tracking tasks.", "AI": {"tldr": "SCas4D introduces a hierarchical, cascaded deformation optimization on top of 3D Gaussian Splatting, refining from parts to points to achieve fast, accurate dynamic scene modeling for tracking and novel-view synthesis.", "motivation": "Dynamic scene modeling needs both accurate deformation capture and high computational efficiency; existing methods struggle to balance precision with training cost for tasks like tracking and novel-view synthesis.", "method": "A cascaded optimization framework (SCas4D) that exploits structural regularities in dynamic scenes: groups of Gaussians share similar transformations. It progressively optimizes deformations from coarse part-level to fine point-level within a 3D Gaussian Splatting representation.", "result": "Converges within ~100 iterations per time frame; matches prior methods\u2019 quality using only about 1/20 of their training iterations; demonstrates strong performance on self-supervised articulated object segmentation, novel view synthesis, and dense point tracking.", "conclusion": "Leveraging hierarchical deformation structure via cascaded refinements yields efficient and accurate dynamic scene modeling, substantially cutting training iterations while maintaining competitive visual and tracking performance."}}
{"id": "2510.06700", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06700", "abs": "https://arxiv.org/abs/2510.06700", "authors": ["Leonardo Bertolazzi", "Sandro Pezzelle", "Raffaelle Bernardi"], "title": "How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects", "comment": null, "summary": "Both humans and large language models (LLMs) exhibit content effects: biases\nin which the plausibility of the semantic content of a reasoning problem\ninfluences judgments regarding its logical validity. While this phenomenon in\nhumans is best explained by the dual-process theory of reasoning, the\nmechanisms behind content effects in LLMs remain unclear. In this work, we\naddress this issue by investigating how LLMs encode the concepts of validity\nand plausibility within their internal representations. We show that both\nconcepts are linearly represented and strongly aligned in representational\ngeometry, leading models to conflate plausibility with validity. Using steering\nvectors, we demonstrate that plausibility vectors can causally bias validity\njudgements, and vice versa, and that the degree of alignment between these two\nconcepts predicts the magnitude of behavioral content effects across models.\nFinally, we construct debiasing vectors that disentangle these concepts,\nreducing content effects and improving reasoning accuracy. Our findings advance\nunderstanding of how abstract logical concepts are represented in LLMs and\nhighlight representational interventions as a path toward more logical systems.", "AI": {"tldr": "LLMs linearly encode both logical validity and semantic plausibility along highly aligned directions, causing them to conflate the two; steering and debiasing these directions modulates and improves reasoning.", "motivation": "Content effects\u2014where plausible content biases judgments of logical validity\u2014are well-established in humans via dual-process theory, but the mechanisms in LLMs are unknown. The paper aims to uncover how LLMs internally represent validity vs. plausibility and why content effects arise.", "method": "Probe internal representations to identify linear directions corresponding to validity and plausibility; analyze their geometric alignment; use steering vectors to causally bias one concept with the other; measure how alignment predicts behavioral content effects across models; construct debiasing vectors to disentangle concepts and evaluate impact on reasoning accuracy.", "result": "Validity and plausibility are each linearly decodable and strongly aligned in representation space. Steering along the plausibility (or validity) vector biases judgments of the other. The alignment magnitude predicts observed content-effect strength across models. Debiasing vectors reduce alignment, attenuate content effects, and increase reasoning accuracy.", "conclusion": "LLMs encode abstract logical concepts in aligned linear subspaces, explaining content effects as representational conflation. Targeted representational interventions (steering/debiasing vectors) can mitigate bias and yield more logically consistent reasoning."}}
{"id": "2510.06743", "categories": ["cs.CV", "cs.AI", "cs.CL", "68T50"], "pdf": "https://arxiv.org/pdf/2510.06743", "abs": "https://arxiv.org/abs/2510.06743", "authors": ["Maria Levchenko"], "title": "Evaluating LLMs for Historical Document OCR: A Methodological Framework for Digital Humanities", "comment": "The First Workshop on Natural Language Processing and Language Models\n  for Digital Humanities (LM4DH 2025). RANLP 2025", "summary": "Digital humanities scholars increasingly use Large Language Models for\nhistorical document digitization, yet lack appropriate evaluation frameworks\nfor LLM-based OCR. Traditional metrics fail to capture temporal biases and\nperiod-specific errors crucial for historical corpus creation. We present an\nevaluation methodology for LLM-based historical OCR, addressing contamination\nrisks and systematic biases in diplomatic transcription. Using 18th-century\nRussian Civil font texts, we introduce novel metrics including Historical\nCharacter Preservation Rate (HCPR) and Archaic Insertion Rate (AIR), alongside\nprotocols for contamination control and stability testing. We evaluate 12\nmultimodal LLMs, finding that Gemini and Qwen models outperform traditional OCR\nwhile exhibiting over-historicization: inserting archaic characters from\nincorrect historical periods. Post-OCR correction degrades rather than improves\nperformance. Our methodology provides digital humanities practitioners with\nguidelines for model selection and quality assessment in historical corpus\ndigitization.", "AI": {"tldr": "Proposes and validates a tailored evaluation framework for LLM-based historical OCR, introducing HCPR and AIR metrics, contamination/stability protocols, and shows top LLMs beat traditional OCR yet anachronistically insert archaic characters; na\u00efve post-correction worsens outcomes.", "motivation": "Digital humanists increasingly rely on LLMs for digitizing historical texts, but existing OCR metrics miss temporally specific errors and biases (e.g., anachronistic character use) and do not control for training-data contamination\u2014issues that threaten the reliability of historical corpora.", "method": "Design an evaluation methodology for LLM-based historical OCR focused on diplomatic transcription. Use 18th\u2011century Russian Civil font materials; define Historical Character Preservation Rate (HCPR) and Archaic Insertion Rate (AIR); implement contamination-control and stability-testing protocols; benchmark 12 multimodal LLMs against conventional OCR systems.", "result": "Gemini and Qwen multimodal models outperform traditional OCR on overall transcription quality but exhibit over-historicization by inserting characters from incorrect periods; post-OCR correction pipelines reduce, rather than improve, accuracy; the proposed metrics and protocols reveal systematic temporal biases not captured by standard measures.", "conclusion": "The framework and metrics enable nuanced assessment of LLM-based historical OCR, highlighting risks of anachronistic outputs and the counterproductive effects of generic post-correction. Practitioners should use the proposed protocols for model selection and quality control when building historical corpora."}}
{"id": "2510.06727", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06727", "abs": "https://arxiv.org/abs/2510.06727", "authors": ["Miao Lu", "Weiwei Sun", "Weihua Du", "Zhan Ling", "Xuesong Yao", "Kang Liu", "Jiecao Chen"], "title": "Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management", "comment": null, "summary": "We study reinforcement learning (RL) fine-tuning of large language model\n(LLM) agents for long-horizon multi-turn tool use, where context length quickly\nbecomes a fundamental bottleneck. Existing RL pipelines can suffer from\ndegraded instruction following, excessive rollout costs, and most importantly,\nstrict context limits. To address these challenges, we introduce\nsummarization-based context management to training. In specific, it\nperiodically compresses the tool using history by LLM-generated summaries that\nretain task-relevant information to keep a compact context while enabling the\nagent to scale beyond the fixed context window. Building on this formulation,\nwe derive a policy gradient representation that seamlessly enables standard LLM\nRL infrastructures to optimize both tool-use behaviors as well as summarization\nstrategies in an end-to-end fashion. We instantiate this framework with\n\\underline{SU}mmarization augmented \\underline{P}olicy \\underline{O}ptimization\n(\\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond\na fixed context limit. Experiments on interactive function calling and\nsearching tasks demonstrate that \\texttt{SUPO} significantly improves the\nsuccess rate while maintaining the same or even lower working context length\ncompared to baselines. We also demonstrate that for complex searching tasks,\n\\texttt{SUPO} can further improve the evaluation performance when scaling\ntest-time maximum round of summarization beyond that of training time. Our\nresults establish summarization-based context management as a principled and\nscalable approach for training RL agents beyond a fixed context length limit.", "AI": {"tldr": "SUPO is an RL algorithm that co-optimizes tool-use actions and LLM-generated summaries, enabling long-horizon multi-turn tool use beyond fixed context windows and improving success rates with equal or lower working context.", "motivation": "LLM agents trained with RL struggle on long, multi-turn tool-use tasks because context windows are limited; existing RL pipelines incur high rollout costs, degrade instruction following, and are strictly bounded by context length.", "method": "Introduce summarization-based context management: periodically compress interaction histories into task-relevant summaries generated by the LLM. Derive a policy-gradient formulation that treats summarization as part of the policy, allowing end-to-end optimization of both tool-use behavior and summarization strategy within standard LLM RL infrastructure. Instantiate this as SUPO (Summarization-augmented Policy Optimization).", "result": "On interactive function-calling and searching benchmarks, SUPO substantially boosts success rates while maintaining the same or lower effective context length than baselines. For complex search tasks, allowing more summarization rounds at test time than at training further improves performance.", "conclusion": "Summarization-based context management provides a principled, scalable path to training RL LLM agents beyond fixed context limits; SUPO validates this approach with improved effectiveness and efficiency."}}
{"id": "2510.06746", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06746", "abs": "https://arxiv.org/abs/2510.06746", "authors": ["Zhiliang Zhu", "Tao Zeng", "Tao Yang", "Guoliang Luo", "Jiyong Zeng"], "title": "DeRainMamba: A Frequency-Aware State Space Model with Detail Enhancement for Image Deraining", "comment": "accepted by IEEE SPL", "summary": "Image deraining is crucial for improving visual quality and supporting\nreliable downstream vision tasks. Although Mamba-based models provide efficient\nsequence modeling, their limited ability to capture fine-grained details and\nlack of frequency-domain awareness restrict further improvements. To address\nthese issues, we propose DeRainMamba, which integrates a Frequency-Aware\nState-Space Module (FASSM) and Multi-Directional Perception Convolution\n(MDPConv). FASSM leverages Fourier transform to distinguish rain streaks from\nhigh-frequency image details, balancing rain removal and detail preservation.\nMDPConv further restores local structures by capturing anisotropic gradient\nfeatures and efficiently fusing multiple convolution branches. Extensive\nexperiments on four public benchmarks demonstrate that DeRainMamba consistently\noutperforms state-of-the-art methods in PSNR and SSIM, while requiring fewer\nparameters and lower computational costs. These results validate the\neffectiveness of combining frequency-domain modeling and spatial detail\nenhancement within a state-space framework for single image deraining.", "AI": {"tldr": "DeRainMamba augments Mamba-based state-space models with a frequency-aware module and multi-directional convolutions to better separate rain streaks from true image details, achieving state-of-the-art deraining quality with fewer parameters and lower compute.", "motivation": "Mamba-based models are efficient for sequence modeling but struggle with fine-grained spatial details and lack frequency-domain awareness. Single-image deraining critically relies on distinguishing rain streaks (often high-frequency) from genuine high-frequency textures; existing methods trade off removal vs. detail preservation.", "method": "Introduce two components within a state-space framework: (1) Frequency-Aware State-Space Module (FASSM) that uses Fourier-domain processing to differentiate and balance rain removal and detail retention; (2) Multi-Directional Perception Convolution (MDPConv) to capture anisotropic gradient features and fuse multiple convolutional branches for local structure restoration. Combined into DeRainMamba.", "result": "Across four public deraining benchmarks, DeRainMamba consistently surpasses prior methods in PSNR and SSIM while using fewer parameters and lower computational cost (no exact numbers provided in the abstract).", "conclusion": "Coupling frequency-domain modeling with spatial detail enhancement inside a state-space (Mamba) architecture is an effective recipe for single-image deraining, improving quality-efficiency trade-offs and supporting more reliable downstream vision tasks."}}
{"id": "2510.06730", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06730", "abs": "https://arxiv.org/abs/2510.06730", "authors": ["Manuel Frank", "Haithem Afli"], "title": "PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs", "comment": null, "summary": "Current evaluations of sentence embedding models typically rely on static\ntest beds such as the Massive Text Embedding Benchmark (MTEB). While\ninvaluable, repeated tuning on a fixed suite can inflate reported performance\nand obscure real-world robustness. We introduce the Paraphrasing Text Embedding\nBenchmark (PTEB), a dynamic protocol that stochastically generates\nmeaning-preserving paraphrases at evaluation time and aggregates results across\nmultiple runs. Using a cost-efficient LLM-based method grounded in semantic\ntextual similarity gold ratings, we show that LLMs generate token-diverse but\nsemantically preserving, paraphrases. Across 7 MTEB tasks, we validate our\nhypothesis that the performance of sentence encoders is sensitive to changes in\ntoken space even when semantics remain fixed. We also observe that smaller\nmodels are not disproportionately affected relative to larger ones. Our results\nare statistically robust over multiple runs and we extended our experiments to\n3 multilingual datasets covering 10 languages. More generally, we aim to\npropose a new evaluation paradigm in NLP that relies less on static,\npre-defined benchmarks but shifts towards dynamic, stochastic evaluation\nleveraging eval-time compute.", "AI": {"tldr": "They propose PTEB, a dynamic paraphrase-based evaluation protocol for sentence embeddings that generates meaning-preserving paraphrases at evaluation time using LLMs and aggregates results over multiple runs. It reveals that encoder performance is sensitive to token-level changes despite fixed semantics, shows no disproportionate harm to smaller models, is statistically robust, and generalizes to multilingual settings\u2014arguing for a shift from static to stochastic, compute-at-eval evaluation.", "motivation": "Static, fixed benchmarks (e.g., MTEB) risk overfitting and may mask robustness issues. The authors aim to create an evaluation that tests invariance to paraphrasing\u2014i.e., whether embeddings remain stable when wording changes but semantics do not\u2014reflecting real-world variation and discouraging tuning to a static test set.", "method": "Design a dynamic evaluation protocol (PTEB) that stochastically generates meaning-preserving paraphrases at evaluation time using a cost-efficient LLM pipeline guided by semantic textual similarity (STS) gold ratings. Evaluate multiple runs and aggregate results to reduce variance. Apply across 7 MTEB tasks and extend to 3 multilingual datasets (10 languages) to test robustness. Assess token diversity and semantic preservation of generated paraphrases.", "result": "LLMs produce paraphrases that are token-diverse yet semantically consistent per STS. Sentence encoders show sensitivity to paraphrase-induced token changes even when meaning is fixed. Smaller models are not disproportionately impacted relative to larger ones. Results are statistically robust across runs and hold in multilingual evaluations.", "conclusion": "Static test suites alone are insufficient for robust evaluation of sentence embeddings. Dynamic, stochastic, eval-time protocols like PTEB better capture robustness to paraphrase variation and should complement or partially replace fixed benchmarks in NLP evaluation."}}
{"id": "2510.06751", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06751", "abs": "https://arxiv.org/abs/2510.06751", "authors": ["Junhan Zhu", "Hesong Wang", "Mingluo Su", "Zefang Wang", "Huan Wang"], "title": "OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot", "comment": null, "summary": "Large-scale text-to-image diffusion models, while powerful, suffer from\nprohibitive computational cost. Existing one-shot network pruning methods can\nhardly be directly applied to them due to the iterative denoising nature of\ndiffusion models. To bridge the gap, this paper presents OBS-Diff, a novel\none-shot pruning framework that enables accurate and training-free compression\nof large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff\nrevitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex\narchitectures of modern diffusion models and supporting diverse pruning\ngranularity, including unstructured, N:M semi-structured, and structured (MHA\nheads and FFN neurons) sparsity; (ii) To align the pruning criteria with the\niterative dynamics of the diffusion process, by examining the problem from an\nerror-accumulation perspective, we propose a novel timestep-aware Hessian\nconstruction that incorporates a logarithmic-decrease weighting scheme,\nassigning greater importance to earlier timesteps to mitigate potential error\naccumulation; (iii) Furthermore, a computationally efficient group-wise\nsequential pruning strategy is proposed to amortize the expensive calibration\nprocess. Extensive experiments show that OBS-Diff achieves state-of-the-art\none-shot pruning for diffusion models, delivering inference acceleration with\nminimal degradation in visual quality.", "AI": {"tldr": "OBS-Diff is a training-free, one-shot pruning framework that adapts Optimal Brain Surgeon to large text-to-image diffusion models with timestep-aware Hessian weighting and group-wise sequential pruning, achieving faster inference with minimal quality loss.", "motivation": "Large text-to-image diffusion models are computationally expensive. Existing one-shot pruning methods for standard networks fail to account for the iterative denoising process where pruning errors can accumulate across timesteps. There is a need for an accurate, training-free compression method tailored to diffusion dynamics and compatible with multiple sparsity types.", "method": "(1) Adapt Optimal Brain Surgeon (OBS) to diffusion architectures and multiple pruning granularities (unstructured, N:M semi-structured, and structured pruning of MHA heads and FFN neurons). (2) Introduce a timestep-aware Hessian that weights earlier timesteps more (logarithmic decrease) to align pruning importance with error accumulation dynamics. (3) Use a computationally efficient group-wise sequential pruning strategy to amortize calibration costs. One-shot, training-free pruning across the model.", "result": "Experiments demonstrate state-of-the-art one-shot pruning for diffusion models, providing inference acceleration while maintaining visual quality with minimal degradation.", "conclusion": "By aligning OBS-based pruning with the iterative nature of diffusion via timestep-aware Hessians and efficient group-wise pruning, OBS-Diff delivers flexible, accurate, training-free compression that preserves image quality and speeds up inference."}}
{"id": "2510.06732", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.06732", "abs": "https://arxiv.org/abs/2510.06732", "authors": ["Tiancheng Xing", "Jerry Li", "Yixuan Du", "Xiyang Hu"], "title": "Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization", "comment": "10 pages, 3 figures", "summary": "Large language models (LLMs) are increasingly used as rerankers in\ninformation retrieval, yet their ranking behavior can be steered by small,\nnatural-sounding prompts. To expose this vulnerability, we present Rank\nAnything First (RAF), a two-stage token optimization method that crafts concise\ntextual perturbations to consistently promote a target item in LLM-generated\nrankings while remaining hard to detect. Stage 1 uses Greedy Coordinate\nGradient to shortlist candidate tokens at the current position by combining the\ngradient of the rank-target with a readability score; Stage 2 evaluates those\ncandidates under exact ranking and readability losses using an entropy-based\ndynamic weighting scheme, and selects a token via temperature-controlled\nsampling. RAF generates ranking-promoting prompts token-by-token, guided by\ndual objectives: maximizing ranking effectiveness and preserving linguistic\nnaturalness. Experiments across multiple LLMs show that RAF significantly\nboosts the rank of target items using naturalistic language, with greater\nrobustness than existing methods in both promoting target items and maintaining\nnaturalness. These findings underscore a critical security implication:\nLLM-based reranking is inherently susceptible to adversarial manipulation,\nraising new challenges for the trustworthiness and robustness of modern\nretrieval systems. Our code is available at: https://github.com/glad-lab/RAF.", "AI": {"tldr": "RAF is a two-stage token-level optimization method that crafts short, natural-sounding prompts to reliably push a target item higher in LLM-based rerankings, outperforming prior attacks and revealing a significant security vulnerability.", "motivation": "LLMs used as rerankers can have their rankings steered by subtle prompt changes. The authors aim to systematically expose and quantify this vulnerability by generating adversarial yet natural prompts that promote specific targets while remaining hard to detect.", "method": "Rank Anything First (RAF) generates an adversarial prompt token-by-token with dual objectives: maximize the target\u2019s rank and preserve linguistic naturalness. Stage 1 (Greedy Coordinate Gradient) shortlists candidate tokens by combining the gradient signal of a rank-target objective with a readability score. Stage 2 evaluates these candidates using exact ranking and readability losses with an entropy-based dynamic weighting scheme, then selects tokens via temperature-controlled sampling. The process iterates to produce concise perturbations.", "result": "Across multiple LLMs, RAF consistently improves the rank of target items while keeping the text naturalistic, and shows greater robustness than existing methods in both effectiveness and naturalness.", "conclusion": "LLM-based reranking is inherently susceptible to adversarial prompt manipulation, posing security and trustworthiness risks for retrieval systems. The authors release code to facilitate further study and defenses."}}
{"id": "2510.06757", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06757", "abs": "https://arxiv.org/abs/2510.06757", "authors": ["Sheng Fu", "Junchao Zhang", "Kailun Yang"], "title": "Transforming Noise Distributions with Histogram Matching: Towards a Single Denoiser for All", "comment": "12 pages", "summary": "Supervised Gaussian denoisers exhibit limited generalization when confronted\nwith out-of-distribution noise, due to the diverse distributional\ncharacteristics of different noise types. To bridge this gap, we propose a\nhistogram matching approach that transforms arbitrary noise towards a target\nGaussian distribution with known intensity. Moreover, a mutually reinforcing\ncycle is established between noise transformation and subsequent denoising.\nThis cycle progressively refines the noise to be converted, making it\napproximate the real noise, thereby enhancing the noise transformation effect\nand further improving the denoising performance. We tackle specific noise\ncomplexities: local histogram matching handles signal-dependent noise,\nintrapatch permutation processes channel-related noise, and frequency-domain\nhistogram matching coupled with pixel-shuffle down-sampling breaks spatial\ncorrelation. By applying these transformations, a single Gaussian denoiser\ngains remarkable capability to handle various out-of-distribution noises,\nincluding synthetic noises such as Poisson, salt-and-pepper and repeating\npattern noises, as well as complex real-world noises. Extensive experiments\ndemonstrate the superior generalization and effectiveness of our method.", "AI": {"tldr": "Turn arbitrary noise into Gaussian via histogram matching and iterate this with a Gaussian denoiser, enabling one supervised Gaussian denoiser to handle many out-of-distribution noise types and real-world noise.", "motivation": "Supervised Gaussian denoisers fail on out-of-distribution noise because different noise sources have distinct distributions (signal dependence, channel correlations, spatial correlations). A universal, training-free way to map diverse noises into a known Gaussian distribution would unlock reuse of existing denoisers.", "method": "Preprocess noisy images to morph the noise toward a target Gaussian with known variance using: (1) local histogram matching for signal-dependent noise, (2) intrapatch permutation to address channel-related correlations, and (3) frequency-domain histogram matching with pixel-shuffle downsampling to break spatial correlation. Establish an iterative loop where denoising output informs and refines the noise transformation, progressively aligning the residual noise with the true noise statistics before feeding it again to the Gaussian denoiser.", "result": "A single supervised Gaussian denoiser, combined with the proposed transformations, effectively handles diverse synthetic noises (Poisson, salt-and-pepper, repeating pattern) and complex real-world noise, showing superior generalization in extensive experiments.", "conclusion": "Iteratively transforming heterogeneous noise to a canonical Gaussian form is an effective, general strategy that upgrades off-the-shelf Gaussian denoisers to robust, broad-spectrum denoisers without retraining."}}
{"id": "2510.06738", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06738", "abs": "https://arxiv.org/abs/2510.06738", "authors": ["Boyi Zeng", "Lin Chen", "Ziwei He", "Xinbing Wang", "Zhouhan Lin"], "title": "AWM: Accurate Weight-Matrix Fingerprint for Large Language Models", "comment": null, "summary": "Protecting the intellectual property of large language models (LLMs) is\ncrucial, given the substantial resources required for their training.\nConsequently, there is an urgent need for both model owners and third parties\nto determine whether a suspect LLM is trained from scratch or derived from an\nexisting base model. However, the intensive post-training processes that models\ntypically undergo-such as supervised fine-tuning, extensive continued\npretraining, reinforcement learning, multi-modal extension, pruning, and\nupcycling-pose significant challenges to reliable identification. In this work,\nwe propose a training-free fingerprinting method based on weight matrices. We\nleverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel\nAlignment (CKA) similarity to neutralize the effects of parameter\nmanipulations, yielding a highly robust and high-fidelity similarity metric. On\na comprehensive testbed of 60 positive and 90 negative model pairs, our method\ndemonstrates exceptional robustness against all six aforementioned\npost-training categories while exhibiting a near-zero risk of false positives.\nBy achieving perfect scores on all classification metrics, our approach\nestablishes a strong basis for reliable model lineage verification. Moreover,\nthe entire computation completes within 30s on an NVIDIA 3090 GPU. The code is\navailable at https://github.com/LUMIA-Group/AWM.", "AI": {"tldr": "Training-free weight-matrix fingerprinting for LLM lineage verification using LAP-aligned unbiased CKA; robust to many post-training changes; perfect classification on tested pairs; runs in ~30s on a 3090.", "motivation": "LLM training is resource-intensive, so owners need reliable ways to prove or detect whether a suspect model is derived from an existing base model. Post-training steps (SFT, continued pretraining, RL, multimodal extension, pruning, upcycling) can obscure lineage, making prior identification methods brittle or costly.", "method": "Compute inter-model similarity directly from weight matrices without extra training. Use a Linear Assignment Problem (LAP) to align components (e.g., layers/neurons/heads) and an unbiased Centered Kernel Alignment (CKA) similarity to measure correspondence, thereby neutralizing parameter permutations and manipulations. The resulting metric serves as a robust fingerprint for model relatedness.", "result": "On a testbed with 60 positive (related) and 90 negative (unrelated) model pairs, the method remains robust across six post-training categories, achieves near-zero false positives, and reports perfect scores across standard classification metrics. End-to-end runtime is about 30 seconds on an NVIDIA RTX 3090.", "conclusion": "Weight-matrix\u2013based, training-free fingerprinting with LAP-aligned unbiased CKA provides a reliable, efficient basis for LLM lineage verification, robust to common post-training transformations and practical for real-world enforcement and auditing."}}
{"id": "2510.06769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06769", "abs": "https://arxiv.org/abs/2510.06769", "authors": ["Gianmarco Perantoni", "Lorenzo Bruzzone"], "title": "A deep multiple instance learning approach based on coarse labels for high-resolution land-cover mapping", "comment": "14 pages, 4 figures, accepted conference paper at SPIE REMOTE\n  SENSING, 3-7 September 2023, Amsterdam, Netherlands", "summary": "The quantity and the quality of the training labels are central problems in\nhigh-resolution land-cover mapping with machine-learning-based solutions. In\nthis context, weak labels can be gathered in large quantities by leveraging on\nexisting low-resolution or obsolete products. In this paper, we address the\nproblem of training land-cover classifiers using high-resolution imagery (e.g.,\nSentinel-2) and weak low-resolution reference data (e.g., MODIS -derived\nland-cover maps). Inspired by recent works in Deep Multiple Instance Learning\n(DMIL), we propose a method that trains pixel-level multi-class classifiers and\npredicts low-resolution labels (i.e., patch-level classification), where the\nactual high-resolution labels are learned implicitly without direct\nsupervision. This is achieved with flexible pooling layers that are able to\nlink the semantics of the pixels in the high-resolution imagery to the\nlow-resolution reference labels. Then, the Multiple Instance Learning (MIL)\nproblem is re-framed in a multi-class and in a multi-label setting. In the\nformer, the low-resolution annotation represents the majority of the pixels in\nthe patch. In the latter, the annotation only provides us information on the\npresence of one of the land-cover classes in the patch and thus multiple labels\ncan be considered valid for a patch at a time, whereas the low-resolution\nlabels provide us only one label. Therefore, the classifier is trained with a\nPositive-Unlabeled Learning (PUL) strategy. Experimental results on the 2020\nIEEE GRSS Data Fusion Contest dataset show the effectiveness of the proposed\nframework compared to standard training strategies.", "AI": {"tldr": "Train pixel-level land-cover classifiers from high-resolution imagery using only weak, low-resolution labels by framing the task as Deep Multiple Instance Learning with flexible pooling, including a Positive-Unlabeled variant; achieves better results than standard weak-label training on the GRSS DFC 2020 dataset.", "motivation": "High-resolution land-cover mapping needs many accurate labels, which are costly. However, abundant weak labels exist from low-resolution or outdated products. The goal is to leverage these weak labels to learn accurate pixel-level classifiers for high-resolution imagery.", "method": "Use Sentinel-2 high-resolution images with weak MODIS-derived patch labels. Model pixels as instances within patches and employ Deep Multiple Instance Learning. Introduce flexible pooling layers to map pixel-level predictions to patch-level supervision. Reframe MIL in two settings: (1) multi-class, where the patch label corresponds to the majority class within the patch; (2) multi-label presence-only, where the provided patch label indicates at least one pixel belongs to that class while others remain unlabeled, trained via Positive-Unlabeled Learning. The network learns pixel-level multi-class predictions implicitly through patch-level losses.", "result": "On the 2020 IEEE GRSS Data Fusion Contest dataset, the proposed framework outperforms standard training strategies that rely on weak labels, demonstrating improved effectiveness in learning from low-resolution supervision.", "conclusion": "Weak low-resolution labels can effectively supervise high-resolution pixel-level land-cover mapping when cast as DMIL with flexible pooling and PUL for presence-only labels. This approach learns fine-grained semantics without direct pixel annotations and yields superior performance to baseline weak-label training."}}
{"id": "2510.06747", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06747", "abs": "https://arxiv.org/abs/2510.06747", "authors": ["I-Fan Lin", "Faegheh Hasibi", "Suzan Verberne"], "title": "TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs", "comment": null, "summary": "In this paper, we propose a training-free and label-free method for short\ntext clustering that can be used on top of any existing embedder. In the\ncontext of customer-facing chatbots, companies are dealing with large amounts\nof user utterances that need to be clustered according to their intent. In\nthese commercial settings, no labeled data is typically available, and the\nnumber of clusters is not known. Our method is based on iterative vector\nupdating: it constructs sparse vectors based on representative texts, and then\niteratively refines them through LLM guidance. Our method achieves comparable\nor superior results to state-of-the-art methods that use contrastive learning,\nbut without assuming prior knowledge of clusters or labels. Experiments on\ndiverse datasets and smaller LLMs show that our method is model agnostic and\ncan be applied to any embedder, with relatively small LLMs, and different\nclustering methods. We also show that our method scales to large datasets,\nreducing the computational cost of the LLM. These low-resource, adaptable\nsettings and the scalability of our method make it more aligned with real-world\nscenarios than existing clustering methods.", "AI": {"tldr": "Training- and label-free short-text clustering that plugs into any embedder and iteratively refines sparse representative vectors with small LLM guidance, matching or surpassing contrastive-learning SOTA while scaling efficiently without knowing the number of clusters.", "motivation": "Commercial chatbots receive massive, unlabeled user utterances with unknown intent counts. Existing methods often need labels, contrastive pretraining, or fixed cluster counts, which is impractical and costly. A model-agnostic, low-resource, scalable clustering approach is needed for real-world deployments.", "method": "Start with any text embedder; build sparse vectors from representative texts; iteratively update/refine these vectors using guidance from a (small) LLM. The process is training-free and label-free, agnostic to the underlying embedder, and compatible with different clustering algorithms. It includes strategies that reduce LLM calls to scale to large datasets.", "result": "On diverse datasets, the approach achieves comparable or superior performance to state-of-the-art contrastive learning methods despite not assuming prior knowledge of labels or the number of clusters. It works with smaller LLMs, different embedders, and various clustering methods, and scales to large datasets with reduced LLM compute cost.", "conclusion": "The method offers a practical, adaptable, and scalable solution for real-world intent clustering in short texts, aligning better with low-resource commercial settings than existing supervised or contrastive approaches."}}
{"id": "2510.06783", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06783", "abs": "https://arxiv.org/abs/2510.06783", "authors": ["Akshit Singh", "Shyam Marjit", "Wei Lin", "Paul Gavrikov", "Serena Yeung-Levy", "Hilde Kuehne", "Rogerio Feris", "Sivan Doveh", "James Glass", "M. Jehanzeb Mirza"], "title": "TTRV: Test-Time Reinforcement Learning for Vision Language Models", "comment": null, "summary": "Existing methods for extracting reward signals in Reinforcement Learning\ntypically rely on labeled data and dedicated training splits, a setup that\ncontrasts with how humans learn directly from their environment. In this work,\nwe propose TTRV to enhance vision language understanding by adapting the model\non the fly at inference time, without the need for any labeled data.\nConcretely, we enhance the Group Relative Policy Optimization (GRPO) framework\nby designing rewards based on the frequency of the base model's output, while\ninferring on each test sample multiple times. Further, we also propose to\ncontrol the diversity of the model's output by simultaneously rewarding the\nmodel for obtaining low entropy of the output empirical distribution. Our\napproach delivers consistent gains across both object recognition and visual\nquestion answering (VQA), with improvements of up to 52.4% and 29.8%,\nrespectively, and average boosts of 24.6% and 10.0% across 16\ndatasets.Remarkably, on image recognition, TTRV applied to InternVL 8B\nsurpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining\nhighly competitive on VQA, demonstrating that test-time reinforcement learning\ncan match or exceed the strongest proprietary models. Finally, we find many\ninteresting properties of test-time RL for VLMs: for example, even in extremely\ndata-constrained scenarios, where adaptation is performed on a single randomly\nchosen unlabeled test example, TTRV still yields non-trivial improvements of up\nto 5.5% in recognition tasks.", "AI": {"tldr": "TTRV is a label-free, test-time reinforcement learning approach that adapts a vision-language model on each inference by rewarding frequent and low-entropy outputs via an enhanced GRPO scheme, yielding large gains on recognition and VQA and even surpassing GPT-4o on image recognition benchmarks.", "motivation": "Most RL-based reward extraction for VLMs depends on labeled data and predefined splits, unlike human learning from the environment. The goal is to remove this reliance and boost performance by adapting at inference without labels.", "method": "Extend Group Relative Policy Optimization to test time: for each test sample, generate multiple outputs, compute rewards based on the frequency of the base model\u2019s outputs (mode-seeking) and encourage low entropy in the empirical output distribution to control diversity/consistency, then update the model on-the-fly without labeled supervision.", "result": "Consistent improvements across 16 datasets: up to 52.4% (object recognition) and 29.8% (VQA); average gains of 24.6% and 10.0%, respectively. With InternVL 8B on image recognition, TTRV averages 2.3% above GPT-4o over 8 benchmarks. Even adapting on a single unlabeled test example yields up to 5.5% gains in recognition.", "conclusion": "Label-free test-time RL with frequency- and entropy-based rewards can substantially enhance VLMs, matching or surpassing top proprietary systems and remaining effective even under extreme data scarcity."}}
{"id": "2510.06749", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06749", "abs": "https://arxiv.org/abs/2510.06749", "authors": ["Eitan Klinger", "Zihao Huang", "Tran Minh Nguyen", "Emma Jayeon Park", "Yige Chen", "Yang Gu", "Qingyu Gao", "Siliang Liu", "Mengyang Qiu", "Jungyeul Park"], "title": "A Formal Framework for Fluency-based Multi-Reference Evaluation in Grammatical Error Correction", "comment": "Submitted to ACL Rolling Review - October 2025 for EACL 2026", "summary": "Evaluating grammatical error correction requires metrics that reflect the\ndiversity of valid human corrections rather than privileging a single\nreference. Existing frameworks, largely edit-based and English-centric, rely on\nrigid alignments between system and reference edits, limiting their\napplicability in multilingual and generative settings. This paper introduces a\nformal framework for \\textit{fluency-based multi-reference evaluation}, framing\n$n$-gram similarity as an aggregation problem over multiple legitimate\ncorrections. Within this formulation, we instantiate GLEU through four\naggregation strategies--\\textsc{select-best}, \\textsc{simple-average},\n\\textsc{weighted-average}, and \\textsc{merged-counts}--and analyze their\nproperties of boundedness, monotonicity, and sensitivity to reference\nvariation. Empirical results on Czech, Estonian, Ukrainian, and Chinese corpora\nshow that these strategies capture complementary aspects of fluency and\ncoverage. The framework unifies multi-reference evaluation into a principled,\nfluency-oriented approach that incorporates linguistic diversity without\npenalizing legitimate variation.", "AI": {"tldr": "Proposes a fluency-oriented, multi-reference evaluation framework for grammatical error correction that aggregates n-gram similarity across multiple valid corrections, instantiates four GLEU aggregation strategies, analyzes their properties, and shows cross-lingual effectiveness without penalizing legitimate variation.", "motivation": "Current GEC metrics are largely edit-based, single-reference, and English-centric, relying on rigid edit alignments that miss the diversity of valid human corrections and do not generalize well to multilingual or generative settings.", "method": "Formalize multi-reference evaluation as an n-gram aggregation problem over multiple legitimate corrections. Instantiate GLEU using four aggregation strategies\u2014select-best, simple-average, weighted-average, and merged-counts\u2014and analyze boundedness, monotonicity, and sensitivity to reference variation. Evaluate empirically on Czech, Estonian, Ukrainian, and Chinese corpora.", "result": "Across four non-English corpora, the proposed aggregation strategies capture complementary aspects of fluency and coverage, exhibiting robustness to reference variation and better reflecting diverse valid corrections.", "conclusion": "A unified, fluency-based multi-reference framework enables more principled and multilingual GEC evaluation by aggregating evidence across valid references, incorporating linguistic diversity without penalizing legitimate variation."}}
{"id": "2510.06791", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06791", "abs": "https://arxiv.org/abs/2510.06791", "authors": ["Changlin Song", "Yunzhong Hou", "Michael Randall Barnes", "Rahul Shome", "Dylan Campbell"], "title": "Extreme Amodal Face Detection", "comment": null, "summary": "Extreme amodal detection is the task of inferring the 2D location of objects\nthat are not fully visible in the input image but are visible within an\nexpanded field-of-view. This differs from amodal detection, where the object is\npartially visible within the input image, but is occluded. In this paper, we\nconsider the sub-problem of face detection, since this class provides\nmotivating applications involving safety and privacy, but do not tailor our\nmethod specifically to this class. Existing approaches rely on image sequences\nso that missing detections may be interpolated from surrounding frames or make\nuse of generative models to sample possible completions. In contrast, we\nconsider the single-image task and propose a more efficient, sample-free\napproach that makes use of the contextual cues from the image to infer the\npresence of unseen faces. We design a heatmap-based extreme amodal object\ndetector that addresses the problem of efficiently predicting a lot (the\nout-of-frame region) from a little (the image) with a selective coarse-to-fine\ndecoder. Our method establishes strong results for this new task, even\noutperforming less efficient generative approaches.", "AI": {"tldr": "They introduce a single-image, sample-free, heatmap-based detector that uses contextual cues with a selective coarse-to-fine decoder to locate faces (and objects) outside the image frame, outperforming heavier generative approaches.", "motivation": "Detecting objects that are entirely outside the current field-of-view (but would be visible with a broader view) matters for safety and privacy (e.g., anticipating people just off-camera). Prior work needs video sequences for interpolation or slow, sampling-based generative models. There is a need for an efficient, single-image solution that leverages scene context.", "method": "Formulate \u201cextreme amodal detection\u201d and instantiate it for faces without class-specific tailoring. Use a heatmap-based predictor with a selective coarse-to-fine decoder to infer the 2D locations of out-of-frame objects from a single image by exploiting contextual cues, avoiding sampling/generative completion.", "result": "On this newly defined task, their method achieves strong performance and surpasses less efficient generative baselines while being computationally more efficient.", "conclusion": "Context from a single image is sufficient to infer likely locations of unseen, out-of-frame objects. A heatmap-based, coarse-to-fine, sample-free detector provides an effective and efficient baseline for extreme amodal detection, with demonstrated gains over generative approaches."}}
{"id": "2510.06750", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06750", "abs": "https://arxiv.org/abs/2510.06750", "authors": ["Jaeseong Lee", "Dayoung Kwon", "seung-won hwang"], "title": "Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs", "comment": null, "summary": "Large Reasoning Models (LRMs) excel in structured tasks by emulating\ndeliberate human reasoning but often suffer from overthinking, degrading\nperformance and wasting resources. One possible baseline is to deploy both LLM\nand LRM, then route input by predicting whether it requires reasoning and may\ncause overthinking. However, deploying multiple models can be costly or\nimpractical. We propose a superposed deployment strategy with a lightweight,\ntraining-free regulation to optimize inference by switching one model on and\noff. Instead of routing, we selectively unlearn from LRM at inference, scaling\ndown computation while preserving reasoning. By analyzing the cumulative energy\nof singular values, we identify optimal low-rank projections to adjust\nreasoning just right.", "AI": {"tldr": "Training-free, low\u2011rank projection at inference time dials a single LRM\u2019s reasoning strength to curb overthinking and cut compute, avoiding dual\u2011model routing while preserving performance.", "motivation": "LRMs can overthink\u2014taking excessive reasoning steps that hurt accuracy and waste computation. A common fix routes inputs between an LLM and an LRM, but maintaining multiple models is costly and impractical.", "method": "Use a superposed, single\u2011model deployment with a lightweight, training\u2011free regulator. At inference, selectively \u201cunlearn\u201d portions of the LRM by applying optimal low\u2011rank projections determined via cumulative singular value energy analysis, effectively turning reasoning capacity up or down without routing or retraining.", "result": "Reduces computation while maintaining necessary reasoning quality; mitigates overthinking by adjusting the model\u2019s reasoning intensity on the fly; removes the need for multi\u2011model routing. (Abstract does not report numbers.)", "conclusion": "A simple, training\u2011free inference control mechanism can right\u2011size reasoning in LRMs\u2014balancing accuracy and efficiency\u2014by projecting onto low\u2011rank subspaces identified through singular value energy, offering a practical alternative to multi\u2011model deployments."}}
{"id": "2510.06809", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06809", "abs": "https://arxiv.org/abs/2510.06809", "authors": ["Teng Wang", "Haojun Jiang", "Yuxuan Wang", "Zhenguo Sun", "Shiji Song", "Gao Huang"], "title": "VA-Adapter: Adapting Ultrasound Foundation Model to Echocardiography Probe Guidance", "comment": null, "summary": "Echocardiography is a critical tool for detecting heart diseases. Recently,\nultrasound foundation models have demonstrated remarkable capabilities in\ncardiac ultrasound image analysis. However, obtaining high-quality ultrasound\nimages is a prerequisite for accurate diagnosis. Due to the exceptionally high\noperational difficulty of cardiac ultrasound, there is a shortage of highly\nskilled personnel, which hinders patients from receiving timely examination\nservices. In this paper, we aim to adapt the medical knowledge learned by\nfoundation models from vast datasets to the probe guidance task, which is\ndesigned to provide real-time operational recommendations for junior\nsonographers to acquire high-quality ultrasound images. Moreover, inspired by\nthe practice where experts optimize action decisions based on past\nexplorations, we meticulously design a parameter-efficient Vision-Action\nAdapter (VA-Adapter) to enable foundation model's image encoder to encode\nvision-action sequences, thereby enhancing guidance performance. With built-in\nsequential reasoning capabilities in a compact design, the VA-Adapter enables a\npre-trained ultrasound foundation model to learn precise probe adjustment\nstrategies by fine-tuning only a small subset of parameters. Extensive\nexperiments demonstrate that the VA-Adapter can surpass strong probe guidance\nmodels. Our code will be released after acceptance.", "AI": {"tldr": "They propose a compact, parameter\u2011efficient Vision\u2011Action Adapter that plugs into an ultrasound foundation model to turn it into a real\u2011time probe guidance system, encoding vision\u2013action sequences and outperforming strong baselines with minimal fine\u2011tuning.", "motivation": "High\u2011quality cardiac ultrasound imaging is essential for diagnosis, but skilled operators are scarce and probe manipulation is difficult. Foundation models encode broad medical knowledge, yet are not directly suited for sequential, action\u2011conditioned guidance. The work aims to transfer this knowledge to real\u2011time probe control to help junior sonographers acquire diagnostic views.", "method": "Introduce a Vision\u2011Action Adapter (VA\u2011Adapter) that augments a pre\u2011trained ultrasound foundation model\u2019s image encoder to process vision\u2013action sequences and perform sequential reasoning. The adapter is parameter\u2011efficient, enabling learning of probe adjustment strategies via fine\u2011tuning only a small subset of parameters.", "result": "Across extensive experiments, the VA\u2011Adapter reportedly surpasses strong probe guidance models in performance (details not provided in the abstract). Code will be released upon acceptance.", "conclusion": "Adapting foundation models with a lightweight, sequence\u2011aware adapter enables accurate, real\u2011time probe guidance while keeping training efficient. This approach can help mitigate operator shortages by improving the acquisition of high\u2011quality cardiac ultrasound images."}}
{"id": "2510.06774", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06774", "abs": "https://arxiv.org/abs/2510.06774", "authors": ["Lei Xu", "Pierre Beckmann", "Marco Valentino", "Andr\u00e9 Freitas"], "title": "Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition", "comment": null, "summary": "Neuro-symbolic NLP methods aim to leverage the complementary strengths of\nlarge language models and formal logical solvers. However, current approaches\nare mostly static in nature, i.e., the integration of a target solver is\npredetermined at design time, hindering the ability to employ diverse formal\ninference strategies. To address this, we introduce an adaptive,\nmulti-paradigm, neuro-symbolic inference framework that: (1) automatically\nidentifies formal reasoning strategies from problems expressed in natural\nlanguage; and (2) dynamically selects and applies specialized formal logical\nsolvers via autoformalization interfaces. Extensive experiments on individual\nand multi-paradigm reasoning tasks support the following conclusions: LLMs are\neffective at predicting the necessary formal reasoning strategies with an\naccuracy above 90 percent. This enables flexible integration with formal\nlogical solvers, resulting in our framework outperforming competing baselines\nby 27 percent and 6 percent compared to GPT-4o and DeepSeek-V3.1, respectively.\nMoreover, adaptive reasoning can even positively impact pure LLM methods,\nyielding gains of 10, 5, and 6 percent on zero-shot, CoT, and symbolic CoT\nsettings with GPT-4o. Finally, although smaller models struggle with adaptive\nneuro-symbolic reasoning, post-training offers a viable path to improvement.\nOverall, this work establishes the foundations for adaptive LLM-symbolic\nreasoning, offering a path forward for unifying material and formal inferences\non heterogeneous reasoning challenges.", "AI": {"tldr": "They propose an adaptive neuro\u2011symbolic framework that predicts which formal reasoning strategy a problem needs from natural language and dynamically routes it to specialized logical solvers via autoformalization, yielding large gains over strong LLM baselines and also improving pure LLM prompting strategies.", "motivation": "Most neuro\u2011symbolic systems hard\u2011wire a single solver or reasoning style at design time, limiting flexibility across heterogeneous tasks. The field needs a way to automatically recognize the right formal paradigm and invoke the appropriate solver on demand.", "method": "Build a multi\u2011paradigm inference pipeline that (1) uses an LLM to classify the required formal reasoning strategy from NL problem statements, (2) autoformalizes problems into solver\u2011compatible representations, and (3) selects and executes specialized logical solvers dynamically; evaluated on single\u2011 and multi\u2011paradigm benchmarks.", "result": "The LLM predicts strategies with >90% accuracy; the framework beats baselines by 27% over GPT\u20114o and 6% over DeepSeek\u2011V3.1; adaptive routing also boosts pure LLM settings by +10% (zero\u2011shot), +5% (CoT), and +6% (symbolic CoT) with GPT\u20114o; smaller models underperform but improve after post\u2011training.", "conclusion": "Adaptive selection and integration of symbolic solvers guided by LLMs is effective and practical, laying groundwork for unifying natural\u2011language (material) and formal inference across diverse reasoning tasks, with room to strengthen smaller models via post\u2011training and broaden solver coverage."}}
{"id": "2510.06820", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06820", "abs": "https://arxiv.org/abs/2510.06820", "authors": ["Mitchell Keren Taraday", "Shahaf Wagner", "Chaim Baskin"], "title": "Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking", "comment": "preprint", "summary": "Multimodal retrieval still leans on embedding-based models like CLIP for fast\nvector search over pre-computed image embeddings. Yet, unlike text retrieval,\nwhere joint-encoder rerankers are standard, comparable vision--language\nrerankers are largely absent. We find that seminal joint encoders such as BLIP\nare severely bottlenecked by an expensive visual feature-extraction stage,\npreventing practical deployment at scale. Motivated by this bottleneck, we\nintroduce EDJE, an Efficient Discriminative Joint Encoder that precomputes\nvision tokens offline and compresses them via a lightweight attention-based\nadapter, so online inference runs only a compact joint encoder over a small set\nof visual tokens plus the text. EDJE preserves strong retrieval performance\nwhile drastically reducing storage and online compute, enabling high-throughput\ninference. Specifically, EDJE processes 50k image--text pairs/second while\nrequiring 49kB of disk storage per image, matching prior art on Flickr\n(zero-shot) and COCO (fine-tuned) retrieval. The implementation and checkpoints\nwill be made publicly available shortly.", "AI": {"tldr": "EDJE is an efficient joint vision\u2013language reranker that precomputes and compresses image tokens offline, allowing a compact online joint encoder to rerank image\u2013text pairs at high throughput while matching prior retrieval accuracy.", "motivation": "Embedding-based models (e.g., CLIP) enable fast candidate retrieval but lack the accuracy of joint-encoder rerankers. Existing joint encoders like BLIP are impractical at scale due to the heavy, online visual feature-extraction step. The goal is to make joint reranking feasible at web scale without sacrificing quality.", "method": "Precompute per-image vision tokens offline and compress them with a lightweight attention-based adapter. At query time, run a small joint encoder only over the compressed visual tokens plus the text, drastically cutting compute and memory. This preserves joint reasoning while avoiding expensive per-query visual backbone passes.", "result": "EDJE achieves 50k image\u2013text pair evaluations per second and requires 49 kB storage per image, while matching prior state-of-the-art on Flickr (zero-shot) and COCO (fine-tuned) retrieval benchmarks. It reduces online compute and storage requirements and enables high-throughput inference.", "conclusion": "Decoupling heavy visual feature extraction from online inference makes joint-encoder reranking practical at scale. EDJE maintains strong retrieval performance with minimal storage and compute, offering a deployable alternative to traditional joint encoders. Code and checkpoints will be released."}}
{"id": "2510.06780", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06780", "abs": "https://arxiv.org/abs/2510.06780", "authors": ["Luca Giordano", "Simon Razniewski"], "title": "Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness", "comment": null, "summary": "Large Language Models (LLMs) encode substantial factual knowledge, yet\nmeasuring and systematizing this knowledge remains challenging. Converting it\ninto structured format, for example through recursive extraction approaches\nsuch as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key\nopen questions include whether such extraction can terminate, whether its\noutputs are reproducible, and how robust they are to variations. We\nsystematically study LLM knowledge materialization using miniGPTKBs\n(domain-specific, tractable subcrawls), analyzing termination, reproducibility,\nand robustness across three categories of metrics: yield, lexical similarity,\nand semantic similarity. We experiment with four variations (seed, language,\nrandomness, model) and three illustrative domains (from history, entertainment,\nand finance). Our findings show (i) high termination rates, though\nmodel-dependent; (ii) mixed reproducibility; and (iii) robustness that varies\nby perturbation type: high for seeds and temperature, lower for languages and\nmodels. These results suggest that LLM knowledge materialization can reliably\nsurface core knowledge, while also revealing important limitations.", "AI": {"tldr": "They systematically evaluate turning LLM knowledge into structured mini knowledge bases via recursive extraction, testing termination, reproducibility, and robustness across metrics, domains, and perturbations; extraction usually terminates, reproducibility is mixed, and robustness is strong to seeds/temperature but weaker to language/model changes.", "motivation": "LLMs contain rich facts, but it\u2019s unclear how to reliably materialize that knowledge into structured form. Key uncertainties are whether recursive extraction processes will finish, whether their outputs are repeatable, and how sensitive they are to changes such as seeds, language, randomness, or model choice.", "method": "Construct domain-specific miniGPTKBs (small, tractable subcrawls) and evaluate knowledge materialization with metrics on yield, lexical similarity, and semantic similarity. Probe robustness via four perturbations (seed, language, randomness/temperature, and model) across three domains (history, entertainment, finance). Analyze termination behavior, reproducibility across runs, and robustness to perturbations.", "result": "High termination rates overall but dependent on the LLM; reproducibility varies across settings; robustness is relatively high for seed and temperature variations but lower when changing language or model.", "conclusion": "Recursive LLM knowledge materialization can reliably surface core, stable facts, but it has notable weaknesses in reproducibility and cross-language/model robustness, implying the need for careful methodology, model selection, and possibly standardization or control mechanisms."}}
{"id": "2510.06827", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06827", "abs": "https://arxiv.org/abs/2510.06827", "authors": ["Jaeseok Jeong", "Junho Kim", "Gayoung Lee", "Yunjey Choi", "Youngjung Uh"], "title": "StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance", "comment": "Accepted to ICCV 2025; CVPRW AI4CC 2024 (Best Paper + Oral)", "summary": "In the domain of text-to-image generation, diffusion models have emerged as\npowerful tools. Recently, studies on visual prompting, where images are used as\nprompts, have enabled more precise control over style and content. However,\nexisting methods often suffer from content leakage, where undesired elements of\nthe visual style prompt are transferred along with the intended style. To\naddress this issue, we 1) extend classifier-free guidance (CFG) to utilize\nswapping self-attention and propose 2) negative visual query guidance (NVQG) to\nreduce the transfer of unwanted contents. NVQG employs negative score by\nintentionally simulating content leakage scenarios that swap queries instead of\nkey and values of self-attention layers from visual style prompts. This simple\nyet effective method significantly reduces content leakage. Furthermore, we\nprovide careful solutions for using a real image as visual style prompts.\nThrough extensive evaluation across various styles and text prompts, our method\ndemonstrates superiority over existing approaches, reflecting the style of the\nreferences, and ensuring that resulting images match the text prompts. Our code\nis available \\href{https://github.com/naver-ai/StyleKeeper}{here}.", "AI": {"tldr": "StyleKeeper reduces content leakage in visual prompting for diffusion-based text-to-image generation by extending classifier-free guidance with swapped self-attention and introducing Negative Visual Query Guidance (NVQG), yielding better style transfer while preserving text-specified content.", "motivation": "Visual style prompts often leak unwanted content (objects/layout) into generated images, undermining control: users want only style transferred, not scene/content from the reference image.", "method": "(1) Extend classifier-free guidance (CFG) to operate with swapping self-attention signals. (2) Negative Visual Query Guidance (NVQG): construct a negative guidance term by simulating content leakage\u2014swap queries (Q) in self-attention using the visual style prompt (instead of swapping keys/values)\u2014and use its negative score to suppress leakage. Provide practical procedures to use real images as style prompts.", "result": "Across diverse styles and text prompts, the approach significantly lowers content leakage, better matches the text description, and more faithfully reflects the reference style than prior visual prompting baselines; qualitative and quantitative evaluations show superiority. Code released.", "conclusion": "A simple, effective, and easily integrable attention-guided negative guidance method improves style control without carrying over unwanted content from style images; practical for real-image prompts and ready for adoption via open-source code."}}
{"id": "2510.06800", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.06800", "abs": "https://arxiv.org/abs/2510.06800", "authors": ["Haotian Wu", "Shufan Jiang", "Chios Chen", "Yiyang Feng", "Hehai Lin", "Heqing Zou", "Yao Shu", "Yanran Li", "Chengwei Qin"], "title": "FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline", "comment": null, "summary": "As large language models (LLMs) advance in role-playing (RP) tasks, existing\nbenchmarks quickly become obsolete due to their narrow scope, outdated\ninteraction paradigms, and limited adaptability across diverse application\nscenarios. To address this gap, we introduce FURINA-Builder, a novel\nmulti-agent collaboration pipeline that automatically constructs fully\ncustomizable RP benchmarks at any scale. It enables evaluation of arbitrary\ncharacters across diverse scenarios and prompt formats, as the first benchmark\nbuilder in RP area for adaptable assessment. FURINA-Builder simulates dialogues\nbetween a test character and other characters drawn from a well-constructed\ncharacter-scene pool, while an LLM judge selects fine-grained evaluation\ndimensions and adjusts the test character's responses into final test\nutterances. Using this pipeline, we build FURINA-Bench, a new comprehensive\nrole-playing benchmark featuring both established and synthesized test\ncharacters, each assessed with dimension-specific evaluation criteria. Human\nevaluation and preliminary separability analysis justify our pipeline and\nbenchmark design. We conduct extensive evaluations of cutting-edge LLMs and\nfind that o3 and DeepSeek-R1 achieve the best performance on English and\nChinese RP tasks, respectively. Across all models, established characters\nconsistently outperform synthesized ones, with reasoning capabilities further\namplifying this disparity. Interestingly, we observe that model scale does not\nmonotonically reduce hallucinations. More critically, for reasoning LLMs, we\nuncover a novel trade-off: reasoning improves RP performance but simultaneously\nincreases RP hallucinations. This trade-off extends to a broader Pareto\nfrontier between RP performance and reliability for all LLMs. These findings\ndemonstrate the effectiveness of FURINA-Builder and the challenge posed by\nFURINA-Bench.", "AI": {"tldr": "Proposes FURINA-Builder, a multi-agent LLM pipeline that auto-generates scalable, customizable role\u2011playing (RP) benchmarks (FURINA-Bench). It flexibly evaluates arbitrary characters across scenarios and prompts, and reveals a performance\u2013reliability trade-off: stronger reasoning boosts RP scores but increases hallucinations; model scale doesn\u2019t consistently reduce them.", "motivation": "Existing RP benchmarks are narrow, outdated in interaction styles, and hard to adapt across scenarios, characters, and prompt formats. Rapid LLM progress renders fixed benchmarks obsolete, motivating an automated, scalable, and customizable benchmark builder for robust RP assessment.", "method": "A multi-agent collaboration pipeline: (1) sample a test character and partner characters/scenes from a curated character\u2013scene pool; (2) simulate dialogues; (3) an LLM judge selects fine-grained evaluation dimensions and edits/normalizes the test character\u2019s replies into final test utterances; (4) assemble tasks with dimension-specific criteria. Using this, they construct FURINA-Bench with both established and synthesized characters, validated via human evaluation and separability checks.", "result": "On extensive evaluations, o3 leads in English RP and DeepSeek-R1 in Chinese. Established characters consistently outperform synthesized ones, with the gap widening for reasoning models. Model scale does not monotonically reduce hallucinations. Reasoning LLMs show a novel trade-off: improved RP scores but higher RP hallucinations, reflecting a Pareto frontier between RP performance and reliability.", "conclusion": "FURINA-Builder enables adaptive, large-scale RP benchmarking, and FURINA-Bench poses a challenging, diverse testbed. Findings highlight a fundamental performance\u2013reliability trade-off in RP, guiding future work toward balancing reasoning strength with hallucination control and more robust evaluation practices."}}
{"id": "2510.06829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06829", "abs": "https://arxiv.org/abs/2510.06829", "authors": ["Mikihiro Ikura", "Arren Glover", "Masayoshi Mizuno", "Chiara Bartolozzi"], "title": "Lattice-allocated Real-time Line Segment Feature Detection and Tracking Using Only an Event-based Camera", "comment": "12 pages, 13 figures, 6 tables, ICCV Workshop NeVi2025", "summary": "Line segment extraction is effective for capturing geometric features of\nhuman-made environments. Event-based cameras, which asynchronously respond to\ncontrast changes along edges, enable efficient extraction by reducing redundant\ndata. However, recent methods often rely on additional frame cameras or\nstruggle with high event rates. This research addresses real-time line segment\ndetection and tracking using only a modern, high-resolution (i.e., high event\nrate) event-based camera. Our lattice-allocated pipeline consists of (i)\nvelocity-invariant event representation, (ii) line segment detection based on a\nfitting score, (iii) and line segment tracking by perturbating endpoints.\nEvaluation using ad-hoc recorded dataset and public datasets demonstrates\nreal-time performance and higher accuracy compared to state-of-the-art\nevent-only and event-frame hybrid baselines, enabling fully stand-alone event\ncamera operation in real-world settings.", "AI": {"tldr": "Real-time, event-only line segment detection and tracking that remains robust at high event rates via a lattice-allocated pipeline, outperforming event-only and hybrid baselines on public and custom datasets.", "motivation": "Line segments compactly encode man-made geometry, but existing event-based methods often require additional frame cameras or break down at modern high-resolution (high event-rate) settings. A standalone, real-time event-only solution is needed for efficiency and deployment simplicity.", "method": "A three-stage, lattice-allocated pipeline: (i) velocity-invariant event representation to normalize motion effects, (ii) line segment detection using a fitting score to select candidates, and (iii) tracking by perturbing endpoints to maintain segments over time, all operating on event streams without frames.", "result": "On both ad-hoc and public datasets, the method runs in real time and achieves higher accuracy than state-of-the-art event-only and event-frame hybrid approaches, handling high event rates from modern sensors.", "conclusion": "The proposed pipeline enables fully stand-alone event camera operation for reliable, real-time line segment extraction and tracking in real-world environments, advancing the practicality of event-based vision."}}
{"id": "2510.06805", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.06805", "abs": "https://arxiv.org/abs/2510.06805", "authors": ["Andr\u00e9 Greiner-Petter", "Maik Fr\u00f6be", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp", "Akiko Aizawa", "Martin Potthast"], "title": "Overview of the Plagiarism Detection Task at PAN 2025", "comment": "Working Notes at PAN at CLEF 2025", "summary": "The generative plagiarism detection task at PAN 2025 aims at identifying\nautomatically generated textual plagiarism in scientific articles and aligning\nthem with their respective sources. We created a novel large-scale dataset of\nautomatically generated plagiarism using three large language models: Llama,\nDeepSeek-R1, and Mistral. In this task overview paper, we outline the creation\nof this dataset, summarize and compare the results of all participants and four\nbaselines, and evaluate the results on the last plagiarism detection task from\nPAN 2015 in order to interpret the robustness of the proposed approaches. We\nfound that the current iteration does not invite a large variety of approaches\nas naive semantic similarity approaches based on embedding vectors provide\npromising results of up to 0.8 recall and 0.5 precision. In contrast, most of\nthese approaches underperform significantly on the 2015 dataset, indicating a\nlack in generalizability.", "AI": {"tldr": "Overview of PAN 2025\u2019s generative plagiarism detection task: introduces a large LLM-generated plagiarism dataset (Llama, DeepSeek-R1, Mistral), compares participant systems and baselines. Simple embedding-based semantic similarity achieves up to 0.8 recall/0.5 precision on the new dataset but fails badly on PAN 2015, exposing weak generalization.", "motivation": "Address the growing need to detect AI-generated plagiarism in scientific texts and align plagiarized passages to sources; provide a large, standardized benchmark and assess robustness and diversity of current approaches.", "method": "Construct a large-scale dataset of automatically generated plagiarism using three LLMs; run a shared task with participant systems and four baselines (including embedding-based similarity); evaluate systems both on the new dataset and on the prior PAN 2015 dataset to gauge robustness and generalization.", "result": "On the new dataset, naive semantic similarity baselines using embeddings obtain promising performance (up to 0.8 recall and 0.5 precision), suggesting the task setup favors such methods. However, these approaches perform significantly worse on the PAN 2015 dataset, indicating poor generalizability across distributions and plagiarism types.", "conclusion": "The current task configuration encourages narrow solution strategies and reveals a generalization gap. Future work should diversify data and task design, or develop more robust methods, to ensure transferability beyond the newly constructed dataset."}}
{"id": "2510.06842", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06842", "abs": "https://arxiv.org/abs/2510.06842", "authors": ["Kanglei Zhou", "Qingyi Pan", "Xingxing Zhang", "Hubert P. H. Shum", "Frederick W. B. Li", "Xiaohui Liang", "Liyuan Wang"], "title": "Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph Regularization", "comment": "Extended Version of MAGR (ECCV 2024 Oral Presentation)", "summary": "Action Quality Assessment (AQA) quantifies human actions in videos,\nsupporting applications in sports scoring, rehabilitation, and skill\nevaluation. A major challenge lies in the non-stationary nature of quality\ndistributions in real-world scenarios, which limits the generalization ability\nof conventional methods. We introduce Continual AQA (CAQA), which equips AQA\nwith Continual Learning (CL) capabilities to handle evolving distributions\nwhile mitigating catastrophic forgetting. Although parameter-efficient\nfine-tuning of pretrained models has shown promise in CL for image\nclassification, we find it insufficient for CAQA. Our empirical and theoretical\nanalyses reveal two insights: (i) Full-Parameter Fine-Tuning (FPFT) is\nnecessary for effective representation learning; yet (ii) uncontrolled FPFT\ninduces overfitting and feature manifold shift, thereby aggravating forgetting.\nTo address this, we propose Adaptive Manifold-Aligned Graph Regularization\n(MAGR++), which couples backbone fine-tuning that stabilizes shallow layers\nwhile adapting deeper ones with a two-step feature rectification pipeline: a\nmanifold projector to translate deviated historical features into the current\nrepresentation space, and a graph regularizer to align local and global\ndistributions. We construct four CAQA benchmarks from three datasets with\ntailored evaluation protocols and strong baselines, enabling systematic\ncross-dataset comparison. Extensive experiments show that MAGR++ achieves\nstate-of-the-art performance, with average correlation gains of 3.6% offline\nand 12.2% online over the strongest baseline, confirming its robustness and\neffectiveness. Our code is available at https://github.com/ZhouKanglei/MAGRPP.", "AI": {"tldr": "They define a Continual Action Quality Assessment setting and propose MAGR++, a controlled full-parameter fine-tuning framework that stabilizes shallow layers, adapts deeper layers, projects historical features onto the current manifold, and aligns distributions via graph regularization, achieving state-of-the-art gains (+3.6% offline, +12.2% online correlation).", "motivation": "AQA models face non-stationary quality distributions in real-world deployments, leading to poor generalization and catastrophic forgetting. Prior continual learning successes with parameter-efficient tuning in classification do not transfer well to AQA; effective representation learning seems to require full-parameter tuning without incurring overfitting or feature manifold shift.", "method": "Introduce the CAQA setting and analyze why PEFT underperforms. Show that full-parameter fine-tuning is needed but risky. Propose MAGR++: (1) backbone fine-tuning with layer-wise control\u2014stabilize shallow layers and adapt deeper ones; (2) a two-step feature rectification pipeline\u2014manifold projector to map historical features into the current representation space; (3) a graph regularizer to align local and global feature distributions over time. Build four CAQA benchmarks across three datasets with tailored protocols and strong baselines for systematic comparison.", "result": "MAGR++ consistently outperforms strong baselines, yielding average correlation improvements of 3.6% in offline and 12.2% in online continual settings, demonstrating robustness to evolving distributions. Code is released for reproducibility.", "conclusion": "Full-parameter tuning is necessary for CAQA but must be controlled to avoid overfitting and manifold drift. MAGR++ effectively mitigates forgetting by rectifying historical features and aligning feature distributions, setting a new state of the art on newly established CAQA benchmarks."}}
{"id": "2510.06811", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06811", "abs": "https://arxiv.org/abs/2510.06811", "authors": ["Philipp Mondorf", "Mingyang Wang", "Sebastian Gerstner", "Ahmad Dawar Hakimi", "Yihong Liu", "Leonor Veloso", "Shijia Zhou", "Hinrich Sch\u00fctze", "Barbara Plank"], "title": "BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods", "comment": "The 8th BlackboxNLP Workshop (Shared Task), 6 pages", "summary": "The Circuit Localization track of the Mechanistic Interpretability Benchmark\n(MIB) evaluates methods for localizing circuits within large language models\n(LLMs), i.e., subnetworks responsible for specific task behaviors. In this\nwork, we investigate whether ensembling two or more circuit localization\nmethods can improve performance. We explore two variants: parallel and\nsequential ensembling. In parallel ensembling, we combine attribution scores\nassigned to each edge by different methods-e.g., by averaging or taking the\nminimum or maximum value. In the sequential ensemble, we use edge attribution\nscores obtained via EAP-IG as a warm start for a more expensive but more\nprecise circuit identification method, namely edge pruning. We observe that\nboth approaches yield notable gains on the benchmark metrics, leading to a more\nprecise circuit identification approach. Finally, we find that taking a\nparallel ensemble over various methods, including the sequential ensemble,\nachieves the best results. We evaluate our approach in the BlackboxNLP 2025 MIB\nShared Task, comparing ensemble scores to official baselines across multiple\nmodel-task combinations.", "AI": {"tldr": "Ensembling circuit-localization methods\u2014via parallel score aggregation and a sequential warm-start-plus-pruning scheme\u2014improves precision on the MIB Circuit Localization track; a meta-parallel ensemble that also includes the sequential variant performs best across model\u2013task settings in the BlackboxNLP 2025 shared task.", "motivation": "Single circuit-localization methods can be noisy, biased, or suboptimal for different tasks; combining complementary attribution signals may yield more reliable and precise identification of the edges/subnetworks responsible for specific behaviors in LLMs.", "method": "Two ensemble strategies: (1) Parallel ensembling aggregates per-edge attribution scores from multiple methods (e.g., mean/min/max). (2) Sequential ensembling uses EAP-IG edge attributions as a warm start for a more expensive but more precise circuit identification procedure (edge pruning). They also form a higher-level parallel ensemble that includes the sequential ensemble alongside other methods. Evaluation is on MIB benchmark metrics across multiple LLM\u2013task combinations, compared with official baselines.", "result": "Both parallel and sequential ensembles produce notable gains on benchmark metrics, yielding more precise circuit identification than individual methods and baselines. The best overall performance comes from a parallel ensemble that pools multiple methods, including the sequential ensemble\u2019s output.", "conclusion": "Ensembling is an effective strategy for circuit localization in LLMs. Aggregating heterogeneous attribution signals and seeding pruning with inexpensive attributions improves accuracy and robustness over single-method approaches, with a meta-parallel ensemble delivering the top results on the shared task."}}
{"id": "2510.06855", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.06855", "abs": "https://arxiv.org/abs/2510.06855", "authors": ["Hyungrok Jung", "Daneul Kim", "Seunggyun Lim", "Jeany Son", "Jonghyun Choi"], "title": "Online Generic Event Boundary Detection", "comment": "ICCV 2025", "summary": "Generic Event Boundary Detection (GEBD) aims to interpret long-form videos\nthrough the lens of human perception. However, current GEBD methods require\nprocessing complete video frames to make predictions, unlike humans processing\ndata online and in real-time. To bridge this gap, we introduce a new task,\nOnline Generic Event Boundary Detection (On-GEBD), aiming to detect boundaries\nof generic events immediately in streaming videos. This task faces unique\nchallenges of identifying subtle, taxonomy-free event changes in real-time,\nwithout the access to future frames. To tackle these challenges, we propose a\nnovel On-GEBD framework, Estimator, inspired by Event Segmentation Theory (EST)\nwhich explains how humans segment ongoing activity into events by leveraging\nthe discrepancies between predicted and actual information. Our framework\nconsists of two key components: the Consistent Event Anticipator (CEA), and the\nOnline Boundary Discriminator (OBD). Specifically, the CEA generates a\nprediction of the future frame reflecting current event dynamics based solely\non prior frames. Then, the OBD measures the prediction error and adaptively\nadjusts the threshold using statistical tests on past errors to capture\ndiverse, subtle event transitions. Experimental results demonstrate that\nEstimator outperforms all baselines adapted from recent online video\nunderstanding models and achieves performance comparable to prior offline-GEBD\nmethods on the Kinetics-GEBD and TAPOS datasets.", "AI": {"tldr": "Defines and tackles Online Generic Event Boundary Detection by predicting the next frame and using adaptive error-based tests to flag boundaries in real time, achieving SOTA among online methods and near-offline performance on benchmark datasets.", "motivation": "Current GEBD models need full video access, unlike humans who segment events online. There is a need to detect taxonomy-free, subtle event boundaries immediately in streaming video without peeking at future frames.", "method": "Proposes Estimator, inspired by Event Segmentation Theory. It has: (1) Consistent Event Anticipator (CEA) that predicts the next frame from past frames; (2) Online Boundary Discriminator (OBD) that computes prediction errors and applies adaptive thresholds via statistical tests over past errors to decide boundaries.", "result": "On Kinetics-GEBD and TAPOS, Estimator outperforms online baselines adapted from recent online video understanding models and reaches performance comparable to prior offline GEBD methods.", "conclusion": "Prediction\u2013actual discrepancy with adaptive statistical testing is effective for real-time, taxonomy-free event boundary detection. The work establishes On-GEBD as a viable task and sets a strong baseline for future research."}}
{"id": "2510.06825", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06825", "abs": "https://arxiv.org/abs/2510.06825", "authors": ["Chenpeng Wang", "Xiaojie Cheng", "Chunye Wang", "Linfeng Yang", "Lei Zhang"], "title": "Adaptive Tool Generation with Models as Tools and Reinforcement Learning", "comment": null, "summary": "Tool-augmented language models have demonstrated strong capabilities, but\ntheir reliance on live API access creates scalability and reliability\nchallenges during training and deployment. We propose MTR, a simulation-first\ntraining framework for tool-augmented reasoning. Instead of relying on live\nAPIs, MTR learns from complete ReAct traces with schema-validated, simulated\nobservations. Our approach operates through a multi-agent architecture where a\nToolMaker generates task-specific, OpenAI-compatible tool interfaces, an\nAutoAgent produces structured think-act-observe sequences, and a ToolActor\nsimulates realistic responses. Training proceeds in two stages: Stage-1\nSupervised Fine-Tuning (SFT) teaches 'trace grammar' from complete reasoning\nsequences; Stage-2 Group Relative Policy Optimization (GRPO) optimizes strategy\nwith a composite trace reward that balances answer correctness and internal\nconsistency. Across four multi-hop QA benchmarks (HotpotQA, MuSiQue,\n2WikiMultiHopQA, Bamboogle), MTR attains competitive Exact Match (EM) scores to\nlive-API systems and excels on reasoning-intensive tasks, suggesting that\neffective tool reasoning can be learned from structured traces without live\ninteractions.", "AI": {"tldr": "MTR is a simulation-first framework that trains tool-augmented LMs using schema-validated, simulated ReAct traces and a two-stage SFT+GRPO process, matching live-API systems on multi-hop QA while improving reasoning consistency\u2014without needing live tool access.", "motivation": "Live API reliance in tool-augmented models hampers scalability, reliability, and cost during training/deployment. The authors aim to teach robust tool reasoning without depending on live interactions.", "method": "A multi-agent pipeline: (1) ToolMaker auto-generates task-specific, OpenAI-compatible tool interfaces; (2) AutoAgent produces structured think\u2013act\u2013observe (ReAct) traces; (3) ToolActor simulates realistic tool responses with schema validation. Training has two stages: Stage-1 SFT learns the \u2018trace grammar\u2019 from complete reasoning sequences; Stage-2 GRPO optimizes strategies using a composite trace reward that balances final answer correctness with internal consistency across the trace.", "result": "On HotpotQA, MuSiQue, 2WikiMultiHopQA, and Bamboogle, MTR achieves competitive Exact Match scores relative to systems using live APIs and performs especially well on reasoning-intensive tasks.", "conclusion": "Structured, simulation-based training can effectively instill tool-use reasoning, reducing dependence on live APIs. Combining SFT of trace structure with GRPO on a composite reward yields competitive performance and better internal consistency, improving scalability and reliability."}}
{"id": "2510.06858", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06858", "abs": "https://arxiv.org/abs/2510.06858", "authors": ["Adrien Dorise", "Marjorie Bellizzi", "Adrien Girard", "Benjamin Francesconi", "St\u00e9phane May"], "title": "Explaining raw data complexity to improve satellite onboard processing", "comment": "Preprint: European Data Handling & Data Processing Conference (EDHPC)\n  2025", "summary": "With increasing processing power, deploying AI models for remote sensing\ndirectly onboard satellites is becoming feasible. However, new constraints\narise, mainly when using raw, unprocessed sensor data instead of preprocessed\nground-based products. While current solutions primarily rely on preprocessed\nsensor images, few approaches directly leverage raw data. This study\ninvestigates the effects of utilising raw data on deep learning models for\nobject detection and classification tasks. We introduce a simulation workflow\nto generate raw-like products from high-resolution L1 imagery, enabling\nsystemic evaluation. Two object detection models (YOLOv11s and YOLOX-S) are\ntrained on both raw and L1 datasets, and their performance is compared using\nstandard detection metrics and explainability tools. Results indicate that\nwhile both models perform similarly at low to medium confidence thresholds, the\nmodel trained on raw data struggles with object boundary identification at high\nconfidence levels. It suggests that adapting AI architectures with improved\ncontouring methods can enhance object detection on raw images, improving\nonboard AI for remote sensing.", "AI": {"tldr": "Benchmarks object detection on simulated raw satellite data versus L1 imagery; performance is similar at low/medium thresholds, but raw-trained models lose boundary precision at high confidence, implying the need for contour-aware architectures for reliable onboard AI.", "motivation": "Onboard inference can cut latency and downlink needs, but most models assume preprocessed (L1) images. Raw sensor data onboard has different noise, radiometry, and geometry, and it\u2019s unclear how standard detectors behave, blocking practical deployment.", "method": "Build a workflow that converts high-resolution L1 images into raw-like data; train YOLOv11s and YOLOX-S on both raw-like and L1 datasets; compare with standard detection metrics across confidence thresholds and analyze with explainability tools.", "result": "Detectors trained on raw-like data match L1-trained counterparts at low-to-medium confidence but underperform at high confidence due to poorer object boundary localization; explainability indicates boundary uncertainty/contouring issues.", "conclusion": "Raw data is viable for onboard detection with small average losses, but achieving robust high-confidence outputs requires adapting architectures and losses to better model edges/contours; such changes could materially improve onboard remote-sensing AI."}}
{"id": "2510.06826", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06826", "abs": "https://arxiv.org/abs/2510.06826", "authors": ["Kaixiang Mo", "Yuxin Shi", "Weiwei Weng", "Zhiqiang Zhou", "Shuman Liu", "Haibo Zhang", "Anxiang Zeng"], "title": "Mid-Training of Large Language Models: A Survey", "comment": null, "summary": "Large language models (LLMs) are typically developed through large-scale\npre-training followed by task-specific fine-tuning. Recent advances highlight\nthe importance of an intermediate mid-training stage, where models undergo\nmultiple annealing-style phases that refine data quality, adapt optimization\nschedules, and extend context length. This stage mitigates diminishing returns\nfrom noisy tokens, stabilizes convergence, and expands model capability in late\ntraining. Its effectiveness can be explained through gradient noise scale, the\ninformation bottleneck, and curriculum learning, which together promote\ngeneralization and abstraction. Despite widespread use in state-of-the-art\nsystems, there has been no prior survey of mid-training as a unified paradigm.\nWe introduce the first taxonomy of LLM mid-training spanning data distribution,\nlearning-rate scheduling, and long-context extension. We distill practical\ninsights, compile evaluation benchmarks, and report gains to enable structured\ncomparisons across models. We also identify open challenges and propose avenues\nfor future research and practice.", "AI": {"tldr": "Proposes and systematizes an intermediate \u201cmid-training\u201d stage for LLMs\u2014with annealed phases for data curation, optimization schedule tuning, and context-length extension\u2014arguing it improves stability, generalization, and late-stage capability; offers the first taxonomy, benchmarks, and practical guidance, plus open problems.", "motivation": "Standard pre-train \u2192 fine-tune pipelines face diminishing returns from noisy tokens, unstable late-stage optimization, and limited context capabilities. Although many SOTA systems quietly use mid-training practices, there is no unified framework or survey to justify, compare, or evaluate them.", "method": "A survey/synthesis: defines \u2018mid-training\u2019 as multiple annealing-style phases; explains why it helps via gradient noise scale, information bottleneck, and curriculum learning; introduces a taxonomy along three axes (data distribution refinement, learning-rate/optimization schedules, long-context extension); compiles evaluation benchmarks; distills practical heuristics; reports empirical gains to facilitate structured comparisons.", "result": "Delivers the first unified taxonomy of LLM mid-training; curates benchmarks and reports performance gains across settings; provides interpretive lenses that rationalize improved convergence stability, generalization, and abstraction in late training; enables apples-to-apples comparisons across models and recipes.", "conclusion": "Mid-training is a principled, practically effective stage that mitigates noisy-token inefficiency, stabilizes training, and unlocks long-context capabilities. The paper formalizes the paradigm, offers tools to adopt and evaluate it, and highlights open challenges as targets for future research and best practices."}}
{"id": "2510.06876", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06876", "abs": "https://arxiv.org/abs/2510.06876", "authors": ["Samir Abou Haidar", "Alexandre Chariot", "Mehdi Darouich", "Cyril Joly", "Jean-Emmanuel Deschaud"], "title": "HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation", "comment": "Accepted at IROS 2025 (IEEE/RSJ International Conference on\n  Intelligent Robots and Systems)", "summary": "LiDAR semantic segmentation is crucial for autonomous vehicles and mobile\nrobots, requiring high accuracy and real-time processing, especially on\nresource-constrained embedded systems. Previous state-of-the-art methods often\nface a trade-off between accuracy and speed. Point-based and sparse\nconvolution-based methods are accurate but slow due to the complexity of\nneighbor searching and 3D convolutions. Projection-based methods are faster but\nlose critical geometric information during the 2D projection. Additionally,\nmany recent methods rely on test-time augmentation (TTA) to improve\nperformance, which further slows the inference. Moreover, the pre-processing\nphase across all methods increases execution time and is demanding on embedded\nplatforms. Therefore, we introduce HARP-NeXt, a high-speed and accurate LiDAR\nsemantic segmentation network. We first propose a novel pre-processing\nmethodology that significantly reduces computational overhead. Then, we design\nthe Conv-SE-NeXt feature extraction block to efficiently capture\nrepresentations without deep layer stacking per network stage. We also employ a\nmulti-scale range-point fusion backbone that leverages information at multiple\nabstraction levels to preserve essential geometric details, thereby enhancing\naccuracy. Experiments on the nuScenes and SemanticKITTI benchmarks show that\nHARP-NeXt achieves a superior speed-accuracy trade-off compared to all\nstate-of-the-art methods, and, without relying on ensemble models or TTA, is\ncomparable to the top-ranked PTv3, while running 24$\\times$ faster. The code is\navailable at https://github.com/SamirAbouHaidar/HARP-NeXt", "AI": {"tldr": "HARP-NeXt is a LiDAR semantic segmentation network that balances accuracy and real-time speed on embedded platforms via lightweight preprocessing, efficient Conv-SE-NeXt blocks, and multi-scale range\u2013point fusion\u2014matching top accuracy while being dramatically faster and TTA-free.", "motivation": "Autonomous vehicles and robots need highly accurate, real-time LiDAR segmentation on resource-constrained hardware. Existing methods either are accurate but slow (point/sparse conv due to neighbor search and 3D conv), or fast but lose geometry (2D projection). Heavy preprocessing and reliance on test-time augmentation further hurt deployment speed.", "method": "1) Novel lightweight preprocessing to cut computational overhead. 2) Conv-SE-NeXt feature extraction block to capture rich representations without deep stacks. 3) Multi-scale range\u2013point fusion backbone to preserve geometric details across abstraction levels. No ensembles or TTA required.", "result": "On nuScenes and SemanticKITTI, HARP-NeXt yields a superior speed\u2013accuracy trade-off versus state-of-the-art methods; it is comparable in accuracy to top-ranked PTv3 while running 24\u00d7 faster.", "conclusion": "Efficient design choices (lean preprocessing, tailored feature blocks, multi-scale fusion) enable practical, accurate, real-time LiDAR segmentation suitable for embedded platforms; code is publicly available."}}
{"id": "2510.06841", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06841", "abs": "https://arxiv.org/abs/2510.06841", "authors": ["Giorgos Filandrianos", "Orfeas Menis Mastromichalakis", "Wafaa Mohammed", "Giuseppe Attanasio", "Chrysoula Zerva"], "title": "GAMBIT+: A Challenge Set for Evaluating Gender Bias in Machine Translation Quality Estimation Metrics", "comment": "Accepted for publication at the 10th Conference of Machine\n  Translation (WMT25), co-located with EMNLP 2025", "summary": "Gender bias in machine translation (MT) systems has been extensively\ndocumented, but bias in automatic quality estimation (QE) metrics remains\ncomparatively underexplored. Existing studies suggest that QE metrics can also\nexhibit gender bias, yet most analyses are limited by small datasets, narrow\noccupational coverage, and restricted language variety. To address this gap, we\nintroduce a large-scale challenge set specifically designed to probe the\nbehavior of QE metrics when evaluating translations containing gender-ambiguous\noccupational terms. Building on the GAMBIT corpus of English texts with\ngender-ambiguous occupations, we extend coverage to three source languages that\nare genderless or natural-gendered, and eleven target languages with\ngrammatical gender, resulting in 33 source-target language pairs. Each source\ntext is paired with two target versions differing only in the grammatical\ngender of the occupational term(s) (masculine vs. feminine), with all dependent\ngrammatical elements adjusted accordingly. An unbiased QE metric should assign\nequal or near-equal scores to both versions. The dataset's scale, breadth, and\nfully parallel design, where the same set of texts is aligned across all\nlanguages, enables fine-grained bias analysis by occupation and systematic\ncomparisons across languages.", "AI": {"tldr": "They introduce a large, fully parallel multilingual challenge set to test whether MT quality estimation metrics score masculine and feminine translations of gender-ambiguous occupations equally, enabling precise cross-language and by-occupation bias analysis.", "motivation": "Bias in MT is well known, but bias in automatic quality estimation (QE) metrics is underexplored and prior studies are constrained by small, narrow, and limited-language datasets. A broader, controlled resource is needed to diagnose QE gender bias reliably.", "method": "Extend the GAMBIT corpus to create paired translations: for each English source with gender-ambiguous occupations (also extended to 3 genderless/natural-gendered source languages), produce two target translations in 11 grammatically gendered languages that differ only in the occupational term\u2019s grammatical gender (masculine vs. feminine), with all agreement updated. This yields 33 source\u2013target pairs in a fully parallel design, allowing direct score comparisons for each pair.", "result": "A large-scale, multilingual, occupation-rich, fully parallel challenge set where each instance has matched masculine/feminine target versions, enabling straightforward detection of QE bias via score parity checks across 33 language pairs.", "conclusion": "This dataset provides a rigorous, scalable benchmark to assess and compare gender bias in QE metrics across languages and occupations; an unbiased QE metric should assign near-equal scores to the paired masculine/feminine translations."}}
{"id": "2510.06887", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06887", "abs": "https://arxiv.org/abs/2510.06887", "authors": ["Bouthaina Slika", "Fadi Dornaika", "Fares Bougourzi", "Karim Hammoudi"], "title": "Lung Infection Severity Prediction Using Transformers with Conditional TransMix Augmentation and Cross-Attention", "comment": null, "summary": "Lung infections, particularly pneumonia, pose serious health risks that can\nescalate rapidly, especially during pandemics. Accurate AI-based severity\nprediction from medical imaging is essential to support timely clinical\ndecisions and optimize patient outcomes. In this work, we present a novel\nmethod applicable to both CT scans and chest X-rays for assessing lung\ninfection severity. Our contributions are twofold: (i) QCross-Att-PVT, a\nTransformer-based architecture that integrates parallel encoders, a cross-gated\nattention mechanism, and a feature aggregator to capture rich multi-scale\nfeatures; and (ii) Conditional Online TransMix, a custom data augmentation\nstrategy designed to address dataset imbalance by generating mixed-label image\npatches during training. Evaluated on two benchmark datasets, RALO CXR and\nPer-COVID-19 CT, our method consistently outperforms several state-of-the-art\ndeep learning models. The results emphasize the critical role of data\naugmentation and gated attention in improving both robustness and predictive\naccuracy. This approach offers a reliable, adaptable tool to support clinical\ndiagnosis, disease monitoring, and personalized treatment planning. The source\ncode of this work is available at https://github.com/bouthainas/QCross-Att-PVT.", "AI": {"tldr": "They propose a transformer-based model (QCross-Att-PVT) plus a conditional online TransMix augmentation to predict lung infection severity from both chest X-rays and CTs, achieving state-of-the-art results on two benchmarks and highlighting the value of gated attention and targeted augmentation.", "motivation": "Rapid, accurate severity assessment of pneumonia-like lung infections is critical for timely clinical decisions, especially during pandemics, but datasets are imbalanced and existing models may not capture multi-scale features robustly across modalities (CXR and CT).", "method": "1) QCross-Att-PVT: a Transformer with parallel encoders, a cross-gated attention module, and a feature aggregator to capture rich multi-scale representations; 2) Conditional Online TransMix: an on-the-fly, mixed-label patch augmentation tailored to mitigate class imbalance. The framework is modality-agnostic, working with both CXR and CT.", "result": "On RALO CXR and Per-COVID-19 CT benchmarks, the approach consistently outperforms multiple contemporary deep learning baselines, improving robustness and predictive accuracy. The study identifies data augmentation and gated attention as key contributors to performance gains.", "conclusion": "The method provides a reliable, adaptable tool for clinical diagnosis support, monitoring, and personalized treatment planning, with open-source code available for reproducibility and further research."}}
{"id": "2510.06843", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06843", "abs": "https://arxiv.org/abs/2510.06843", "authors": ["Xuhang Chen", "Zhifan Song", "Deyi Ji", "Shuo Gao", "Lanyun Zhu"], "title": "SID: Multi-LLM Debate Driven by Self Signals", "comment": null, "summary": "Large Language Models (LLMs) have exhibited impressive capabilities across\ndiverse application domains. Recent work has explored Multi-LLM Agent Debate\n(MAD) as a way to enhance performance by enabling multiple LLMs to discuss and\nrefine responses iteratively. Nevertheless, existing MAD methods predominantly\nfocus on utilizing external structures, such as debate graphs, using\nLLM-as-a-Judge, while neglecting the application of self signals, such as token\nlogits and attention, that arise during generation. This omission leads to\nredundant computation and potential performance degradation. In this paper, we\nshift the focus to the self signals of multi-LLM debate and introduce a\nSelf-Signals Driven Multi-LLM Debate (SID), which leverages two types of\nself-signals: model-level confidence and token-level semantic focus, to\nadaptively guide the debate process. Our approach enables high-confidence\nagents to exit early at the model level and compress the redundant debate\ncontents based on the attention mechanism. We evaluate our method on various\nLLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental\nresults demonstrate that our method not only outperforms existing MAD\ntechniques in accuracy but also reduces token consumption, highlighting the\neffectiveness of utilizing self signals in enhancing both the performance and\nefficiency of multi-agent debate systems. Our code will be available\nat~\\href{https://github.com/xuhang2019/SID}{\\texttt{https://github.com/xuhang2019/SID}}.", "AI": {"tldr": "SID is a self-signals driven multi-LLM debate framework that uses model confidence and attention-based semantic focus to adaptively control debates, enabling early agent exit and content compression, which improves accuracy while reducing token usage across benchmarks and models.", "motivation": "Existing multi-LLM agent debate methods mostly rely on external scaffolds (e.g., debate graphs, LLM-as-a-judge) and ignore internal generation signals (token logits, attention), causing redundant computation and possible performance drops. The authors aim to exploit these underused self signals to make debates more effective and efficient.", "method": "Introduce SID, which leverages two self signals: (1) model-level confidence to allow high-confidence agents to exit early, and (2) token-level semantic focus (via attention) to compress or filter redundant debate content and guide interactions. SID is applied across both LLMs and multimodal LLMs and adapts the debate process dynamically.", "result": "Across multiple challenging benchmarks and with various (multi)modal LLMs, SID achieves higher accuracy than prior MAD approaches while consuming fewer tokens, indicating improved performance and efficiency. (Abstract does not provide exact numbers.)", "conclusion": "Self signals such as confidence and attention can effectively steer multi-agent debates, reducing redundancy and improving outcomes. SID demonstrates a practical, efficient alternative to external-structure-heavy MAD frameworks, yielding better accuracy with lower token cost."}}
{"id": "2510.06926", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06926", "abs": "https://arxiv.org/abs/2510.06926", "authors": ["Hichem Sahbi"], "title": "Label-frugal satellite image change detection with generative virtual exemplar learning", "comment": null, "summary": "Change detection is a major task in remote sensing which consists in finding\nall the occurrences of changes in multi-temporal satellite or aerial images.\nThe success of existing methods, and particularly deep learning ones, is\ntributary to the availability of hand-labeled training data that capture the\nacquisition conditions and the subjectivity of the user (oracle). In this\npaper, we devise a novel change detection algorithm, based on active learning.\nThe main contribution of our work resides in a new model that measures how\nimportant is each unlabeled sample, and provides an oracle with only the most\ncritical samples (also referred to as virtual exemplars) for further labeling.\nThese exemplars are generated, using an invertible graph convnet, as the\noptimum of an adversarial loss that (i) measures representativity, diversity\nand ambiguity of the data, and thereby (ii) challenges (the most) the current\nchange detection criteria, leading to a better re-estimate of these criteria in\nthe subsequent iterations of active learning. Extensive experiments show the\npositive impact of our label-efficient learning model against comparative\nmethods.", "AI": {"tldr": "They propose a label-efficient active learning framework for remote-sensing change detection that generates and queries the most informative \u201cvirtual exemplars\u201d via an invertible graph convnet optimized with an adversarial loss, improving performance with fewer annotations.", "motivation": "Change detection in multi-temporal satellite/aerial imagery relies heavily on large, hand-labeled datasets that reflect acquisition conditions and annotator subjectivity. Collecting such labels is costly and often impractical, limiting deep models\u2019 effectiveness. A more label-efficient approach is needed.", "method": "An active learning scheme ranks unlabeled samples by importance and queries only the most critical ones. Instead of selecting raw samples, it generates virtual exemplars with an invertible graph convolutional network. These exemplars are obtained by optimizing an adversarial loss that jointly accounts for representativity, diversity, and ambiguity, thereby producing queries that maximally challenge the current change-detection criteria. The model is iteratively updated with oracle-labeled exemplars.", "result": "Extensive experiments indicate the proposed approach outperforms comparative methods under limited labeling budgets, achieving better change-detection performance with fewer annotated samples.", "conclusion": "Generating adversarially informative virtual exemplars within an active learning loop reduces annotation needs and leads to improved change detection. The approach offers a practical, label-efficient alternative to standard supervised methods and can be iteratively refined to adapt to data and annotator bias."}}
{"id": "2510.06847", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06847", "abs": "https://arxiv.org/abs/2510.06847", "authors": ["Pontakorn Trakuekul", "Attapol T. Rutherford", "Jullajak Karnjanaekarin", "Narongkorn Panitsrisit", "Sumana Sumanakul"], "title": "OpenJAI-v1.0: An Open Thai Large Language Model", "comment": null, "summary": "We introduce OpenJAI-v1.0, an open-source large language model for Thai and\nEnglish, developed from the Qwen3-14B model. Our work focuses on boosting\nperformance on practical tasks through carefully curated data across three key\nuse cases: instruction following, long-context understanding, and tool use.\nEvaluation results show that OpenJAI-v1.0 improves on the capabilities of its\nbase model and outperforms other leading open-source Thai models on a diverse\nsuite of benchmarks, while avoiding catastrophic forgetting. OpenJAI-v1.0 is\npublicly released as another alternative NLP resource for the Thai AI\ncommunity.", "AI": {"tldr": "OpenJAI-v1.0 is a bilingual (Thai/English) open-source LLM fine-tuned from Qwen3-14B, optimized for instruction following, long-context understanding, and tool use; it outperforms its base model and other Thai open models on diverse benchmarks without catastrophic forgetting, and is publicly released for the Thai AI community.", "motivation": "Address the lack of strong, practical Thai-capable open LLMs and improve real-world utility (instruction following, extended context handling, and tool integration) while maintaining prior knowledge.", "method": "Curate and use targeted datasets for three key use cases\u2014instruction following, long-context understanding, and tool use\u2014to further train Qwen3-14B; evaluate on diverse benchmarks to measure gains and check for catastrophic forgetting.", "result": "The model improves over Qwen3-14B and surpasses other leading open-source Thai models across multiple benchmarks, with no observed catastrophic forgetting.", "conclusion": "Targeted data curation for practical capabilities yields a stronger Thai/English LLM. OpenJAI-v1.0 is released as a robust, community-accessible resource that advances Thai NLP without sacrificing existing knowledge."}}
{"id": "2510.06928", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06928", "abs": "https://arxiv.org/abs/2510.06928", "authors": ["Ran Yi", "Teng Hu", "Zihan Su", "Lizhuang Ma"], "title": "IAR2: Improving Autoregressive Visual Generation with Semantic-Detail Associated Token Prediction", "comment": null, "summary": "Autoregressive models have emerged as a powerful paradigm for visual content\ncreation, but often overlook the intrinsic structural properties of visual\ndata. Our prior work, IAR, initiated a direction to address this by\nreorganizing the visual codebook based on embedding similarity, thereby\nimproving generation robustness. However, it is constrained by the rigidity of\npre-trained codebooks and the inaccuracies of hard, uniform clustering. To\novercome these limitations, we propose IAR2, an advanced autoregressive\nframework that enables a hierarchical semantic-detail synthesis process. At the\ncore of IAR2 is a novel Semantic-Detail Associated Dual Codebook, which\ndecouples image representations into a semantic codebook for global semantic\ninformation and a detail codebook for fine-grained refinements. It expands the\nquantization capacity from a linear to a polynomial scale, significantly\nenhancing expressiveness. To accommodate this dual representation, we propose a\nSemantic-Detail Autoregressive Prediction scheme coupled with a Local-Context\nEnhanced Autoregressive Head, which performs hierarchical prediction-first the\nsemantic token, then the detail token-while leveraging a local context window\nto enhance spatial coherence. Furthermore, for conditional generation, we\nintroduce a Progressive Attention-Guided Adaptive CFG mechanism that\ndynamically modulates the guidance scale for each token based on its relevance\nto the condition and its temporal position in the generation sequence,\nimproving conditional alignment without sacrificing realism. Extensive\nexperiments demonstrate that IAR2 sets a new state-of-the-art for\nautoregressive image generation, achieving a FID of 1.50 on ImageNet. Our model\nnot only surpasses previous methods in performance but also demonstrates\nsuperior computational efficiency, highlighting the effectiveness of our\nstructured, coarse-to-fine generation strategy.", "AI": {"tldr": "IAR2 is a structured, coarse-to-fine autoregressive image generator that uses a dual codebook (semantic + detail), hierarchical token prediction with local context, and adaptive per-token CFG, achieving state-of-the-art FID 1.50 on ImageNet with improved efficiency.", "motivation": "Standard autoregressive (AR) image models often ignore visual structure, and prior IAR\u2019s codebook reorganization was limited by fixed codebooks and hard, uniform clustering. The goal is to better capture global semantics and fine details while overcoming rigidity and improving robustness and conditional alignment.", "method": "Introduce a Semantic-Detail Associated Dual Codebook that decouples global semantics from fine details, expanding quantization capacity polynomially (via combinatorial semantic-detail pairs). Use a Semantic-Detail Autoregressive Prediction scheme with a Local-Context Enhanced AR head to predict semantic tokens first, then detail tokens, leveraging a local spatial window for coherence. For conditional generation, apply a Progressive Attention-Guided Adaptive CFG that modulates guidance per token based on attention-derived relevance and its position in the sequence.", "result": "Extensive experiments show new SOTA for AR image generation: FID 1.50 on ImageNet. The model surpasses previous methods in quality and is computationally more efficient, indicating better expressiveness and robustness.", "conclusion": "Decoupling semantics and details and predicting them hierarchically yields more expressive, robust, and efficient AR image generation with stronger conditional alignment. IAR2\u2019s structured, coarse-to-fine strategy advances the state of the art in AR image synthesis."}}
{"id": "2510.06866", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06866", "abs": "https://arxiv.org/abs/2510.06866", "authors": ["Wafaa Mohammed", "Vlad Niculae", "Chrysoula Zerva"], "title": "Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding", "comment": null, "summary": "Large language models (LLMs) have emerged as strong contenders in machine\ntranslation.Yet, they still struggle to adequately handle discourse phenomena,\nsuch as pronoun resolution and lexical cohesion at the document level. In this\nstudy, we thoroughly investigate the discourse phenomena performance of LLMs in\ncontext-aware translation. We demonstrate that discourse knowledge is encoded\nwithin LLMs and propose the use of quality-aware decoding (QAD) to effectively\nextract this knowledge, showcasing its superiority over other decoding\napproaches through comprehensive analysis. Furthermore, we illustrate that QAD\nenhances the semantic richness of translations and aligns them more closely\nwith human preferences.", "AI": {"tldr": "LLMs already encode discourse knowledge for MT, but standard decoding underutilizes it; a new quality\u2011aware decoding (QAD) better surfaces this knowledge, improving document\u2011level coherence and human\u2011rated quality.", "motivation": "Despite strong sentence\u2011level MT performance, LLMs still mishandle document\u2011level discourse phenomena (e.g., pronoun resolution, lexical cohesion). The goal is to leverage latent discourse knowledge without retraining models.", "method": "Evaluate LLMs on context\u2011aware translation focused on discourse phenomena. Propose quality\u2011aware decoding (QAD) that selects outputs using quality signals to favor discourse\u2011coherent, semantically rich candidates. Compare QAD to common decoding strategies (greedy, beam, sampling) and analyze outcomes.", "result": "QAD outperforms other decoding approaches on discourse\u2011sensitive metrics, produces semantically richer translations, and aligns more closely with human preferences.", "conclusion": "LLMs do encode discourse knowledge, but decoding strategy determines whether it is expressed. QAD provides an effective, training\u2011free way to enhance document\u2011level MT quality."}}
{"id": "2510.06952", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06952", "abs": "https://arxiv.org/abs/2510.06952", "authors": ["Bing Li", "Wuqi Wang", "Yanan Zhang", "Jingzheng Li", "Haigen Min", "Wei Feng", "Xingyu Zhao", "Jie Zhang", "Qing Guo"], "title": "OBJVanish: Physically Realizable Text-to-3D Adv. Generation of LiDAR-Invisible Objects", "comment": null, "summary": "LiDAR-based 3D object detectors are fundamental to autonomous driving, where\nfailing to detect objects poses severe safety risks. Developing effective 3D\nadversarial attacks is essential for thoroughly testing these detection systems\nand exposing their vulnerabilities before real-world deployment. However,\nexisting adversarial attacks that add optimized perturbations to 3D points have\ntwo critical limitations: they rarely cause complete object disappearance and\nprove difficult to implement in physical environments. We introduce the\ntext-to-3D adversarial generation method, a novel approach enabling physically\nrealizable attacks that can generate 3D models of objects truly invisible to\nLiDAR detectors and be easily realized in the real world. Specifically, we\npresent the first empirical study that systematically investigates the factors\ninfluencing detection vulnerability by manipulating the topology, connectivity,\nand intensity of individual pedestrian 3D models and combining pedestrians with\nmultiple objects within the CARLA simulation environment. Building on the\ninsights, we propose the physically-informed text-to-3D adversarial generation\n(Phy3DAdvGen) that systematically optimizes text prompts by iteratively\nrefining verbs, objects, and poses to produce LiDAR-invisible pedestrians. To\nensure physical realizability, we construct a comprehensive object pool\ncontaining 13 3D models of real objects and constrain Phy3DAdvGen to generate\n3D objects based on combinations of objects in this set. Extensive experiments\ndemonstrate that our approach can generate 3D pedestrians that evade six\nstate-of-the-art (SOTA) LiDAR 3D detectors in both CARLA simulation and\nphysical environments, thereby highlighting vulnerabilities in safety-critical\napplications.", "AI": {"tldr": "They propose Phy3DAdvGen, a physically informed text-to-3D attack that generates pedestrian 3D models which become effectively invisible to LiDAR detectors, validated in both CARLA simulation and real-world tests against six SOTA detectors.", "motivation": "LiDAR object detectors are safety-critical, yet existing point-perturbation attacks rarely cause full object disappearance and are hard to realize physically. There is a need for practical, high-impact adversarial tests to expose weaknesses before deployment.", "method": "(1) Empirical study in CARLA probing vulnerability factors by manipulating pedestrian model topology, connectivity, intensity, and by composing pedestrians with other objects. (2) A text-to-3D adversarial generation pipeline (Phy3DAdvGen) that iteratively optimizes prompts\u2014verbs, objects, and poses\u2014to yield LiDAR-invisible pedestrians. (3) Physical realizability enforced via a curated pool of 13 real-object 3D models; generation is constrained to combinations from this pool. Evaluations target six SOTA LiDAR 3D detectors in simulation and physical environments.", "result": "The generated 3D pedestrian models evade detection across six SOTA LiDAR detectors, achieving practical object disappearance in both simulated CARLA scenarios and real-world setups; experiments are described as extensive, implying consistent cross-model effectiveness.", "conclusion": "Physically realizable, prompt-optimized text-to-3D generation can craft LiDAR-invisible objects, revealing significant vulnerabilities in current 3D detection systems and underscoring the need for stronger defenses and robustness evaluations."}}
{"id": "2510.06870", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06870", "abs": "https://arxiv.org/abs/2510.06870", "authors": ["Yining Wang", "Jinman Zhao", "Chuangxin Zhao", "Shuhao Guan", "Gerald Penn", "Shinan Liu"], "title": "$\u03bb$-GRPO: Unifying the GRPO Frameworks with Learnable Token Preferences", "comment": "9 pages", "summary": "Reinforcement Learning with Human Feedback (RLHF) has been the dominant\napproach for improving the reasoning capabilities of Large Language Models\n(LLMs). Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has\nsimplified this paradigm by replacing the reward and value models with\nrule-based verifiers. A prominent example is Group Relative Policy Optimization\n(GRPO). However, GRPO inherently suffers from a length bias, since the same\nadvantage is uniformly assigned to all tokens of a response. As a result,\nlonger responses distribute the reward over more tokens and thus contribute\ndisproportionately to gradient updates. Several variants, such as DAPO and Dr.\nGRPO, modify the token-level aggregation of the loss, yet these methods remain\nheuristic and offer limited interpretability regarding their implicit token\npreferences. In this work, we explore the possibility of allowing the model to\nlearn its own token preference during optimization. We unify existing\nframeworks under a single formulation and introduce a learnable parameter\n$\\lambda$ that adaptively controls token-level weighting. We use $\\lambda$-GRPO\nto denote our method, and we find that $\\lambda$-GRPO achieves consistent\nimprovements over vanilla GRPO and DAPO on multiple mathematical reasoning\nbenchmarks. On Qwen2.5 models with 1.5B, 3B, and 7B parameters, $\\lambda$-GRPO\nimproves average accuracy by $+1.9\\%$, $+1.0\\%$, and $+1.7\\%$ compared to GRPO,\nrespectively. Importantly, these gains come without any modifications to the\ntraining data or additional computational cost, highlighting the effectiveness\nand practicality of learning token preferences.", "AI": {"tldr": "They propose \u03bb-GRPO, a GRPO variant that learns token-level weighting to fix length bias in rule-verified RL, delivering consistent accuracy gains on math reasoning without extra compute or data changes.", "motivation": "GRPO in RLVR assigns the same advantage to all tokens, causing longer responses to dominate gradients (length bias). Existing fixes (e.g., DAPO, Dr.GRPO) adjust aggregation heuristically and obscure the implied token preferences. The authors want a principled, interpretable way to set token importance.", "method": "Unify RLVR/GRPO-style objectives under a single formulation and introduce a learnable parameter \u03bb that adaptively controls token-level weighting (token preference) during optimization, letting the model learn how to distribute reward across tokens. No changes to data or compute budget.", "result": "On Qwen2.5 1.5B/3B/7B, \u03bb-GRPO improves average accuracy over GRPO by +1.9%, +1.0%, and +1.7%, and outperforms DAPO across multiple math reasoning benchmarks.", "conclusion": "Learning token preferences via \u03bb mitigates GRPO\u2019s length bias and yields consistent, practical gains without added cost, offering a more interpretable and effective alternative to heuristic token-weighting schemes."}}
{"id": "2510.06967", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06967", "abs": "https://arxiv.org/abs/2510.06967", "authors": ["Huanning Dong", "Fan Li", "Ping Kuang", "Jianwen Min"], "title": "Generating Surface for Text-to-3D using 2D Gaussian Splatting", "comment": null, "summary": "Recent advancements in Text-to-3D modeling have shown significant potential\nfor the creation of 3D content. However, due to the complex geometric shapes of\nobjects in the natural world, generating 3D content remains a challenging task.\nCurrent methods either leverage 2D diffusion priors to recover 3D geometry, or\ntrain the model directly based on specific 3D representations. In this paper,\nwe propose a novel method named DirectGaussian, which focuses on generating the\nsurfaces of 3D objects represented by surfels. In DirectGaussian, we utilize\nconditional text generation models and the surface of a 3D object is rendered\nby 2D Gaussian splatting with multi-view normal and texture priors. For\nmulti-view geometric consistency problems, DirectGaussian incorporates\ncurvature constraints on the generated surface during optimization process.\nThrough extensive experiments, we demonstrate that our framework is capable of\nachieving diverse and high-fidelity 3D content creation.", "AI": {"tldr": "DirectGaussian is a text-to-3D method that directly generates surfel-based object surfaces using 2D Gaussian splatting, guided by multi-view normal/texture priors and curvature constraints to ensure geometric consistency, yielding diverse, high-fidelity 3D assets.", "motivation": "Text-to-3D remains hard due to complex real-world geometries and the limitations of relying solely on 2D diffusion priors or committing to rigid 3D representations. There is a need for a method that produces geometrically consistent, detailed surfaces from text prompts.", "method": "Represent object surfaces as surfels and render them with 2D Gaussian splatting. Use conditional text generation to obtain multi-view normal and texture priors for guidance. During optimization, add curvature-based constraints to enforce multi-view geometric consistency of the generated surface.", "result": "Extensive experiments (details not provided in the abstract) show the framework can create diverse 3D content with high fidelity and improved geometric consistency.", "conclusion": "A surfel-based, Gaussian-splatting approach with curvature constraints is an effective path for text-driven 3D surface generation, producing diverse and high-quality results while addressing multi-view consistency challenges."}}
{"id": "2510.06889", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06889", "abs": "https://arxiv.org/abs/2510.06889", "authors": ["Zaid Alyafeai", "Maged S. Al-Shaibani", "Bernard Ghanem"], "title": "MeXtract: Light-Weight Metadata Extraction from Scientific Papers", "comment": null, "summary": "Metadata plays a critical role in indexing, documenting, and analyzing\nscientific literature, yet extracting it accurately and efficiently remains a\nchallenging task. Traditional approaches often rely on rule-based or\ntask-specific models, which struggle to generalize across domains and schema\nvariations. In this paper, we present MeXtract, a family of lightweight\nlanguage models designed for metadata extraction from scientific papers. The\nmodels, ranging from 0.5B to 3B parameters, are built by fine-tuning Qwen 2.5\ncounterparts. In their size family, MeXtract achieves state-of-the-art\nperformance on metadata extraction on the MOLE benchmark. To further support\nevaluation, we extend the MOLE benchmark to incorporate model-specific\nmetadata, providing an out-of-domain challenging subset. Our experiments show\nthat fine-tuning on a given schema not only yields high accuracy but also\ntransfers effectively to unseen schemas, demonstrating the robustness and\nadaptability of our approach. We release all the code, datasets, and models\nopenly for the research community.", "AI": {"tldr": "MeXtract introduces small fine-tuned LLMs (0.5B\u20133B) for scientific-paper metadata extraction, achieving state-of-the-art results on MOLE, extending the benchmark with an out-of-domain subset, and showing strong transfer to unseen schemas.", "motivation": "Accurate, scalable metadata extraction is essential for indexing and analyzing scientific literature, but rule-based or task-specific systems fail to generalize across domains and schema variations.", "method": "Fine-tune a family of Qwen 2.5 models (0.5B\u20133B parameters) for structured metadata extraction; evaluate on the MOLE benchmark and an extended, harder out-of-domain subset featuring model-specific metadata; test schema transfer by training on one schema and evaluating on unseen schemas.", "result": "Within their size class, MeXtract models achieve SOTA performance on MOLE and maintain high accuracy on the new OOD subset; fine-tuning on a given schema transfers effectively to unseen schemas, indicating robustness and adaptability.", "conclusion": "Lightweight, fine-tuned LLMs can perform high-quality metadata extraction and generalize across schemas. The released code, datasets, and models support reproducibility and further research."}}
{"id": "2510.06969", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06969", "abs": "https://arxiv.org/abs/2510.06969", "authors": ["Shoumeng Qiu", "Xinrun Li", "Yang Long", "Xiangyang Xue", "Varun Ojha", "Jian Pu"], "title": "Learning Global Representation from Queries for Vectorized HD Map Construction", "comment": "16 pages", "summary": "The online construction of vectorized high-definition (HD) maps is a\ncornerstone of modern autonomous driving systems. State-of-the-art approaches,\nparticularly those based on the DETR framework, formulate this as an instance\ndetection problem. However, their reliance on independent, learnable object\nqueries results in a predominantly local query perspective, neglecting the\ninherent global representation within HD maps. In this work, we propose\n\\textbf{MapGR} (\\textbf{G}lobal \\textbf{R}epresentation learning for HD\n\\textbf{Map} construction), an architecture designed to learn and utilize a\nglobal representations from queries. Our method introduces two synergistic\nmodules: a Global Representation Learning (GRL) module, which encourages the\ndistribution of all queries to better align with the global map through a\ncarefully designed holistic segmentation task, and a Global Representation\nGuidance (GRG) module, which endows each individual query with explicit,\nglobal-level contextual information to facilitate its optimization. Evaluations\non the nuScenes and Argoverse2 datasets validate the efficacy of our approach,\ndemonstrating substantial improvements in mean Average Precision (mAP) compared\nto leading baselines.", "AI": {"tldr": "MapGR augments DETR-style vectorized HD map construction with learned global representations, using a holistic segmentation-driven learning signal and global guidance to queries, yielding higher mAP on nuScenes and Argoverse2.", "motivation": "DETR-based HD map methods treat vector elements as independent instances via learnable queries, which biases learning toward local features and misses the global structure and context inherent to maps. This can hurt coherence and accuracy of vectorized outputs. The motivation is to inject a global map representation that regularizes and informs per-query predictions.", "method": "Introduce two modules: (1) Global Representation Learning (GRL), which learns a global map representation by supervising the collective query distribution with a holistic segmentation task, aligning queries to the overall map layout; (2) Global Representation Guidance (GRG), which feeds the learned global representation back to each query, providing explicit global context during optimization within a DETR-like framework.", "result": "On nuScenes and Argoverse2, MapGR achieves substantial gains in mean Average Precision over state-of-the-art baselines (exact numbers not provided in the abstract), demonstrating improved vectorized map detection performance.", "conclusion": "Embedding a learned global map representation into query-based HD map construction both regularizes the query set and enriches per-query context, leading to better accuracy and likely more coherent map outputs across diverse datasets."}}
{"id": "2510.06915", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06915", "abs": "https://arxiv.org/abs/2510.06915", "authors": ["Zecheng Tang", "Baibei Ji", "Quantong Qiu", "Haitian Wang", "Xiaobo Liang", "Juntao Li", "Min Zhang"], "title": "LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling", "comment": null, "summary": "Reward model (RM) plays a pivotal role in aligning large language model (LLM)\nwith human preferences. As real-world applications increasingly involve long\nhistory trajectories, e.g., LLM agent, it becomes indispensable to evaluate\nwhether a model's responses are not only high-quality but also grounded in and\nconsistent with the provided context. Yet, current RMs remain confined to\nshort-context settings and primarily focus on response-level attributes (e.g.,\nsafety or helpfulness), while largely neglecting the critical dimension of long\ncontext-response consistency. In this work, we introduce Long-RewardBench, a\nbenchmark specifically designed for long-context RM evaluation, featuring both\nPairwise Comparison and Best-of-N tasks. Our preliminary study reveals that\neven state-of-the-art generative RMs exhibit significant fragility in\nlong-context scenarios, failing to maintain context-aware preference judgments.\nMotivated by the analysis of failure patterns observed in model outputs, we\npropose a general multi-stage training strategy that effectively scales\narbitrary models into robust Long-context RMs (LongRMs). Experiments show that\nour approach not only substantially improves performance on long-context\nevaluation but also preserves strong short-context capability. Notably, our 8B\nLongRM outperforms much larger 70B-scale baselines and matches the performance\nof the proprietary Gemini 2.5 Pro model.", "AI": {"tldr": "They introduce Long-RewardBench to test whether reward models can judge responses with long contexts, find that existing RMs fail at this, and propose a multi-stage training strategy to build LongRMs that significantly improve long-context consistency\u2014an 8B model even beats 70B baselines and matches Gemini 2.5 Pro.", "motivation": "Real-world LLM use (e.g., agents) involves long interaction histories where responses must be grounded in and consistent with extensive context. Existing reward models are tuned for short contexts and generic attributes (helpfulness/safety), neglecting long-context consistency, so there is no reliable way to evaluate or train for this setting.", "method": "1) Build Long-RewardBench, a benchmark for long-context RM evaluation with Pairwise Comparison and Best-of-N tasks to measure context-aware preference judgments. 2) Analyze failure modes of current (generative) RMs on long contexts. 3) Propose a general, multi-stage training pipeline to convert arbitrary models into long-context reward models (LongRMs). The stages are not fully detailed in the abstract but aim to improve context grounding and robustness while retaining short-context skills.", "result": "Empirically, SOTA generative reward models are fragile with long contexts. The proposed training yields substantial gains on long-context evaluation while preserving short-context performance. Their 8B LongRM surpasses much larger 70B baselines and reaches parity with the proprietary Gemini 2.5 Pro.", "conclusion": "Long-context consistency is a critical but overlooked axis for reward models. A dedicated benchmark plus a general multi-stage training approach can produce compact RMs that are robust to long contexts without sacrificing short-context quality, advancing alignment for agentic, real-world LLM deployments."}}
{"id": "2510.06973", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06973", "abs": "https://arxiv.org/abs/2510.06973", "authors": ["Zhantao Yang", "Huangji Wang", "Ruili Feng", "Han Zhang", "Yuting Hu", "Shangwen Zhu", "Junyan Li", "Yu Liu", "Fan Cheng"], "title": "Addressing the ID-Matching Challenge in Long Video Captioning", "comment": null, "summary": "Generating captions for long and complex videos is both critical and\nchallenging, with significant implications for the growing fields of\ntext-to-video generation and multi-modal understanding. One key challenge in\nlong video captioning is accurately recognizing the same individuals who appear\nin different frames, which we refer to as the ID-Matching problem. Few prior\nworks have focused on this important issue. Those that have, usually suffer\nfrom limited generalization and depend on point-wise matching, which limits\ntheir overall effectiveness. In this paper, unlike previous approaches, we\nbuild upon LVLMs to leverage their powerful priors. We aim to unlock the\ninherent ID-Matching capabilities within LVLMs themselves to enhance the\nID-Matching performance of captions. Specifically, we first introduce a new\nbenchmark for assessing the ID-Matching capabilities of video captions. Using\nthis benchmark, we investigate LVLMs containing GPT-4o, revealing key insights\nthat the performance of ID-Matching can be improved through two methods: 1)\nenhancing the usage of image information and 2) increasing the quantity of\ninformation of individual descriptions. Based on these insights, we propose a\nnovel video captioning method called Recognizing Identities for Captioning\nEffectively (RICE). Extensive experiments including assessments of caption\nquality and ID-Matching performance, demonstrate the superiority of our\napproach. Notably, when implemented on GPT-4o, our RICE improves the precision\nof ID-Matching from 50% to 90% and improves the recall of ID-Matching from 15%\nto 80% compared to baseline. RICE makes it possible to continuously track\ndifferent individuals in the captions of long videos.", "AI": {"tldr": "They identify identity matching (consistent recognition of the same people across frames) as the key bottleneck in long\u2011video captioning, build a benchmark to measure it, study LVLMs (incl. GPT\u20114o) to derive two practical levers\u2014use image evidence more and enrich per\u2011person descriptions\u2014and propose RICE, a method that exploits LVLM priors to track individuals in captions, boosting ID\u2011matching precision from 50\u219290% and recall from 15\u219280%.", "motivation": "Long, complex videos require captions that consistently refer to the same individuals across time. Existing methods rarely target this ID\u2011matching need; those that do rely on point\u2011wise matching and don\u2019t generalize well, hurting both caption quality and downstream multimodal/text\u2011to\u2011video tasks.", "method": "1) Introduce a benchmark to assess ID\u2011matching in video captions. 2) Probe LVLMs (e.g., GPT\u20114o) to discover that ID\u2011matching improves by (a) leveraging more image information and (b) providing richer per\u2011individual descriptions. 3) Propose RICE (Recognizing Identities for Captioning Effectively), a captioning approach that unlocks LVLMs\u2019 inherent ID\u2011matching priors via enhanced image usage and increased identity\u2011description granularity, enabling continuous tracking of people across long videos.", "result": "Across experiments, RICE yields superior caption quality and ID\u2011matching. On GPT\u20114o, precision improves from 50% to 90% and recall from 15% to 80% relative to baseline.", "conclusion": "LVLMs contain latent capacity for identity consistency that can be activated with the right strategy. RICE operationalizes this, enabling stable per\u2011person tracking in long\u2011video captions and setting a benchmark for evaluating ID\u2011matching, with implications for text\u2011to\u2011video generation and broader multimodal understanding."}}
{"id": "2510.06917", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.06917", "abs": "https://arxiv.org/abs/2510.06917", "authors": ["Cheng-Han Chiang", "Xiaofei Wang", "Linjie Li", "Chung-Ching Lin", "Kevin Lin", "Shujie Liu", "Zhendong Wang", "Zhengyuan Yang", "Hung-yi Lee", "Lijuan Wang"], "title": "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models", "comment": "Work in progress", "summary": "Current large language models (LLMs) and spoken language models (SLMs) begin\nthinking and taking actions only after the user has finished their turn. This\nprevents the model from interacting during the user's turn and can lead to high\nresponse latency while it waits to think. Consequently, thinking after\nreceiving the full input is not suitable for speech-to-speech interaction,\nwhere real-time, low-latency exchange is important. We address this by noting\nthat humans naturally \"think while listening.\" In this paper, we propose\nSHANKS, a general inference framework that enables SLMs to generate unspoken\nchain-of-thought reasoning while listening to the user input. SHANKS streams\nthe input speech in fixed-duration chunks and, as soon as a chunk is received,\ngenerates unspoken reasoning based on all previous speech and reasoning, while\nthe user continues speaking. SHANKS uses this unspoken reasoning to decide\nwhether to interrupt the user and to make tool calls to complete the task. We\ndemonstrate that SHANKS enhances real-time user-SLM interaction in two\nscenarios: (1) when the user is presenting a step-by-step solution to a math\nproblem, SHANKS can listen, reason, and interrupt when the user makes a\nmistake, achieving 37.1% higher interruption accuracy than a baseline that\ninterrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can\ncomplete 56.9% of the tool calls before the user finishes their turn. Overall,\nSHANKS moves toward models that keep thinking throughout the conversation, not\nonly after a turn ends. Animated illustrations of Shanks can be found at\nhttps://d223302.github.io/SHANKS/", "AI": {"tldr": "SHANKS is an inference framework for spoken language models that performs hidden chain\u2011of\u2011thought reasoning while the user is still speaking, enabling timely interruptions and proactive tool calls; it improves interruption accuracy by 37.1% and completes 56.9% of tool calls before a turn ends.", "motivation": "Conventional LLM/SLM systems wait for the user to finish before thinking or acting, causing high latency and poor interactivity in speech\u2011to\u2011speech settings where real\u2011time feedback, corrections, and tool use are required.", "method": "Stream the user\u2019s speech in fixed\u2011length chunks; after each chunk, generate unspoken (non\u2011vocalized) reasoning conditioned on prior audio and prior reasoning. Use this evolving internal state to (a) decide whether to interrupt the speaker and (b) launch tool calls early to advance the task while the user continues speaking. Evaluate on step\u2011by\u2011step math explanation (interrupt on mistakes) and on tool\u2011augmented dialogue (proactive calls).", "result": "Compared to a baseline that interrupts without intermediate reasoning, SHANKS achieves 37.1% higher interruption accuracy in math\u2011explanation scenarios and completes 56.9% of required tool calls before the user finishes speaking in tool\u2011augmented dialogues.", "conclusion": "Think\u2011while\u2011listening enables lower\u2011latency, more interactive speech agents that reason continuously rather than only post\u2011turn. SHANKS shows this approach can yield earlier, more accurate interventions and actions in real\u2011time conversations."}}
{"id": "2510.06988", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06988", "abs": "https://arxiv.org/abs/2510.06988", "authors": ["Girolamo Macaluso", "Lorenzo Mandelli", "Mirko Bicchierai", "Stefano Berretti", "Andrew D. Bagdanov"], "title": "No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts", "comment": null, "summary": "Diffusion models have recently advanced human motion generation, producing\nrealistic and diverse animations from textual prompts. However, adapting these\nmodels to unseen actions or styles typically requires additional motion capture\ndata and full retraining, which is costly and difficult to scale. We propose a\npost-training framework based on Reinforcement Learning that fine-tunes\npretrained motion diffusion models using only textual prompts, without\nrequiring any motion ground truth. Our approach employs a pretrained\ntext-motion retrieval network as a reward signal and optimizes the diffusion\npolicy with Denoising Diffusion Policy Optimization, effectively shifting the\nmodel's generative distribution toward the target domain without relying on\npaired motion data. We evaluate our method on cross-dataset adaptation and\nleave-one-out motion experiments using the HumanML3D and KIT-ML datasets across\nboth latent- and joint-space diffusion architectures. Results from quantitative\nmetrics and user studies show that our approach consistently improves the\nquality and diversity of generated motions, while preserving performance on the\noriginal distribution. Our approach is a flexible, data-efficient, and\nprivacy-preserving solution for motion adaptation.", "AI": {"tldr": "They fine-tune pretrained motion diffusion models to new actions/styles using only text prompts by optimizing with an RL objective driven by a text\u2013motion retrieval reward, improving motion quality/diversity without needing new motion capture data.", "motivation": "Adapting motion diffusion models to unseen actions or styles typically needs extra mocap data and full retraining, which is expensive, hard to scale, and can raise data-access/privacy issues. A method that adapts using only textual descriptions would be far more data-efficient and practical.", "method": "Post-training reinforcement learning on top of pretrained motion diffusion models. A frozen text\u2013motion retrieval network provides a reward measuring text\u2013motion alignment. They optimize the diffusion generator with Denoising Diffusion Policy Optimization (DDPO) to shift the generative distribution toward the target domain using only prompts, with no paired motion ground truth. Applicable to both latent- and joint-space diffusion architectures.", "result": "On HumanML3D and KIT-ML, across cross-dataset and leave-one-out settings, quantitative metrics and user studies indicate consistent gains in motion quality and diversity while maintaining performance on the original distribution.", "conclusion": "A flexible, data-efficient, and privacy-preserving post-training framework for adapting motion diffusion models to new domains using text-only supervision, avoiding costly data collection and retraining while preserving base-model capabilities."}}
{"id": "2510.06961", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.06961", "abs": "https://arxiv.org/abs/2510.06961", "authors": ["Vaibhav Srivastav", "Steven Zheng", "Eric Bezzam", "Eustache Le Bihan", "Nithin Koluguri", "Piotr \u017belasko", "Somshubra Majumdar", "Adel Moumen", "Sanchit Gandhi"], "title": "Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation", "comment": "Submitted to ICASSP 2026; Leaderboard:\n  https://huggingface.co/spaces/hf-audio/open_asr_leaderboard; Code:\n  https://github.com/huggingface/open_asr_leaderboard", "summary": "Despite rapid progress, ASR evaluation remains saturated with short-form\nEnglish, and efficiency is rarely reported. We present the Open ASR\nLeaderboard, a fully reproducible benchmark and interactive leaderboard\ncomparing 60+ open-source and proprietary systems across 11 datasets, including\ndedicated multilingual and long-form tracks. We standardize text normalization\nand report both word error rate (WER) and inverse real-time factor (RTFx),\nenabling fair accuracy-efficiency comparisons. For English transcription,\nConformer encoders paired with LLM decoders achieve the best average WER but\nare slower, while CTC and TDT decoders deliver much better RTFx, making them\nattractive for long-form and offline use. Whisper-derived encoders fine-tuned\nfor English improve accuracy but often trade off multilingual coverage. All\ncode and dataset loaders are open-sourced to support transparent, extensible\nevaluation.", "AI": {"tldr": "Open ASR Leaderboard is a fully reproducible benchmark and interactive leaderboard that evaluates 60+ ASR systems on 11 datasets (including multilingual and long\u2011form), standardizes text normalization, and reports both WER and inverse RTFx to compare accuracy vs. efficiency; Conformer+LLM decoders lead in WER but are slower, CTC/TDT are much faster, and English\u2011fine\u2011tuned Whisper encoders trade multilingual coverage for accuracy.", "motivation": "ASR evaluation is overly focused on short-form English and rarely measures efficiency, limiting fair comparisons and progress on multilingual and long-form use cases. A standardized, transparent benchmark that jointly measures accuracy and speed is needed.", "method": "Construct a reproducible benchmark and interactive leaderboard covering 60+ open-source and proprietary systems across 11 datasets with dedicated multilingual and long-form tracks. Standardize text normalization and report Word Error Rate (WER) plus inverse real-time factor (RTFx). Open-source all code and dataset loaders for transparent, extensible evaluation.", "result": "Conformer encoders paired with LLM decoders achieve the best average WER in English but are slower. CTC and TDT decoders show much better RTFx, making them preferable for long-form or offline transcription. Whisper-derived encoders fine-tuned for English improve accuracy but often reduce multilingual coverage.", "conclusion": "The Open ASR Leaderboard enables fair, transparent comparisons of ASR systems across accuracy and efficiency, highlights trade-offs between model architectures and decoding strategies, and provides open tooling to support community benchmarking and future extensions."}}
{"id": "2510.07008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07008", "abs": "https://arxiv.org/abs/2510.07008", "authors": ["Gianmarco Perantoni", "Giulio Weikmann", "Lorenzo Bruzzone"], "title": "Bayesian Modelling of Multi-Year Crop Type Classification Using Deep Neural Networks and Hidden Markov Models", "comment": "5 pages, 1 figure, accepted conference paper at IEEE International\n  Geoscience and Remote Sensing Symposium, 7-12 July 2024, Athens, Greece", "summary": "The temporal consistency of yearly land-cover maps is of great importance to\nmodel the evolution and change of the land cover over the years. In this paper,\nwe focus the attention on a novel approach to classification of yearly\nsatellite image time series (SITS) that combines deep learning with Bayesian\nmodelling, using Hidden Markov Models (HMMs) integrated with Transformer\nEncoder (TE) based DNNs. The proposed approach aims to capture both i)\nintricate temporal correlations in yearly SITS and ii) specific patterns in\nmultiyear crop type sequences. It leverages the cascade classification of an\nHMM layer built on top of the TE, discerning consistent yearly crop-type\nsequences. Validation on a multiyear crop type classification dataset spanning\n47 crop types and six years of Sentinel-2 acquisitions demonstrates the\nimportance of modelling temporal consistency in the predicted labels. HMMs\nenhance the overall performance and F1 scores, emphasising the effectiveness of\nthe proposed approach.", "AI": {"tldr": "HMMs stacked on top of a Transformer Encoder enforce temporally consistent multi\u2011year crop\u2011type labels in satellite image time series, improving overall performance and F1 on a 6\u2011year, 47\u2011class Sentinel\u20112 dataset.", "motivation": "Yearly land\u2011cover maps should evolve coherently over time, but per\u2011year classifiers often ignore inter\u2011annual dependencies and valid crop rotation patterns, leading to temporally inconsistent labels. The work aims to model these temporal consistencies explicitly to better capture land\u2011cover change and crop sequences.", "method": "A cascade model: a Transformer Encoder\u2013based DNN captures complex temporal signatures within yearly SITS and produces per\u2011year predictions, while a Hidden Markov Model on top constrains and smooths the label sequence by modeling inter\u2011year transitions, yielding the most probable, temporally consistent crop\u2011type sequence.", "result": "On a multi\u2011year crop classification benchmark (47 crop types across six years of Sentinel\u20112), integrating the HMM with the Transformer improves overall performance and F1 compared to using the deep model alone, highlighting gains from modeling temporal label consistency.", "conclusion": "Coupling deep temporal feature learning (Transformer Encoder) with probabilistic sequence modeling (HMM) produces more coherent and accurate multi\u2011year land\u2011cover classifications, underscoring the value of enforcing temporal consistency in operational mapping pipelines."}}
{"id": "2510.06965", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06965", "abs": "https://arxiv.org/abs/2510.06965", "authors": ["Bryan R. Christ", "Penelope Molitz", "Jonathan Kropko", "Thomas Hartvigsen"], "title": "EDUMATH: Generating Standards-aligned Educational Math Word Problems", "comment": "32 pages, 15 figures", "summary": "Math word problems (MWPs) are critical K-12 educational tools, and\ncustomizing them to students' interests and ability levels can increase\nlearning outcomes. However, teachers struggle to find time to customize MWPs\nfor each student given large class sizes and increasing burnout. We propose\nthat LLMs can support math education by generating MWPs customized to student\ninterests and math education standards. To this end, we use a joint human\nexpert-LLM judge approach to evaluate over 11,000 MWPs generated by open and\nclosed LLMs and develop the first teacher-annotated dataset for\nstandards-aligned educational MWP generation. We show the value of our data by\nusing it to train a 12B open model that matches the performance of larger and\nmore capable open models. We also use our teacher-annotated data to train a\ntext classifier that enables a 30B open LLM to outperform existing closed\nbaselines without any training. Next, we show our models' MWPs are more similar\nto human-written MWPs than those from existing models. We conclude by\nconducting the first study of customized LLM-generated MWPs with grade school\nstudents, finding they perform similarly on our models' MWPs relative to\nhuman-written MWPs but consistently prefer our customized MWPs.", "AI": {"tldr": "They build a teacher-annotated, standards-aligned dataset and evaluation pipeline for customized math word problems (MWPs), train/open-source models steered by this data, and show the generated MWPs are close to human-written, with students performing similarly but preferring the customized ones.", "motivation": "MWPs tailored to students\u2019 interests and ability can improve learning, but teachers lack time to author such content at scale. There is a need for reliable, standards-aligned generation and evaluation resources to let LLMs assist educators.", "method": "Use a joint human expert\u2013LLM judging framework to assess >11k MWPs created by various open/closed LLMs; create the first teacher-annotated dataset for standards-aligned MWP generation; train a 12B open model on this data; train a teacher-label\u2013based text classifier to steer a 30B open LLM without further training; compare similarity to human-written MWPs; run a classroom study with grade school students comparing performance and preference on customized vs human MWPs.", "result": "The 12B open model matches larger open models; the classifier enables a 30B open LLM to outperform closed baselines zero-shot; generated MWPs are more similar to human-written ones than prior models\u2019 outputs; in a student study, performance on customized LLM MWPs is similar to human-written, but students prefer the customized problems.", "conclusion": "Teacher-annotated data and a hybrid human\u2013LLM evaluation enable open LLMs to generate standards-aligned, customized MWPs that are competitive with human-written problems and favored by students, suggesting practical value for classroom adoption."}}
{"id": "2510.07041", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07041", "abs": "https://arxiv.org/abs/2510.07041", "authors": ["Fenghe Tang", "Chengqi Dong", "Wenxin Ma", "Zikang Xu", "Heqin Zhu", "Zihang Jiang", "Rongsheng Wang", "Yuhao Wang", "Chenxu Wu", "Shaohua Kevin Zhou"], "title": "U-Bench: A Comprehensive Understanding of U-Net through 100-Variant Benchmarking", "comment": "54 pages. The project can be accessed at:\n  https://fenghetan9.github.io/ubench. Code is available at:\n  https://github.com/FengheTan9/U-Bench", "summary": "Over the past decade, U-Net has been the dominant architecture in medical\nimage segmentation, leading to the development of thousands of U-shaped\nvariants. Despite its widespread adoption, there is still no comprehensive\nbenchmark to systematically evaluate their performance and utility, largely\nbecause of insufficient statistical validation and limited consideration of\nefficiency and generalization across diverse datasets. To bridge this gap, we\npresent U-Bench, the first large-scale, statistically rigorous benchmark that\nevaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our\ncontributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates\nmodels along three key dimensions: statistical robustness, zero-shot\ngeneralization, and computational efficiency. We introduce a novel metric,\nU-Score, which jointly captures the performance-efficiency trade-off, offering\na deployment-oriented perspective on model progress. (2) Systematic Analysis\nand Model Selection Guidance: We summarize key findings from the large-scale\nevaluation and systematically analyze the impact of dataset characteristics and\narchitectural paradigms on model performance. Based on these insights, we\npropose a model advisor agent to guide researchers in selecting the most\nsuitable models for specific datasets and tasks. (3) Public Availability: We\nprovide all code, models, protocols, and weights, enabling the community to\nreproduce our results and extend the benchmark with future methods. In summary,\nU-Bench not only exposes gaps in previous evaluations but also establishes a\nfoundation for fair, reproducible, and practically relevant benchmarking in the\nnext decade of U-Net-based segmentation models. The project can be accessed at:\nhttps://fenghetan9.github.io/ubench. Code is available at:\nhttps://github.com/FengheTan9/U-Bench.", "AI": {"tldr": "U-Bench is a large-scale, statistically rigorous benchmark evaluating 100 U-Net variants on 28 datasets across 10 modalities, introducing a deployment-oriented U-Score that balances segmentation performance and efficiency, plus a model-selection advisor and full open resources.", "motivation": "Despite thousands of U-Net variants in medical image segmentation, prior comparisons lack statistical rigor, ignore efficiency, and rarely test cross-dataset generalization, leaving practitioners without reliable, deployment-focused guidance.", "method": "Construct a unified benchmark (U-Bench) that: (1) evaluates models on statistical robustness, zero-shot generalization, and computational efficiency; (2) introduces U-Score to jointly assess performance\u2013efficiency trade-offs; (3) performs systematic analyses of dataset/architecture effects; (4) offers a model advisor agent; and (5) releases code, models, protocols, and weights for reproducibility and extensibility.", "result": "Benchmarked 100 U-Net variants across 28 datasets/10 modalities; produced statistically validated comparisons, deployment-oriented rankings via U-Score, and empirical insights into how dataset characteristics and architectural paradigms affect outcomes; packaged a model-selection advisor for practical use.", "conclusion": "U-Bench exposes shortcomings in prior evaluations and establishes a fair, reproducible, and deployment-relevant foundation for comparing U-Net-based segmentation models, providing tools and resources likely to shape model selection and development in the coming years."}}
{"id": "2510.06974", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06974", "abs": "https://arxiv.org/abs/2510.06974", "authors": ["Geng Liu", "Feng Li", "Junjie Mu", "Mengxiao Zhu", "Francesco Pierri"], "title": "Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social Groups", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in user-facing\napplications, raising concerns about their potential to reflect and amplify\nsocial biases. We investigate social identity framing in Chinese LLMs using\nMandarin-specific prompts across ten representative Chinese LLMs, evaluating\nresponses to ingroup (\"We\") and outgroup (\"They\") framings, and extending the\nsetting to 240 social groups salient in the Chinese context. To complement\ncontrolled experiments, we further analyze Chinese-language conversations from\na corpus of real interactions between users and chatbots. Across models, we\nobserve systematic ingroup-positive and outgroup-negative tendencies, which are\nnot confined to synthetic prompts but also appear in naturalistic dialogue,\nindicating that bias dynamics might strengthen in real interactions. Our study\nprovides a language-aware evaluation framework for Chinese LLMs, demonstrating\nthat social identity biases documented in English generalize\ncross-linguistically and intensify in user-facing contexts.", "AI": {"tldr": "They evaluate ingroup (\u201cwe\u201d) vs outgroup (\u201cthey\u201d) framing across 10 Chinese LLMs and real chatbot logs, finding consistent ingroup-positive and outgroup-negative bias that is even stronger in natural interactions, and provide a Mandarin- and China-context-aware evaluation framework.", "motivation": "LLMs are widely deployed and risk reflecting/amplifying social biases. Prior work focuses on English; it\u2019s unclear whether similar social identity framing effects exist in Chinese and how they manifest in real user interactions. There is a need for a culturally and linguistically appropriate evaluation for Chinese LLMs and a test of cross-linguistic generalization.", "method": "Use Mandarin-specific prompts to probe responses under ingroup (\u201cwe\u201d) vs outgroup (\u201cthey\u201d) framing across ten representative Chinese LLMs; expand evaluations to 240 China-salient social groups. Complement controlled prompts with an analysis of a corpus of real Chinese-language user\u2013chatbot conversations to detect framing-linked sentiment/bias patterns.", "result": "Across models, there are systematic ingroup-positive and outgroup-negative tendencies. These effects are not limited to synthetic prompts; they also appear in naturalistic dialogues, with indications that biases may be stronger in real interactions.", "conclusion": "Social identity biases seen in English generalize to Chinese and can intensify in user-facing contexts. The paper contributes a language-aware evaluation framework for Chinese LLMs and underscores the need for mitigation strategies in deployment."}}
{"id": "2510.07058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07058", "abs": "https://arxiv.org/abs/2510.07058", "authors": ["Ori nizan", "Oren Shrout", "Ayellet Tal"], "title": "Concept Retrieval - What and How?", "comment": null, "summary": "A concept may reflect either a concrete or abstract idea. Given an input\nimage, this paper seeks to retrieve other images that share its central\nconcepts, capturing aspects of the underlying narrative. This goes beyond\nconventional retrieval or clustering methods, which emphasize visual or\nsemantic similarity. We formally define the problem, outline key requirements,\nand introduce appropriate evaluation metrics. We propose a novel approach\ngrounded in two key observations: (1) While each neighbor in the embedding\nspace typically shares at least one concept with the query, not all neighbors\nnecessarily share the same concept with one another. (2) Modeling this\nneighborhood with a bimodal Gaussian distribution uncovers meaningful structure\nthat facilitates concept identification. Qualitative, quantitative, and human\nevaluations confirm the effectiveness of our approach. See the package on PyPI:\nhttps://pypi.org/project/coret/", "AI": {"tldr": "Concept-oriented image retrieval method that models a query image\u2019s embedding-space neighborhood with a bimodal Gaussian to disentangle and identify shared concepts, enabling retrieval beyond surface visual/semantic similarity; includes formal problem definition, requirements, and metrics; validated via qualitative, quantitative, and human studies; released as the coret package.", "motivation": "Standard retrieval/clustering focuses on overall visual or semantic similarity and often misses the central concepts or narrative that make images related at a deeper level. The paper aims to retrieve images sharing a query\u2019s core concepts, addressing this gap and formalizing the task with requirements and evaluation metrics.", "method": "Define the concept-oriented retrieval problem and its evaluation criteria. For a query image, collect nearest neighbors in an embedding space. Observe that each neighbor may share at least one concept with the query, but neighbors may not share the same concept among themselves. Fit a bimodal Gaussian to the neighbor distribution to reveal meaningful substructure corresponding to different concepts, then use this structure to identify and rank images aligned with the query\u2019s central concepts.", "result": "Qualitative case studies, quantitative experiments, and human evaluations indicate the approach more effectively retrieves images sharing the query\u2019s central concepts than conventional methods, with the bimodal modeling uncovering useful conceptual groupings.", "conclusion": "Modeling a query\u2019s neighborhood with a bimodal Gaussian enables robust identification of core concepts, yielding improved concept-oriented retrieval relative to standard similarity-based approaches. The work also contributes a formal problem framing, metrics, and a practical implementation (coret)."}}
{"id": "2510.06999", "categories": ["cs.CL", "cs.IR", "I.2.7; H.3.3; K.5.0"], "pdf": "https://arxiv.org/pdf/2510.06999", "abs": "https://arxiv.org/abs/2510.06999", "authors": ["Markus Reuter", "Tobias Lingenberg", "R\u016bta Liepi\u0146a", "Francesca Lagioia", "Marco Lippi", "Giovanni Sartor", "Andrea Passerini", "Burcu Sayin"], "title": "Towards Reliable Retrieval in RAG Systems for Large Legal Datasets", "comment": "Accepted for the 7th Natural Legal Language Processing Workshop (NLLP\n  2025), co-located with EMNLP 2025", "summary": "Retrieval-Augmented Generation (RAG) is a promising approach to mitigate\nhallucinations in Large Language Models (LLMs) for legal applications, but its\nreliability is critically dependent on the accuracy of the retrieval step. This\nis particularly challenging in the legal domain, where large databases of\nstructurally similar documents often cause retrieval systems to fail. In this\npaper, we address this challenge by first identifying and quantifying a\ncritical failure mode we term Document-Level Retrieval Mismatch (DRM), where\nthe retriever selects information from entirely incorrect source documents. To\nmitigate DRM, we investigate a simple and computationally efficient technique\nwhich we refer to as Summary-Augmented Chunking (SAC). This method enhances\neach text chunk with a document-level synthetic summary, thereby injecting\ncrucial global context that would otherwise be lost during a standard chunking\nprocess. Our experiments on a diverse set of legal information retrieval tasks\nshow that SAC greatly reduces DRM and, consequently, also improves text-level\nretrieval precision and recall. Interestingly, we find that a generic\nsummarization strategy outperforms an approach that incorporates legal expert\ndomain knowledge to target specific legal elements. Our work provides evidence\nthat this practical, scalable, and easily integrable technique enhances the\nreliability of RAG systems when applied to large-scale legal document datasets.", "AI": {"tldr": "They identify a key failure mode in legal RAG\u2014retrievers pulling from the wrong document (Document-Level Retrieval Mismatch, DRM)\u2014and mitigate it with Summary-Augmented Chunking (SAC), which prepends a synthetic document summary to each chunk. SAC reduces DRM and improves retrieval precision/recall; generic summaries work better than expert-crafted ones.", "motivation": "RAG\u2019s reliability hinges on retrieval quality. In legal corpora with many long, structurally similar documents, standard chunking loses global context, causing retrievers to select text from entirely incorrect documents (DRM), which propagates hallucinations in downstream LLM answers.", "method": "1) Define and quantify DRM as a retrieval error where the source document is wrong. 2) Propose Summary-Augmented Chunking (SAC): generate a document-level synthetic summary and attach it to every chunk to inject global context into chunk representations. 3) Evaluate across diverse legal IR tasks, comparing generic summarization versus domain-expert targeted summaries.", "result": "SAC substantially lowers DRM rates and boosts text-level retrieval precision and recall. Unexpectedly, a generic summarization strategy outperforms expert-informed, legally targeted summaries.", "conclusion": "A simple, scalable, and easily integrable augmentation\u2014adding document-level summaries to chunks\u2014meaningfully improves RAG reliability on large legal datasets by restoring global context during retrieval, without requiring domain-specific engineering."}}
{"id": "2510.07089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07089", "abs": "https://arxiv.org/abs/2510.07089", "authors": ["Federico Gonzalez", "Estefania Talavera", "Petia Radeva"], "title": "DADO: A Depth-Attention framework for Object Discovery", "comment": "21st International Conference in Computer Analysis of Images and\n  Patterns (CAIP 2025)", "summary": "Unsupervised object discovery, the task of identifying and localizing objects\nin images without human-annotated labels, remains a significant challenge and a\ngrowing focus in computer vision. In this work, we introduce a novel model,\nDADO (Depth-Attention self-supervised technique for Discovering unseen\nObjects), which combines an attention mechanism and a depth model to identify\npotential objects in images. To address challenges such as noisy attention maps\nor complex scenes with varying depth planes, DADO employs dynamic weighting to\nadaptively emphasize attention or depth features based on the global\ncharacteristics of each image. We evaluated DADO on standard benchmarks, where\nit outperforms state-of-the-art methods in object discovery accuracy and\nrobustness without the need for fine-tuning.", "AI": {"tldr": "DADO is a self-supervised method that fuses image attention with monocular depth via dynamic weighting to discover and localize objects without labels, outperforming prior methods on standard benchmarks without fine-tuning.", "motivation": "Unsupervised object discovery is hard due to lack of labels and failure modes like noisy attention maps and complex, multi-depth scenes. The work aims to leverage complementary cues (attention and depth) to improve robustness and accuracy in identifying object regions without supervision.", "method": "Introduce DADO (Depth-Attention self-supervised technique for Discovering unseen Objects): combine an attention mechanism with a depth estimation model; apply dynamic, image-level weighting that adaptively prioritizes attention or depth features depending on global image characteristics to mitigate noise and handle varying depth planes. Entirely self-supervised; no fine-tuning on benchmarks.", "result": "On standard object-discovery benchmarks, DADO achieves state-of-the-art accuracy and robustness, outperforming existing methods while requiring no fine-tuning.", "conclusion": "Adaptive fusion of depth and attention provides a strong, label-free signal for object discovery, yielding better robustness across diverse scenes and validating the utility of dynamic weighting. Results suggest depth-attention synergy is an effective direction for unsupervised object localization and discovery."}}
{"id": "2510.07000", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07000", "abs": "https://arxiv.org/abs/2510.07000", "authors": ["Neel Prabhanjan Rachamalla", "Aravind Konakalla", "Gautam Rajeev", "Ashish Kulkarni", "Chandra Khatri", "Shubham Agarwal"], "title": "Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages", "comment": "EMNLP 2025", "summary": "The effectiveness of Large Language Models (LLMs) depends heavily on the\navailability of high-quality post-training data, particularly\ninstruction-tuning and preference-based examples. Existing open-source\ndatasets, however, often lack multilingual coverage, cultural grounding, and\nsuffer from task diversity gaps that are especially pronounced for Indian\nlanguages. We introduce a human-in-the-loop pipeline that combines translations\nwith synthetic expansion to produce reliable and diverse Indic post-training\ndata. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and\nPragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56\nsub-categories, leveraging 57 diverse datasets. Our dataset protocol\nincorporates several often-overlooked dimensions and emphasize task diversity,\nmulti-turn dialogue, instruction fidelity, safety alignment, and preservation\nof cultural nuance, providing a foundation for more inclusive and effective\nmultilingual LLMs.", "AI": {"tldr": "They propose a human-in-the-loop pipeline that blends translation with synthetic expansion to build high-quality, culturally grounded post-training data for Indian languages, releasing two datasets\u2014Pragyaan-IT (22.5K) and Pragyaan-Align (100K)\u2014spanning 10 languages and diverse task categories.", "motivation": "LLMs rely on rich post-training corpora (instruction-tuning and preference data), but open datasets lack multilingual breadth, cultural grounding, and task diversity\u2014gaps that are acute for Indic languages.", "method": "A human-in-the-loop curation pipeline combining translations and synthetic generation, sourced from 57 datasets, with explicit protocols emphasizing task diversity, multi-turn dialogues, instruction fidelity, safety alignment, and preservation of cultural nuance.", "result": "Creation of two Indic-focused datasets: Pragyaan-IT (22.5K) and Pragyaan-Align (100K), covering 10 languages, 13 broad categories, and 56 sub-categories. The data are positioned as reliable and diverse for post-training.", "conclusion": "The datasets and protocol aim to enable more inclusive and effective multilingual LLMs, particularly for Indic languages, by addressing coverage, cultural grounding, and alignment considerations in post-training data."}}
{"id": "2510.07115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07115", "abs": "https://arxiv.org/abs/2510.07115", "authors": ["R\u00e9mi Kazmierczak", "Steve Azzolin", "Elo\u00efse Berthier", "Goran Frehse", "Gianni Franchi"], "title": "Enhancing Concept Localization in CLIP-based Concept Bottleneck Models", "comment": null, "summary": "This paper addresses explainable AI (XAI) through the lens of Concept\nBottleneck Models (CBMs) that do not require explicit concept annotations,\nrelying instead on concepts extracted using CLIP in a zero-shot manner. We show\nthat CLIP, which is central in these techniques, is prone to concept\nhallucination, incorrectly predicting the presence or absence of concepts\nwithin an image in scenarios used in numerous CBMs, hence undermining the\nfaithfulness of explanations. To mitigate this issue, we introduce Concept\nHallucination Inhibition via Localized Interpretability (CHILI), a technique\nthat disentangles image embeddings and localizes pixels corresponding to target\nconcepts. Furthermore, our approach supports the generation of saliency-based\nexplanations that are more interpretable.", "AI": {"tldr": "CLIP-driven concept bottleneck models can hallucinate concepts, harming explanation faithfulness; the paper proposes CHILI to disentangle and localize concept evidence, yielding more accurate and interpretable saliency-based explanations.", "motivation": "Concept Bottleneck Models promise interpretable predictions via intermediate human concepts, but explicit concept labels are costly. Recent works use CLIP zero-shot concepts instead\u2014however, if CLIP misidentifies concept presence, the resulting explanations are unfaithful. The authors aim to diagnose and mitigate such concept hallucination while improving the interpretability of concept evidence.", "method": "CHILI (Concept Hallucination Inhibition via Localized Interpretability): a technique that disentangles image embeddings and localizes pixels corresponding to each target concept, replacing/globalizing CLIP\u2019s similarity with spatially grounded evidence and producing saliency-based explanations for concepts.", "result": "They show CLIP often hallucinates concept presence/absence in CBM settings and demonstrate that CHILI reduces these hallucinations while producing more interpretable, saliency-based explanations. (No quantitative metrics are provided in the abstract.)", "conclusion": "Grounding concepts via localized, disentangled evidence improves the faithfulness and interpretability of CLIP-based CBMs; CHILI is presented as a practical approach to inhibit hallucination and yield clearer concept-level saliency maps."}}
{"id": "2510.07019", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07019", "abs": "https://arxiv.org/abs/2510.07019", "authors": ["Jusen Du", "Jiaxi Hu", "Tao Zhang", "Weigao Sun", "Yu Cheng"], "title": "Native Hybrid Attention for Efficient Sequence Modeling", "comment": "Technical report, 16 pages", "summary": "Transformers excel at sequence modeling but face quadratic complexity, while\nlinear attention offers improved efficiency but often compromises recall\naccuracy over long contexts. In this work, we introduce Native Hybrid Attention\n(NHA), a novel hybrid architecture of linear and full attention that integrates\nboth intra \\& inter-layer hybridization into a unified layer design. NHA\nmaintains long-term context in key-value slots updated by a linear RNN, and\naugments them with short-term tokens from a sliding window. A single\n\\texttt{softmax attention} operation is then applied over all keys and values,\nenabling per-token and per-head context-dependent weighting without requiring\nadditional fusion parameters. The inter-layer behavior is controlled through a\nsingle hyperparameter, the sliding window size, which allows smooth adjustment\nbetween purely linear and full attention while keeping all layers structurally\nuniform. Experimental results show that NHA surpasses Transformers and other\nhybrid baselines on recall-intensive and commonsense reasoning tasks.\nFurthermore, pretrained LLMs can be structurally hybridized with NHA, achieving\ncompetitive accuracy while delivering significant efficiency gains. Code is\navailable at https://github.com/JusenD/NHA.", "AI": {"tldr": "Native Hybrid Attention (NHA) unifies linear and full attention by maintaining long-term memory via a linear RNN-updated KV cache and mixing it with a sliding-window of recent tokens, then applying a single softmax over all keys/values. It outperforms Transformers and prior hybrids on recall-heavy and commonsense tasks while being more efficient and easy to tune via one window-size hyperparameter.", "motivation": "Transformers have quadratic cost with context length; linear attention scales better but often loses recall over long contexts. The paper aims to bridge this efficiency\u2013recall trade-off with a simple, uniform architecture that preserves long-term information without extra fusion parameters and can be smoothly tuned between linear and full attention.", "method": "Introduce a unified layer that combines intra- and inter-layer hybridization: (1) maintain long-term context in key\u2013value slots updated by a linear RNN; (2) augment with short-term tokens from a sliding window; (3) perform a single softmax attention over the concatenated keys/values, allowing per-token/per-head weighting without additional fusion modules. A single hyperparameter\u2014the sliding-window size\u2014controls behavior across layers, enabling a continuum from purely linear to full attention. The design can retrofit pretrained LLMs.", "result": "Across experiments, NHA surpasses standard Transformers and other hybrid attention baselines on recall-intensive and commonsense reasoning benchmarks. When used to structurally hybridize pretrained LLMs, it attains competitive accuracy with significant efficiency gains. (Abstract does not provide exact metrics.)", "conclusion": "NHA is a simple, parameter-light hybrid attention mechanism that preserves long-term recall while retaining efficiency. Its single hyperparameter makes it easy to trade off cost and accuracy, and its drop-in nature enables efficient retrofitting of existing LLMs."}}
{"id": "2510.07119", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07119", "abs": "https://arxiv.org/abs/2510.07119", "authors": ["Dongki Jung", "Jaehoon Choi", "Yonghan Lee", "Sungmin Eum", "Heesung Kwon", "Dinesh Manocha"], "title": "MoRe: Monocular Geometry Refinement via Graph Optimization for Cross-View Consistency", "comment": null, "summary": "Monocular 3D foundation models offer an extensible solution for perception\ntasks, making them attractive for broader 3D vision applications. In this\npaper, we propose MoRe, a training-free Monocular Geometry Refinement method\ndesigned to improve cross-view consistency and achieve scale alignment. To\ninduce inter-frame relationships, our method employs feature matching between\nframes to establish correspondences. Rather than applying simple least squares\noptimization on these matched points, we formulate a graph-based optimization\nframework that performs local planar approximation using the estimated 3D\npoints and surface normals estimated by monocular foundation models. This\nformulation addresses the scale ambiguity inherent in monocular geometric\npriors while preserving the underlying 3D structure. We further demonstrate\nthat MoRe not only enhances 3D reconstruction but also improves novel view\nsynthesis, particularly in sparse view rendering scenarios.", "AI": {"tldr": "MoRe is a training\u2011free refinement that enforces cross\u2011view consistency and resolves scale ambiguity for monocular 3D foundation model outputs via feature-matched correspondences and a graph-based, locally planar optimization, improving both 3D reconstruction and sparse\u2011view novel view synthesis.", "motivation": "Monocular 3D foundation models are attractive for general 3D perception but suffer from cross-view inconsistency and inherent scale ambiguity, which limit accurate multi\u2011view reconstruction and novel view synthesis without additional training.", "method": "Establish inter-frame correspondences using feature matching. Build a graph-based optimization that uses estimated 3D points and surface normals (from monocular foundation models) to perform local planar approximations across views. This jointly enforces cross-view geometric consistency and aligns scales while preserving underlying 3D structure, avoiding simple least-squares that can distort geometry.", "result": "Compared with direct monocular outputs, MoRe yields more consistent multi-view geometry, better scale alignment, and improved 3D reconstruction quality. It also enhances novel view synthesis, especially in sparse-view rendering settings.", "conclusion": "A practical, training-free refinement layer for monocular 3D foundation models can effectively mitigate scale ambiguity and inter-frame inconsistencies through graph-based, locally planar constraints, boosting both reconstruction fidelity and sparse-view NVS performance."}}
{"id": "2510.07024", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07024", "abs": "https://arxiv.org/abs/2510.07024", "authors": ["Shrestha Ghosh", "Luca Giordano", "Yujia Hu", "Tuan-Phong Nguyen", "Simon Razniewski"], "title": "Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge", "comment": null, "summary": "LLMs are remarkable artifacts that have revolutionized a range of NLP and AI\ntasks. A significant contributor is their factual knowledge, which, to date,\nremains poorly understood, and is usually analyzed from biased samples. In this\npaper, we take a deep tour into the factual knowledge (or beliefs) of a\nfrontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited\nset of 100 million beliefs of one of the strongest currently available frontier\nLLMs, GPT-4.1. We find that the models' factual knowledge differs quite\nsignificantly from established knowledge bases, and that its accuracy is\nsignificantly lower than indicated by previous benchmarks. We also find that\ninconsistency, ambiguity and hallucinations are major issues, shedding light on\nfuture research opportunities concerning factual LLM knowledge.", "AI": {"tldr": "Analyzing 100M recursively elicited \u201cbeliefs\u201d from GPT\u20114.1 (GPTKB v1.5), the authors find LLM factual knowledge diverges from canonical knowledge bases, achieves lower true accuracy than prior benchmarks imply, and exhibits substantial inconsistency, ambiguity, and hallucinations.", "motivation": "Factual knowledge in LLMs remains poorly characterized and prior evaluations rely on small or biased samples, potentially overstating reliability. A comprehensive, data-driven picture is needed to understand what LLMs \u2018know\u2019 and how trustworthy those beliefs are.", "method": "Leverage GPTKB v1.5\u2014a large-scale (100M) corpus of GPT\u20114.1-elicited beliefs obtained via recursive prompting\u2014and compare these beliefs against established knowledge bases. Quantify accuracy and analyze patterns of inconsistency, ambiguity, and hallucination across the belief set.", "result": "LLM beliefs differ markedly from entries in established knowledge bases; measured factual accuracy is notably lower than suggested by prior benchmarks; and the belief set reveals widespread inconsistency, ambiguity, and hallucinations.", "conclusion": "Current benchmarks likely overestimate LLM factual reliability. The study highlights the need for better evaluation protocols and datasets, and points to reducing inconsistency, ambiguity, and hallucinations as key directions for improving factual knowledge in LLMs."}}
{"id": "2510.07126", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.07126", "abs": "https://arxiv.org/abs/2510.07126", "authors": ["Jan Fiszer", "Dominika Ciupek", "Maciej Malawski"], "title": "Validation of Various Normalization Methods for Brain Tumor Segmentation: Can Federated Learning Overcome This Heterogeneity?", "comment": null, "summary": "Deep learning (DL) has been increasingly applied in medical imaging, however,\nit requires large amounts of data, which raises many challenges related to data\nprivacy, storage, and transfer. Federated learning (FL) is a training paradigm\nthat overcomes these issues, though its effectiveness may be reduced when\ndealing with non-independent and identically distributed (non-IID) data. This\nstudy simulates non-IID conditions by applying different MRI intensity\nnormalization techniques to separate data subsets, reflecting a common cause of\nheterogeneity. These subsets are then used for training and testing models for\nbrain tumor segmentation. The findings provide insights into the influence of\nthe MRI intensity normalization methods on segmentation models, both training\nand inference. Notably, the FL methods demonstrated resilience to\ninconsistently normalized data across clients, achieving the 3D Dice score of\n92%, which is comparable to a centralized model (trained using all data). These\nresults indicate that FL is a solution to effectively train high-performing\nmodels without violating data privacy, a crucial concern in medical\napplications. The code is available at:\nhttps://github.com/SanoScience/fl-varying-normalization.", "AI": {"tldr": "Federated learning for brain tumor segmentation remains robust when clients use different MRI intensity normalization methods, achieving a 3D Dice of ~92% comparable to centralized training, suggesting FL can address privacy concerns without sacrificing performance under this type of non-IID heterogeneity.", "motivation": "Deep learning in medical imaging needs large, pooled datasets, but privacy, storage, and transfer barriers prevent centralization. Federated learning mitigates these issues, yet its performance can degrade with non-IID data. A common, realistic source of non-IID distribution across sites is differing MRI intensity normalization. The study aims to quantify how such normalization heterogeneity affects segmentation training and inference, and whether FL remains effective.", "method": "Simulate non-IID conditions by splitting data into subsets and applying different MRI intensity normalization techniques to each subset (mimicking site-specific preprocessing). Train brain tumor segmentation models under federated learning across these subsets and compare to a centralized model trained on all data. Evaluate performance (3D Dice) and analyze the influence of normalization differences on both training and inference.", "result": "Federated learning methods showed resilience to inconsistently normalized client data, reaching a 3D Dice score of approximately 92%, comparable to the centralized baseline trained with all data. The study provides empirical insights into how normalization choices impact segmentation performance in both training and inference settings.", "conclusion": "Federated learning can effectively train high-performing brain tumor segmentation models even when clients use different MRI intensity normalization techniques, thereby enabling privacy-preserving collaboration without notable loss of accuracy. The released code supports reproducibility and further investigation."}}
{"id": "2510.07037", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07037", "abs": "https://arxiv.org/abs/2510.07037", "authors": ["Rajvee Sheth", "Samridhi Raj Sinha", "Mahavir Patil", "Himanshu Beniwal", "Mayank Singh"], "title": "Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models", "comment": null, "summary": "Code-switching (CSW), the alternation of languages and scripts within a\nsingle utterance, remains a fundamental challenge for multiling ual NLP, even\namidst the rapid advances of large language models (LLMs). Most LLMs still\nstruggle with mixed-language inputs, limited CSW datasets, and evaluation\nbiases, hindering deployment in multilingual societies. This survey provides\nthe first comprehensive analysis of CSW-aware LLM research, reviewing\n\\total{unique_references} studies spanning five research areas, 12 NLP tasks,\n30+ datasets, and 80+ languages. We classify recent advances by architecture,\ntraining strategy, and evaluation methodology, outlining how LLMs have reshaped\nCSW modeling and what challenges persist. The paper concludes with a roadmap\nemphasizing the need for inclusive datasets, fair evaluation, and\nlinguistically grounded models to achieve truly multilingual intelligence. A\ncurated collection of all resources is maintained at\nhttps://github.com/lingo-iitgn/awesome-code-mixing/.", "AI": {"tldr": "A survey of code-switching\u2013aware large language models that categorizes recent work across architectures, training strategies, and evaluations, covers 12 tasks over 80+ languages and 30+ datasets, identifies persistent challenges, and proposes a roadmap toward fair, linguistically grounded, multilingual intelligence, with resources curated online.", "motivation": "Code-switching is common in multilingual settings, but most LLMs still falter on mixed-language inputs due to scarce datasets and evaluation biases. A comprehensive synthesis is needed to map progress, reveal gaps, and guide future research and deployment.", "method": "Literature review of CSW-focused LLM research across five areas, 12 NLP tasks, 30+ datasets, and 80+ languages. The survey classifies advances by model architecture, training strategy, and evaluation methodology, and analyzes how LLMs have reshaped CSW modeling. It also compiles a curated repository of resources.", "result": "Provides the first broad taxonomy and analysis of CSW-aware LLMs, detailing trends, strengths, and limitations across datasets, tasks, and languages. Highlights that while LLMs have improved CSW modeling, they still struggle with mixed-language inputs, limited data coverage, and biased evaluations.", "conclusion": "Achieving robust multilingual intelligence requires inclusive, representative datasets, fair and standardized evaluation protocols, and models grounded in linguistic principles. The survey offers a roadmap and a curated resource list to accelerate progress."}}
{"id": "2510.07129", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07129", "abs": "https://arxiv.org/abs/2510.07129", "authors": ["Sarah Cechnicka", "Matthew Baugh", "Weitong Zhang", "Mischa Dombrowski", "Zhe Li", "Johannes C. Paetzold", "Candice Roufosse", "Bernhard Kainz"], "title": "Graph Conditioned Diffusion for Controllable Histopathology Image Generation", "comment": null, "summary": "Recent advances in Diffusion Probabilistic Models (DPMs) have set new\nstandards in high-quality image synthesis. Yet, controlled generation remains\nchallenging, particularly in sensitive areas such as medical imaging. Medical\nimages feature inherent structure such as consistent spatial arrangement, shape\nor texture, all of which are critical for diagnosis. However, existing DPMs\noperate in noisy latent spaces that lack semantic structure and strong priors,\nmaking it difficult to ensure meaningful control over generated content. To\naddress this, we propose graph-based object-level representations for\nGraph-Conditioned-Diffusion. Our approach generates graph nodes corresponding\nto each major structure in the image, encapsulating their individual features\nand relationships. These graph representations are processed by a transformer\nmodule and integrated into a diffusion model via the text-conditioning\nmechanism, enabling fine-grained control over generation. We evaluate this\napproach using a real-world histopathology use case, demonstrating that our\ngenerated data can reliably substitute for annotated patient data in downstream\nsegmentation tasks. The code is available here.", "AI": {"tldr": "Introduce graph-conditioned diffusion that uses object-level graphs (nodes for major structures + relations) processed by a transformer and injected via text-conditioning to enable fine-grained control in medical image synthesis; validated on histopathology where synthetic data can replace annotated data for segmentation.", "motivation": "High-quality diffusion models lack semantic control because their latent spaces are noisy and unstructured, which is problematic in medical imaging where spatial arrangement, shape, and texture carry diagnostic significance.", "method": "Construct a graph with nodes representing key anatomical/structural entities and their relationships; encode these graphs with a transformer; integrate the resulting representation into a diffusion model through the text-conditioning mechanism to steer generation at object/structure level.", "result": "In a real-world histopathology scenario, the method produces images whose use in downstream segmentation tasks can reliably substitute for annotated patient data (suggesting comparable utility to real labels).", "conclusion": "Graph-based, object-level conditioning restores semantic structure and controllability to diffusion generation in medical imaging, enabling fine-grained control and viable synthetic data for training; implementation is available (code link referenced)."}}
{"id": "2510.07048", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.07048", "abs": "https://arxiv.org/abs/2510.07048", "authors": ["Yuntao Gui", "James Cheng"], "title": "Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models", "comment": null, "summary": "Despite their remarkable natural language understanding capabilities, Large\nLanguage Models (LLMs) have been underutilized for retrieval tasks. We present\nSearch-R3, a novel framework that addresses this limitation by adapting LLMs to\ngenerate search embeddings as a direct output of their reasoning process. Our\napproach exploits LLMs' chain-of-thought capabilities, allowing them to produce\nmore effective embeddings by reasoning step-by-step through complex semantic\nanalyses. We implement this through three complementary mechanisms. (1) a\nsupervised learning stage enables the model's ability to produce quality\nembeddings, (2) a reinforcement learning (RL) methodology that optimizes\nembedding generation alongside reasoning, and (3) a specialized RL environment\nthat efficiently handles evolving embedding representations without requiring\ncomplete corpus re-encoding at each training iteration. Our extensive\nevaluations on diverse benchmarks demonstrate that Search-R3 significantly\noutperforms prior methods by unifying the reasoning and embedding generation\nprocesses. This integrated post-training approach represents a substantial\nadvancement in handling complex knowledge-intensive tasks that require both\nsophisticated reasoning and effective information retrieval. Project page:\nhttps://github.com/ytgui/Search-R3", "AI": {"tldr": "Search-R3 trains LLMs to output search embeddings as a byproduct of chain-of-thought reasoning, using supervised learning plus RL and a specialized RL environment to avoid constant re-encoding, achieving strong gains on retrieval benchmarks.", "motivation": "LLMs reason well but traditional retrieval uses separate embedding models that don\u2019t leverage step-by-step semantic reasoning. Unifying reasoning with embedding generation could improve retrieval for knowledge-intensive tasks.", "method": "Three-part framework: (1) supervised stage to teach the LLM to produce high-quality embeddings; (2) reinforcement learning that jointly optimizes reasoning traces and embedding quality; (3) a custom RL training environment that supports evolving embeddings without re-encoding the whole corpus every iteration. Embeddings are produced directly from the model\u2019s chain-of-thought.", "result": "Across diverse retrieval benchmarks, Search-R3 significantly outperforms prior methods, indicating that coupling reasoning with embedding generation yields better retrieval effectiveness.", "conclusion": "Integrating reasoning and embedding generation during post-training advances retrieval for complex, knowledge-intensive tasks, offering both effectiveness gains and practical training efficiency. Open-source resources are available at the provided project page."}}
{"id": "2510.07135", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07135", "abs": "https://arxiv.org/abs/2510.07135", "authors": ["Karim El Khoury", "Maxime Zanella", "Christophe De Vleeschouwer", "Benoit Macq"], "title": "Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models", "comment": null, "summary": "Remote Sensing Vision-Language Models (RSVLMs) have shown remarkable\npotential thanks to large-scale pretraining, achieving strong zero-shot\nperformance on various tasks. However, their ability to generalize in low-data\nregimes, such as few-shot learning, remains insufficiently explored. In this\nwork, we present the first structured benchmark for evaluating few-shot\nadaptation methods on RSVLMs. We conduct comprehensive experiments across ten\nremote sensing scene classification datasets, applying five widely used\nfew-shot adaptation strategies to three state-of-the-art RSVLMs with varying\nbackbones. Our findings reveal that models with similar zero-shot performance\ncan exhibit markedly different behavior under few-shot adaptation, with some\nRSVLMs being inherently more amenable to such adaptation than others. The\nvariability of performance and the absence of a clear winner among existing\nmethods highlight the need for the development of more robust methods for\nfew-shot adaptation tailored to RS. To facilitate future research, we provide a\nreproducible benchmarking framework and open-source code to systematically\nevaluate RSVLMs under few-shot conditions. The source code is publicly\navailable on Github: https://github.com/elkhouryk/fewshot_RSVLMs", "AI": {"tldr": "They introduce the first structured benchmark for few-shot adaptation of remote sensing vision-language models (RSVLMs), evaluating 5 adaptation strategies on 3 SOTA models across 10 scene classification datasets. Results show large variability\u2014models with similar zero-shot scores adapt very differently\u2014and no single method dominates. They release a reproducible framework and code to spur research.", "motivation": "RSVLMs perform well in zero-shot settings due to large-scale pretraining, but their behavior when only a few labeled examples are available (few-shot adaptation) is underexplored. Practitioners in remote sensing often face low-label regimes, so understanding and improving few-shot adaptation is important.", "method": "Build a structured, reproducible benchmark for few-shot adaptation in RS: test five widely used adaptation strategies on three state-of-the-art RSVLMs with different backbones, evaluated across ten remote sensing scene classification datasets.", "result": "Despite similar zero-shot performance, RSVLMs show markedly different few-shot adaptation behavior; some models are inherently more adaptable. Performance varies widely across methods and datasets, with no clear overall winner among existing strategies.", "conclusion": "Current few-shot adaptation methods are inconsistent for RSVLMs, indicating a need for more robust, RS-tailored approaches. The authors provide an open-source, reproducible benchmarking framework to facilitate systematic evaluation and future method development (code: https://github.com/elkhouryk/fewshot_RSVLMs)."}}
{"id": "2510.07060", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07060", "abs": "https://arxiv.org/abs/2510.07060", "authors": ["Miriam Wanner", "Sophia Hager", "Anjalie Field"], "title": "Does Local News Stay Local?: Online Content Shifts in Sinclair-Acquired Stations", "comment": null, "summary": "Local news stations are often considered to be reliable sources of\nnon-politicized information, particularly local concerns that residents care\nabout. Because these stations are trusted news sources, viewers are\nparticularly susceptible to the information they report. The Sinclair Broadcast\ngroup is a broadcasting company that has acquired many local news stations in\nthe last decade. We investigate the effects of local news stations being\nacquired by Sinclair: how does coverage change? We use computational methods to\ninvestigate changes in internet content put out by local news stations before\nand after being acquired by Sinclair and in comparison to national news\noutlets. We find that there is clear evidence that local news stations report\nmore frequently on national news at the expense of local topics, and that their\ncoverage of polarizing national topics increases.", "AI": {"tldr": "After Sinclair acquires local TV stations, their online news shifts from locally focused reporting toward more national\u2014and more polarizing\u2014topics.", "motivation": "Local TV news is widely trusted and central to residents\u2019 information about community issues. Sinclair\u2019s rapid consolidation raises concerns that editorial direction may change, potentially altering audiences\u2019 information diets and the local civic information ecosystem.", "method": "Computational analysis of internet content produced by local stations before and after Sinclair acquisition, benchmarked against national outlets; topic classification to distinguish local vs. national coverage and identification of polarizing national topics.", "result": "Post-acquisition, stations increase the share of national news, reduce local-topic coverage, and devote more attention to polarizing national issues.", "conclusion": "Sinclair ownership is associated with a systematic shift away from local reporting toward national, more polarizing content, suggesting media consolidation can diminish locally relevant information available to communities."}}
{"id": "2510.07143", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07143", "abs": "https://arxiv.org/abs/2510.07143", "authors": ["Chenfei Liao", "Wensong Wang", "Zichen Wen", "Xu Zheng", "Yiyu Wang", "Haocong He", "Yuanhuiyi Lyu", "Lutao Jiang", "Xin Zou", "Yuqian Fu", "Bin Ren", "Linfeng Zhang", "Xuming Hu"], "title": "Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods", "comment": null, "summary": "Recent endeavors to accelerate inference in Multimodal Large Language Models\n(MLLMs) have primarily focused on visual token compression. The effectiveness\nof these methods is typically assessed by measuring the accuracy drop on\nestablished benchmarks, comparing model performance before and after\ncompression. However, these benchmarks are originally designed to assess the\nperception and reasoning capabilities of MLLMs, rather than to evaluate\ncompression techniques. As a result, directly applying them to visual token\ncompression introduces a task mismatch. Strikingly, our investigation reveals\nthat simple image downsampling consistently outperforms many advanced\ncompression methods across multiple widely used benchmarks. Through extensive\nexperiments, we make the following observations: (i) Current benchmarks are\nnoisy for the visual token compression task. (ii) Down-sampling is able to\nserve as a data filter to evaluate the difficulty of samples in the visual\ntoken compression task. Motivated by these findings, we introduce VTC-Bench, an\nevaluation framework that incorporates a data filtering mechanism to denoise\nexisting benchmarks, thereby enabling fairer and more accurate assessment of\nvisual token compression methods. All data and code are available at\nhttps://github.com/Chenfei-Liao/VTC-Bench.", "AI": {"tldr": "Simple image downsampling beats many sophisticated visual token compression methods on current MLLM benchmarks, revealing a task\u2013benchmark mismatch; the authors propose VTC-Bench, an evaluation framework with data filtering to denoise benchmarks and fairly assess compression methods.", "motivation": "Existing MLLM benchmarks were designed to test perception/reasoning, not the quality of visual token compression. Using them directly to judge compression methods can be misleading. The surprising strength of naive downsampling suggests that current evaluations are noisy and misaligned with the compression task, motivating a purpose-built assessment approach.", "method": "Run extensive experiments comparing naive image downsampling with advanced visual token compression techniques across multiple commonly used MLLM benchmarks. Analyze performance sensitivity to compression and use downsampling as a proxy data filter to gauge sample difficulty. Build VTC-Bench, an evaluation framework that integrates a data filtering mechanism to denoise existing benchmarks for the compression task.", "result": "Across several widely used benchmarks, naive downsampling consistently outperforms many advanced compression approaches. Two key observations emerge: (i) current benchmarks are noisy for evaluating visual token compression; (ii) downsampling can act as a data filter to assess sample difficulty in this task. Code and data are released publicly.", "conclusion": "There is a fundamental mismatch between current benchmarks and the visual token compression objective, leading to unreliable comparisons. VTC-Bench addresses this by filtering/denoising data to provide fairer, more accurate evaluation of visual token compression methods."}}
{"id": "2510.07061", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07061", "abs": "https://arxiv.org/abs/2510.07061", "authors": ["Amir Hossein Yari", "Kalmit Kulkarni", "Ahmad Raza Khan", "Fajri Koto"], "title": "Revisiting Metric Reliability for Fine-grained Evaluation of Machine Translation and Summarization in Indian Languages", "comment": "18 pages, 14 figures", "summary": "While automatic metrics drive progress in Machine Translation (MT) and Text\nSummarization (TS), existing metrics have been developed and validated almost\nexclusively for English and other high-resource languages. This narrow focus\nleaves Indian languages, spoken by over 1.5 billion people, largely overlooked,\ncasting doubt on the universality of current evaluation practices. To address\nthis gap, we introduce ITEM, a large-scale benchmark that systematically\nevaluates the alignment of 26 automatic metrics with human judgments across six\nmajor Indian languages, enriched with fine-grained annotations. Our extensive\nevaluation, covering agreement with human judgments, sensitivity to outliers,\nlanguage-specific reliability, inter-metric correlations, and resilience to\ncontrolled perturbations, reveals four central findings: (1) LLM-based\nevaluators show the strongest alignment with human judgments at both segment\nand system levels; (2) outliers exert a significant impact on metric-human\nagreement; (3) in TS, metrics are more effective at capturing content fidelity,\nwhereas in MT, they better reflect fluency; and (4) metrics differ in their\nrobustness and sensitivity when subjected to diverse perturbations.\nCollectively, these findings offer critical guidance for advancing metric\ndesign and evaluation in Indian languages.", "AI": {"tldr": "ITEM is a benchmark that evaluates 26 MT and summarization metrics against human judgments in six major Indian languages, finding LLM-based evaluators align best, outliers strongly affect agreement, TS metrics better capture content fidelity while MT metrics better reflect fluency, and robustness varies across metrics.", "motivation": "Most automatic MT/TS metrics are built and validated for English or other high-resource languages, leaving Indian languages under-evaluated and casting doubt on the universality of current evaluation practices. There is a need for a systematic, human-grounded benchmark for Indian languages.", "method": "Construct ITEM, a large-scale benchmark with fine-grained annotations across six major Indian languages. Evaluate 26 automatic metrics for MT and TS on multiple dimensions: segment- and system-level agreement with human judgments, sensitivity to outliers, language-specific reliability, inter-metric correlations, and resilience under controlled perturbations.", "result": "Four main findings: (1) LLM-based evaluators have the strongest alignment with human judgments at both segment and system levels; (2) outliers significantly influence metric\u2013human agreement; (3) for TS, metrics better capture content fidelity, whereas for MT, they better reflect fluency; (4) metrics differ substantially in robustness and sensitivity to various perturbations.", "conclusion": "ITEM offers a principled foundation for evaluating and designing metrics for Indian languages. LLM-based evaluators are promising but sensitivity to outliers and task-specific strengths/weaknesses must be considered. Future metric development should emphasize robustness, outlier handling, and distinct objectives across MT and TS."}}
{"id": "2510.07190", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07190", "abs": "https://arxiv.org/abs/2510.07190", "authors": ["Yihao Zhi", "Chenghong Li", "Hongjie Liao", "Xihe Yang", "Zhengwentai Sun", "Jiahao Chang", "Xiaodong Cun", "Wensen Feng", "Xiaoguang Han"], "title": "MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis", "comment": "Accepted by SIGGRAPH Asia 2025 conference track", "summary": "Recent breakthroughs in video generation, powered by large-scale datasets and\ndiffusion techniques, have shown that video diffusion models can function as\nimplicit 4D novel view synthesizers. Nevertheless, current methods primarily\nconcentrate on redirecting camera trajectory within the front view while\nstruggling to generate 360-degree viewpoint changes. In this paper, we focus on\nhuman-centric subdomain and present MV-Performer, an innovative framework for\ncreating synchronized novel view videos from monocular full-body captures. To\nachieve a 360-degree synthesis, we extensively leverage the MVHumanNet dataset\nand incorporate an informative condition signal. Specifically, we use the\ncamera-dependent normal maps rendered from oriented partial point clouds, which\neffectively alleviate the ambiguity between seen and unseen observations. To\nmaintain synchronization in the generated videos, we propose a multi-view\nhuman-centric video diffusion model that fuses information from the reference\nvideo, partial rendering, and different viewpoints. Additionally, we provide a\nrobust inference procedure for in-the-wild video cases, which greatly mitigates\nthe artifacts induced by imperfect monocular depth estimation. Extensive\nexperiments on three datasets demonstrate our MV-Performer's state-of-the-art\neffectiveness and robustness, setting a strong model for human-centric 4D novel\nview synthesis.", "AI": {"tldr": "MV-Performer is a multi-view human-centric video diffusion framework that generates synchronized 360\u00b0 novel-view videos from a single monocular full-body capture by conditioning on camera-dependent normal maps derived from oriented partial point clouds, achieving state-of-the-art quality and robustness across datasets.", "motivation": "Video diffusion models can implicitly perform 4D novel view synthesis, but existing methods mainly handle small front-view redirections and fail to produce coherent 360\u00b0 viewpoint changes, especially for humans and in-the-wild videos where monocular depth is noisy. A solution is needed to disambiguate seen vs. unseen surfaces and keep multi-view videos temporally synchronized.", "method": "Leverage the MVHumanNet dataset and introduce an informative conditioning signal: camera-dependent normal maps rendered from oriented partial point clouds to guide view synthesis. Propose a multi-view human-centric video diffusion model that fuses the reference monocular video, the partial renderings, and target viewpoints to maintain cross-view temporal synchronization. Add a robust inference procedure tailored for in-the-wild inputs to mitigate artifacts from imperfect monocular depth estimation.", "result": "Across three datasets, MV-Performer achieves state-of-the-art performance for human-centric 4D novel view synthesis, producing synchronized, high-quality 360\u00b0 novel-view videos and demonstrating robustness to real-world inputs with noisy depth.", "conclusion": "Conditioning video diffusion on camera-dependent normal maps and fusing multi-source signals enables reliable, synchronized 360\u00b0 human novel-view video generation from monocular inputs. MV-Performer establishes a strong, robust SOTA baseline for human-centric 4D novel view synthesis."}}
{"id": "2510.07074", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07074", "abs": "https://arxiv.org/abs/2510.07074", "authors": ["Fred Philippy", "Laura Bernardy", "Siwen Guo", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish", "comment": "Paper under review; Dataset available at\n  https://huggingface.co/datasets/fredxlpy/LuxInstruct", "summary": "Instruction tuning has become a key technique for enhancing the performance\nof large language models, enabling them to better follow human prompts.\nHowever, low-resource languages such as Luxembourgish face severe limitations\ndue to the lack of high-quality instruction datasets. Traditional reliance on\nmachine translation often introduces semantic misalignment and cultural\ninaccuracies. In this work, we address these challenges by creating a\ncross-lingual instruction tuning dataset for Luxembourgish, without resorting\nto machine-generated translations into it. Instead, by leveraging aligned data\nfrom English, French, and German, we build a high-quality dataset that\npreserves linguistic and cultural nuances. We provide evidence that\ncross-lingual instruction tuning not only improves representational alignment\nacross languages but also the model's generative capabilities in Luxembourgish.\nThis highlights how cross-lingual data curation can avoid the common pitfalls\nof machine-translated data and directly benefit low-resource language\ndevelopment.", "AI": {"tldr": "They curate a cross-lingual instruction-tuning dataset for Luxembourgish using aligned English/French/German data (not MT into Luxembourgish), yielding better alignment and generation in Luxembourgish.", "motivation": "Low-resource languages lack high-quality instruction datasets; relying on machine translation causes semantic drift and cultural inaccuracies. A method is needed to improve instruction-following in Luxembourgish without these pitfalls.", "method": "Construct a cross-lingual instruction-tuning dataset for Luxembourgish by leveraging aligned data from English, French, and German, explicitly avoiding machine-generated translations into Luxembourgish to preserve linguistic and cultural nuances.", "result": "Cross-lingual instruction tuning improves representational alignment across languages and enhances the model\u2019s generative capabilities in Luxembourgish, compared to MT-based approaches.", "conclusion": "Carefully curated cross-lingual data is an effective alternative to MT for instruction tuning in low-resource languages, directly benefiting Luxembourgish model performance while avoiding MT-induced errors."}}
{"id": "2510.07191", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07191", "abs": "https://arxiv.org/abs/2510.07191", "authors": ["Soroosh Tayebi Arasteh", "Mina Shaigan", "Christiane Kuhl", "Jakob Nikolas Kather", "Sven Nebelung", "Daniel Truhn"], "title": "Resolution scaling governs DINOv3 transfer performance in chest radiograph classification", "comment": null, "summary": "Self-supervised learning (SSL) has advanced visual representation learning,\nbut its value in chest radiography, a high-volume imaging modality with\nfine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL\nmodels through Gram-anchored self-distillation. Whether these design choices\nimprove transfer learning for chest radiography has not been systematically\ntested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across\nseven datasets (n>814,000). Two representative backbones were evaluated:\nViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and\n1024x1024 pixels. We additionally assessed frozen features from a 7B model. The\nprimary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2\nachieved comparable performance on adult datasets. Increasing resolution to\n512x512 yielded consistent improvements for DINOv3 over both DINOv2 and\nImageNet. In contrast, results in pediatric cohort showed no differences across\ninitializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models\nusing frozen DINOv3-7B features underperformed relative to fully finetuned\n86-89M-parameter backbones, highlighting the importance of domain adaptation.\nScaling to 1024x1024 did not further improve accuracy. Resolution-related gains\nwere most evident for boundary-dependent and small focal abnormalities. In\nchest radiography, higher input resolution is critical for leveraging the\nbenefits of modern self-supervised models. 512x512 pixels represent a practical\nupper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest\nperformance, while larger inputs offer minimal return on cost. Clinically,\nthese findings support use of finetuned, mid-sized backbones at 512x512 for\nchest radiograph interpretation, with the greatest gains expected in detecting\nsubtle or boundary-centered lesions relevant to emergency and critical care\nsettings.", "AI": {"tldr": "Large-scale benchmark (>814k images) shows that for chest radiography, DINOv3 self-supervised initialization paired with ConvNeXt-B and 512\u00d7512 inputs outperforms DINOv2 and ImageNet baselines; 1024\u00d71024 brings no gains, pediatric results show no differences, and frozen 7B features underperform finetuning.", "motivation": "Clarify whether modern SSL\u2014specifically DINOv3 with Gram-anchored self-distillation\u2014offers tangible benefits for chest radiograph transfer learning, an imaging domain with fine-grained, boundary-centric abnormalities where SSL\u2019s value is uncertain.", "method": "Compare DINOv3 vs DINOv2 vs ImageNet initialization across seven chest X-ray datasets (>814k studies). Evaluate two backbones (ViT-B/16, ConvNeXt-B) at three input resolutions (224, 512, 1024). Also test models using frozen features from a DINOv3-7B model. Primary metric: mean AUROC across labels; examine lesion-type sensitivity (small/boundary-dependent).", "result": "At 224\u00d7224, DINOv3 \u2248 DINOv2 on adult datasets. Moving to 512\u00d7512 consistently improves DINOv3 over DINOv2 and ImageNet. No meaningful differences across initializations in pediatric cohort. ConvNeXt-B > ViT-B/16 in all settings. Frozen DINOv3-7B features underperform fully finetuned 86\u201389M backbones, indicating need for domain adaptation. Scaling to 1024\u00d71024 yields no further accuracy gains. Resolution gains are strongest for boundary-dependent and small focal findings.", "conclusion": "In chest radiography, higher but moderate resolution (512\u00d7512) is key to realizing benefits of modern SSL. Best-performing recipe: finetuned, mid-sized ConvNeXt-B with DINOv3 initialization at 512\u00d7512. Larger inputs have minimal ROI, and frozen foundation features are inferior to task-specific finetuning. Gains are most pronounced for subtle or boundary-centered lesions relevant to acute care."}}
{"id": "2510.07081", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07081", "abs": "https://arxiv.org/abs/2510.07081", "authors": ["Fanheng Kong", "Jingyuan Zhang", "Yahui Liu", "Zirui Wu", "Yu Tian", "Victoria W.", "Guorui Zhou"], "title": "Accelerating Diffusion LLM Inference via Local Determinism Propagation", "comment": "21 pages, 4 figures. Under review", "summary": "Diffusion large language models (dLLMs) represent a significant advancement\nin text generation, offering parallel token decoding capabilities. However,\nexisting open-source implementations suffer from quality-speed trade-offs that\nimpede their practical deployment. Conservative sampling strategies typically\ndecode only the most confident token per step to ensure quality (i.e., greedy\ndecoding), at the cost of inference efficiency due to repeated redundant\nrefinement iterations--a phenomenon we term delayed decoding. Through\nsystematic analysis of dLLM decoding dynamics, we characterize this delayed\ndecoding behavior and propose a training-free adaptive parallel decoding\nstrategy, named LocalLeap, to address these inefficiencies. LocalLeap is built\non two fundamental empirical principles: local determinism propagation centered\non high-confidence anchors and progressive spatial consistency decay. By\napplying these principles, LocalLeap identifies anchors and performs localized\nrelaxed parallel decoding within bounded neighborhoods, achieving substantial\ninference step reduction through early commitment of already-determined tokens\nwithout compromising output quality. Comprehensive evaluation on various\nbenchmarks demonstrates that LocalLeap achieves 6.94$\\times$ throughput\nimprovements and reduces decoding steps to just 14.2\\% of the original\nrequirement, achieving these gains with negligible performance impact. The\nsource codes are available at: https://github.com/friedrichor/LocalLeap.", "AI": {"tldr": "LocalLeap is a training-free, adaptive parallel decoding strategy for diffusion LLMs that commits high-confidence tokens early using local anchors, cutting decoding steps to ~14.2% and boosting throughput by ~6.94x with negligible quality loss.", "motivation": "Diffusion LLMs enable parallel token generation but suffer from a quality\u2013speed trade-off: conservative (greedy) sampling often decodes only the most confident token per step, causing redundant refinement iterations\u2014\"delayed decoding\"\u2014that slows inference. There is a need to exploit parallelism without degrading output quality.", "method": "Analyze dLLM decoding dynamics and formalize delayed decoding. Propose LocalLeap, a training-free adaptive parallel decoding scheme based on two empirical principles: (1) local determinism propagation from high-confidence \u201canchor\u201d tokens; (2) progressive spatial consistency decay away from those anchors. The algorithm identifies anchors and performs relaxed, localized parallel decoding within bounded neighborhoods, enabling early commitment of tokens likely already determined.", "result": "Across multiple benchmarks, LocalLeap reduces decoding steps to 14.2% of the original requirement and increases throughput by about 6.94x, while maintaining negligible performance degradation.", "conclusion": "Addressing delayed decoding via anchor-centered, localized parallel updates makes diffusion LLM decoding substantially more efficient without retraining, improving practicality and deployment readiness. Open-source code is provided."}}
{"id": "2510.07206", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07206", "abs": "https://arxiv.org/abs/2510.07206", "authors": ["Shirin Shoushtari", "Yi Wang", "Xiao Shi", "M. Salman Asif", "Ulugbek S. Kamilov"], "title": "EigenScore: OOD Detection using Covariance in Diffusion Models", "comment": null, "summary": "Out-of-distribution (OOD) detection is critical for the safe deployment of\nmachine learning systems in safety-sensitive domains. Diffusion models have\nrecently emerged as powerful generative models, capable of capturing complex\ndata distributions through iterative denoising. Building on this progress,\nrecent work has explored their potential for OOD detection. We propose\nEigenScore, a new OOD detection method that leverages the eigenvalue spectrum\nof the posterior covariance induced by a diffusion model. We argue that\nposterior covariance provides a consistent signal of distribution shift,\nleading to larger trace and leading eigenvalues on OOD inputs, yielding a clear\nspectral signature. We further provide analysis explicitly linking posterior\ncovariance to distribution mismatch, establishing it as a reliable signal for\nOOD detection. To ensure tractability, we adopt a Jacobian-free subspace\niteration method to estimate the leading eigenvalues using only forward\nevaluations of the denoiser. Empirically, EigenScore achieves SOTA performance,\nwith up to 5% AUROC improvement over the best baseline. Notably, it remains\nrobust in near-OOD settings such as CIFAR-10 vs CIFAR-100, where existing\ndiffusion-based methods often fail.", "AI": {"tldr": "EigenScore uses the eigenvalue spectrum (trace and top eigenvalues) of the posterior covariance from a diffusion model as an OOD score, estimated efficiently without Jacobians; it yields state-of-the-art OOD detection, including challenging near-OOD cases.", "motivation": "OOD detection is essential for safety-critical deployment, yet existing diffusion-based detectors often fail on near-distribution shifts. A more principled and reliable signal of distribution mismatch is needed, along with a tractable way to compute it on top of diffusion models.", "method": "Define an OOD score from the posterior covariance induced by a diffusion model\u2019s denoiser; use its spectral properties\u2014particularly the trace and leading eigenvalues\u2014which are argued to inflate under distribution shift. Provide analysis linking posterior covariance magnitude to distribution mismatch. For efficiency, estimate leading eigenvalues via a Jacobian-free subspace iteration using only forward passes of the denoiser.", "result": "Across benchmarks, EigenScore achieves state-of-the-art AUROC, improving up to 5% over the best baselines, and remains robust in near-OOD scenarios such as CIFAR-10 vs CIFAR-100 where prior diffusion-based methods underperform.", "conclusion": "The posterior covariance spectrum is a reliable, theoretically grounded, and practical signal for OOD detection in diffusion models. EigenScore delivers strong and robust performance while remaining computationally tractable."}}
{"id": "2510.07083", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07083", "abs": "https://arxiv.org/abs/2510.07083", "authors": ["Miriam Wanner", "Leif Azzopardi", "Paul Thomas", "Soham Dan", "Benjamin Van Durme", "Nick Craswell"], "title": "All Claims Are Equal, but Some Claims Are More Equal Than Others: Importance-Sensitive Factuality Evaluation of LLM Generations", "comment": null, "summary": "Existing methods for evaluating the factuality of large language model (LLM)\nresponses treat all claims as equally important. This results in misleading\nevaluations when vital information is missing or incorrect as it receives the\nsame weight as peripheral details, raising the question: how can we reliably\ndetect such differences when there are errors in key information? Current\napproaches that measure factuality tend to be insensitive to omitted or false\nkey information. To investigate this lack of sensitivity, we construct\nVITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses\ndesigned to omit or falsify key information. Using this dataset, we demonstrate\nthe insensitivities of existing evaluation metrics to key information errors.\nTo address this gap, we introduce VITAL, a set of metrics that provide greater\nsensitivity in measuring the factuality of responses by incorporating the\nrelevance and importance of claims with respect to the query. Our analysis\ndemonstrates that VITAL metrics more reliably detect errors in key information\nthan previous methods. Our dataset, metrics, and analysis provide a foundation\nfor more accurate and robust assessment of LLM factuality.", "AI": {"tldr": "They show that common LLM factuality metrics miss errors/omissions in key information. They build a benchmark (VITALERRORS) and propose importance-aware metrics (VITAL) that better detect such vital errors.", "motivation": "Factuality evaluators typically weight all claims equally, so omissions or falsifications of crucial facts can be masked by correct but peripheral details. The authors want metrics that reflect the relevance and importance of claims to the user\u2019s query.", "method": "1) Construct VITALERRORS: 6,733 minimally edited LLM responses where key information is omitted or falsified. 2) Empirically demonstrate existing metrics\u2019 insensitivity to these vital errors. 3) Introduce VITAL, a metric suite that incorporates claim relevance/importance to the query when scoring factuality.", "result": "Existing factuality metrics under-detect errors in key information on VITALERRORS. VITAL metrics show greater sensitivity and more reliably flag vital omissions or falsehoods compared to prior methods.", "conclusion": "Importance-aware evaluation is necessary for reliable factuality assessment. The provided benchmark, metrics, and analysis offer a more accurate and robust way to detect key-information errors in LLM responses."}}
{"id": "2510.07217", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07217", "abs": "https://arxiv.org/abs/2510.07217", "authors": ["Wen Ye", "Zhaocheng Liu", "Yuwei Gui", "Tingyu Yuan", "Yunyue Su", "Bowen Fang", "Chaoyang Zhao", "Qiang Liu", "Liang Wang"], "title": "GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation", "comment": "30 pages, 21 figures, accepted to EMNLP 2025 findings", "summary": "Text-to-image synthesis has made remarkable progress, yet accurately\ninterpreting complex and lengthy prompts remains challenging, often resulting\nin semantic inconsistencies and missing details. Existing solutions, such as\nfine-tuning, are model-specific and require training, while prior automatic\nprompt optimization (APO) approaches typically lack systematic error analysis\nand refinement strategies, resulting in limited reliability and effectiveness.\nMeanwhile, test-time scaling methods operate on fixed prompts and on noise or\nsample numbers, limiting their interpretability and adaptability. To solve\nthese, we introduce a flexible and efficient test-time prompt optimization\nstrategy that operates directly on the input text. We propose a plug-and-play\nmulti-agent system called GenPilot, integrating error analysis,\nclustering-based adaptive exploration, fine-grained verification, and a memory\nmodule for iterative optimization. Our approach is model-agnostic,\ninterpretable, and well-suited for handling long and complex prompts.\nSimultaneously, we summarize the common patterns of errors and the refinement\nstrategy, offering more experience and encouraging further exploration.\nExperiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7%\ndemonstrate the strong capability of our methods in enhancing the text and\nimage consistency and structural coherence of generated images, revealing the\neffectiveness of our test-time prompt optimization strategy. The code is\navailable at https://github.com/27yw/GenPilot.", "AI": {"tldr": "GenPilot is a model-agnostic, test-time prompt optimization system for text-to-image generation that iteratively rewrites input prompts via a multi-agent process (error analysis, adaptive exploration, verification, and memory), improving text\u2013image alignment and structural coherence with up to 16.9% and 5.7% gains on DPG-bench and Geneval.", "motivation": "Modern text-to-image models often misinterpret long, complex prompts, causing semantic omissions and inconsistencies. Existing fixes\u2014model fine-tuning or prior automatic prompt optimization\u2014either require training and are model-specific, or lack systematic diagnosis and refinement. Test-time scaling methods tune sampling or noise but leave prompts unchanged, limiting interpretability and adaptability. A model-agnostic, interpretable, test-time method that operates directly on the prompt is needed.", "method": "Introduce GenPilot, a plug-and-play multi-agent framework that, at test time, iteratively edits the input prompt. Components include: (1) error analysis to detect semantic/structural failures; (2) clustering-based adaptive exploration to diversify and focus candidate prompt edits; (3) fine-grained verification to assess candidates; and (4) a memory module to accumulate findings across iterations. The system summarizes common error patterns and guides refinements accordingly. It is designed to work across different T2I models without retraining.", "result": "On DPG-bench and Geneval, GenPilot yields improvements up to 16.9% and 5.7%, respectively, enhancing text\u2013image consistency and structural coherence. These results indicate reliable gains over fixed-prompt baselines and prior APO/test-time scaling strategies.", "conclusion": "Test-time prompt optimization via GenPilot is an effective, interpretable, and model-agnostic approach for complex prompts in text-to-image synthesis. It systematically diagnoses and refines prompts, improves alignment and structure, and provides reusable error/refinement patterns. Open-source code is provided for reproducibility."}}
{"id": "2510.07096", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.07096", "abs": "https://arxiv.org/abs/2510.07096", "authors": ["Zhu Li", "Yuqing Zhang", "Xiyuan Gao", "Shekhar Nayak", "Matt Coler"], "title": "Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided Sarcastic Speech Synthesis", "comment": null, "summary": "Sarcasm is a subtle form of non-literal language that poses significant\nchallenges for speech synthesis due to its reliance on nuanced semantic,\ncontextual, and prosodic cues. While existing speech synthesis research has\nfocused primarily on broad emotional categories, sarcasm remains largely\nunexplored. In this paper, we propose a Large Language Model (LLM)-enhanced\nRetrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach\ncombines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture\npragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic\nexemplars retrieved via a Retrieval Augmented Generation (RAG) module, which\nprovide expressive reference patterns of sarcastic delivery. Integrated within\na VITS backbone, this dual conditioning enables more natural and contextually\nappropriate sarcastic speech. Experiments demonstrate that our method\noutperforms baselines in both objective measures and subjective evaluations,\nyielding improvements in speech naturalness, sarcastic expressivity, and\ndownstream sarcasm detection.", "AI": {"tldr": "LLM-enhanced, retrieval-augmented TTS that conditions a VITS backbone on LLaMA-3 semantic embeddings and retrieved prosodic exemplars to synthesize contextually appropriate sarcastic speech, outperforming baselines in naturalness, expressivity, and sarcasm detection.", "motivation": "Sarcasm depends on nuanced semantic, contextual, and prosodic cues and is poorly handled by current TTS systems that focus on broad emotions. There is a need for models that can encode pragmatic incongruity and discourse-level signals to render sarcasm naturally.", "method": "LoRA-fine-tuned LLaMA 3 provides semantic embeddings capturing sarcasm-related pragmatic cues. A RAG module retrieves prosodic exemplars representing sarcastic delivery patterns. These two signals jointly condition a VITS TTS backbone (dual conditioning) to generate sarcasm-aware speech.", "result": "Experiments show superiority over baselines in objective metrics and human evaluations, with reported gains in speech naturalness, sarcastic expressivity, and performance on downstream sarcasm detection.", "conclusion": "Combining LLM-derived semantic representations with retrieved prosodic references within a VITS framework effectively adds sarcasm awareness to TTS, suggesting retrieval-augmented, LLM-informed conditioning is a promising direction for expressive speech synthesis."}}
{"id": "2510.07249", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07249", "abs": "https://arxiv.org/abs/2510.07249", "authors": ["Jiaben Chen", "Zixin Wang", "Ailing Zeng", "Yang Fu", "Xueyang Yu", "Siyuan Cen", "Julian Tanke", "Yihang Chen", "Koichi Saito", "Yuki Mitsufuji", "Chuang Gan"], "title": "TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation", "comment": "Project page: https://talkcuts.github.io/", "summary": "In this work, we present TalkCuts, a large-scale dataset designed to\nfacilitate the study of multi-shot human speech video generation. Unlike\nexisting datasets that focus on single-shot, static viewpoints, TalkCuts offers\n164k clips totaling over 500 hours of high-quality human speech videos with\ndiverse camera shots, including close-up, half-body, and full-body views. The\ndataset includes detailed textual descriptions, 2D keypoints and 3D SMPL-X\nmotion annotations, covering over 10k identities, enabling multimodal learning\nand evaluation. As a first attempt to showcase the value of the dataset, we\npresent Orator, an LLM-guided multi-modal generation framework as a simple\nbaseline, where the language model functions as a multi-faceted director,\norchestrating detailed specifications for camera transitions, speaker\ngesticulations, and vocal modulation. This architecture enables the synthesis\nof coherent long-form videos through our integrated multi-modal video\ngeneration module. Extensive experiments in both pose-guided and audio-driven\nsettings show that training on TalkCuts significantly enhances the\ncinematographic coherence and visual appeal of generated multi-shot speech\nvideos. We believe TalkCuts provides a strong foundation for future work in\ncontrollable, multi-shot speech video generation and broader multimodal\nlearning.", "AI": {"tldr": "TalkCuts is a large-scale, richly annotated dataset for multi-shot human speech video generation, paired with an LLM-directed baseline (Orator). Training on TalkCuts improves cinematographic coherence and visual appeal of generated multi-shot speech videos.", "motivation": "Existing resources emphasize single-shot, static viewpoints, limiting progress on generating coherent, controllable multi-shot speech videos. The field needs data and tools that capture realistic camera transitions, body configurations, and multimodal cues for evaluation and learning.", "method": "Build TalkCuts: 164k clips (>500 hours) of human speech with diverse camera shots (close-up, half-, full-body), textual descriptions, 2D keypoints, and 3D SMPL-X motion across 10k+ identities. Propose Orator, an LLM-guided multimodal generation framework where the LLM acts as a director specifying camera transitions, gestures, and vocal modulation; an integrated module synthesizes long-form videos. Evaluate in pose-guided and audio-driven settings.", "result": "Models trained on TalkCuts produce multi-shot speech videos with significantly better cinematographic coherence and visual appeal. Orator demonstrates the dataset\u2019s utility, yielding coherent long-form outputs in both pose-guided and audio-driven experiments.", "conclusion": "TalkCuts offers a robust foundation for controllable, multi-shot speech video generation and broader multimodal learning; the dataset and baseline enable future research on coherent, directive, and multimodal video synthesis."}}
{"id": "2510.07098", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07098", "abs": "https://arxiv.org/abs/2510.07098", "authors": ["Guo Yutong", "Wanying Wang", "Yue Wu", "Zichen Miao", "Haoyu Wang"], "title": "TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription", "comment": null, "summary": "Table Visual Question Answering (Table VQA) is typically addressed by large\nvision-language models (VLMs). While such models can answer directly from\nimages, they often miss fine-grained details unless scaled to very large sizes,\nwhich are computationally prohibitive, especially for mobile deployment. A\nlighter alternative is to have a small VLM perform OCR and then use a large\nlanguage model (LLM) to reason over structured outputs such as Markdown tables.\nHowever, these representations are not naturally optimized for LLMs and still\nintroduce substantial errors. We propose TALENT (Table VQA via Augmented\nLanguage-Enhanced Natural-text Transcription), a lightweight framework that\nleverages dual representations of tables. TALENT prompts a small VLM to produce\nboth OCR text and natural language narration, then combines them with the\nquestion for reasoning by an LLM. This reframes Table VQA as an LLM-centric\nmultimodal reasoning task, where the VLM serves as a perception-narration\nmodule rather than a monolithic solver. Additionally, we construct ReTabVQA, a\nmore challenging Table VQA dataset requiring multi-step quantitative reasoning\nover table images. Experiments show that TALENT enables a small VLM-LLM\ncombination to match or surpass a single large VLM at significantly lower\ncomputational cost on both public datasets and ReTabVQA.", "AI": {"tldr": "TALENT reframes Table VQA by using a small VLM to transcribe and narrate tables, then offloading reasoning to an LLM, achieving large-VLM-level accuracy at much lower compute; also introduces a harder dataset, ReTabVQA.", "motivation": "Large VLMs can read tables directly but miss fine-grained details unless scaled, which is computationally expensive and unsuitable for mobile settings. Using OCR-to-structured formats (e.g., Markdown) for LLMs still yields errors and is not LLM-optimized. The authors seek a lightweight, accurate, and LLM-friendly alternative.", "method": "TALENT uses a small VLM as a perception\u2013narration module to produce two complementary outputs from a table image: (1) OCR text and (2) a natural-language narration describing the table. These, combined with the user question, are fed to an LLM for reasoning. The task is reframed as LLM-centric multimodal reasoning. They also build ReTabVQA, a dataset requiring multi-step quantitative reasoning over table images.", "result": "Across public benchmarks and the new ReTabVQA, a small VLM + LLM pipeline with TALENT matches or surpasses a single large VLM\u2019s performance while using significantly less computation.", "conclusion": "Dual table representations (OCR + natural-language narration) enable efficient, accurate LLM-driven reasoning for Table VQA, offering a practical alternative to large VLMs. ReTabVQA provides a more challenging benchmark for multi-step quantitative reasoning over tables."}}
{"id": "2510.07277", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07277", "abs": "https://arxiv.org/abs/2510.07277", "authors": ["Franco Javier Arellano", "Jos\u00e9 Ignacio Orlando"], "title": "Evaluating Fundus-Specific Foundation Models for Diabetic Macular Edema Detection", "comment": "Accepted for publication at SIPAIM 2025", "summary": "Diabetic Macular Edema (DME) is a leading cause of vision loss among patients\nwith Diabetic Retinopathy (DR). While deep learning has shown promising results\nfor automatically detecting this condition from fundus images, its application\nremains challenging due the limited availability of annotated data. Foundation\nModels (FM) have emerged as an alternative solution. However, it is unclear if\nthey can cope with DME detection in particular. In this paper, we\nsystematically compare different FM and standard transfer learning approaches\nfor this task. Specifically, we compare the two most popular FM for retinal\nimages--RETFound and FLAIR--and an EfficientNet-B0 backbone, across different\ntraining regimes and evaluation settings in IDRiD, MESSIDOR-2 and\nOCT-and-Eye-Fundus-Images (OEFI). Results show that despite their scale, FM do\nnot consistently outperform fine-tuned CNNs in this task. In particular, an\nEfficientNet-B0 ranked first or second in terms of area under the ROC and\nprecision/recall curves in most evaluation settings, with RETFound only showing\npromising results in OEFI. FLAIR, on the other hand, demonstrated competitive\nzero-shot performance, achieving notable AUC-PR scores when prompted\nappropriately. These findings reveal that FM might not be a good tool for\nfine-grained ophthalmic tasks such as DME detection even after fine-tuning,\nsuggesting that lightweight CNNs remain strong baselines in data-scarce\nenvironments.", "AI": {"tldr": "Benchmarking foundation models (RETFound, FLAIR) against a fine-tuned EfficientNet-B0 for DME detection on multiple fundus datasets shows FMs do not consistently outperform a lightweight CNN; EfficientNet often ranks best, while FLAIR offers competitive zero-shot results with careful prompting.", "motivation": "DME is a major cause of vision loss in DR, but annotated fundus data are scarce. Foundation models promise strong performance with limited labels; it is unclear whether they provide advantages for fine-grained ophthalmic tasks like DME detection.", "method": "Systematic comparison of RETFound and FLAIR (as foundation models) versus an EfficientNet-B0 backbone under different training regimes (including zero-shot and fine-tuning) and evaluation settings on IDRiD, MESSIDOR-2, and OEFI. Performance measured mainly via AUROC and AUC-PR; FLAIR evaluated with prompt-based zero-shot inference.", "result": "EfficientNet-B0 ranks first or second in most evaluation settings by AUROC and AUC-PR. RETFound shows promising results primarily on OEFI. FLAIR exhibits competitive zero-shot performance, achieving strong AUC-PR when prompted appropriately. Overall, FMs do not consistently surpass a well-tuned CNN.", "conclusion": "For DME detection from fundus images, foundation models may not offer consistent gains over fine-tuned lightweight CNNs, even after fine-tuning. In data-scarce environments, EfficientNet-like models remain strong baselines, while FM zero-shot capabilities can be useful but are not broadly superior."}}
{"id": "2510.07105", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07105", "abs": "https://arxiv.org/abs/2510.07105", "authors": ["Taylor Sorensen", "Yejin Choi"], "title": "Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning", "comment": "NLPerspectives: The 4th Workshop on Perspectivist Approaches to\n  Natural Language Processing at EMNLP 2025", "summary": "Many natural language processing (NLP) tasks involve subjectivity, ambiguity,\nor legitimate disagreement between annotators. In this paper, we outline our\nsystem for modeling human variation. Our system leverages language models'\n(LLMs) in-context learning abilities, along with a two-step meta-learning\ntraining procedure for 1) post-training on many datasets requiring in-context\nlearning and 2) specializing the model via in-context meta-learning to the\nparticular data distribution of interest. We also evaluate the performance of\nour system submission to the Learning With Disagreements (LeWiDi) competition,\nwhere it was the overall winner on both tasks. Additionally, we perform an\nablation study to measure the importance of each system component. We find that\nincluding rater examples in-context is crucial for our system's performance,\ndataset-specific fine-tuning is helpful on the larger datasets, post-training\non other in-context datasets is helpful on one of the competition datasets, and\nthat performance improves with model scale.", "AI": {"tldr": "They propose a meta-learned, in-context LLM framework that models human variation by conditioning on rater examples, winning the LeWiDi competition and showing that rater-aware prompts, task-specific tuning, and model scale notably boost performance.", "motivation": "Many NLP tasks contain genuine annotator disagreement; instead of forcing a single \u201cgold\u201d label, the goal is to model human variation and predict rater-specific or distribution-aware outcomes.", "method": "A two-step approach: (1) post-train an LLM on many datasets to strengthen general in-context learning; (2) specialize via in-context meta-learning to the target distribution, conditioning on rater examples. They also explore dataset-specific fine-tuning and scaling effects.", "result": "Their system won both tasks in the LeWiDi competition. Ablations show: including rater examples in-context is crucial; dataset-specific fine-tuning helps, especially on larger datasets; post-training on other ICL datasets helps on one competition dataset; and larger models perform better.", "conclusion": "Explicitly modeling rater variation with rater-conditioned in-context learning, combined with meta-learning and selective fine-tuning, yields state-of-the-art performance on disagreement-heavy NLP tasks, with benefits amplified by model scale."}}
{"id": "2510.07302", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07302", "abs": "https://arxiv.org/abs/2510.07302", "authors": ["Inzamamul Alam", "Md Tanvir Islam", "Khan Muhammad", "Simon S. Woo"], "title": "SpecGuard: Spectral Projection-based Advanced Invisible Watermarking", "comment": "ICCV 2025 Accepted Paper", "summary": "Watermarking embeds imperceptible patterns into images for authenticity\nverification. However, existing methods often lack robustness against various\ntransformations primarily including distortions, image regeneration, and\nadversarial perturbation, creating real-world challenges. In this work, we\nintroduce SpecGuard, a novel watermarking approach for robust and invisible\nimage watermarking. Unlike prior approaches, we embed the message inside hidden\nconvolution layers by converting from the spatial domain to the frequency\ndomain using spectral projection of a higher frequency band that is decomposed\nby wavelet projection. Spectral projection employs Fast Fourier Transform\napproximation to transform spatial data into the frequency domain efficiently.\nIn the encoding phase, a strength factor enhances resilience against diverse\nattacks, including adversarial, geometric, and regeneration-based distortions,\nensuring the preservation of copyrighted information. Meanwhile, the decoder\nleverages Parseval's theorem to effectively learn and extract the watermark\npattern, enabling accurate retrieval under challenging transformations. We\nevaluate the proposed SpecGuard based on the embedded watermark's invisibility,\ncapacity, and robustness. Comprehensive experiments demonstrate the proposed\nSpecGuard outperforms the state-of-the-art models. To ensure reproducibility,\nthe full code is released on\n\\href{https://github.com/inzamamulDU/SpecGuard_ICCV_2025}{\\textcolor{blue}{\\textbf{GitHub}}}.", "AI": {"tldr": "SpecGuard embeds watermarks in high-frequency spectral components of hidden convolutional features using wavelet-based decomposition and FFT projection, with a strength-controlled encoder and a Parseval\u2019s-theorem-guided decoder, achieving robust, invisible, and higher-capacity watermarking that outperforms prior methods under diverse attacks.", "motivation": "Existing image watermarking methods struggle to remain both imperceptible and robust against real-world transformations, including geometric distortions, regeneration (e.g., model-based re-synthesis), and adversarial perturbations. The paper aims to close this robustness gap without sacrificing invisibility or capacity.", "method": "Convert feature representations from spatial to frequency domain via an efficient FFT-based spectral projection; decompose via wavelets to isolate higher-frequency bands; embed message bits into these bands within hidden convolution layers using a tunable strength factor to balance invisibility and robustness; train a decoder that leverages Parseval\u2019s theorem to reliably learn and extract the watermark after challenging transformations.", "result": "On benchmarks measuring invisibility, capacity, and robustness to adversarial, geometric, and regeneration-based distortions, SpecGuard outperforms state-of-the-art watermarking methods. The abstract claims comprehensive experiments and public code for reproducibility.", "conclusion": "Frequency-domain embedding in high-frequency bands, combined with a theoretically grounded decoder and strength-controlled encoding, yields a practical watermarking approach that is more robust and imperceptible than existing methods, with demonstrated superiority across multiple attack scenarios."}}
{"id": "2510.07118", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07118", "abs": "https://arxiv.org/abs/2510.07118", "authors": ["Manish Nagaraj", "Sakshi Choudhary", "Utkarsh Saxena", "Deepak Ravikumar", "Kaushik Roy"], "title": "TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning", "comment": null, "summary": "Instruction tuning is essential for aligning large language models (LLMs) to\ndownstream tasks and commonly relies on large, diverse corpora. However, small,\nhigh-quality subsets, known as coresets, can deliver comparable or superior\nresults, though curating them remains challenging. Existing methods often rely\non coarse, sample-level signals like gradients, an approach that is\ncomputationally expensive and overlooks fine-grained features. To address this,\nwe introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a\nforward-only, token-centric framework. Instead of using gradients, TRIM\noperates by matching underlying representational patterns identified via\nattention-based \"fingerprints\" from a handful of target samples. Such an\napproach makes TRIM highly efficient and uniquely sensitive to the structural\nfeatures that define a task. Coresets selected by our method consistently\noutperform state-of-the-art baselines by up to 9% on downstream tasks and even\nsurpass the performance of full-data fine-tuning in some settings. By avoiding\nexpensive backward passes, TRIM achieves this at a fraction of the\ncomputational cost. These findings establish TRIM as a scalable and efficient\nalternative for building high-quality instruction-tuning datasets.", "AI": {"tldr": "TRIM is a forward-only, token-centric data selection method that uses multi-layer attention \u201cfingerprints\u201d to pick small, high-quality instruction-tuning coresets, outperforming SOTA baselines by up to 9% and sometimes even full-data fine-tuning, at much lower compute (no gradients/backward passes).", "motivation": "Instruction tuning typically needs large, diverse datasets, but well-chosen small coresets can match or beat full-data performance. Existing coreset methods often rely on gradient-based, sample-level signals that are costly and overlook fine-grained token-level structure. There is a need for an efficient, fine-grained approach that captures task-relevant structure without expensive backward passes.", "method": "TRIM builds interpretable, multi-layer attention-based \u201cfingerprints\u201d from a few target samples, then uses forward passes to match these representational patterns across candidate data. It selects examples whose token-level attention patterns align with the target task\u2019s structure, avoiding gradients/backprop and emphasizing token-centric, multi-layer signals for sensitivity and efficiency.", "result": "Across downstream tasks, TRIM-selected coresets consistently outperform state-of-the-art data selection baselines by up to 9% and, in some cases, even surpass full-data fine-tuning. Because it avoids backward passes, it achieves these gains at a fraction of the computational cost.", "conclusion": "TRIM offers a scalable, efficient alternative for constructing high-quality instruction-tuning datasets, leveraging attention-based fingerprints to capture task-structural features and deliver strong performance with lower compute."}}
{"id": "2510.07310", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07310", "abs": "https://arxiv.org/abs/2510.07310", "authors": ["Siyoon Jin", "Seongchan Kim", "Dahyun Chung", "Jaeho Lee", "Hyunwook Choi", "Jisu Nam", "Jiyoung Kim", "Seungryong Kim"], "title": "MATRIX: Mask Track Alignment for Interaction-aware Video Generation", "comment": "Project Page is available at: https://cvlab-kaist.github.io/MATRIX/", "summary": "Video DiTs have advanced video generation, yet they still struggle to model\nmulti-instance or subject-object interactions. This raises a key question: How\ndo these models internally represent interactions? To answer this, we curate\nMATRIX-11K, a video dataset with interaction-aware captions and multi-instance\nmask tracks. Using this dataset, we conduct a systematic analysis that\nformalizes two perspectives of video DiTs: semantic grounding, via\nvideo-to-text attention, which evaluates whether noun and verb tokens capture\ninstances and their relations; and semantic propagation, via video-to-video\nattention, which assesses whether instance bindings persist across frames. We\nfind both effects concentrate in a small subset of interaction-dominant layers.\nMotivated by this, we introduce MATRIX, a simple and effective regularization\nthat aligns attention in specific layers of video DiTs with multi-instance mask\ntracks from the MATRIX-11K dataset, enhancing both grounding and propagation.\nWe further propose InterGenEval, an evaluation protocol for interaction-aware\nvideo generation. In experiments, MATRIX improves both interaction fidelity and\nsemantic alignment while reducing drift and hallucination. Extensive ablations\nvalidate our design choices. Codes and weights will be released.", "AI": {"tldr": "They analyze how video Diffusion Transformers represent multi-instance interactions, curate a labeled video dataset (MATRIX-11K), find a small set of interaction-dominant layers responsible for grounding and temporal binding, and propose a targeted attention-regularization method (MATRIX) that improves interaction fidelity and reduces drift; plus an evaluation protocol (InterGenEval).", "motivation": "Video DiTs often fail at scenes with multiple entities and subject\u2013object interactions. The authors ask whether and where such interactions are encoded inside the models, and how to leverage that knowledge to improve generation quality.", "method": "1) Build MATRIX-11K: videos with interaction-aware captions and multi-instance mask tracks. 2) Analyze DiTs from two angles: (a) semantic grounding via video-to-text attention to test whether noun/verb tokens attend to the correct instances/relations; (b) semantic propagation via video-to-video attention to test whether instance bindings persist across frames. 3) Identify interaction-dominant layers where these effects concentrate. 4) MATRIX: apply a regularization that aligns attention maps in those layers with the provided mask tracks. 5) Propose InterGenEval to benchmark interaction-aware video generation.", "result": "Grounding and propagation signals concentrate in a small subset of layers. Applying MATRIX in those layers improves interaction fidelity and semantic alignment, while reducing temporal drift and hallucinations. Extensive ablations support the design. Code and model weights are to be released.", "conclusion": "Understanding attention patterns reveals key layers governing interactions; aligning their attention with instance tracks is a simple, effective way to enhance interaction modeling in video DiTs. The dataset and evaluation protocol provide infrastructure for further research."}}
{"id": "2510.07141", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07141", "abs": "https://arxiv.org/abs/2510.07141", "authors": ["Samuel Joseph Amouyal", "Aya Meltzer-Asscher", "Jonathan Berant"], "title": "Comparing human and language models sentence processing difficulties on complex structures", "comment": "Data and code will be released soon", "summary": "Large language models (LLMs) that fluently converse with humans are a reality\n- but do LLMs experience human-like processing difficulties? We systematically\ncompare human and LLM sentence comprehension across seven challenging\nlinguistic structures. We collect sentence comprehension data from humans and\nfive families of state-of-the-art LLMs, varying in size and training procedure\nin a unified experimental framework. Our results show LLMs overall struggle on\nthe target structures, but especially on garden path (GP) sentences. Indeed,\nwhile the strongest models achieve near perfect accuracy on non-GP structures\n(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).\nAdditionally, when ranking structures based on average performance, rank\ncorrelation between humans and models increases with parameter count. For each\ntarget structure, we also collect data for their matched baseline without the\ndifficult structure. Comparing performance on the target vs. baseline\nsentences, the performance gap observed in humans holds for LLMs, with two\nexceptions: for models that are too weak performance is uniformly low across\nboth sentence types, and for models that are too strong the performance is\nuniformly high. Together, these reveal convergence and divergence in human and\nLLM sentence comprehension, offering new insights into the similarity of humans\nand LLMs.", "AI": {"tldr": "Human vs. LLM sentence comprehension compared on seven difficult constructions; LLMs, even very strong ones, especially struggle with garden\u2011path sentences; larger models\u2019 difficulty rankings align more with humans; target\u2013baseline performance gaps are human\u2011like except for very weak or very strong models.", "motivation": "Test whether LLMs exhibit human\u2011like processing difficulties in sentence comprehension, and to what extent model size/training lead to convergence with human behavior.", "method": "Unified experimental framework collecting accuracy on seven challenging syntactic structures from humans and five LLM families of varying sizes/training. For each structure, matched baseline sentences without the difficult feature were included. Analyses included accuracy by structure (GP vs. non\u2011GP) and rank correlations between human and model performance as a function of parameter count.", "result": "Overall, LLMs underperform on the difficult target structures, with the largest deficits on garden\u2011path (GP) sentences. The strongest model attains near\u2011ceiling performance on non\u2011GP items (93.7% for GPT\u20115) but low accuracy on GP items (46.8%). Rank correlation between human and model performance increases with model size. Human\u2011like performance gaps between target and baseline hold, except when models are too weak (uniformly low) or too strong (uniformly high).", "conclusion": "LLMs show both convergence and divergence with human sentence processing: larger models mirror human difficulty rankings more closely, yet even state\u2011of\u2011the\u2011art models falter on GP sentences. This suggests partial alignment with human psycholinguistic patterns but indicates remaining deficits in handling incremental ambiguity and reanalysis."}}
{"id": "2510.07313", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07313", "abs": "https://arxiv.org/abs/2510.07313", "authors": ["Zezhong Qian", "Xiaowei Chi", "Yuming Li", "Shizun Wang", "Zhiyuan Qin", "Xiaozhu Ju", "Sirui Han", "Shanghang Zhang"], "title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation", "comment": null, "summary": "Wrist-view observations are crucial for VLA models as they capture\nfine-grained hand-object interactions that directly enhance manipulation\nperformance. Yet large-scale datasets rarely include such recordings, resulting\nin a substantial gap between abundant anchor views and scarce wrist views.\nExisting world models cannot bridge this gap, as they require a wrist-view\nfirst frame and thus fail to generate wrist-view videos from anchor views\nalone. Amid this gap, recent visual geometry models such as VGGT emerge with\ngeometric and cross-view priors that make it possible to address extreme\nviewpoint shifts. Inspired by these insights, we propose WristWorld, the first\n4D world model that generates wrist-view videos solely from anchor views.\nWristWorld operates in two stages: (i) Reconstruction, which extends VGGT and\nincorporates our Spatial Projection Consistency (SPC) Loss to estimate\ngeometrically consistent wrist-view poses and 4D point clouds; (ii) Generation,\nwhich employs our video generation model to synthesize temporally coherent\nwrist-view videos from the reconstructed perspective. Experiments on Droid,\nCalvin, and Franka Panda demonstrate state-of-the-art video generation with\nsuperior spatial consistency, while also improving VLA performance, raising the\naverage task completion length on Calvin by 3.81% and closing 42.4% of the\nanchor-wrist view gap.", "AI": {"tldr": "WristWorld is a two-stage 4D world model that generates wrist-view videos purely from anchor-view inputs by first reconstructing wrist-view geometry (poses + 4D point clouds) using an extended VGGT with a Spatial Projection Consistency loss, then synthesizing temporally coherent videos from that reconstructed viewpoint, achieving SOTA spatial consistency and improving VLA performance.", "motivation": "Fine-grained hand\u2013object interactions are best captured from wrist-view cameras, but large-scale datasets rarely include wrist views, creating a performance gap between abundant anchor views and scarce wrist views. Existing world models cannot synthesize wrist-view videos from anchor views because they require a wrist-view first frame. Recent visual geometry models (e.g., VGGT) provide geometric and cross-view priors that could enable generation under extreme viewpoint shifts, motivating a method to bridge this gap.", "method": "Two-stage pipeline: (i) Reconstruction\u2014extend VGGT and introduce a Spatial Projection Consistency (SPC) loss to estimate geometrically consistent wrist-view camera poses and dense 4D point clouds from anchor views. (ii) Generation\u2014use a video generation model to render temporally coherent wrist-view videos from the reconstructed viewpoint, yielding a 4D world model capable of cross-view synthesis without any wrist-view inputs.", "result": "On Droid, Calvin, and Franka Panda datasets, WristWorld achieves state-of-the-art wrist-view video generation with superior spatial consistency. It also boosts VLA performance, increasing average task completion length on Calvin by 3.81% and closing 42.4% of the anchor\u2013wrist view performance gap.", "conclusion": "Generating wrist-view videos from anchor views alone is feasible and beneficial for manipulation: WristWorld bridges the anchor\u2013wrist data gap, produces spatially consistent wrist-view sequences, and measurably improves VLA outcomes. The approach leverages geometric priors (VGGT) and a new SPC loss to handle extreme viewpoint shifts, setting a strong baseline for cross-view 4D world modeling."}}
{"id": "2510.07167", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07167", "abs": "https://arxiv.org/abs/2510.07167", "authors": ["Lekang Jiang", "Wenjun Sun", "Stephan Goetz"], "title": "Reasoning for Hierarchical Text Classification: The Case of Patents", "comment": "15 pages, 10 tables, 3 figures", "summary": "Hierarchical text classification (HTC) assigns documents to multiple levels\nof a pre-defined taxonomy. Automated patent subject classification represents\none of the hardest HTC scenarios because of domain knowledge difficulty and a\nhuge number of labels. Prior approaches only output a flat label set, which\noffers little insight into the reason behind predictions. Therefore, we propose\nReasoning for Hierarchical Classification (RHC), a novel framework that\nreformulates HTC as a step-by-step reasoning task to sequentially deduce\nhierarchical labels. RHC trains large language models (LLMs) in two stages: a\ncold-start stage that aligns outputs with chain-of-thought (CoT) reasoning\nformat and a reinforcement learning (RL) stage to enhance multi-step reasoning\nability. RHC demonstrates four advantages in our experiments. (1)\nEffectiveness: RHC surpasses previous baselines and outperforms the supervised\nfine-tuning counterparts by approximately 3% in accuracy and macro F1. (2)\nExplainability: RHC produces natural-language justifications before prediction\nto facilitate human inspection. (3) Scalability: RHC scales favorably with\nmodel size with larger gains compared to standard fine-tuning. (4)\nApplicability: Beyond patents, we further demonstrate that RHC achieves\nstate-of-the-art performance on other widely used HTC benchmarks, which\nhighlights its broad applicability.", "AI": {"tldr": "RHC reframes hierarchical text classification as a step-by-step reasoning process, training LLMs with CoT alignment and RL to sequentially predict taxonomy paths, yielding ~3% gains in accuracy and macro-F1, better explainability, and strong scalability across patents and other HTC benchmarks.", "motivation": "HTC\u2014especially patent classification\u2014demands domain knowledge and must assign labels across multiple taxonomy levels. Prior models often output flat labels with limited interpretability and reasoning transparency, offering little insight into how predictions map to the hierarchy. The authors aim to improve both performance and explainability by enabling explicit multi-step reasoning aligned with the taxonomy.", "method": "They propose RHC, which reformulates HTC as sequential reasoning along the taxonomy. Training has two stages: (1) a cold-start stage to align model outputs with chain-of-thought (CoT) reasoning formats; (2) a reinforcement learning stage to strengthen multi-step reasoning and decision quality across levels. The model produces natural-language justifications before emitting hierarchical labels.", "result": "Across patent classification and other HTC benchmarks, RHC surpasses prior baselines and supervised fine-tuning by about 3% in accuracy and macro-F1. It also provides textual rationales, scales favorably with larger model sizes, and achieves state-of-the-art results beyond patents.", "conclusion": "Treating HTC as explicit reasoning improves performance, interpretability, and scalability. RHC offers a general, effective approach for hierarchical labeling tasks and demonstrates broad applicability beyond patents."}}
{"id": "2510.07316", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07316", "abs": "https://arxiv.org/abs/2510.07316", "authors": ["Gangwei Xu", "Haotong Lin", "Hongcheng Luo", "Xianqi Wang", "Jingfeng Yao", "Lianghui Zhu", "Yuechuan Pu", "Cheng Chi", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye", "Sida Peng", "Xin Yang"], "title": "Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers", "comment": "NeurIPS 2025. Project page: https://pixel-perfect-depth.github.io/", "summary": "This paper presents Pixel-Perfect Depth, a monocular depth estimation model\nbased on pixel-space diffusion generation that produces high-quality,\nflying-pixel-free point clouds from estimated depth maps. Current generative\ndepth estimation models fine-tune Stable Diffusion and achieve impressive\nperformance. However, they require a VAE to compress depth maps into latent\nspace, which inevitably introduces \\textit{flying pixels} at edges and details.\nOur model addresses this challenge by directly performing diffusion generation\nin the pixel space, avoiding VAE-induced artifacts. To overcome the high\ncomplexity associated with pixel-space generation, we introduce two novel\ndesigns: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which\nincorporate semantic representations from vision foundation models into DiT to\nprompt the diffusion process, thereby preserving global semantic consistency\nwhile enhancing fine-grained visual details; and 2) Cascade DiT Design that\nprogressively increases the number of tokens to further enhance efficiency and\naccuracy. Our model achieves the best performance among all published\ngenerative models across five benchmarks, and significantly outperforms all\nother models in edge-aware point cloud evaluation.", "AI": {"tldr": "Pixel-Perfect Depth is a monocular depth estimator that performs diffusion directly in pixel space, avoiding VAE-induced flying-pixel artifacts, and uses semantic prompting plus a cascaded DiT to achieve state-of-the-art accuracy and edge fidelity.", "motivation": "Latent-space generative depth methods (fine-tuned Stable Diffusion) rely on a VAE that compresses depth maps, which introduces edge/detail artifacts known as flying pixels. The goal is to produce cleaner, more geometrically consistent depth and point clouds without these artifacts.", "method": "Replace latent-space generation with pixel-space diffusion. Introduce (1) Semantics-Prompted DiT (SP-DiT) that injects semantic representations from vision foundation models to guide diffusion for globally consistent semantics and fine details; and (2) a Cascade DiT that progressively increases token counts to control computation while improving resolution/accuracy.", "result": "Achieves best performance among published generative depth models on five benchmarks and shows large gains on edge-aware point cloud evaluation, producing flying-pixel-free point clouds from predicted depth.", "conclusion": "Pixel-space diffusion guided by semantic prompts and a cascaded token design effectively removes VAE-induced artifacts, yielding superior depth accuracy and cleaner edges, establishing a new state of the art for generative monocular depth and downstream point cloud quality."}}
{"id": "2510.07169", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07169", "abs": "https://arxiv.org/abs/2510.07169", "authors": ["Yike Zhao", "Simin Guo", "Ziqing Yang", "Shifan Han", "Dahua Lin", "Fei Tan"], "title": "More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning", "comment": "12 pages, 3 figures, submitted to EMNLP 2025 Industry Track", "summary": "The reasoning capabilities of Large Language Models (LLMs) play a critical\nrole in many downstream tasks, yet depend strongly on the quality of training\ndata. Despite various proposed data construction methods, their practical\nutility in real-world pipelines remains underexplored. In this work, we conduct\na comprehensive analysis of open-source datasets and data synthesis techniques\nfor mathematical reasoning, evaluating them under a unified pipeline designed\nto mirror training and deployment scenarios. We further distill effective data\nselection strategies and identify practical methods suitable for industrial\napplications. Our findings highlight that structuring data in more\ninterpretable formats, or distilling from stronger models often outweighs\nsimply scaling up data volume. This study provides actionable guidance for\nintegrating training data to enhance LLM capabilities, supporting both\ncost-effective data curation and scalable model enhancement. We hope this work\nwill inspire further research on how to balance \"more data\" versus \"better\ndata\" for real-world reasoning tasks.", "AI": {"tldr": "Evaluates open-source math-reasoning datasets and synthesis methods in a unified, deployment-like pipeline and finds that structured/ distilled data beats merely scaling volume, yielding practical, cost-effective data selection strategies for improving LLM reasoning.", "motivation": "LLM reasoning quality hinges on training data, yet practitioners lack clear, empirically grounded guidance on which data sources and construction methods work best in real-world pipelines and under budget constraints.", "method": "Comprehensive analysis of mathematical-reasoning datasets and data synthesis techniques, all trained and assessed under a consistent pipeline designed to mirror real deployment. The study compares data volume vs. data quality (e.g., interpretable structure, distillation from stronger models) and distills selection strategies for practical use.", "result": "Data organized in interpretable formats and data distilled from stronger models often yield larger gains than simply increasing dataset size. The study extracts effective selection strategies and identifies methods suitable for industrial settings.", "conclusion": "Actionable guidance is provided for integrating and curating training data to enhance LLM reasoning in a cost-effective, scalable manner. The work advocates prioritizing \u201cbetter data\u201d (structured, distilled) over sheer quantity and invites further research on balancing quality and scale for real-world reasoning tasks."}}
{"id": "2510.07317", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07317", "abs": "https://arxiv.org/abs/2510.07317", "authors": ["Natacha Kuete Meli", "Shuteng Wang", "Marcel Seelbach Benkner", "Michele Sasdelli", "Tat-Jun Chin", "Tolga Birdal", "Michael Moeller", "Vladislav Golyanik"], "title": "Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms", "comment": "44 pages, 23 figures and 6 tables", "summary": "Quantum-enhanced Computer Vision (QeCV) is a new research field at the\nintersection of computer vision, optimisation theory, machine learning and\nquantum computing. It has high potential to transform how visual signals are\nprocessed and interpreted with the help of quantum computing that leverages\nquantum-mechanical effects in computations inaccessible to classical (i.e.\nnon-quantum) computers. In scenarios where existing non-quantum methods cannot\nfind a solution in a reasonable time or compute only approximate solutions,\nquantum computers can provide, among others, advantages in terms of better time\nscalability for multiple problem classes. Parametrised quantum circuits can\nalso become, in the long term, a considerable alternative to classical neural\nnetworks in computer vision. However, specialised and fundamentally new\nalgorithms must be developed to enable compatibility with quantum hardware and\nunveil the potential of quantum computational paradigms in computer vision.\nThis survey contributes to the existing literature on QeCV with a holistic\nreview of this research field. It is designed as a quantum computing reference\nfor the computer vision community, targeting computer vision students,\nscientists and readers with related backgrounds who want to familiarise\nthemselves with QeCV. We provide a comprehensive introduction to QeCV, its\nspecifics, and methodologies for formulations compatible with quantum hardware\nand QeCV methods, leveraging two main quantum computational paradigms, i.e.\ngate-based quantum computing and quantum annealing. We elaborate on the\noperational principles of quantum computers and the available tools to access,\nprogram and simulate them in the context of QeCV. Finally, we review existing\nquantum computing tools and learning materials and discuss aspects related to\npublishing and reviewing QeCV papers, open challenges and potential social\nimplications.", "AI": {"tldr": "A survey of Quantum\u2011enhanced Computer Vision (QeCV) that introduces the field, explains quantum paradigms (gate-based and annealing), details hardware\u2011compatible formulations and tools, and outlines resources, challenges, and implications for the CV community.", "motivation": "Classical computer vision often faces intractable or slow optimization and only approximate solutions. Quantum computing may offer better scalability and new modeling capacity (e.g., parametrized quantum circuits), but CV researchers need a consolidated reference to understand principles, methods, tooling, and research practices for QeCV.", "method": "Holistic literature review and tutorial-style exposition: introduce QeCV concepts; map vision problems to quantum-compatible formulations; cover two main quantum paradigms (gate-based circuits and quantum annealing); describe operational principles of quantum hardware; survey programming/simulation tools and learning materials; discuss publishing/reviewing practices, open challenges, and social implications.", "result": "A structured synthesis of the QeCV landscape: conceptual framework, taxonomy of methods across quantum paradigms, guidance for hardware-aware formulations, overview of accessible toolchains and simulators, curated resources for learning, and identification of current research gaps and evaluation considerations.", "conclusion": "QeCV is promising for certain classes of vision problems due to potential scalability and alternative learning models, but realizing benefits requires fundamentally new, hardware-compatible algorithms and careful methodology. The community now has a reference roadmap, yet key challenges (algorithm design, practicality on near-term devices, evaluation standards, and societal impacts) remain open."}}
{"id": "2510.07173", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07173", "abs": "https://arxiv.org/abs/2510.07173", "authors": ["Md Tawkat Islam Khondaker", "Julia Harrington", "Shady Shehata"], "title": "NurseLLM: The First Specialized Language Model for Nursing", "comment": "EMNLP 2025 Industry Track", "summary": "Recent advancements in large language models (LLMs) have significantly\ntransformed medical systems. However, their potential within specialized\ndomains such as nursing remains largely underexplored. In this work, we\nintroduce NurseLLM, the first nursing-specialized LLM tailored for multiple\nchoice question-answering (MCQ) tasks. We develop a multi-stage data generation\npipeline to build the first large scale nursing MCQ dataset to train LLMs on a\nbroad spectrum of nursing topics. We further introduce multiple nursing\nbenchmarks to enable rigorous evaluation. Our extensive experiments demonstrate\nthat NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of\ncomparable size on different benchmarks, underscoring the importance of a\nspecialized LLM for the nursing domain. Finally, we explore the role of\nreasoning and multi-agent collaboration systems in nursing, highlighting their\npromise for future research and applications.", "AI": {"tldr": "Introduces NurseLLM, a nursing-specialized LLM for MCQ answering, trained on a newly built large-scale nursing MCQ dataset and evaluated on new nursing benchmarks; it outperforms comparable general and medical LLMs and explores reasoning and multi-agent collaboration for future work.", "motivation": "LLMs have advanced medical AI broadly, but nursing\u2014despite distinct knowledge and workflows\u2014lacks dedicated models, datasets, and benchmarks. Since MCQs are central to nursing education/evaluation, a specialized LLM and resources are needed to address this gap.", "method": "Build a multi-stage pipeline to generate a large-scale, topic-diverse nursing MCQ dataset; construct multiple nursing benchmarks; train/fine-tune NurseLLM; conduct extensive comparisons against state-of-the-art general-purpose and medical LLMs; explore reasoning strategies and multi-agent collaboration systems for nursing tasks.", "result": "Across introduced benchmarks, NurseLLM surpasses similarly sized state-of-the-art general and medical-specialized LLMs, indicating benefits from domain specialization.", "conclusion": "Specialized LLMs for nursing yield measurable gains on MCQs; the new dataset and benchmarks provide infrastructure for progress. Reasoning and multi-agent approaches appear promising, motivating further research and practical applications in nursing contexts."}}
{"id": "2510.07319", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07319", "abs": "https://arxiv.org/abs/2510.07319", "authors": ["Ci-Siang Lin", "Min-Hung Chen", "I-Jieh Liu", "Chien-Yi Wang", "Sifei Liu", "Yu-Chiang Frank Wang"], "title": "Temporal Prompting Matters: Rethinking Referring Video Object Segmentation", "comment": null, "summary": "Referring Video Object Segmentation (RVOS) aims to segment the object\nreferred to by the query sentence in the video. Most existing methods require\nend-to-end training with dense mask annotations, which could be\ncomputation-consuming and less scalable. In this work, we rethink the RVOS\nproblem and aim to investigate the key to this task. Based on existing\nfoundation segmentation models, we decompose the RVOS task into referring,\nvideo, and segmentation factors, and propose a Temporal Prompt Generation and\nSelection (Tenet) framework to address the referring and video factors while\nleaving the segmentation problem to foundation models. To efficiently adapt\nimage-based foundation segmentation models to referring video object\nsegmentation, we leverage off-the-shelf object detectors and trackers to\nproduce temporal prompts associated with the referring sentence. While\nhigh-quality temporal prompts could be produced, they can not be easily\nidentified from confidence scores. To tackle this issue, we propose Prompt\nPreference Learning to evaluate the quality of the produced temporal prompts.\nBy taking such prompts to instruct image-based foundation segmentation models,\nwe would be able to produce high-quality masks for the referred object,\nenabling efficient model adaptation to referring video object segmentation.\nExperiments on RVOS benchmarks demonstrate the effectiveness of the Tenet\nframework.", "AI": {"tldr": "Tenet reframes RVOS by offloading mask prediction to foundation segmentation models and focusing on generating and selecting high\u2011quality temporal prompts from detectors/trackers via preference learning, achieving strong results with far less dense-mask training.", "motivation": "End-to-end RVOS methods depend on costly dense mask annotations and heavy training, limiting scalability. The authors aim to exploit powerful image foundation segmentation models and reduce supervision/training by isolating the core challenges: linking text to objects over time and providing good guidance for segmentation.", "method": "Decompose RVOS into three factors: (1) referring (language grounding), (2) video (temporal association), and (3) segmentation. Use off-the-shelf object detectors and trackers to produce temporal prompts (object tubes) aligned with the query sentence. Recognizing that detector/track confidence is unreliable for prompt quality, introduce Prompt Preference Learning to assess and select the best temporal prompts. Feed the selected prompts to image-based foundation segmentation models to generate masks for the referred object, enabling efficient adaptation to RVOS without dense mask training.", "result": "On standard RVOS benchmarks, Tenet yields effective performance\u2014producing high-quality masks for referred objects\u2014while avoiding end-to-end dense-mask training. (Abstract reports empirical effectiveness but no specific metrics.)", "conclusion": "Carefully generated and preference-learned temporal prompts, combined with foundation segmentation models, provide a scalable and efficient solution to RVOS. Tenet demonstrates that handling referring and temporal factors well is sufficient to achieve strong RVOS performance without heavy annotation or training burdens."}}
{"id": "2510.07175", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07175", "abs": "https://arxiv.org/abs/2510.07175", "authors": ["Jongwook Han", "Woojung Song", "Jonggeun Lee", "Yohan Jo"], "title": "Quantifying Data Contamination in Psychometric Evaluations of LLMs", "comment": "12 pages, 1 figure", "summary": "Recent studies apply psychometric questionnaires to Large Language Models\n(LLMs) to assess high-level psychological constructs such as values,\npersonality, moral foundations, and dark traits. Although prior work has raised\nconcerns about possible data contamination from psychometric inventories, which\nmay threaten the reliability of such evaluations, there has been no systematic\nattempt to quantify the extent of this contamination. To address this gap, we\npropose a framework to systematically measure data contamination in\npsychometric evaluations of LLMs, evaluating three aspects: (1) item\nmemorization, (2) evaluation memorization, and (3) target score matching.\nApplying this framework to 21 models from major families and four widely used\npsychometric inventories, we provide evidence that popular inventories such as\nthe Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40)\nexhibit strong contamination, where models not only memorize items but can also\nadjust their responses to achieve specific target scores.", "AI": {"tldr": "They introduce a framework to quantify data contamination in LLM psychometric tests and show that commonly used inventories (e.g., BFI\u201144, PVQ\u201140) are heavily contaminated\u2014models memorize items and can tune answers to hit target scores.", "motivation": "Psychometric questionnaires are increasingly used to assess LLMs\u2019 values, personality, and other traits, but suspected training-data contamination may invalidate such evaluations. There was no systematic way to measure how much contamination exists.", "method": "Propose and apply a contamination-measurement framework along three axes: (1) item memorization, (2) evaluation memorization, and (3) target score matching. Evaluate 21 LLMs across major families on four widely used psychometric inventories.", "result": "Find strong contamination for popular inventories, notably BFI\u201144 and PVQ\u201140: models not only recall questionnaire items but also adjust responses to achieve specific target scores.", "conclusion": "Standard psychometric evaluations of LLMs are unreliable without addressing contamination. The framework enables diagnosing contamination and supports the need for decontaminated or adaptive inventories and stricter evaluation protocols."}}
{"id": "2510.07177", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07177", "abs": "https://arxiv.org/abs/2510.07177", "authors": ["Yong-En Tian", "Yu-Chien Tang", "An-Zi Yen", "Wen-Chih Peng"], "title": "CARPAS: Towards Content-Aware Refinement of Provided Aspects for Summarization in Large Language Models", "comment": "22 pages, 17 figures", "summary": "Aspect-based summarization has attracted significant attention for its\nability to generate more fine-grained and user-aligned summaries. While most\nexisting approaches assume a set of predefined aspects as input, real-world\nscenarios often present challenges where these given aspects may be incomplete,\nirrelevant, or entirely missing from the document. Users frequently expect\nsystems to adaptively refine or filter the provided aspects based on the actual\ncontent. In this paper, we initiate this novel task setting, termed\nContent-Aware Refinement of Provided Aspects for Summarization (CARPAS), with\nthe aim of dynamically adjusting the provided aspects based on the document\ncontext before summarizing. We construct three new datasets to facilitate our\npilot experiments, and by using LLMs with four representative prompting\nstrategies in this task, we find that LLMs tend to predict an overly\ncomprehensive set of aspects, which often results in excessively long and\nmisaligned summaries. Building on this observation, we propose a preliminary\nsubtask to predict the number of relevant aspects, and demonstrate that the\npredicted number can serve as effective guidance for the LLMs, reducing the\ninference difficulty, and enabling them to focus on the most pertinent aspects.\nOur extensive experiments show that the proposed approach significantly\nimproves performance across all datasets. Moreover, our deeper analyses uncover\nLLMs' compliance when the requested number of aspects differs from their own\nestimations, establishing a crucial insight for the deployment of LLMs in\nsimilar real-world applications.", "AI": {"tldr": "Introduces CARPAS, a task that refines user-provided aspects based on document content before summarization; shows LLMs over-predict aspects and proposes predicting the number of relevant aspects to guide LLMs, yielding consistent gains across three new datasets and insights on compliance to requested aspect counts.", "motivation": "Real-world aspect-based summarization often receives incomplete, irrelevant, or missing aspects. Users expect systems to filter/refine aspects to match the document, but current methods assume fixed, correct aspect sets.", "method": "Define the CARPAS setting; build three datasets; evaluate LLMs under four prompting strategies and observe over-comprehensive aspect selection; introduce a subtask to predict the number of relevant aspects and use this count to guide LLM aspect selection and summarization; analyze LLM compliance when the requested number differs from their internal estimates.", "result": "LLMs tend to over-predict aspects, producing long, misaligned summaries. Predicting and enforcing the number of relevant aspects effectively focuses generation, reduces inference difficulty, and significantly improves performance across all datasets. LLMs generally comply with externally requested aspect counts, even when conflicting with their estimates.", "conclusion": "Controlling aspect count is a simple, effective lever for CARPAS: number prediction guides LLMs to produce concise, relevant summaries and improves metrics. The compliance behavior offers a practical mechanism for deployment, suggesting count-based constraints can steer LLMs in real-world aspect-based summarization workflows."}}
{"id": "2510.07178", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07178", "abs": "https://arxiv.org/abs/2510.07178", "authors": ["Imry Ziv", "Nur Lan", "Emmanuel Chemla", "Roni Katzir"], "title": "Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish the Possible from the Impossible", "comment": "15 pages, 4 figures", "summary": "Are large language models (LLMs) sensitive to the distinction between humanly\npossible languages and humanly impossible languages? This question is taken by\nmany to bear on whether LLMs and humans share the same innate learning biases.\nPrevious work has attempted to answer it in the positive by comparing LLM\nlearning curves on existing language datasets and on \"impossible\" datasets\nderived from them via various perturbation functions. Using the same\nmethodology, we examine this claim on a wider set of languages and impossible\nperturbations. We find that in most cases, GPT-2 learns each language and its\nimpossible counterpart equally easily, in contrast to previous claims. We also\napply a more lenient condition by testing whether GPT-2 provides any kind of\nseparation between the whole set of natural languages and the whole set of\nimpossible languages. By considering cross-linguistic variance in various\nmetrics computed on the perplexity curves, we show that GPT-2 provides no\nsystematic separation between the possible and the impossible. Taken together,\nthese perspectives show that LLMs do not share the human innate biases that\nshape linguistic typology.", "AI": {"tldr": "Testing GPT-2 on natural vs. systematically perturbed (\"impossible\") languages, the authors find it learns both about equally well and shows no reliable separation\u2014contradicting claims that LLMs share human innate linguistic biases.", "motivation": "Assess whether LLMs reflect human-like innate constraints on learnable languages by checking sensitivity to the possible/impossible language distinction that shapes linguistic typology.", "method": "Replicate and broaden prior methodology: construct \"impossible\" datasets by perturbing multiple real languages; train/evaluate GPT-2 across languages; compare learning/perplexity curves and derived metrics; test both pairwise (each language vs. its perturbed counterpart) and global separation (all natural vs. all impossible).", "result": "Across most languages and perturbations, GPT-2 learns natural and impossible datasets equally easily; analyzing variance in perplexity-derived metrics shows no systematic separation between possible and impossible sets, contrary to earlier positive findings.", "conclusion": "Under this evaluation, GPT-2 does not exhibit the human innate learning biases implicated in linguistic typology; thus, LLM success does not imply human-like constraints on learnability."}}
{"id": "2510.07213", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07213", "abs": "https://arxiv.org/abs/2510.07213", "authors": ["Chengzhi Zhong", "Fei Cheng", "Qianying Liu", "Yugo Murawaki", "Chenhui Chu", "Sadao Kurohashi"], "title": "Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models", "comment": "Work in progress. Our code will be available at:\n  https://github.com/ku-nlp/language-specific-dimensions", "summary": "Large language models exhibit strong multilingual capabilities despite\nlimited exposure to non-English data. Prior studies show that English-centric\nlarge language models map multilingual content into English-aligned\nrepresentations at intermediate layers and then project them back into\ntarget-language token spaces in the final layer. From this observation, we\nhypothesize that this cross-lingual transition is governed by a small and\nsparse set of dimensions, which occur at consistent indices across the\nintermediate to final layers. Building on this insight, we introduce a simple,\ntraining-free method to identify and manipulate these dimensions, requiring\nonly as few as 50 sentences of either parallel or monolingual data. Experiments\non a multilingual generation control task reveal the interpretability of these\ndimensions, demonstrating that the interventions in these dimensions can switch\nthe output language while preserving semantic content, and that it surpasses\nthe performance of prior neuron-based approaches at a substantially lower cost.", "AI": {"tldr": "They find sparse, consistent latent dimensions that govern cross\u2011lingual transitions in LLMs and introduce a training\u2011free, small\u2011data method to identify and tweak these dimensions to switch output languages while keeping meaning, outperforming prior neuron\u2011based controls at lower cost.", "motivation": "Despite strong multilingual performance, English\u2011centric LLMs see little non\u2011English data. Prior work suggests they translate content into English\u2011aligned internal representations and then back to target languages. The authors aim to understand and control this mechanism in a simple, interpretable, and data\u2011efficient way.", "method": "Hypothesize that cross\u2011lingual transitions are controlled by a small, sparse set of dimensions that recur at consistent indices across intermediate-to-final layers. Propose a training\u2011free procedure that, using as few as ~50 sentences (parallel or monolingual), identifies these dimensions and intervenes on them to steer the model\u2019s output language while preserving semantics.", "result": "On multilingual generation control benchmarks, manipulating the identified dimensions reliably switches the output language without changing content. The approach outperforms previous neuron\u2011based methods and does so with substantially lower computational and data costs.", "conclusion": "Cross\u2011lingual behavior in LLMs is mediated by sparse, stable dimensions that can be cheaply located and controlled. This offers an interpretable and efficient mechanism for language control in multilingual generation, with better performance and lower cost than neuron\u2011level baselines."}}
{"id": "2510.07203", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07203", "abs": "https://arxiv.org/abs/2510.07203", "authors": ["Benjamin Akera", "Evelyn Nafula Ouma", "Gilbert Yiga", "Patrick Walukagga", "Phionah Natukunda", "Trevor Saaka", "Solomon Nsumba", "Lilian Teddy Nabukeera", "Joel Muhanguzi", "Imran Sekalala", "Nimpamya Janat Namara", "Engineer Bainomugisha", "Ernest Mwebaze", "John Quinn"], "title": "Sunflower: A New Approach To Expanding Coverage of African Languages in Large Language Models", "comment": null, "summary": "There are more than 2000 living languages in Africa, most of which have been\nbypassed by advances in language technology. Current leading LLMs exhibit\nstrong performance on a number of the most common languages (e.g. Swahili or\nYoruba), but prioritise support for the languages with the most speakers first,\nresulting in piecemeal ability across disparate languages. We contend that a\nregionally focussed approach is more efficient, and present a case study for\nUganda, a country with high linguistic diversity. We describe the development\nof Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the\nart comprehension in the majority of all Ugandan languages. These models are\nopen source and can be used to reduce language barriers in a number of\nimportant practical applications.", "AI": {"tldr": "They propose a regional strategy for African NLP and introduce Sunflower 14B/32B\u2014Qwen 3\u2013based, open-source LLMs\u2014achieving state-of-the-art comprehension across most Ugandan languages.", "motivation": "Africa has 2000+ languages, but most are overlooked by current LLMs, which focus on a few high-speaker languages (e.g., Swahili, Yoruba), leading to fragmented support. The authors aim to efficiently improve coverage for diverse, under-served languages, using Uganda as a case study.", "method": "Adopt a regionally focused approach: develop two models (Sunflower 14B and 32B) built on Qwen 3, tailored to Ugandan languages to enhance comprehension. Models are released openly.", "result": "Sunflower 14B/32B attain state-of-the-art comprehension for the majority of Ugandan languages, according to the authors.", "conclusion": "A country- or region-focused development strategy can rapidly elevate language technology for many under-served languages and help reduce language barriers in practical applications; open-sourcing enables broader use and impact."}}
{"id": "2510.07227", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07227", "abs": "https://arxiv.org/abs/2510.07227", "authors": ["Arjun Krishnakumar", "Rhea Sanjay Sukthanker", "Hannan Javed Mahadik", "Gabriela Kadlecov\u00e1", "Vladyslav Moroshan", "Timur Carstensen", "Frank Hutter", "Aaron Klein"], "title": "Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation", "comment": null, "summary": "Small Language models (SLMs) offer an efficient and accessible alternative to\nLarge Language Models (LLMs), delivering strong performance while using far\nfewer resources. We introduce a simple and effective framework for pretraining\nSLMs that brings together three complementary ideas. First, we identify\nstructurally sparse sub-network initializations that consistently outperform\nrandomly initialized models of similar size under the same compute budget.\nSecond, we use evolutionary search to automatically discover high-quality\nsub-network initializations, providing better starting points for pretraining.\nThird, we apply knowledge distillation from larger teacher models to speed up\ntraining and improve generalization. Together, these components make SLM\npretraining substantially more efficient: our best model, discovered using\nevolutionary search and initialized with LLM weights, matches the validation\nperplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining\ntokens. We release all code and models at\nhttps://github.com/whittle-org/whittle/, offering a practical and reproducible\npath toward cost-efficient small language model development at scale.", "AI": {"tldr": "A simple three-part pretraining recipe\u2014sparse subnetwork initialization, evolutionary search, and knowledge distillation\u2014lets small language models match Pythia-level perplexity using 9.2\u00d7 fewer pretraining tokens.", "motivation": "Make small language models more cost- and data-efficient so they can offer strong performance without the heavy compute and data demands of large language models.", "method": "(1) Identify structurally sparse sub-network initializations that outperform random inits under equal compute. (2) Use evolutionary search to automatically discover high-quality sparse initializations, often seeded from LLM weights. (3) Apply knowledge distillation from larger teacher models during pretraining to accelerate learning and improve generalization.", "result": "The best SLM found via evolutionary search and initialized with LLM weights matches the validation perplexity of a comparable Pythia SLM while using 9.2\u00d7 fewer pretraining tokens; code and models are released for reproducibility.", "conclusion": "Combining optimized sparse initializations, automated search, and distillation substantially improves SLM pretraining efficiency, providing a practical, reproducible path to high-quality small models at much lower data/compute cost."}}
{"id": "2510.07221", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07221", "abs": "https://arxiv.org/abs/2510.07221", "authors": ["Benjamin Akera", "Evelyn Nafula", "Patrick Walukagga", "Gilbert Yiga", "John Quinn", "Ernest Mwebaze"], "title": "How much speech data is necessary for ASR in African languages? An evaluation of data scaling in Kinyarwanda and Kikuyu", "comment": null, "summary": "The development of Automatic Speech Recognition (ASR) systems for\nlow-resource African languages remains challenging due to limited transcribed\nspeech data. While recent advances in large multilingual models like OpenAI's\nWhisper offer promising pathways for low-resource ASR development, critical\nquestions persist regarding practical deployment requirements. This paper\naddresses two fundamental concerns for practitioners: determining the minimum\ndata volumes needed for viable performance and characterizing the primary\nfailure modes that emerge in production systems. We evaluate Whisper's\nperformance through comprehensive experiments on two Bantu languages:\nsystematic data scaling analysis on Kinyarwanda using training sets from 1 to\n1,400 hours, and detailed error characterization on Kikuyu using 270 hours of\ntraining data. Our scaling experiments demonstrate that practical ASR\nperformance (WER < 13\\%) becomes achievable with as little as 50 hours of\ntraining data, with substantial improvements continuing through 200 hours (WER\n< 10\\%). Complementing these volume-focused findings, our error analysis\nreveals that data quality issues, particularly noisy ground truth\ntranscriptions, account for 38.6\\% of high-error cases, indicating that careful\ndata curation is as critical as data volume for robust system performance.\nThese results provide actionable benchmarks and deployment guidance for teams\ndeveloping ASR systems across similar low-resource language contexts. We\nrelease accompanying and models see\nhttps://github.com/SunbirdAI/kinyarwanda-whisper-eval", "AI": {"tldr": "Whisper-based ASR for low-resource African languages can reach practical accuracy with modest data, but label quality is a major bottleneck; careful curation rivals sheer volume in importance.", "motivation": "Low-resource African languages lack transcribed speech, hindering ASR deployment. Despite progress with large multilingual models (e.g., Whisper), practitioners still need concrete guidance on (1) how much data is minimally required and (2) what failure modes dominate in production.", "method": "Evaluate Whisper on two Bantu languages: (a) Kinyarwanda scaling study with training sets from 1 to 1,400 hours to map word error rate (WER) vs. data volume; (b) Kikuyu error characterization using 270 training hours to attribute high-error cases to specific causes, notably transcription noise.", "result": "For Kinyarwanda, practical ASR performance is attainable with ~50 hours (WER < 13%), improving further to <10% WER around 200 hours; gains continue with more data. For Kikuyu, 38.6% of high-error cases stem from noisy ground-truth transcriptions, highlighting data quality issues as a primary error source.", "conclusion": "Actionable benchmarks: ~50 hours yields usable systems; ~200 hours approaches strong performance. However, label noise significantly degrades outcomes, so rigorous data curation is as critical as increasing data volume. Released models/resources provide a reference for similar low-resource deployments."}}
{"id": "2510.07231", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07231", "abs": "https://arxiv.org/abs/2510.07231", "authors": ["Donggyu Lee", "Sungwon Park", "Yerin Hwang", "Hyunwoo Oh", "Hyoshin Kim", "Jungwon Kim", "Meeyoung Cha", "Sangyoon Park", "Jihee Kim"], "title": "Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships", "comment": null, "summary": "Causal reasoning is fundamental for Large Language Models (LLMs) to\nunderstand genuine cause-and-effect relationships beyond pattern matching.\nExisting benchmarks suffer from critical limitations such as reliance on\nsynthetic data and narrow domain coverage. We introduce a novel benchmark\nconstructed from casually identified relationships extracted from top-tier\neconomics and finance journals, drawing on rigorous methodologies including\ninstrumental variables, difference-in-differences, and regression discontinuity\ndesigns. Our benchmark comprises 40,379 evaluation items covering five task\ntypes across domains such as health, environment, technology, law, and culture.\nExperimental results on eight state-of-the-art LLMs reveal substantial\nlimitations, with the best model achieving only 57.6\\% accuracy. Moreover,\nmodel scale does not consistently translate to superior performance, and even\nadvanced reasoning models struggle with fundamental causal relationship\nidentification. These findings underscore a critical gap between current LLM\ncapabilities and demands of reliable causal reasoning in high-stakes\napplications.", "AI": {"tldr": "Introduces a large, real\u2011world causal reasoning benchmark derived from rigorously identified causal studies (IV, DiD, RDD) across multiple domains; eight SOTA LLMs perform poorly (best 57.6% accuracy), showing that model scale and \u201creasoning\u201d branding don\u2019t ensure causal understanding, highlighting a sizable gap for high\u2011stakes use.", "motivation": "Causal reasoning is crucial for trustworthy decisions, but existing LLM benchmarks rely heavily on synthetic data and narrow domains, limiting ecological validity and masking true capabilities. The paper seeks a more realistic, rigorous testbed grounded in established causal identification methods.", "method": "Curate causally identified relationships from top-tier economics and finance journals that use instrumental variables, difference-in-differences, and regression discontinuity designs; build a 40,379\u2011item benchmark spanning five task types and domains including health, environment, technology, law, and culture; evaluate eight state-of-the-art LLMs on these tasks.", "result": "Across models, performance is weak; the best model achieves only 57.6% accuracy. Larger models are not consistently better, and models marketed for reasoning still falter on basic causal relationship identification.", "conclusion": "There is a significant mismatch between current LLM capabilities and the demands of reliable causal reasoning in real-world, high-stakes settings. Further research, training strategies, and evaluation methods are needed before LLMs can be trusted for causal inference tasks."}}
{"id": "2510.07243", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07243", "abs": "https://arxiv.org/abs/2510.07243", "authors": ["Joseph Enguehard", "Morgane Van Ermengem", "Kate Atkinson", "Sujeong Cha", "Arijit Ghosh Chowdhury", "Prashanth Kallur Ramaswamy", "Jeremy Roghair", "Hannah R Marlowe", "Carina Suzana Negreanu", "Kitty Boxall", "Diana Mincu"], "title": "LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation", "comment": "Published in Natural Legal Language Processing - EMNLP Workshop 2025", "summary": "Evaluating large language model (LLM) outputs in the legal domain presents\nunique challenges due to the complex and nuanced nature of legal analysis.\nCurrent evaluation approaches either depend on reference data, which is costly\nto produce, or use standardized assessment methods, both of which have\nsignificant limitations for legal applications.\n  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its\nreliability and effectiveness in legal contexts depend heavily on evaluation\nprocesses unique to the legal industry and how trustworthy the evaluation\nappears to the human legal expert. This is where existing evaluation methods\ncurrently fail and exhibit considerable variability.\n  This paper aims to close the gap: a) we break down lengthy responses into\n'Legal Data Points' (LDPs), self-contained units of information, and introduce\na novel, reference-free evaluation methodology that reflects how lawyers\nevaluate legal answers; b) we demonstrate that our method outperforms a variety\nof baselines on both our proprietary dataset and an open-source dataset\n(LegalBench); c) we show how our method correlates more closely with human\nexpert evaluations and helps improve inter-annotator agreement; and finally d)\nwe open source our Legal Data Points for a subset of LegalBench used in our\nexperiments, allowing the research community to replicate our results and\nadvance research in this vital area of LLM evaluation on legal\nquestion-answering.", "AI": {"tldr": "Proposes a reference-free, lawyer-aligned evaluation of legal LLM answers by decomposing outputs into \u201cLegal Data Points\u201d (LDPs); shows higher agreement with experts and better performance than baselines on proprietary data and LegalBench; releases LDPs for part of LegalBench.", "motivation": "Legal LLM outputs are hard to evaluate: reference sets are costly, standardized rubrics miss legal nuance, and generic LLM-as-a-Judge is unreliable/trustworthiness-sensitive in legal settings. A domain-specific, credible, and scalable evaluation is needed.", "method": "Break long answers into self-contained Legal Data Points (LDPs) and evaluate these atomic units via a novel, reference-free procedure designed to mirror how lawyers assess arguments. Compare against multiple baselines and measure correlation with expert judgments and inter-annotator agreement. Release LDP annotations for a subset of LegalBench.", "result": "Their LDP-based method outperforms several baselines on both a proprietary dataset and LegalBench, aligns more closely with human expert evaluations, and improves inter-annotator agreement. They open-source LDPs for the LegalBench subset used.", "conclusion": "Decomposing legal answers into LDPs enables a more reliable, trustworthy, and scalable evaluation for legal QA, better matching expert assessment. The released resources support reproducibility and future research in legal LLM evaluation."}}
{"id": "2510.07230", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07230", "abs": "https://arxiv.org/abs/2510.07230", "authors": ["Ziyi Wang", "Yuxuan Lu", "Yimeng Zhang", "Jing Huang", "Dakuo Wang"], "title": "Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping", "comment": null, "summary": "Simulating step-wise human behavior with Large Language Models (LLMs) has\nbecome an emerging research direction, enabling applications in various\npractical domains. While prior methods, including prompting, supervised\nfine-tuning (SFT), and reinforcement learning (RL), have shown promise in\nmodeling step-wise behavior, they primarily learn a population-level policy\nwithout conditioning on a user's persona, yielding generic rather than\npersonalized simulations. In this work, we pose a critical question: how can\nLLM agents better simulate personalized user behavior? We introduce\nCustomer-R1, an RL-based method for personalized, step-wise user behavior\nsimulation in online shopping environments. Our policy is conditioned on an\nexplicit persona, and we optimize next-step rationale and action generation via\naction correctness reward signals. Experiments on the OPeRA dataset emonstrate\nthat Customer-R1 not only significantly outperforms prompting and SFT-based\nbaselines in next-action prediction tasks, but also better matches users'\naction distribution, indicating higher fidelity in personalized behavior\nsimulation.", "AI": {"tldr": "Customer-R1 is an RL-based, persona-conditioned LLM policy that generates step-wise rationales and actions for online shopping, optimizing with action-correctness rewards; it outperforms prompting and SFT and better matches user action distributions on OPeRA.", "motivation": "Existing step-wise behavior simulators learn population-level policies and ignore user-specific personas, leading to generic, less faithful simulations. The goal is to achieve personalized, higher-fidelity user behavior modeling.", "method": "Condition the LLM policy on an explicit user persona and train with reinforcement learning. Optimize both next-step rationale and action generation using action correctness as the reward signal in an online shopping setting; evaluate against prompting and supervised fine-tuning baselines on the OPeRA dataset.", "result": "Customer-R1 significantly improves next-action prediction accuracy over prompting and SFT baselines and better aligns with users\u2019 empirical action distributions, indicating more faithful personalized behavior simulation.", "conclusion": "Persona-conditioned RL that jointly optimizes rationales and actions enhances personalized, step-wise user simulations in e-commerce, offering higher fidelity than non-personalized prompting/SFT approaches."}}
{"id": "2510.07284", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07284", "abs": "https://arxiv.org/abs/2510.07284", "authors": ["MohammadHossein Rezaei", "Robert Vacareanu", "Zihao Wang", "Clinton Wang", "Yunzhong He", "Afra Feyza Aky\u00fcrek"], "title": "Online Rubrics Elicitation from Pairwise Comparisons", "comment": null, "summary": "Rubrics provide a flexible way to train LLMs on open-ended long-form answers\nwhere verifiable rewards are not applicable and human preferences provide\ncoarse signals. Prior work shows that reinforcement learning with rubric-based\nrewards leads to consistent gains in LLM post-training. Most existing\napproaches rely on rubrics that remain static over the course of training. Such\nstatic rubrics, however, are vulnerable to reward-hacking type behaviors and\nfail to capture emergent desiderata that arise during training. We introduce\nOnline Rubrics Elicitation (OnlineRubrics), a method that dynamically curates\nevaluation criteria in an online manner through pairwise comparisons of\nresponses from current and reference policies. This online process enables\ncontinuous identification and mitigation of errors as training proceeds.\nEmpirically, this approach yields consistent improvements of up to 8% over\ntraining exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as\nwell as the validation sets of expert questions and rubrics. We qualitatively\nanalyze the elicited criteria and identify prominent themes such as\ntransparency, practicality, organization, and reasoning.", "AI": {"tldr": "They propose OnlineRubrics, which updates evaluation rubrics on-the-fly using pairwise comparisons between a current and a reference policy, reducing reward hacking and improving long-form LLM post-training by up to ~8% over static rubrics.", "motivation": "Static, hand-crafted rubrics used for RL post-training of LLMs on open-ended tasks give coarse, fixed signals that can be gamed and cannot reflect new failure modes that emerge during training. A dynamic, adaptive signal is needed when verifiable rewards are unavailable.", "method": "Online Rubrics Elicitation: iteratively compare outputs from the current policy to a reference policy; from these pairwise judgments, elicit or refine rubric criteria and use them to shape rewards during RL. The process continually identifies new errors and updates evaluation criteria, closing the loop between model behavior and reward design.", "result": "Across benchmarks (AlpacaEval, GPQA, ArenaHard) and expert-validated sets, the online approach yields consistent gains up to 8% versus training with static rubrics. Qualitative analysis shows learned criteria emphasize transparency, practicality, organization, and reasoning.", "conclusion": "Dynamic, online-elicited rubrics offer a more robust and adaptive reward signal than static rubrics for long-form, open-ended LLM training, mitigating reward hacking and capturing emergent desiderata, leading to measurable performance improvements."}}
{"id": "2510.07315", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07315", "abs": "https://arxiv.org/abs/2510.07315", "authors": ["Ming Zhong", "Xiang Zhou", "Ting-Yun Chang", "Qingze Wang", "Nan Xu", "Xiance Si", "Dan Garrette", "Shyam Upadhyay", "Jeremiah Liu", "Jiawei Han", "Benoit Schillings", "Jiao Sun"], "title": "Vibe Checker: Aligning Code Evaluation with Human Preference", "comment": "Preprint", "summary": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage\nLLMs to generate and iteratively refine code through natural language\ninteractions until it passes their vibe check. Vibe check is tied to real-world\nhuman preference and goes beyond functionality: the solution should feel right,\nread cleanly, preserve intent, and remain correct. However, current code\nevaluation remains anchored to pass@k and captures only functional correctness,\noverlooking the non-functional instructions that users routinely apply. In this\npaper, we hypothesize that instruction following is the missing piece\nunderlying vibe check that represents human preference in coding besides\nfunctional correctness. To quantify models' code instruction following\ncapabilities with measurable signals, we present VeriCode, a taxonomy of 30\nverifiable code instructions together with corresponding deterministic\nverifiers. We use the taxonomy to augment established evaluation suites,\nresulting in Vibe Checker, a testbed to assess both code instruction following\nand functional correctness. Upon evaluating 31 leading LLMs, we show that even\nthe strongest models struggle to comply with multiple instructions and exhibit\nclear functional regression. Most importantly, a composite score of functional\ncorrectness and instruction following correlates the best with human\npreference, with the latter emerging as the primary differentiator on\nreal-world programming tasks. Our work identifies core factors of the vibe\ncheck, providing a concrete path for benchmarking and developing models that\nbetter align with user preferences in coding.", "AI": {"tldr": "They argue that instruction following\u2014not just functional correctness\u2014is central to users\u2019 \u201cvibe check\u201d for code. They introduce VeriCode (30 verifiable code-instruction types with deterministic checkers) and Vibe Checker (evaluation suite augmenting standard code benchmarks). Across 31 LLMs, instruction adherence is weak\u2014especially under multi-instruction prompts\u2014and a composite metric (functionality + instruction following) best aligns with human preference.", "motivation": "Current code benchmarks (e.g., pass@k) focus on functional correctness only, while real users care about non-functional qualities such as style, readability, intent preservation, and constraint adherence. The authors posit that measurable instruction following is the missing ingredient representing human preference in coding tasks.", "method": "Define a taxonomy of 30 verifiable code instructions and build deterministic verifiers for each. Augment existing code evaluation suites with these verifiers to create Vibe Checker. Evaluate 31 LLMs, measuring both functional correctness and instruction adherence, analyze multi-instruction compliance, and correlate metrics with human preference data.", "result": "Even top models struggle to follow multiple instructions and show functional regression. A composite score combining functional correctness and instruction adherence correlates best with human preference, with instruction following emerging as the key differentiator on real-world coding tasks.", "conclusion": "Instruction following is a core component of the coding \u201cvibe check.\u201d VeriCode and Vibe Checker provide concrete, automatable signals to benchmark and improve LLMs toward user-aligned coding behavior, suggesting the community should move beyond pass@k-only evaluation."}}
{"id": "2510.07233", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07233", "abs": "https://arxiv.org/abs/2510.07233", "authors": ["Zhivar Sourati", "Zheng Wang", "Marianne Menglin Liu", "Yazhe Hu", "Mengqing Guo", "Sujeeth Bharadwaj", "Kyu Han", "Tao Sheng", "Sujith Ravi", "Morteza Dehghani", "Dan Roth"], "title": "LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding", "comment": null, "summary": "Question answering over visually rich documents (VRDs) requires reasoning not\nonly over isolated content but also over documents' structural organization and\ncross-page dependencies. However, conventional retrieval-augmented generation\n(RAG) methods encode content in isolated chunks during ingestion, losing\nstructural and cross-page dependencies, and retrieve a fixed number of pages at\ninference, regardless of the specific demands of the question or context. This\noften results in incomplete evidence retrieval and degraded answer quality for\nmulti-page reasoning tasks. To address these limitations, we propose LAD-RAG, a\nnovel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs\na symbolic document graph that captures layout structure and cross-page\ndependencies, adding it alongside standard neural embeddings to yield a more\nholistic representation of the document. During inference, an LLM agent\ndynamically interacts with the neural and symbolic indices to adaptively\nretrieve the necessary evidence based on the query. Experiments on\nMMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG\nimproves retrieval, achieving over 90% perfect recall on average without any\ntop-k tuning, and outperforming baseline retrievers by up to 20% in recall at\ncomparable noise levels, yielding higher QA accuracy with minimal latency.", "AI": {"tldr": "LAD-RAG is a layout-aware, dynamic RAG framework for QA over visually rich, multi-page documents; it builds a symbolic layout graph alongside neural embeddings and uses an LLM agent to adapt retrieval to the query, delivering >90% perfect recall and higher QA accuracy with minimal latency across benchmarks.", "motivation": "QA over visually rich documents demands reasoning over layout structure and cross-page dependencies. Conventional RAG chunks content independently and uses fixed top-k retrieval, which breaks structural links and often misses necessary evidence for multi-page reasoning.", "method": "During ingestion, LAD-RAG constructs a symbolic document graph that encodes layout structure and cross-page dependencies and stores it with standard neural embeddings to form a hybrid index. During inference, an LLM agent dynamically interacts with the symbolic and neural indices to adaptively retrieve only the evidence needed for the query, rather than using a fixed top-k.", "result": "Across MMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA, LAD-RAG achieves over 90% perfect recall on average without top-k tuning, improves recall by up to 20% at comparable noise versus baseline retrievers, and yields higher QA accuracy with minimal latency.", "conclusion": "Integrating document layout structure with dynamic, query-contingent retrieval substantially improves evidence coverage and QA performance for multi-page, visually rich documents without tuning overhead, positioning LAD-RAG as an efficient and effective approach for VRD QA."}}
{"id": "2510.07318", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07318", "abs": "https://arxiv.org/abs/2510.07318", "authors": ["Yunhao Fang", "Weihao Yu", "Shu Zhong", "Qinghao Ye", "Xuehan Xiong", "Lai Wei"], "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling", "comment": "Code: https://github.com/ByteDance-Seed/AHN", "summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.", "AI": {"tldr": "Hybrid memory for long-context Transformers: keep a lossless sliding KV window as short-term memory and compress older context into a fixed-size, learnable long-term memory via an Artificial Hippocampus Network (AHN), yielding near/full-attention quality with much lower compute and memory.", "motivation": "Long-sequence modeling faces a trade-off: RNN-like models are efficient but lossy with fixed-size memory, while Transformer attention is lossless but costly as context grows. Inspired by the Multi-Store Model of human memory, the work aims to combine fidelity and efficiency for scalable long-context inference.", "method": "Maintain the Transformer's sliding-window KV cache as lossless short-term memory. Introduce a learnable module, the Artificial Hippocampus Network (AHN), that recurrently compresses out-of-window tokens into a fixed-size long-term memory. AHN is instantiated with modern RNN-like architectures (Mamba2, DeltaNet, Gated DeltaNet).", "result": "Across LV-Eval and InfiniteBench long-context benchmarks, AHN-augmented models consistently outperform sliding-window baselines and match or surpass full attention while reducing costs. Example: augmenting Qwen2.5-3B-Instruct with AHN cuts inference FLOPs by 40.5% and KV cache memory by 74.0%, and improves LV-Eval (128k) score from 4.41 to 5.88.", "conclusion": "A two-store memory framework effectively reconciles efficiency and fidelity in long-sequence modeling. By pairing a lossless short-term KV window with a compact learnable long-term memory, models achieve strong long-context performance with substantially lower compute and memory overhead. Code is released for reproducibility."}}
{"id": "2510.07238", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07238", "abs": "https://arxiv.org/abs/2510.07238", "authors": ["Xunyi Jiang", "Dingyi Chang", "Julian McAuley", "Xin Xu"], "title": "When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation", "comment": null, "summary": "The rapid evolution of large language models (LLMs) and the real world has\noutpaced the static nature of widely used evaluation benchmarks, raising\nconcerns about their reliability for evaluating LLM factuality. While\nsubstantial works continue to rely on the popular but old benchmarks, their\ntemporal misalignment with real-world facts and modern LLMs, and their effects\non LLM factuality evaluation remain underexplored. Therefore, in this work, we\npresent a systematic investigation of this issue by examining five popular\nfactuality benchmarks and eight LLMs released across different years. An\nup-to-date fact retrieval pipeline and three metrics are tailored to quantify\nbenchmark aging and its impact on LLM factuality evaluation. Experimental\nresults and analysis illustrate that a considerable portion of samples in the\nwidely used factuality benchmarks are outdated, leading to unreliable\nassessments of LLM factuality. We hope our work can provide a testbed to assess\nthe reliability of a benchmark for LLM factuality evaluation and inspire more\nresearch on the benchmark aging issue. Codes are available in\nhttps://github.com/JiangXunyi/BenchAge.", "AI": {"tldr": "Many widely used LLM factuality benchmarks have aged and contain outdated facts, making current evaluations unreliable. The authors quantify this aging across five benchmarks and show its impact on assessing eight LLMs, offering a retrieval pipeline, metrics, and a testbed (BenchAge).", "motivation": "LLMs and real\u2011world facts evolve quickly, but factuality benchmarks are static and old. Continued reliance on them risks misleading conclusions about model factuality. The field lacks a systematic way to measure how benchmark staleness affects evaluation outcomes.", "method": "They analyze five popular factuality benchmarks and eight LLMs released over multiple years. They build an up\u2011to\u2011date fact retrieval pipeline to verify each benchmark item against current sources and design three metrics to quantify (1) how outdated items are, and (2) how aging alters factuality scores and model rankings. They run experiments to compare evaluations with and without aged items and conduct error analyses.", "result": "A substantial fraction of benchmark items are outdated. This staleness meaningfully distorts factuality assessments and can change model performance estimates and comparative rankings. The authors provide reproducible code and a testbed (BenchAge) to measure benchmark aging and its effects.", "conclusion": "Benchmark aging is prevalent and undermines the reliability of LLM factuality evaluation. The provided pipeline and metrics enable auditing and updating benchmarks. The community should maintain or redesign benchmarks to be time\u2011aware or refreshable to ensure trustworthy factuality assessment."}}
{"id": "2510.07239", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07239", "abs": "https://arxiv.org/abs/2510.07239", "authors": ["Christos Ziakas", "Nicholas Loo", "Nishita Jain", "Alessandra Russo"], "title": "Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts", "comment": null, "summary": "Automated red-teaming has emerged as a scalable approach for auditing Large\nLanguage Models (LLMs) prior to deployment, yet existing approaches lack\nmechanisms to efficiently adapt to model-specific vulnerabilities at inference.\nWe introduce Red-Bandit, a red-teaming framework that adapts online to identify\nand exploit model failure modes under distinct attack styles (e.g.,\nmanipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA\nexperts, each specialized for a particular attack style, using reinforcement\nlearning that rewards the generation of unsafe prompts via a rule-based safety\nmodel. At inference, a multi-armed bandit policy dynamically selects among\nthese attack-style experts based on the target model's response safety,\nbalancing exploration and exploitation. Red-Bandit achieves state-of-the-art\nresults on AdvBench under sufficient exploration (ASR@10), while producing more\nhuman-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy\nserves as a diagnostic tool for uncovering model-specific vulnerabilities by\nindicating which attack styles most effectively elicit unsafe behaviors.", "AI": {"tldr": "Red-Bandit is an adaptive automated red-teaming framework that trains attack-style LoRA experts with RL and uses a multi-armed bandit at inference to target model-specific vulnerabilities, achieving SOTA attack success while keeping prompts readable.", "motivation": "Automated red-teaming often fails to adapt in real time to a target model\u2019s idiosyncratic weaknesses. The authors aim to make auditing scalable and more effective by dynamically discovering which attack styles best elicit unsafe behavior from different LLMs.", "method": "Post-train multiple parameter-efficient LoRA experts, each dedicated to an attack style (e.g., manipulation, slang). Use reinforcement learning with rewards from a rule-based safety model to encourage generating prompts that induce unsafe responses. At inference, deploy a multi-armed bandit policy to select among experts based on observed response safety, balancing exploration and exploitation and providing signals about vulnerable styles.", "result": "On AdvBench, the approach reaches state-of-the-art ASR@10 under sufficient exploration and produces more human-readable prompts (lower perplexity). The bandit\u2019s arm-selection patterns also reveal which attack styles most effectively induce unsafe responses for a given target model.", "conclusion": "An online-adaptive red-teaming pipeline that is effective and interpretable, improving attack success and readability and offering diagnostic insights into model-specific weaknesses. Its performance depends on exploration strategy and the accuracy of the rule-based safety reward."}}
{"id": "2510.07242", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07242", "abs": "https://arxiv.org/abs/2510.07242", "authors": ["Leitian Tao", "Ilia Kulikov", "Swarnadeep Saha", "Tianlu Wang", "Jing Xu", "Yixuan Li", "Jason E Weston", "Ping Yu"], "title": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense", "comment": "20 pages", "summary": "Post-training for reasoning of large language models (LLMs) increasingly\nrelies on verifiable rewards: deterministic checkers that provide 0-1\ncorrectness signals. While reliable, such binary feedback is brittle--many\ntasks admit partially correct or alternative answers that verifiers\nunder-credit, and the resulting all-or-nothing supervision limits learning.\nReward models offer richer, continuous feedback, which can serve as a\ncomplementary supervisory signal to verifiers. We introduce HERO (Hybrid\nEnsemble Reward Optimization), a reinforcement learning framework that\nintegrates verifier signals with reward-model scores in a structured way. HERO\nemploys stratified normalization to bound reward-model scores within\nverifier-defined groups, preserving correctness while refining quality\ndistinctions, and variance-aware weighting to emphasize challenging prompts\nwhere dense signals matter most. Across diverse mathematical reasoning\nbenchmarks, HERO consistently outperforms RM-only and verifier-only baselines,\nwith strong gains on both verifiable and hard-to-verify tasks. Our results show\nthat hybrid reward design retains the stability of verifiers while leveraging\nthe nuance of reward models to advance reasoning.", "AI": {"tldr": "HERO is a reinforcement-learning framework that blends binary verifier feedback with continuous reward-model scores using stratified normalization and variance-aware weighting, yielding more informative yet stable rewards and improving LLM mathematical reasoning over RM-only and verifier-only baselines.", "motivation": "Binary verifiers provide reliable but brittle 0/1 signals that under-credit partially correct or alternative answers, limiting learning. Reward models offer nuanced, continuous feedback but can be noisy. A hybrid approach aims to preserve verifier correctness while exploiting reward-model granularity.", "method": "Integrate verifier signals with reward-model scores in RL: (1) stratified normalization that bounds and calibrates reward-model scores within groups defined by verifier outcomes, maintaining correctness constraints while differentiating quality; (2) variance-aware weighting that emphasizes prompts where reward signals are especially informative (high variance or difficulty). Applied to mathematical reasoning tasks.", "result": "Across diverse math reasoning benchmarks, HERO consistently surpasses both reward-model-only and verifier-only training, with notable gains on tasks that are fully verifiable and those that are hard to verify.", "conclusion": "Hybrid reward design combining verifiers with calibrated reward-model signals can retain the stability of verifiers while leveraging nuanced feedback, advancing LLM reasoning performance."}}
{"id": "2510.07248", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07248", "abs": "https://arxiv.org/abs/2510.07248", "authors": ["Jonggeun Lee", "Woojung Song", "Jongwook Han", "Haesung Pyun", "Yohan Jo"], "title": "Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models", "comment": "15 pages, 4 figures", "summary": "Small language models (SLMs) offer significant computational advantages for\ntool-augmented AI systems, yet they struggle with tool-use tasks, particularly\nin selecting appropriate tools and identifying correct parameters. A common\nfailure mode is schema misalignment: models hallucinate plausible but\nnon-existent tool names that reflect naming conventions internalized during\npretraining but absent from the provided tool schema. Rather than forcing\nmodels to adapt to arbitrary schemas, we propose adapting schemas to align with\nmodels' pretrained knowledge. We introduce PA-Tool (Pretraining-Aligned Tool\nSchema Generation), a training-free method that leverages peakedness-a signal\nfrom contamination detection indicating pretraining familiarity-to\nautomatically rename tool components. By generating multiple candidates and\nselecting those with highest output concentration across samples, PA-Tool\nidentifies pretrain-aligned naming patterns. Experiments on MetaTool and\nRoTBench show improvements of up to 17% points, with schema misalignment errors\nreduced by 80%. PA-Tool enables small models to approach state-of-the-art\nperformance while maintaining computational efficiency for adaptation to new\ntools without retraining. Our work demonstrates that schema-level interventions\ncan unlock the tool-use potential of resource-efficient models by adapting\nschemas to models rather than models to schemas.", "AI": {"tldr": "PA-Tool is a training-free method that renames tool schemas to match small language models\u2019 pretraining-induced naming expectations, sharply improving tool-use without retraining.", "motivation": "SLMs are attractive for tool-augmented systems due to efficiency, but they frequently fail by hallucinating plausible yet nonexistent tool/parameter names\u2014a schema misalignment rooted in pretraining priors. Rather than retraining models to arbitrary schemas, adapt the schemas to the models\u2019 existing knowledge.", "method": "Use a contamination-detection signal (\u201cpeakedness\u201d) as a proxy for model familiarity with candidate names. Generate multiple renaming candidates for tool components (tools, functions, parameters) and select those with highest output concentration across samples, yielding a pretraining-aligned schema. No finetuning; works as a preprocessing layer. Evaluated on MetaTool and RoTBench.", "result": "Up to +17 percentage points overall improvement and ~80% reduction in schema-misalignment errors. SLMs approach SOTA tool-use performance while maintaining computational efficiency and requiring no retraining for new tools.", "conclusion": "Schema-level interventions\u2014specifically aligning tool names/parameters to models\u2019 pretraining priors\u2014can unlock SLM tool-use capabilities. PA-Tool provides a practical, training-free path to better tool selection/parameterization and faster adaptation to new tools."}}
{"id": "2510.07290", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07290", "abs": "https://arxiv.org/abs/2510.07290", "authors": ["Guangliang Liu", "Haitao Mao", "Bochuan Cao", "Zhiyu Xue", "Xitong Zhang", "Rongrong Wang", "Kristen Marie Johnson"], "title": "On the Convergence of Moral Self-Correction in Large Language Models", "comment": "19pages, 7 figures", "summary": "Large Language Models (LLMs) are able to improve their responses when\ninstructed to do so, a capability known as self-correction. When instructions\nprovide only a general and abstract goal without specific details about\npotential issues in the response, LLMs must rely on their internal knowledge to\nimprove response quality, a process referred to as intrinsic self-correction.\nThe empirical success of intrinsic self-correction is evident in various\napplications, but how and why it is effective remains unknown. Focusing on\nmoral self-correction in LLMs, we reveal a key characteristic of intrinsic\nself-correction: performance convergence through multi-round interactions; and\nprovide a mechanistic analysis of this convergence behavior. Based on our\nexperimental results and analysis, we uncover the underlying mechanism of\nconvergence: consistently injected self-correction instructions activate moral\nconcepts that reduce model uncertainty, leading to converged performance as the\nactivated moral concepts stabilize over successive rounds. This paper\ndemonstrates the strong potential of moral self-correction by showing that it\nexhibits a desirable property of converged performance.", "AI": {"tldr": "Intrinsic moral self-correction in LLMs converges over multiple rounds because repeated self-correction instructions activate stable moral concepts that reduce uncertainty.", "motivation": "Despite widespread use of self-correction prompts, it is unclear how and why LLMs improve without explicit error feedback. The paper seeks a mechanistic account, focusing on the moral domain where normative concepts may guide revisions.", "method": "Study multi-round self-correction on moral tasks. Empirically track performance across rounds and analyze internal behavior to explain convergence, arguing that repeated generic correction instructions activate moral concepts that persist and reduce uncertainty.", "result": "Performance improves and then stabilizes (converges) over successive self-correction rounds. Analysis links this to the sustained activation of moral concepts and decreased predictive uncertainty as rounds progress.", "conclusion": "Intrinsic moral self-correction exhibits a desirable convergence property: repeated generic correction prompts prime stable moral concepts, lowering uncertainty and yielding consistent performance. This suggests designing prompts to activate relevant concepts can systematically enhance reliability."}}
{"id": "2510.07300", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07300", "abs": "https://arxiv.org/abs/2510.07300", "authors": ["Xue Zhang", "Yunlong Liang", "Fandong Meng", "Songming Zhang", "Kaiyu Huang", "Yufeng Chen", "Jinan Xu", "Jie Zhou"], "title": "Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning", "comment": "13 pages, 8 tables, 4 figures", "summary": "Large Reasoning Models (LRMs) have achieved remarkable performance on complex\nreasoning tasks by adopting the \"think-then-answer\" paradigm, which enhances\nboth accuracy and interpretability. However, current LRMs exhibit two critical\nlimitations when processing non-English languages: (1) They often struggle to\nmaintain input-output language consistency; (2) They generally perform poorly\nwith wrong reasoning paths and lower answer accuracy compared to English. These\nlimitations significantly degrade the user experience for non-English speakers\nand hinder the global deployment of LRMs. To address these limitations, we\npropose M-Thinker, which is trained by the GRPO algorithm that involves a\nLanguage Consistency (LC) reward and a novel Cross-lingual Thinking Alignment\n(CTA) reward. Specifically, the LC reward defines a strict constraint on the\nlanguage consistency between the input, thought, and answer. Besides, the CTA\nreward compares the model's non-English reasoning paths with its English\nreasoning path to transfer its own reasoning capability from English to\nnon-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B\nmodels not only achieve nearly 100% language consistency and superior\nperformance on two multilingual benchmarks (MMATH and PolyMath), but also\nexhibit excellent generalization on out-of-domain languages.", "AI": {"tldr": "They introduce M-Thinker, a multilingual reasoning model trained with RL that enforces strict language consistency and aligns non\u2011English reasoning paths with English ones, yielding near\u2011perfect consistency and improved accuracy on multilingual math benchmarks.", "motivation": "Current Large Reasoning Models degrade on non\u2011English inputs: they mix languages between input, thoughts, and answers, and they reason less accurately than in English\u2014hurting user experience and global deployment.", "method": "Train with GRPO using two rewards: (1) Language Consistency (LC) to strictly enforce the same language across input, chain\u2011of\u2011thought, and final answer; (2) Cross\u2011lingual Thinking Alignment (CTA) that compares a sample\u2019s non\u2011English reasoning path to its English counterpart to transfer reasoning competence. Apply iterative RL to 1.5B and 7B models (M\u2011Thinker).", "result": "M\u2011Thinker\u20111.5B/7B achieve nearly 100% language consistency, outperform baselines on MMATH and PolyMath, and generalize well to out\u2011of\u2011domain languages.", "conclusion": "Cross\u2011lingual alignment plus language\u2011consistency rewards effectively transfer strong English reasoning to other languages, enabling multilingual, think\u2011then\u2011answer models with higher accuracy and consistent outputs."}}
{"id": "2510.07309", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07309", "abs": "https://arxiv.org/abs/2510.07309", "authors": ["Yue Li", "Ran Tao", "Derek Hommel", "Yusuf Denizay D\u00f6nder", "Sungyong Chang", "David Mimno", "Unso Eun Seo Jo"], "title": "Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain", "comment": "20 pages, 6 figures, under review for ACL ARR", "summary": "In the business domain, where data-driven decision making is crucial,\ntext-to-SQL is fundamental for easy natural language access to structured data.\nWhile recent LLMs have achieved strong performance in code generation, existing\ntext-to-SQL benchmarks remain focused on factual retrieval of past records. We\nintroduce CORGI, a new benchmark specifically designed for real-world business\ncontexts. CORGI is composed of synthetic databases inspired by enterprises such\nas Doordash, Airbnb, and Lululemon. It provides questions across four\nincreasingly complex categories of business queries: descriptive, explanatory,\npredictive, and recommendational. This challenge calls for causal reasoning,\ntemporal forecasting, and strategic recommendation, reflecting multi-level and\nmulti-step agentic intelligence. We find that LLM performance drops on\nhigh-level questions, struggling to make accurate predictions and offer\nactionable plans. Based on execution success rate, the CORGI benchmark is about\n21\\% more difficult than the BIRD benchmark. This highlights the gap between\npopular LLMs and the need for real-world business intelligence. We release a\npublic dataset and evaluation framework, and a website for public submissions.", "AI": {"tldr": "CORGI is a business-oriented text-to-SQL benchmark with synthetic enterprise-style databases and queries spanning descriptive, explanatory, predictive, and recommendational tasks, revealing that current LLMs struggle with higher-level causal, temporal, and strategic reasoning and that CORGI is ~21% harder than BIRD.", "motivation": "Existing text-to-SQL benchmarks mostly test factual retrieval of past records, whereas real-world business analytics demands prediction, causal explanation, and actionable recommendations. The authors seek to expose and measure LLM gaps relevant to practical business intelligence.", "method": "They build synthetic databases modeled after enterprises (e.g., DoorDash, Airbnb, Lululemon) and craft queries across four increasing complexity tiers: descriptive, explanatory, predictive, and recommendational. They evaluate LLMs using execution success rate, compare with BIRD, and release a dataset, evaluation framework, and submission website.", "result": "LLMs show marked performance degradation on higher-level business questions, failing to make accurate predictions and produce actionable plans; CORGI is about 21% more difficult than BIRD by execution success rate.", "conclusion": "Business-relevant text-to-SQL requires multi-step, agentic intelligence\u2014causal reasoning, temporal forecasting, and strategic recommendation\u2014beyond current LLM capabilities. CORGI provides a challenging, public benchmark to spur progress toward real-world business intelligence."}}
